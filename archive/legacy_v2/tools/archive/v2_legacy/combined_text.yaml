- role: system
  type: document
  source: .pytest_cache\README.md
  timestamp: 1766133252148
  content: "# pytest cache directory #\r\n\r\nThis directory contains data from the pytest's cache plugin,\r\nwhich provides the `--lf` and `--ff` options, as well as the `cache` fixture.\r\n\r\n**Do not** commit this to version control.\r\n\r\nSee [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\r\n"
- role: system
  type: document
  source: CHANGELOG.md
  timestamp: 1767282468016
  content: |-
    # Context-Engine Changelog

    ## [2.3.0] - 2026-01-01 "Stability & Passive Text Ingestion"

    ### Added
    - **Auto-Resurrection System**: Ghost Engine (headless browser) automatically restarts on WebSocket disconnect
      - `ResurrectionManager` class handles browser lifecycle
      - Retry logic: 3 attempts with 2-second delays
      - Comprehensive logging of resurrection events
      - Works on Windows (Edge/Chrome) and Linux/Mac (Chrome)
    - **Passive Context Ingestion**: New `watchdog.py` service for continuous context growth
      - Monitors `context/` folder for `.txt`, `.md`, `.markdown` files
      - Batch processing with configurable interval (default 5s)
      - Real-time mode (watchdog library) and polling mode (fallback)
      - `/v1/memory/ingest` endpoint for receiving ingested files
    - **Model Server Improvements**: Enhanced `/models/{file_path:path}` endpoint
      - Proper streaming for large binary files
      - Correct MIME type handling (`.wasm`, `.safetensors`, `.json`, etc.)
      - Path traversal protection
      - Range request support with proper headers
      - 1-week cache TTL for immutable model files

    ### Fixed
    - **Model Loading**: All `chat.html` appConfig instances now use local bridge URLs
      - Ensures reliable local model loading
      - Fallback to HuggingFace only when files not available locally
      - Prevents 404 errors from hardcoded external URLs
    - **Connection Stability**: WebSocket handler properly triggers auto-resurrection
      - Cleans up pending requests before restart
      - Prevents orphaned request queues
    - **File Size Validation**: Watchdog service enforces max 1MB file size (configurable)
    - **Security**: Added path validation to prevent directory traversal attacks

    ### Changed
    - **Architecture**: Pivot from Vision/Ollama to Text-Only + Watchdog model
      - Removed heavy Vision processing pipeline
      - Simplified to pure text ingestion for stability
      - Focus on reliable model serving and auto-recovery
    - **Model Configuration**: Unified model URL scheme across all UI components
      - All models now served via local bridge redirect
      - Consistent URL format: `window.location.origin + "/models/{model_id}/resolve/main/"`
    - **Logging**: Enhanced logging system with per-component log files
      - `logs/webgpu_bridge.log`
      - `logs/resurrection.log`
      - `logs/memory_api.log`
      - `logs/model_server.log`

    ### Removed
    - Vision processing pipeline (Ollama integration)

    ## [2.3.1] - 2026-01-01 "Process Management & Streaming UX"

    ### Added
    - **Enhanced Process Management**: Improved `ResurrectionManager` with process cleanup
      - `kill_existing_browsers()` method to terminate existing browser processes before launching new ones
      - Explicit `--remote-debugging-port=9222` flag to prevent port conflicts
      - Increased initialization wait time from 3 to 5 seconds
    - **File Ingestion Improvements**: Enhanced `watchdog.py` with debounce and hash checking
      - Debounce functionality to wait for silence before processing file changes
      - MD5 hash checking to prevent duplicate ingestion of unchanged files
      - Proper cleanup of debounce timers
    - **Streaming CLI Client**: Updated `anchor.py` to use streaming for better UX
      - Changed from `stream=False` to `stream=True` for real-time character display
      - Added line-by-line processing of streaming responses
      - Maintained conversation history accumulation while providing immediate feedback
    - **New Standards**: Added Standards 016-018 for process management, file ingestion, and streaming UX

    ### Fixed
    - **Zombie Process Risk**: Fixed issue where browser resurrection would fail due to port conflicts
    - **Autosave Flood**: Fixed excessive database writes from editor autosave features
    - **CLI Latency**: Fixed hanging terminal during long model responses

    ### Changed
    - **Process Management**: Enhanced browser process management with proper cleanup
    - **File Ingestion**: Improved file monitoring with debounce and hash checking
    - **CLI UX**: Enhanced terminal experience with streaming responses

    ## [2.3.2] - 2026-01-01 "Ingestion Expansion & Memory Persistence"

    ### Added
    - **Code File Support**: Extended `watchdog.py` to monitor code extensions
      - Added `.py`, `.js`, `.html`, `.css`, `.json`, `.yaml`, `.yml`, `.sh`, `.bat`, `.ts`, `.tsx`, `.jsx`, `.xml`, `.sql`, `.rs`, `.go`, `.cpp`, `.c`, `.h`, `.hpp` to monitored extensions
    - **Browser Profile Cleanup**: Enhanced `ResurrectionManager` with temporary profile management
      - Added `--user-data-dir` with unique timestamp to prevent profile conflicts
      - Added `_cleanup_old_profiles()` method to remove old temporary directories
      - Added performance optimization flags for headless browser
    - **Chat Session Persistence**: Updated `anchor.py` to auto-save conversations
      - Created `context/sessions/` directory for chat logs
      - Added `save_message_to_session()` to persist each message to markdown files
      - Ensures conversation history survives CLI crashes and becomes ingested context

    ### Fixed
    - **Ingestion Blind Spot**: Fixed system being blind to code files in user's context
    - **Memory Leak Risk**: Fixed potential disk space issues from temporary browser profiles
    - **Lost Context Risk**: Fixed loss of conversation history on CLI crashes

    ## [2.3.3] - 2026-01-01 "Session Recorder & Text-File Source of Truth"

    ### Added
    - **Daily Session Files**: Updated `anchor.py` to create daily markdown files (`chat_YYYY-MM-DD.md`)
      - Uses `ensure_session_file()` to create dated session files
      - Formats messages with timestamps: `### ROLE [HH:MM:SS]`
      - Stores in `../context/sessions/` directory relative to tools/
    - **Text-File Source of Truth**: Implemented "Database is Cache" philosophy
      - All chat history saved to text files for cross-machine sync
      - Files automatically ingested by watchdog service
      - Creates infinite feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat
    - **Session Tracking**: Added session file path display in CLI startup

    ### Changed
    - **File Structure**: Changed from timestamped individual files to daily consolidated files
    - **Relative Pathing**: Uses relative path (`../context/sessions/`) for proper directory structure
    - **Markdown Format**: Improved formatting with headers and timestamps for better readability

    ## [2.2.12] - 2025-12-31 "UI Endpoint Fixes & Log File Verification"

    ### Fixed
    - **UI Endpoints**: Fixed `/context` and `/sidecar` endpoints to properly serve HTML files with correct paths
    - **File Response**: Corrected FileResponse paths to use absolute paths for reliable file serving
    - **Endpoint Accessibility**: Resolved 500 errors for UI endpoints by fixing file path resolution
    - **Static File Serving**: Enhanced static file serving for HTML interfaces

    ### Changed
    - **Path Resolution**: Updated endpoint handlers to use proper absolute path resolution for static files
    - **UI Serving**: Improved reliability of UI file serving from the bridge
    - **Error Handling**: Enhanced error handling for file serving operations

    ## [2.2.11] - 2025-12-31 "Coroutines and Async Fixes"

    ### Fixed
    - **Async Warnings**: Fixed "coroutine was never awaited" warnings by properly implementing startup event handlers
    - **Event Loop Integration**: Corrected async function calls to work properly with FastAPI's event loop

    - **Resource Management**: Fixed resource cleanup in WebSocket handlers to prevent leaks
    - **Startup Sequence**: Ensured logging system initializes properly with the application lifecycle

    ### Changed
    - **Async Handling**: Improved async/await patterns throughout the bridge for better stability
    - **Error Handling**: Enhanced error handling for async operations with proper cleanup
    - **Initialization**: Refined startup sequence to ensure all components initialize correctly

    ## [2.2.10] - 2025-12-31 "Log File System Implementation"

    ### Added
    - **Logs Directory**: Created `logs/` directory to store individual component logs
    - **File-based Logging**: Each system component now writes to its own log file (e.g., `chat-api.log`, `memory-api.log`, `websocket-bridge.log`)
    - **Log Truncation**: Implemented automatic log truncation to keep only last 1000 lines per file
    - **Individual Log Files**: Separate log files for each component for easier debugging

    ### Changed
    - **Log Storage**: Moved from in-memory only to file-based persistent logging
    - **Log Management**: Added automatic log rotation and truncation to prevent disk space issues
    - **API Endpoints**: All API endpoints now write to both central buffer and individual log files

    ## [2.2.9] - 2025-12-31 "Complete Process Log Capture"

    ### Added
    - **Process Logging**: Added logging for all major system processes (chat, memory search, WebSocket connections)
    - **Error Tracking**: Enhanced error logging with detailed context and request IDs
    - **Status Monitoring**: Added detailed status messages for connection states and process flow

    ### Changed
    - **Log Collection**: Enhanced centralized log collection with comprehensive process monitoring
    - **API Endpoints**: All API endpoints now log detailed request/response information
    - **WebSocket Handler**: Improved WebSocket connection logging with detailed status updates
    - **Error Handling**: Enhanced error messages with better context and correlation IDs

    ## [2.2.8] - 2025-12-31 "Universal Log Collection System"

    ### Added
    - **Log Collection**: Added centralized log collection system in `webgpu_bridge.py` with global log buffer
    - **API Endpoints**: Added `/logs/recent` and `/logs/collect` endpoints for log aggregation
    - **Standard 013**: Created universal log collection standard for all system components
    - **Cross-Platform Logging**: Implemented logging from all system components (Python, JavaScript, WebSocket)

    ### Changed
    - **Log Viewer**: Updated `log-viewer.html` to consume logs from the new centralized endpoint
    - **WebSocket Logging**: Enhanced WebSocket connection to send detailed status messages to log viewer
    - **System Integration**: All components now route logs through the central collection system

    ## [2.2.7] - 2025-12-31 "Ghost Engine Startup Improvements"

    ### Fixed
    - **Connection Issues**: Fixed Ghost Engine startup to ensure proper WebSocket connection establishment
    - **Process Launch**: Improved startup scripts to properly launch Ghost Engine with correct parameters
    - **CPU-Only Mode**: Enhanced CPU-only mode startup with appropriate browser flags
    - **Low-Resource Mode**: Fixed low-resource mode startup with conservative GPU settings

    ### Changed
    - **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` with better Ghost Engine launch parameters
    - **Connection Timing**: Improved timing between server and Ghost Engine startup
    - **User Feedback**: Added clearer status messages during startup process

    ## [2.2.6] - 2025-12-31 "WebGPU Adapter Error Handling"

    ### Fixed
    - **WebGPU Errors**: Added specific handling for "No WebGPU Adapter found" errors on Snapdragon/limited GPU devices
    - **Error Messages**: Improved error messages to guide users when WebGPU is unavailable
    - **Graceful Degradation**: System now provides helpful guidance instead of failing silently

    ### Changed
    - **Chat Endpoint**: Enhanced `/v1/chat/completions` to handle WebGPU adapter errors gracefully
    - **Error Response**: More informative error messages for GPU-related issues
    - **User Guidance**: Clear instructions for users with unsupported GPU configurations

    ## [2.2.5] - 2025-12-31 "Log Viewer Consolidation"

    ### Added
    - **Single Panel**: Consolidated all logs into one unified panel for easier monitoring
    - **Stream Collection**: All app processes now stream to single consolidated view
    - **Efficiency**: Removed unused chat and context panels that were empty

    ### Changed
    - **Log Viewer**: `tools/log-viewer.html` now shows all logs in single panel
    - **UI Simplification**: Streamlined interface for better usability
    - **Copy Functionality**: Simplified copy to clipboard for all logs at once

    ## [2.2.4] - 2025-12-31 "Chat Client & Bridge Reorientation"

    ### Added
    - **Chat Client**: Converted `tools/anchor.py` from Shell Executor to Chat Client interface
    - **Stream Accumulation**: Enhanced bridge to properly accumulate chat stream responses
    - **Terminal Chat**: Added conversation history and context management to CLI

    ### Fixed
    - **Chat Response**: Fixed issue where chat responses were cut off due to stream handling
    - **Bridge Protocol**: Improved WebSocket message handling for complete response delivery
    - **Conversation Flow**: Added proper conversation history management in CLI

    ### Changed
    - **Architecture**: Shifted from command execution to chat interface in terminal client
    - **API Handling**: Bridge now accumulates streaming responses for non-streaming API compatibility
    - **User Experience**: Terminal client now provides full chat experience with context

    ## [2.2.3] - 2025-12-31 "Ghost Engine Startup Fix"

    ### Fixed
    - **JavaScript Disabled**: Removed `--disable-javascript` flag that was preventing Ghost Engine from starting
    - **WASM Engine**: Fixed issue where WebAssembly AI engine couldn't load with JavaScript disabled
    - **WebSocket Connection**: Resolved "Failed to fetch" errors by enabling JavaScript in headless browser
    - **Memory Search**: Fixed context search functionality by ensuring Ghost Engine starts properly

    ### Changed
    - **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` to enable JavaScript
    - **Ghost Engine**: Headless browser now properly loads WASM AI engine and connects to bridge

    ## [2.2.2] - 2025-12-31 "Context Search Fix"

    ### Fixed
    - **Memory Search**: Fixed 503 errors when Ghost Engine is disconnected by providing helpful error messages
    - **WebSocket Handling**: Improved handling of search result responses from Ghost Engine
    - **Timeout Management**: Added proper timeout handling for search requests
    - **Error Messages**: Enhanced error messages to guide users when Ghost Engine is not connected

    ### Changed
    - **Search Endpoint**: Improved `/v1/memory/search` to handle disconnected Ghost Engine gracefully
    - **Chat Endpoint**: Enhanced error handling for chat completions when Ghost Engine is unavailable

    ## [2.2.1] - 2025-12-31 "UI Consolidation"

    ### Removed
    - **Sidecar Interface**: Removed duplicate sidecar.html interface to consolidate to single Context UI
    - **Redundant Endpoints**: Streamlined UI access to focus on single interface

    ### Added
    - **Context UI**: Single, focused interface for retrieval and search functionality
    - **Endpoint Consolidation**: Both `/sidecar` and `/context` now serve the same Context UI

    ### Changed
    - **UI Strategy**: Shifted from multiple similar interfaces to single, focused Context UI
    - **User Experience**: Simplified navigation with single interface for context retrieval

    ## [2.2.0] - 2025-12-31 "Text-Only Architecture Pivot"

    ### Removed
    - **Vision Engine**: Removed Python-based vision_engine.py and Ollama dependency
    - **Image Processing**: Removed all /v1/vision/* endpoints and image-related functionality
    - **External Dependencies**: Eliminated heavy Python/Ollama dependencies for lightweight operation

    ### Added
    - **Text-Only Focus**: Streamlined architecture focusing purely on text context and memory
    - **Simplified Bridge**: Cleaned webgpu_bridge.py with only essential context relay functionality
    - **Memory Builder**: Reinforced tools/memory-builder.html as the primary background processor
    - **Browser-Native Processing**: Leverage Ghost Engine (WebGPU) for all processing needs

    ### Changed
    - **Architecture**: Shifted from multi-component system to lightweight, browser-native approach
    - **Processing Model**: Memory processing now handled by Qwen 1.5B in WebGPU (memory-builder.html)
    - **Dependency Management**: Eliminated external inference servers (Ollama) in favor of browser-native models
    - **Sidecar Interface**: Simplified to focus solely on retrieval and search functionality

    ## [2.1.0] - 2025-12-31 "Daemon Eyes & Passive Observation"

    ### Added
    - **Daemon Eyes**: Implemented "Digital Proprioception". System now observes user screen activity via `sidecar.html` toggle.
    - **Vision Pipeline**: Integrated `vision_engine.py` to convert images/screenshots into semantic text memories.
    - **Live Context Loop**: Added `POST /v1/vision/screenshot` for non-blocking background context ingestion.
    - **Unified Sidecar**: Merged Retrieval and Vision tools into `tools/sidecar.html`.
    - **Context UI**: Added `tools/context.html` for simplified read-only context retrieval with scrollable display and one-click copy
    - **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:
        - `GET /sidecar` - Serves the unified control center
        - `GET /context` - Serves the read-only context retrieval UI
        - `POST /v1/vision/ingest` - Handles image upload and VLM processing
        - `POST /v1/vision/screenshot` - Handles background screenshot processing
        - `POST /v1/memory/search` - Implements memory graph search functionality

    ### Changed
    - **Context Strategy**: Shifted from "Manual Copy-Paste" to "Passive Accumulation + Manual Retrieval".
    - **Bridge Architecture**: `webgpu_bridge.py` now manages background tasks (FastAPI `BackgroundTasks`) for image processing to prevent UI freezing.
    - **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs

    ## [2.0.3] - 2025-12-31 "Browser-Based Control Center & VLM Integration"

    ### Added
    - **Vision Engine**: Created `tools/vision_engine.py` for Python-powered image analysis using Ollama backend
    - **Browser Control Center**: Implemented `tools/sidecar.html` with dual tabs for context retrieval and vision ingestion
    - **Context UI**: Added `tools/context.html` for manual context retrieval with scrollable display and one-click copy
    - **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:
        - `GET /sidecar` - Serves the sidecar dashboard
        - `GET /context` - Serves the context retrieval UI
        - `POST /v1/vision/ingest` - Handles image upload and VLM processing
        - `POST /v1/memory/search` - Implements memory graph search functionality
    - **VLM Integration**: Full integration pipeline from image upload ‚Üí Python VLM ‚Üí memory graph ingestion

    ### Changed
    - **Bridge Enhancement**: Extended `webgpu_bridge.py` to serve UI files and orchestrate vision processing
    - **Memory Search**: Implemented placeholder search functionality with realistic response structure
    - **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs

    ## [2.0.2] - 2025-12-31 "Test Suite Organization & Pipeline Verification"

    ### Added
    - **Test Directory Structure**: Created dedicated `tests/` directory in project root for all test files
    - **Test File Migration**: Moved all test files from `tools/` and `scripts/` to new `tests/` directory:
        - `test_model_loading.py` ‚Üí `tests/test_model_loading.py`
        - `test_model_availability.py` ‚Üí `tests/test_model_availability.py`
        - `test_orchestrator.py` ‚Üí `tests/test_orchestrator.py`
        - `model_test.html` ‚Üí `tests/model_test.html`
        - `test_gpu_fixes.py` ‚Üí `tests/test_gpu_fixes.py`
    - **Test Configuration Updates**: Updated test files to use correct port (8000 instead of 8080) for current architecture
    - **Comprehensive Test Suite**: Enhanced test coverage for model loading, endpoint accessibility, and data pipeline verification

    ### Changed
    - **Project Organization**: Consolidated all test assets into dedicated directory for better maintainability
    - **Test Architecture**: Updated test configurations to match current Anchor Core unified architecture (port 8000)

    ## [2.0.1] - 2025-12-30 "Server Stability & Endpoint Fixes"

    ### Fixed
    - **Server Startup Issues**: Resolved server hanging issues caused by problematic path parameter syntax (`:path`) in route definitions that prevented proper server startup
    - **Missing Endpoints**: Added critical missing endpoints (`/v1/models/pull`, `/v1/models/pull/status`, `/v1/gpu/lock`, `/v1/gpu/status`, etc.) that were documented but missing from implementation
    - **Endpoint Accessibility**: Verified all documented endpoints are now accessible and responding properly
    - **Model Availability**: Improved model availability testing showing that `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` and `Qwen2.5-7B-Instruct-q4f16_1-MLC` have most files available (missing only `params.json`)

    ### Architecture Shift
    - **Unified Anchor Core**: Consolidated Bridge, File Server, and UI into a single process (`webgpu_bridge.py`) running on **Port 8000**.
    - **Single Origin**: Eliminated CORS issues and port confusion. UI, API, and Models are served from the same origin.
    - **Protocol**:
        - Brain: `http://localhost:8000/chat.html`
        - Terminal: `http://localhost:8000/terminal.html`
        - API: `http://localhost:8000/v1/...`

    ### Removed / Archived
    - **CLI Bloat**: Deleted `anchor.py` and `sov.py` in favor of web-based `terminal.html` or direct API calls.
    - **Legacy Scripts**: Archived `start-bridge.bat`, `start-ghost-shell.bat`, `launch-ghost.ps1`, `hot_reload_gpu.py`, and others to `archive/v2_ghost_shell/`.

    ### Added
    - **start-anchor.bat**: Single-click launcher that starts the Core and the Ghost Engine (Minimized Browser).

    ## [1.2.4] - 2025-12-29 "Ghost & Shell Architecture"

    ### Added
    - **Ghost & Shell Architecture**: Implemented headless Ghost engine with native Anchor shell for OS integration
    - **Auto-Ignition Protocol**: Added auto-start sequence for headless browser with `?headless=true` parameter
    - **Anchor Terminal**: Created `tools/anchor.py` for native PowerShell interface with natural language processing
    - **Spawn Endpoint**: Added `/v1/system/spawn_shell` to launch native terminals from dashboard
    - **Neural Shell Protocol**: Enhanced `/v1/shell/exec` to process natural language to PowerShell commands
    - **UTF-8 Encoding Fix**: Added Windows encoding enforcement to prevent Unicode crashes in bridge
    - **Minimized Window Approach**: Updated `scripts/launch-ghost.ps1` to use `--start-minimized` for proper GPU access
    - **Unified Startup**: Consolidated to single `start-ghost-shell.bat` script launching complete architecture

    ### Changed
    - **Renamed Kernel**: Migrated from `sovereign.js` to `anchor.js` with updated imports across all components
    - **Simplified Bridge**: Streamlined `webgpu_bridge.py` with essential functionality only
    - **Updated Neural Terminal**: Modified `tools/neural-terminal.html` to use new shell protocol
    - **Dashboard Integration**: Added Anchor Shell button to `tools/index.html`
    - **Startup Scripts**: Consolidated multiple startup scripts to single unified approach

    ### Fixed
    - **Windows Encoding**: Resolved Unicode encoding crashes with UTF-8 enforcement
    - **Bridge Authorization**: Fixed authentication token consistency across components
    - **Headless GPU Access**: Resolved WebGPU initialization issues with minimized window approach
    - **Model Loading**: Fixed auto-load sequence for Ghost engine with proper model selection
    - **Cache Bypass Protocol**: Implemented "Stealth Mode" for browser AI engine by overriding Cache API and modifying static file headers to force browser to treat models as "data in RAM" rather than "persistent storage", bypassing strict security policies.
    - **NoCacheStaticFiles**: Custom StaticFiles class with `Cache-Control: no-store` headers to prevent browser cache API usage when serving models through the bridge.
    - **Neural Shell Protocol**: Activated "The Hands" (Layer 3) in `webgpu_bridge.py` (`/v1/shell/exec`), allowing the browser to execute system commands on the host.
    - **Neural Terminal**: `tools/neural-terminal.html` provides a matrix-style command interface for direct shell access from the browser.
    - **Hot Reload Improvement**: Fixed `start-sovereign-console-hotreload.bat` port conflicts and added `/file-mod-time` endpoint to `smart_gpu_bridge.py` for correct frontend reloading.
    - **Root Mic (Audio Input)**: Renamed `sovereign-mic.html` to `root-mic.html` and added "Summarize & Clarify" feature using the local Qwen2.5 model.
    - **Long-Form Transcription**: Fixed Whisper pipeline to support recordings >30s using chunking and striding.
    - **CozoDB Corruption Recovery**: Enhanced error handling for IndexedDB corruption with automatic fallback to in-memory database, manual recovery button, and timeout protection against hanging WASM calls.
    - **Bulk CozoDB Import Tool**: Added `tools/prepare_cozo_import.py` to transform `combined_memory.json` into the canonical `relations` payload (`cozo_import_memory.json`) for atomic bulk imports into CozoDB.
    - **Import Safety & Verification**: Added recommended import procedure and a post-import verification + backup step to avoid Schema Detachment.
    - **WebGPU Bridge**: `webgpu_bridge.py` for proxying OpenAI API requests to browser workers.
    - **Chat Worker**: `webgpu-server-chat.html` for running LLMs in the browser.
    - **Embed Worker**: `webgpu-server-embed.html` for running embedding models in the browser.
    - **Mobile Chat**: `mobile-chat.html` for a lightweight, mobile-friendly UI.
    - **Log Viewer**: `log-viewer.html` for real-time server log monitoring.

    ### Changed
    - **Model Loading**: Updated `model-server-chat.html` to use bridge-based model URLs (`http://localhost:8080/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.
    - **Ingestion Defaults**: Recommended batch size increased to 100 to prevent long-running slow writes that can desync CozoDB's in-memory metadata.
    - **Git Configuration**: Added `models/` directory to `.gitignore` to prevent committing large binary model files.

    ---

    ## [1.2.3] - 2025-12-19 "Snapdragon Optimization"

    ### Added
    - **Qwen3 Support**: Added `Qwen3-4B-Instruct` to the verified model list.
    - **Llama 3.2 Support**: Added `Llama-3.2-1B-Instruct` as the recommended lightweight model.
    - **Buffer Override**: Implemented `appConfig` overrides to force high-end performance on 256MB GPUs (fixing Adreno throttling).

    ### Changed
    - **Portable Launchers**: All scripts now use `--user-data-dir="%~dp0browser_data"` for fully portable, clean-running instances.
    - **Model Config**: Refactored `CreateMLCEngine` initialization to handle both URL-based and ID-based model definitions reliably.

    ---

    ## [1.2.2] - 2025-12-18 "Hermes & CozoDB Fixes"

    ### Fixed
    - **Hermes Model Support**: Fixed 404 errors for OpenHermes and NeuralHermes by mapping them to the verified `Mistral-v0.3` WASM library.
    - **CozoDB Date Formatting**: Removed `strftime` dependency from WASM queries (causing `no_implementation` errors) and moved date formatting to client-side JavaScript.
    - **Drag-and-Drop Import**: Fixed handling of CozoDB `relations` export format in drag-and-drop ingestion.
    - **Documentation**: Established `specs/mlc-urls.md` as a registry for verified WASM binaries.

    ---

    ## [1.2.1] - 2025-12-15 "DeepSeek & CozoDB Stabilization"

    ### Fixed
    - **CozoDB Initialization**: Resolved `CozoDb.new_from_path is not a function` error by switching to `CozoDb.new_from_indexed_db` for persistent browser storage (IndexedDB backend).
    - **WASM Memory Access**: Fixed "memory access out of bounds" error in `sovereign-db-builder.html` and `unified-coda.html` by correctly stringifying JSON parameters passed to `db.run()`.
    - **DeepSeek Configuration**: Fixed "Cannot find model record" error in `unified-coda.html` by decoupling the internal model ID from the HuggingFace URL.

    ### Added
    - **Sovereign Hub**: Created `tools/index.html` as a central dashboard for the Console, Builder, and Log Viewer.
    - **Log Viewer Upgrade**: Refactored `tools/log-viewer.html` to use `BroadcastChannel` for real-time, polling-free log updates from the console.
    - **Expanded File Support**: Updated `sovereign-db-builder.html` to support ingestion of a wider range of code and config files (ts, rs, go, sql, ini, xml, etc.).

    ## [1.2.0] - 2025-12-15 "Sovereign Architecture"

    ### Added
    - **Sovereign Console**: Created `tools/unified-coda.html`, a standalone WASM-based chat console with local CozoDB (OPFS) and Transformers.js.
    - **Sovereign DB Builder**: Created `tools/sovereign-db-builder.html` for ingesting JSON logs into the browser-based database.
    - **Model Support**: Expanded `unified-coda.html` to support the full range of MLC-compatible models (Llama 3.2, Qwen 2.5, Gemma 2, etc.).

    ### Changed
    - **Log Management**: Updated backend logging to truncate files at 500KB to prevent disk bloat.

    ## [1.1.0] - 2025-12-14 "Browser Stability & Bridge Fixes"

    ### Fixed
    - **WebGPU Bridge**: Patched `tools/webgpu_bridge.py` to accept any model name, resolving 503 errors during embedding requests.
    - **LLM Client**: Updated `backend/src/llm.py` to correctly identify and use the configured embedding model (`nomic-embed-text-v1.5`).
    - **Coda Chat**: Modified `backend/src/recipes/coda_chat.py` to sanitize and truncate `retrieve_memory` outputs. Large JSON payloads were causing `Maximum call stack size exceeded` errors in the browser-based LLM worker.

    ## [1.0.0] - 2025-12-08 "Infinite Context Pipeline"

    ### Added
    - **Phase 1: Hardware Foundation**: All LLM servers now boot with 65,536 context window and Flash Attention enabled
    - **Phase 2: Context Rotation Protocol**: ContextManager automatically rotates context when exceeding 55k tokens
    - **Phase 3: Graph-R1 Enhancement**: GraphReasoner now retrieves ContextGist memories for historical continuity
    - **ContextGist Nodes**: Neo4j storage for compressed historical context summaries with chronological links
    - **Context Shifting Logic**: Intelligent distillation of old content using Distiller agent with gist creation
    - **Documentation Structure**: Organized specs/ directories at root, backend, and anchor levels with spec.md, plan.md, tasks.md
    - **Infinite Context Pipeline**: Complete end-to-end implementation enabling unlimited context window management

    ### Changed
    - **Upgraded Context Windows**: All start scripts now default to 64k context for infinite work capability
    - **Enhanced Memory Architecture**: Neo4j now stores both active memories and ContextGist historical summaries
    - **Improved ContextManager**: Added check_and_rotate_context() logic with automatic gist creation and storage
    - **Extended GraphReasoner**: Updated retrieval queries to include ContextGist nodes alongside regular memories
    - **Optimized Distiller Integration**: Enhanced _chunk_and_distill functionality for context rotation use cases
    - **Refined Archivist Agent**: Now coordinates context rotation and gist management operations

    ### Fixed
    - **Context Limit Elimination**: Fixed issue where systems would crash when reaching context limits
    - **Memory Continuity**: Resolved problems with historical context access across conversation boundaries
    - **Performance Optimization**: Fixed inefficiencies in large context handling with 64k window support
    - **Rotation Logic**: Fixed issues with context preservation during rotation cycles

    ---

    ## [0.9.0] - 2025-12-07 "Reka & Local Proxy"

    ### Added
    - **Reka Configuration**: Full support for Reka-Flash-3-21B (Q4_K_S) with 16k context, stop tokens, and optimized LLaMa server flags.
    - **Local API Proxy**: Added `scripts/local_api_proxy.py` to enforce static API keys for local LLaMa instances (fixes Cline extension "OpenAI API Key" requirement).
    - **VS Code Integration**: Added `.vscode/settings.json` template and `VSCODE_CLINE_SETUP.md` for seamless local development.
    - **MCP Health**: Added `/health` endpoint to Unified Launcher for better compatibility.

    ### Fixed
    - **MCP Routing**: Resolved duplicate `/mcp` prefix in Unified Launcher routes (`/mcp/tools` is now accessible).
    - **LLM Client**: Added `stop` token support to API payloads and local GGUF generation.

    ## [0.8.0] - 2025-12-06 "Archivist Protocol"

    ### Added
    - **Archivist Ingestion**: Implemented `POST /archivist/ingest` endpoint to accept live data from the browser.
    - **Memory Schema**: Enforced **Directive INJ-A1** (`PlaintextMemory`) for immutable "Page-Store" records.
    - **Modular DOM Adapters**:
        - `GeminiAdapter`: Clean extraction for Google Gemini.
        - `ChatGPTAdapter`: Clean extraction for ChatGPT.
        - `ClaudeAdapter`: Clean extraction for Claude.ai.
        - `GenericAdapter`: Universal fallback for any webpage.
    - **Extension UI**: Added **[Save to Memory]** button to the Side Panel for manual ingestion.

    ### Fixed
    - **Encoding Crash**: Resolved Windows `charmap` error by enforcing `PYTHONIOENCODING='utf-8'`.
    - **Server Stability**: Fixed startup crashes caused by `MemoryWeaver` resource contention.

    ## [0.7.0] - 2025-12-06 "Operation Concrete"

    ### Added
    - **Browser Bridge**: A Chrome Extension (MV3) capable of:
        - **Voice**: Streaming chat interface via Side Panel.
        - **Sight**: Context injection (reading active tab).
        - **Hands**: JavaScript execution on active pages (User-ratified).
    - **Backend Architecture**: Migrated from monolithic scripts to **Modular Recipes** (MAX Agentic Cookbook standard).
        - `CodaChatRecipe`: Handles orchestration, context, and tool execution.
    - **Persistence**: Side panel now saves chat history to local storage.
    - **Markdown Support**: Chat interface renders code blocks and syntax highlighting.

    ### Changed
    - **Identity**: System formally renamed from "Sybil" to **"Coda"**.
    - **Documentation**: Adopted `specs/` based documentation policy.

    ### Fixed
    - **Audit Logger**: Patched critical `NameError` in streaming endpoints.
    - **Security**: Hardened extension execution via `world: "MAIN"` to bypass strict CSP on some sites.

    ---

    ## [0.6.0] - 2025-11-30 "Operation MCP Integrated"

    ### Added
    - **MCP Integration**: Complete integration of MCP server into main ECE Core server
    - **Unified Endpoint**: All MCP functionality now available at `/mcp` on main server (port 8000)
    - **Memory Tools**: Enhanced MCP tools for memory operations:
        - `add_memory` - Add to Neo4j memory graph
        - `search_memories` - Search memory graph with relationships
        - `get_summaries` - Get session summaries
    - **Configuration**: New `mcp_enabled` setting in config.yaml to toggle integration
    - **Authentication**: MCP endpoints now inherit main server authentication settings

    ### Changed
    - **Architecture**: MCP server no longer runs as separate process, now integrated into main ECE server
    - **Endpoints**: MCP tools now accessed via `/mcp/tools` and `/mcp/call` instead of separate server
    - **Deployment**: Simplified deployment - no need to start separate MCP service
    - **Resources**: Reduced memory footprint by eliminating duplicate server processes

    ### Fixed
    - **Connection Issues**: Resolved intermittent connection failures between ECE and external MCP server
    - **Latency**: Reduced tool call latency by eliminating inter-service communication overhead
    - **Synchronization**: Fixed race conditions in concurrent tool executions

    ---

    ## [0.5.1] - 2025-11-29 "Memory Weaver Security Audit"

    ### Added
    - **Security Hardening**: Added input validation for all GraphReasoner queries
    - **Audit Trail**: Enhanced logging for all automated relationship repairs
    - **Circuit Breakers**: Added fail safes for Weaver operations

    ### Changed
    - **Weaver Engine**: Refactored to use parameterized queries, preventing Cypher injection
    - **Permission Model**: Strengthened access controls for relationship modification operations

    ### Fixed
    - **Cypher Injection**: Patched vulnerability in Neo4j relationship queries
    - **Race Conditions**: Fixed concurrency issues in automated repair operations
    - **Resource Exhaustion**: Added limits to prevent DoS via excessive repair requests

    ---

    ## [0.5.0] - 2025-11-28 "Memory Weaver (Automated Repair)"

    ### Added
    - **Memory Weaver Engine**: Automated system for detecting and repairing broken relationships in Neo4j
    - **Similarity Detection**: Embedding-based relationship discovery for linking related memories
    - **Audit System**: Complete traceability for all automated repairs with `auto_commit_run_id`
    - **Rollback Capability**: Deterministic reversal of automated changes via `rollback_commits_by_run.py`
    - **Scheduler**: Background maintenance tasks for continuous graph integrity

    ### Changed
    - **Graph Maintenance**: Automated relationship repair now runs as background process
    - **Quality Assurance**: Enhanced relationship validation with similarity scoring
    - **Traceability**: All automated changes now logged with unique run identifiers

    ### Fixed
    - **Orphaned Nodes**: Automatically discovers and connects isolated memories
    - **Broken Links**: Repairs missing relationships between related concepts
    - **Data Drift**: Corrects inconsistent metadata across related nodes

    ---

    ## [0.4.0] - 2025-11-25 "Graph-R1 Implementation"

    ### Added
    - **Graph Reasoner**: Iterative "Think ‚Üí Query ‚Üí Retrieve ‚Üí Rethink" reasoning engine
    - **Q-Learning Retrieval**: Reinforcement learning for optimized memory access patterns
    - **Markovian Reasoning**: Chunked thinking with state preservation across context shifts
    - **Multi-Hop Queries**: Complex graph traversal for answering compound questions
    - **Cognitive Agents**: Plugin architecture for specialized reasoning tasks

    ### Changed
    - **Retrieval Method**: Replaced simple vector search with Graph-R1 retrieval
    - **Memory Access**: Graph-based traversal now primary method for context assembly
    - **Agent Architecture**: Modular cognitive agents for specialized tasks
    - **Context Building**: Enhanced context with relationship-aware retrieval

    ### Fixed
    - **Context Relevance**: Improved precision of memory retrieval
    - **Chain of Thought**: Better preservation of reasoning pathways
    - **Memory Decay**: Reduced loss of historical context in long conversations

    ---

    ## [0.3.1] - 2025-11-20 "Security Hardening"

    ### Added
    - **API Authentication**: Token-based authentication for all endpoints
    - **Rate Limiting**: Request throttling to prevent abuse
    - **Input Sanitization**: Enhanced validation for all user inputs
    - **Audit Logging**: Comprehensive logging of all sensitive operations
    - **Secure Defaults**: Safe configuration presets for common deployment scenarios

    ### Changed
    - **Security Model**: Implemented zero-trust architecture
    - **Credential Handling**: Secure storage and transmission of API keys
    - **Access Controls**: Granular permissions for different API endpoints

    ### Fixed
    - **Authentication Bypass**: Patched critical vulnerability in API access
    - **Data Exposure**: Resolved information disclosure in error messages
    - **Injection Attacks**: Fixed potential SQL injection in Neo4j queries

    ---

    ## [0.3.0] - 2025-11-15 "Neo4j Migration Complete"

    ### Added
    - **Neo4j Integration**: Complete migration from SQLite to Neo4j graph database
    - **Redis Cache**: Hot cache layer for active session management
    - **Graph Schema**: Formal schema definition for memory relationships
    - **Migration Tools**: Scripts to migrate existing SQLite data to Neo4j
    - **Backup System**: Automated graph backup and restoration procedures

    ### Changed
    - **Storage Architecture**: Tiered storage (Redis hot cache + Neo4j persistent)
    - **Query Language**: Cypher queries for graph operations
    - **Relationship Modeling**: Graph-based connections between memories
    - **Indexing Strategy**: Graph-based indices for faster retrieval

    ### Fixed
    - **Performance**: Significantly improved query performance for complex relationships
    - **Scalability**: Better handling of large-scale memory graphs
    - **Consistency**: Stronger data integrity with ACID-compliant transactions

    ---

    ## [0.2.0] - 2025-10-30 "Cognitive Agents"

    ### Added
    - **Verifier Agent**: Fact-checking via empirical distrust protocol
    - **Archivist Agent**: Memory maintenance and staleness detection
    - **Distiller Agent**: Content summarization and extraction
    - **Agent Framework**: Plugin system for extensible cognitive capabilities
    - **Truth Scoring**: Provenance-aware fact-checking with primary source priority

    ### Changed
    - **Memory Hygiene**: Automated maintenance of memory quality
    - **Verification Process**: Evidence-based fact-checking system
    - **Quality Assurance**: Continuous assessment of memory reliability
    - **Maintenance Schedule**: Regular memory grooming operations

    ### Fixed
    - **Hallucinations**: Reduced false information in responses
    - **Stale Information**: Automatic detection and updating of outdated memories
    - **Data Quality**: Improved content validation and cleaning procedures

    ---

    ## [0.1.0] - 2025-09-15 "Initial Architecture"

    ### Added
    - **Core Backend**: Initial ECE_Core with SQLite memory system
    - **Anchor Interface**: Terminal interface for user interaction
    - **Basic Memory**: Text-based memory storage and retrieval
    - **LLM Integration**: Support for various local LLM servers
    - **Plugin System**: Extensible tool architecture (UTCP)

    ### Changed
    - **Foundation**: Established core architecture patterns
    - **API Design**: Defined RESTful API structure for components

    ### Fixed
    - **Basic Functionality**: Initial implementation of core features
- role: system
  type: document
  source: README.md
  timestamp: 1767460951071
  content: |-
    # Context Engine (Sovereign Edition)

    > **Philosophy:** Your mind, augmented. Your data, sovereign. Your tools, open.

    A **Headless Node.js** cognitive extraction system. No browser dependencies. No cloud. No installation.
    Just you, Node.js, and your infinite context.

    ---

    ## ‚ö° Quick Start

    1.  **Download** this repository.
    2.  **Install** Node.js dependencies: `cd server && npm install`
    3.  **Launch** the unified system: `cd server && npm start`
    4.  **Access** the API at `http://localhost:3000`

    *That's it. You are running a headless context engine with persistent Graph Memory.*

    ---

    ## üèóÔ∏è Architecture

    The system now runs in `server/` using Node.js with direct CozoDB integration.

    ### 1. The Sovereign Loop
    ```mermaid
    graph TD
        User -->|Input| API["HTTP API"]

        subgraph "SOVEREIGN ENGINE (Port 3000)"
            Server[Express Server]
            API["API Endpoints"]
            Cozo["CozoDB Node"]
        end

        API -->|REST| Server
        Server -->|Direct| Cozo["CozoDB (RocksDB)"]

        subgraph File_Watcher ["Context Watcher"]
            Watcher[chokidar] -->|Monitor| Context["context/ directory"]
            Watcher -->|Ingest| Cozo
        end

        subgraph Cognitive_Engine
            Server -->|Query| Cozo
            Cozo -->|Results| Server
        end
    ```

    ### 2. Core Components
    *   **Engine**: `src/index.js` - Node.js server with Express, CORS, and body-parser.
    *   **Memory**: `CozoDB (Node)` - Stores relations (`*memory`) with RocksDB persistence.
    *   **Ingestion**: `chokidar` - Watches `context/` directory for file changes and auto-ingests.
    *   **Core**: `server/` - Node.js monolith handling API, ingestion, and database operations.
    *   **Data**: `context/` - Directory for storing context files that are automatically monitored.

    ---

    ## üèõÔ∏è Node.js Monolith Architecture

    The system now features the unified Node.js monolith architecture for simplified deployment:

    *   **Sovereign Engine**: Single-process Node.js server handling API, ingestion, and database on port 3000
    *   **CozoDB Integration**: Direct integration with `cozo-node` using RocksDB backend
    *   **File Watcher**: `chokidar` monitors `context/` directory for automatic ingestion
    *   **API Endpoints**: Standardized endpoints for ingestion, querying, and health checks

    ### Getting Started with Node.js Monolith
    1. Install dependencies: `cd server && npm install`
    2. Start the unified system: `cd server && npm start`
    3. Access the health check: `http://localhost:3000/health`
    4. Use the API endpoints for ingestion and querying

    ### API Endpoints
    *   `POST /v1/ingest` - Content ingestion endpoint
    *   `POST /v1/query` - CozoDB query execution endpoint
    *   `GET /health` - Service health verification endpoint

    ### Migration Benefits
    * **Eliminated Browser Dependencies**: No more fragile headless browser architecture
    * **Reduced Resource Consumption**: Lower memory and CPU usage
    * **Improved Platform Compatibility**: Works on Termux/Linux environments
    * **Enhanced Stability**: More reliable operation without browser quirks
    * **Simplified Deployment**: Single Node.js process instead of complex browser bridge

    ---

    ## üîÑ Context Collection & Migration

    The system now includes comprehensive context collection and legacy migration:

    *   **Legacy Migration**: `migrate_history.js` consolidates legacy session files into YAML/JSON
    *   **Context Collection**: `read_all.js` aggregates content from project directories
    *   **File Monitoring**: Automatic ingestion of new files in `context/` directory
    *   **Multi-Format Output**: Generates text, JSON, and YAML formats for maximum compatibility
    *   **Archive Strategy**: Legacy V2 artifacts archived to `archive/v2_python_bridge/`

    ### Data Migration Process
    1. Legacy session files consolidated from `context/Coding-Notes/Notebook/history/important-context/sessions/`
    2. Converted to YAML format in `context/full_history.yaml` and `context/full_history.json`
    3. Auto-ingested into CozoDB for persistent storage
    4. Legacy Python infrastructure archived for historical reference

    ### Context Collection Strategy
    *   **File Discovery**: Recursive scanning of `context/` directory
    *   **Format Support**: Handles .json, .md, .yaml, .txt, .py, .js, .html, .css, .sh, .ps1, .bat
    *   **Exclusion Rules**: Skips common build directories, logs, and combined outputs
    *   **Encoding Detection**: Robust encoding handling for various file types
    *   **Structured Output**: Generates both JSON and YAML memory files

    ---

    ## üìö Documentation

    *   **Architecture**: [specs/spec.md](specs/spec.md)
    *   **Roadmap**: [specs/plan.md](specs/plan.md)
    *   **Migration Guide**: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)
    *   **Autonomous Execution**: [specs/protocols/001-autonomous-execution.md](specs/protocols/001-autonomous-execution.md)

    ---

    ## üßπ Legacy Support
    The old Python/Browser Bridge (V2) has been **archived**.
    *   Legacy artifacts: `archive/v2_python_bridge/`
    *   Legacy code: `webgpu_bridge.py`, `anchor_watchdog.py`, `start-anchor.bat`, etc.
    *   Migration guide: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)
- role: system
  type: document
  source: docs\low_resource_mode.md
  timestamp: 1767254282418
  content: "# Low-Resource Mode for Anchor Core\r\n\r\nThis guide explains how to optimize the Anchor Core for phones, small laptops, and other low-resource devices.\r\n\r\n## Environment Variables\r\n\r\nThe system supports two environment variables for optimization:\r\n\r\n### `LOW_RESOURCE_MODE`\r\n- Set to `true` to enable conservative settings for low-resource devices\r\n- Reduces GPU buffer size to 64MB\r\n- Limits concurrent operations to 1\r\n- Uses smaller models and reduced context windows\r\n\r\n### `CPU_ONLY_MODE`\r\n- Set to `true` to force CPU-only processing (no GPU)\r\n- Useful when GPU is unavailable or causing crashes\r\n- Slower but more stable on constrained hardware\r\n\r\n## Setting Environment Variables\r\n\r\n### Windows Command Prompt:\r\n```cmd\r\nset LOW_RESOURCE_MODE=true\r\nstart-anchor.bat\r\n```\r\n\r\n### Windows PowerShell:\r\n```powershell\r\n$env:LOW_RESOURCE_MODE=\"true\"\r\n.\\start-anchor.bat\r\n```\r\n\r\n### Linux/Mac:\r\n```bash\r\nexport LOW_RESOURCE_MODE=true\r\n./start-anchor.sh\r\n```\r\n\r\n## Conservative Settings Applied\r\n\r\nWhen `LOW_RESOURCE_MODE` is enabled, the system applies:\r\n\r\n- **GPU Buffer**: 64MB (vs 256MB default)\r\n- **Model**: Phi-3.5-mini (smallest recommended)\r\n- **Context Window**: 2048 tokens (vs 4096+ default)\r\n- **Batch Size**: 1 (vs 4+ default)\r\n- **WebGL Contexts**: 1 max (vs 16+ default)\r\n- **Cache Size**: 128MB (vs 1GB+ default)\r\n- **Timeouts**: 120 seconds (vs 30s default)\r\n\r\n## For Phones and Tablets\r\n\r\nFor mobile devices, use both settings:\r\n```cmd\r\nset LOW_RESOURCE_MODE=true\r\nset CPU_ONLY_MODE=true\r\nstart-anchor.bat\r\n```\r\n\r\n## Model Recommendations for Low-Resource Devices\r\n\r\n- `Phi-3.5-mini-instruct-q4f16_1-MLC` - Smallest recommended model\r\n- `Qwen2-0.5B-Instruct-q4f16_1-MLC` - If available, even smaller\r\n- Avoid models > 1.5B parameters on devices with < 1GB VRAM\r\n\r\n## Troubleshooting\r\n\r\n### GPU Crashes\r\n- Enable `LOW_RESOURCE_MODE=true`\r\n- Consider `CPU_ONLY_MODE=true` for stability\r\n\r\n### Slow Performance\r\n- Use the smallest available models\r\n- Reduce context window size\r\n- Close other GPU-intensive applications\r\n\r\n### Memory Issues\r\n- Enable conservative memory settings\r\n- Clear browser cache regularly\r\n- Use single-threaded mode"
- role: system
  type: document
  source: docs\sidecar_vision_guide.md
  timestamp: 1767254282419
  content: "# Anchor Core: Browser-Based Control Center & Vision Integration\r\n\r\n## Overview\r\n\r\nThe Anchor Core now includes a browser-based control center that provides unified access to system functionality through two main interfaces:\r\n\r\n1. **Sidecar Dashboard** (`http://localhost:8000/sidecar`) - Dual-tab interface for context retrieval and vision processing\r\n2. **Context UI** (`http://localhost:8000/context`) - Manual context retrieval with scrollable display and copy functionality\r\n\r\n## Components\r\n\r\n### Vision Engine (`tools/vision_engine.py`)\r\n- Python-powered Vision Language Model (VLM) integration\r\n- Currently configured for Ollama backend with LLaVA model\r\n- Handles image analysis and converts to text descriptions for memory storage\r\n\r\n### Sidecar Dashboard (`tools/sidecar.html`)\r\n- **Retrieve Tab**: Query the memory graph and retrieve context\r\n- **Vision Tab**: Drag-and-drop image processing with VLM analysis\r\n- Real-time processing logs\r\n\r\n### Context UI (`tools/context.html`)\r\n- Manual context retrieval interface\r\n- Scrollable text display for reviewing context\r\n- One-click copy functionality for pasting into other tools\r\n\r\n## Setup\r\n\r\n### Prerequisites\r\n1. Ensure Ollama is installed and running:\r\n   ```bash\r\n   ollama serve\r\n   ```\r\n\r\n2. Pull a vision model (e.g., LLaVA):\r\n   ```bash\r\n   ollama pull llava\r\n   ```\r\n\r\n### Launch\r\n1. Start the Anchor Core:\r\n   ```bash\r\n   start-anchor.bat\r\n   ```\r\n\r\n2. Access the interfaces:\r\n   - Sidecar: `http://localhost:8000/sidecar`\r\n   - Context UI: `http://localhost:8000/context`\r\n\r\n## Usage\r\n\r\n### Context Retrieval\r\n1. Open `http://localhost:8000/context`\r\n2. Enter a query in the search field (e.g., \"Project Specs\")\r\n3. Click \"Fetch Context\" to retrieve relevant information\r\n4. Review the context in the scrollable text area\r\n5. Click \"üìã Copy to Clipboard\" to copy the context for use elsewhere\r\n\r\n### Vision Processing\r\n1. Open `http://localhost:8000/sidecar`\r\n2. Go to the \"Vision\" tab\r\n3. Drag and drop an image or click to upload\r\n4. The image will be processed by the VLM\r\n5. Results will be stored in the memory graph automatically\r\n\r\n### Memory Search\r\n1. Use the \"Retrieve\" tab in the sidecar\r\n2. Enter your query and click \"Fetch Context\"\r\n3. Copy the results to use in other applications\r\n\r\n## Endpoints\r\n\r\n- `GET /sidecar` - Serve the sidecar dashboard\r\n- `GET /context` - Serve the context UI\r\n- `POST /v1/vision/ingest` - Process uploaded images with VLM\r\n- `POST /v1/memory/search` - Search the memory graph\r\n\r\n## Architecture\r\n\r\nThe system follows a unified architecture where:\r\n- The WebGPU Bridge (`webgpu_bridge.py`) serves UI files and orchestrates components\r\n- Vision processing happens in Python via the Vision Engine\r\n- Memory storage uses the graph database\r\n- Communication between components happens via WebSockets"
- role: system
  type: document
  source: extension\background.js
  timestamp: 1767462987183
  content: |-
    chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
      if (request.action === 'queryMemories') {
        queryMemoriesFromCozoDB(request.query)
          .then(memories => ({
            success: true,
            memories: memories,
            summary: generateSummary(memories)
          }))
          .then(result => sendResponse(result))
          .catch(err => {
            console.error('[Sovereign] Error querying memories:', err);
            sendResponse({
              success: false,
              error: err.message,
              summary: null
            });
          });
        return true; // Keep channel open for async response
      }
    });

    async function queryMemoriesFromCozoDB(userInput) {
      try {
        // Attempt to hit the Local Bridge (Node.js Sovereign Engine)
        // The bridge now runs on port 3000 as part of the Node.js monolith
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 10000); // 10 second timeout

        // Try the new Node.js server on port 3000
        const port = 3000;
        let response = null;
        let lastError = null;

        try {
          // First, try to ping the health endpoint to see if there's a bridge running
          const healthUrl = `http://localhost:${port}/health`;
          let healthResponse;

          try {
            healthResponse = await fetch(healthUrl, {
              method: 'GET',
              signal: controller.signal
            });
          } catch (healthError) {
            console.log(`[Sovereign] Health check failed for port ${port}:`, healthError.message);
            throw healthError;
          }

          if (!healthResponse.ok) {
            console.log(`[Sovereign] Health check failed for port ${port}, status: ${healthResponse.status}`);
            throw new Error(`Health check failed with status: ${healthResponse.status}`);
          }

          console.log(`[Sovereign] Bridge detected on port ${port}, testing query...`);

          // Now try the query endpoint (the new Node.js endpoint)
          const queryUrl = `http://localhost:${port}/v1/query`;

          // Try the query without authentication (Node.js server is open)
          response = await fetch(queryUrl, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              query: userInput, // This should be a CozoDB query string
              params: {}
            }),
            signal: controller.signal
          });

          if (response.ok) {
            console.log(`[Sovereign] Successfully connected to bridge at port ${port}`);
            clearTimeout(timeoutId);
            return response.json();
          } else {
            console.log(`[Sovereign] Query failed with status: ${response.status}`);
            throw new Error(`Query failed with status: ${response.status}`);
          }
        } catch (e) {
          lastError = e;
          console.log(`[Sovereign] Trying bridge port ${port} failed:`, e.message);
        }

        // If all ports failed, use the last error
        clearTimeout(timeoutId);
        throw lastError || new Error(`Node.js bridge on port 3000 unavailable. Is the Sovereign Engine running?`);
      } catch (e) {
        console.warn('[Sovereign] Backend unavailable, falling back to simulated response...', e.message);
        // In the future, this can connect directly to IndexedDB via shared worker
        // For now, return simulated data
        return [
          {
            content: "This is a simulated memory based on your input: " + userInput.substring(0, 100) + "...",
            timestamp: new Date().toISOString(),
            relevance: 0.8
          }
        ];
      }
    }

    function generateSummary(memories) {
      if (!memories || memories.length === 0) return null;

      const maxMemories = 3;
      const relevant = memories.slice(0, maxMemories);

      return relevant
        .map((m, idx) => `[Memory ${idx + 1}] ${m.content.substring(0, 150)}...`)
        .join('\n');
    }
- role: system
  type: document
  source: extension\content.js
  timestamp: 1767463066043
  content: |-
    // Platform-specific DOM selectors
    const SELECTORS = {
        'gemini.google.com': 'div[contenteditable="true"], textarea',
        'chatgpt.openai.com': 'textarea, div[contenteditable="true"]'
    };

    let textArea = null;
    let inputTimeout = null;
    const PAUSE_THRESHOLD = 3000; // 3 seconds

    // 1. Detect the active text input
    function detectTextArea() {
        const domain = window.location.hostname;
        const selector = SELECTORS[domain];
        if (!selector) return null;
        return document.querySelector(selector);
    }

    // 2. Extract text from the input
    function getVisibleText() {
        if (!textArea) return "";
        return textArea.value || textArea.textContent || "";
    }

    // 3. Monitor for user pauses
    function setupPauseDetector() {
        if (!textArea) return;

        textArea.addEventListener('input', () => {
            clearTimeout(inputTimeout);
            inputTimeout = setTimeout(() => {
                const text = getVisibleText();
                if (text.length > 10) { // Only query if meaningful text exists
                    console.log('[Sovereign] 3-second pause detected, querying memories...');
                    chrome.runtime.sendMessage(
                        { action: 'queryMemories', query: text },
                        (response) => {
                            if (response && response.success) injectContext(response);
                        }
                    );
                }
            }, PAUSE_THRESHOLD);
        });
    }

    // 4. Inject the retrieved context
    async function injectContext(contextData) {
        if (!contextData.summary) return;

        const timestamp = new Date().toLocaleTimeString();
        const summary = `\n\n[Sovereign Context Injection at ${timestamp}]\n${contextData.summary}\n---\n`;

        // For contenteditable (Gemini/modern apps)
        if (textArea.isContentEditable || textArea.getAttribute('contenteditable') === 'true') {
            // Simple append - in production this might need Range/Selection manipulation for cursors
            textArea.textContent = textArea.textContent + summary;
        }
        // For standard textarea (ChatGPT legacy)
        else {
            textArea.value += summary;
        }

        // Also send the current text content to the server for ingestion
        try {
            const currentText = getVisibleText();
            if (currentText && currentText.length > 10) { // Only send if meaningful content exists
                const response = await fetch('http://localhost:3000/v1/ingest', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        content: currentText,
                        filename: `extension_capture_${Date.now()}.txt`,
                        source: `extension://${window.location.hostname}`
                    })
                });

                if (response.ok) {
                    console.log('[Sovereign] Content successfully sent for ingestion');
                } else {
                    console.warn('[Sovereign] Ingestion request failed:', response.status);
                }
            }
        } catch (e) {
            console.warn('[Sovereign] Error sending content for ingestion:', e.message);
        }

        // Notify user
        displayIndicator('\u2713 Context injected', 'success'); // Using Unicode checkmark
    }

    // 5. UI Feedback
    function displayIndicator(message, type) {
        let indicator = document.getElementById('sovereign-indicator');
        if (!indicator) {
            indicator = document.createElement('div');
            indicator.id = 'sovereign-indicator';
            indicator.style.position = 'fixed';
            indicator.style.bottom = '20px';
            indicator.style.right = '20px';
            indicator.style.padding = '10px 15px';
            indicator.style.borderRadius = '5px';
            indicator.style.zIndex = '9999';
            indicator.style.fontFamily = 'monospace';
            document.body.appendChild(indicator);
        }

        indicator.textContent = message;
        indicator.style.background = type === 'success' ? '#238636' : '#da3633';
        indicator.style.color = '#ffffff';

        setTimeout(() => indicator.remove(), 5000);
    }

    // Handle messages from popup
    chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
        if (request.action === 'testInjection') {
            // For testing purposes, inject a sample context
            const testData = {
                summary: "This is a test injection from Sovereign Context Bridge.\n\n[Sample Memory] Example context for testing purposes..."
            };
            injectContext(testData);
            sendResponse({ success: true });
            return true; // Keep channel open for async response
        }
    });

    // --- 6. Robust Initialization ---
    function startSovereignObserver() {
        // Safety Check: If body isn't ready, wait for next frame
        if (!document.body) {
            console.warn("[Sovereign] document.body not ready, retrying...");
            requestAnimationFrame(startSovereignObserver);
            return;
        }

        console.log("[Sovereign] Body detected. Eyes opening...");

        // Main Logic
        if (!textArea) {
            textArea = detectTextArea();
            if (textArea) {
                console.log("[Sovereign] Input Found on Init.");
                setupPauseDetector();
            }
        }

        // Watch for dynamic changes
        const observer = new MutationObserver(() => {
            if (!textArea) {
                textArea = detectTextArea();
                if (textArea) console.log("[Sovereign] Input Found via Mutation.");
            }
        });

        observer.observe(document.body, { childList: true, subtree: true });
    }

    // Start only when DOM is ready
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', startSovereignObserver);
    } else {
        startSovereignObserver();
    }

    // --- DEBUG: CLICK-TO-LOG REFLEX ---
    // This allows us to manually verify what the extension sees when we touch the UI.
    document.addEventListener('click', (event) => {
        const target = event.target;
        const detected = detectTextArea();

        console.group("üëÅÔ∏è [Sovereign Debug] Retina Scan");
        console.log("üñ±Ô∏è Clicked Element:", target);
        console.log("üè∑Ô∏è Clicked Class:", target.className);

        if (detected) {
            console.log("%c‚úÖ Active Input Detected:", "color:green;font-weight:bold", detected);
            console.log("üìù Current Value:", detected.value || detected.innerText || detected.textContent);
        } else {
            console.log("%c‚ùå No Input Detected via Selector", "color:red;font-weight:bold");
            console.log("üîç Current Selector for Domain:", SELECTORS[window.location.hostname] || "NONE");
        }
        console.groupEnd();
    });
- role: system
  type: document
  source: extension\images\README.md
  timestamp: 1766591401813
  content: "# Extension Icons\r\n\r\nThis directory contains the following icon files:\r\n\r\n- `icon-16.png` - 16x16 pixel icon (minimal valid PNG)\r\n- `icon-32.png` - 32x32 pixel icon (minimal valid PNG)\r\n- `icon-128.png` - 128x128 pixel icon (minimal valid PNG)\r\n\r\nThese icons represent the Sovereign Context Bridge extension.\r\n\r\nNote: The current files are minimal valid PNGs for extension loading. Replace with actual designed icons for production use."
- role: system
  type: document
  source: extension\popup.html
  timestamp: 1766591401825
  content: "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <meta charset=\"UTF-8\">\r\n  <style>\r\n    body { background: #0f1115; color: #e2e8f0; font-family: sans-serif; width: 250px; padding: 15px; }\r\n    h3 { margin-top: 0; color: #00ff88; font-weight: 300; border-bottom: 1px solid #333; padding-bottom: 10px; }\r\n    .stat-row { display: flex; justify-content: space-between; margin-bottom: 8px; font-size: 0.9rem; }\r\n    .val { font-weight: bold; color: #58a6ff; }\r\n    button { width: 100%; padding: 8px; margin-top: 10px; background: #2d2d2d; border: 1px solid #444; color: #fff; cursor: pointer; border-radius: 4px; }\r\n    button:hover { background: #333; border-color: #00ff88; }\r\n    .status-ok { color: #00ff88; }\r\n    .status-err { color: #ff4444; }\r\n  </style>\r\n</head>\r\n<body>\r\n  <h3>&#129300 Sovereign Bridge</h3>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Status:</span>\r\n    <span id=\"status-badge\" class=\"status-err\">&#9679; Offline</span>\r\n  </div>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Memories:</span>\r\n    <span id=\"mem-count\" class=\"val\">0</span>\r\n  </div>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Last Inject:</span>\r\n    <span id=\"last-inject\" class=\"val\">-</span>\r\n  </div>\r\n\r\n  <button id=\"settings-btn\">&#9881;&#65039 Settings</button>\r\n  <button id=\"test-inject-btn\">&#129512 Test Injection</button>\r\n\r\n  <script src=\"popup.js\"></script>\r\n</body>\r\n</html>"
- role: system
  type: document
  source: extension\popup.js
  timestamp: 1766591401826
  content: "document.addEventListener('DOMContentLoaded', async () => {\r\n    const statusBadge = document.getElementById('status-badge');\r\n\r\n    // Check connection to Local Bridge\r\n    try {\r\n        // Using a more robust approach to handle CORS issues\r\n        const controller = new AbortController();\r\n        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout\r\n\r\n        const res = await fetch('http://localhost:8080/health', {\r\n            signal: controller.signal,\r\n            mode: 'cors', // Explicitly set CORS mode\r\n            credentials: 'omit' // Don't send credentials\r\n        });\r\n\r\n        clearTimeout(timeoutId);\r\n\r\n        if (res.ok) {\r\n            statusBadge.textContent = \"‚óè Online\";\r\n            statusBadge.className = \"status-ok\";\r\n        } else {\r\n            statusBadge.textContent = \"‚óè Offline\";\r\n            statusBadge.className = \"status-err\";\r\n        }\r\n    } catch (e) {\r\n        // Handle network errors, CORS errors, and timeouts\r\n        console.warn('[Sovereign] Backend connection failed:', e.message);\r\n        statusBadge.textContent = \"‚óè Offline\";\r\n        statusBadge.className = \"status-err\";\r\n    }\r\n\r\n    document.getElementById('test-inject-btn').addEventListener('click', () => {\r\n        // Trigger manual test injection\r\n        chrome.tabs.query({active: true, currentWindow: true}, (tabs) => {\r\n            chrome.tabs.sendMessage(tabs[0].id, { action: 'testInjection' }, (response) => {\r\n                if (chrome.runtime.lastError) {\r\n                    console.log('[Sovereign] Test injection not available on this page');\r\n                } else {\r\n                    console.log('[Sovereign] Test injection triggered');\r\n                }\r\n            });\r\n        });\r\n    });\r\n\r\n    // Add settings button functionality\r\n    document.getElementById('settings-btn').addEventListener('click', () => {\r\n        // For now, just show a message - in the future this could open options page\r\n        alert('Sovereign Context Bridge Settings\\n\\nConfigure extension preferences here.');\r\n    });\r\n});"
- role: system
  type: document
  source: extension\test_connection.html
  timestamp: 1767463024884
  content: |-
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Extension Server Connection Test</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                max-width: 600px;
                margin: 50px auto;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .container {
                background: white;
                padding: 30px;
                border-radius: 8px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            button {
                background-color: #007bff;
                color: white;
                border: none;
                padding: 12px 24px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 16px;
            }
            button:hover {
                background-color: #0056b3;
            }
            .result {
                margin-top: 20px;
                padding: 15px;
                border-radius: 4px;
                display: none;
            }
            .success {
                background-color: #d4edda;
                color: #155724;
                border: 1px solid #c3e6cb;
            }
            .error {
                background-color: #f8d7da;
                color: #721c24;
                border: 1px solid #f5c6cb;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Extension Server Connection Test</h1>
            <p>Click the button below to test if the extension can connect to the Sovereign Engine server.</p>
            
            <button id="ping">Ping Server</button>
            
            <div id="result" class="result"></div>
        </div>

        <script>
            document.getElementById('ping').onclick = async () => {
                const resultDiv = document.getElementById('result');
                resultDiv.style.display = 'none';
                
                try {
                    const res = await fetch('http://localhost:3000/health');
                    const data = await res.json();
                    
                    resultDiv.className = 'result success';
                    resultDiv.innerHTML = `<strong>Success!</strong><br>Server Status: ${JSON.stringify(data, null, 2)}`;
                    resultDiv.style.display = 'block';
                } catch (e) {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = `<strong>Connection Failed:</strong><br>${e.message}`;
                    resultDiv.style.display = 'block';
                }
            };
        </script>
    </body>
    </html>
- role: system
  type: document
  source: logs\README.md
  timestamp: 1767254282420
  content: "# Logs Directory\r\n\r\nThis directory contains individual log files for each system component to facilitate debugging and monitoring.\r\n\r\n## Log File Naming Convention\r\n\r\nEach component writes to its own log file named after the source:\r\n\r\n- `system.log` - System startup and general operations\r\n- `chat_api.log` - Chat API requests and responses\r\n- `memory_api.log` - Memory search API operations\r\n- `websocket_bridge.log` - WebSocket connection events\r\n- `python_stdout.log` - Python standard output\r\n- `python_stderr.log` - Python standard error\r\n\r\n## Log Rotation\r\n\r\nEach log file is automatically truncated to keep only the last 1000 lines to prevent excessive disk usage.\r\n\r\n## Log Format\r\n\r\nEach log entry follows this format:\r\n```\r\n[YYYY-MM-DD HH:MM:SS] [LEVEL] Message content\r\n```\r\n\r\nWhere LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG\r\n\r\n## Accessing Logs\r\n\r\n- **Real-time viewing**: Use the log viewer at `http://localhost:8000/log-viewer.html`\r\n- **File access**: Individual log files are available in this directory\r\n- **API access**: Recent logs available via `/logs/recent` endpoint"
- role: system
  type: document
  source: read_all.js
  timestamp: 1767482252033
  content: |
    const fs = require('fs');
    const path = require('path');
    const yaml = require('js-yaml');

    /**
     * Aggregates all readable text content from a directory and its subdirectories
     * into a single YAML file under 100k characters, respecting .gitignore rules.
     */
    function createFullCorpusRecursive() {
      // Set the root directory to scan as the directory containing this script
      const rootDirToScan = path.dirname(__filename);

      const outputYamlFile = path.join(rootDirToScan, 'combined_text.yaml');

      console.log(`Scanning Target Directory: ${rootDirToScan}`);

      // Read and parse .gitignore file
      const gitignorePath = path.join(rootDirToScan, '.gitignore');
      let gitignorePatterns = [];
      if (fs.existsSync(gitignorePath)) {
        const gitignoreContent = fs.readFileSync(gitignorePath, 'utf-8');
        gitignorePatterns = gitignoreContent
          .split('\n')
          .map(line => line.trim())
          .filter(line => line && !line.startsWith('#'))
          .map(pattern => {
            // Convert gitignore pattern to a regex
            let regexPattern = pattern
              .replace(/\./g, '\\.') // Escape dots
              .replace(/\*/g, '.*')  // Convert * to .*
              .replace(/\?/g, '.'); // Convert ? to .

            // Add start and end anchors
            if (!pattern.startsWith('/')) {
              regexPattern = '.*' + regexPattern;
            } else {
              regexPattern = regexPattern.substring(1);
            }

            if (!pattern.endsWith('/**') && !pattern.endsWith('/*')) {
              regexPattern += '$';
            }

            return new RegExp(regexPattern);
          });
      }

      const textExtensions = new Set([
        '.md', '.poml', '.yaml', '.yml', '.txt',
        '.py', '.js', '.ts', '.css', '.sh', '.ps1', '.html', '.bat'
      ]);

      // Files to exclude from the corpus itself to avoid recursion
      const excludeFiles = new Set([
        path.basename(outputYamlFile),
        'package-lock.json',
        'yarn.lock'
      ]);

      const filesToProcess = [];

      function walkDirectory(currentPath) {
        const items = fs.readdirSync(currentPath);

        for (const item of items) {
          const itemPath = path.join(currentPath, item);
          const stat = fs.statSync(itemPath);

          // Check if this path matches any gitignore pattern
          const relPath = path.relative(rootDirToScan, itemPath);
          const isIgnored = gitignorePatterns.some(pattern => pattern.test(relPath) || pattern.test(relPath + '/'));

          if (isIgnored) {
            continue; // Skip this file/directory if it matches gitignore
          }

          if (stat.isDirectory()) {
            walkDirectory(itemPath);
          } else if (stat.isFile()) {
            const ext = path.extname(item).toLowerCase();
            if (textExtensions.has(ext) && !excludeFiles.has(item)) {
              filesToProcess.push(itemPath);
            }
          }
        }
      }

      walkDirectory(rootDirToScan);
      filesToProcess.sort();

      if (filesToProcess.length === 0) {
        console.log(`No processable files found in '${rootDirToScan}'.`);
        return;
      }

      console.log(`Found ${filesToProcess.length} files to process.`);

      const memoryRecords = [];
      let totalCharCount = 0;
      const maxChars = 200000; // 200k characters limit

      for (const filePath of filesToProcess) {
        if (totalCharCount >= maxChars) {
          console.log(`Reached character limit of ${maxChars}, stopping processing.`);
          break;
        }

        console.log(`Processing '${filePath}'...`);
        try {
          // Get file metadata
          const fileStats = fs.statSync(filePath);
          const modTime = fileStats.mtimeMs; // milliseconds timestamp
          const relPath = path.relative(rootDirToScan, filePath);

          // Read file content
          const rawContent = fs.readFileSync(filePath);
          // For simplicity in JS, we'll assume UTF-8, but could implement encoding detection
          const decodedContent = rawContent.toString('utf-8');

          // Check if adding this file would exceed the character limit
          if (totalCharCount + decodedContent.length > maxChars) {
            // Truncate content to fit within the limit
            const remainingChars = maxChars - totalCharCount;
            if (remainingChars > 0) {
              const truncatedContent = decodedContent.substring(0, remainingChars);
              memoryRecords.push({
                role: 'system',
                type: 'document',
                source: relPath,
                timestamp: Math.floor(modTime), // Convert to integer
                content: truncatedContent
              });
              totalCharCount += truncatedContent.length;
              console.log(`Truncated content for '${relPath}' to fit within character limit.`);
            }
            break; // Reached the limit, stop processing
          } else {
            // Add full content
            memoryRecords.push({
              role: 'system',
              type: 'document',
              source: relPath,
              timestamp: Math.floor(modTime), // Convert to integer
              content: decodedContent
            });
            totalCharCount += decodedContent.length;
          }

        } catch (e) {
          console.log(`An unexpected error occurred with file '${filePath}': ${e.message}`);
        }
      }

      // Generate YAML Memory File
      console.log(`Generating YAML Memory: ${outputYamlFile}`);

      // Custom YAML representer for multiline strings
      const schema = yaml.DEFAULT_SCHEMA.extend([
        new yaml.Type('!long-string', {
          kind: 'scalar',
          predicate: (data) => typeof data === 'string' && data.includes('\n'),
          represent: (data) => ({ value: data, style: '|' })
        })
      ]);

      // Use default representer with multiline string style
      const yamlContent = yaml.dump(memoryRecords, {
        lineWidth: -1, // Don't wrap lines
        noRefs: true,
        quotingType: '"', // Use double quotes when needed
        forceQuotes: false
      });

      fs.writeFileSync(outputYamlFile, yamlContent, 'utf-8');

      console.log('\nCorpus aggregation complete.');
      console.log(`YAML Memory: '${outputYamlFile}'`);
      console.log(`Total characters: ${totalCharCount}`);
      console.log(`Files processed: ${memoryRecords.length}`);
    }

    // Run the function if this script is executed directly
    if (require.main === module) {
      createFullCorpusRecursive();
    }

    module.exports = { createFullCorpusRecursive };
- role: system
  type: document
  source: scripts\CHANGELOG.md
  timestamp: 1766774826762
  content: "# Changelog\r\n\r\nAll notable changes to the `scripts/` module will be documented in this file.\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **Smart GPU Bridge**: Added `StaticFiles` mount at `/models` to serve local model artifacts.\r\n- **On-Demand Downloads**: Added `POST /v1/models/pull` and `GET /v1/models/pull/status` to handle server-side model downloading from Hugging Face.\r\n- **Shared Module**: Integrated `scripts.download_models` for reusable download logic.\r\n\r\n### Fixed\r\n- **CORS/Auth Collision**: Reordered `CORSMiddleware` to wrap the entire application (including Auth middleware) to ensure CORS headers are sent even on 401 Unauthorized responses.\r\n- **Authentication**: Exempted `/models` path from Token Verification to allow browser-side fetching of artifacts without credentials.\r\n- **Import Error**: Fixed `ModuleNotFoundError` by changing import to `from download_models import ...` for direct script execution.\r\n"
- role: system
  type: document
  source: scripts\README.md
  timestamp: 1766448510017
  content: |
    # Scripts Directory

    Contains utility scripts for continuous integration and local environment setup.
- role: system
  type: document
  source: scripts\ci\check_docs.py
  timestamp: 1766241700595
  content: "#!/usr/bin/env python3\r\n\"\"\"CI doc check script (Sovereign Era).\r\n\r\nVerifies the presence of critical spec files defined in specs/doc_policy.md.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n\r\nREPO_ROOT = Path(__file__).resolve().parents[2]\r\n\r\n\r\ndef main() -> int:\r\n    # 1. Check Core Specs (per specs/doc_policy.md Rule 3)\r\n    expected = [\r\n        REPO_ROOT / \"specs\" / \"spec.md\",\r\n        REPO_ROOT / \"specs\" / \"plan.md\",\r\n        REPO_ROOT / \"specs\" / \"tasks.md\",\r\n        REPO_ROOT / \"specs\" / \"doc_policy.md\",\r\n    ]\r\n    \r\n    missing = [str(p) for p in expected if not p.exists()]\r\n    if missing:\r\n        print(\"[FAIL] Missing core specification files:\")\r\n        for m in missing:\r\n            print(f\"  - {m}\")\r\n        return 2\r\n\r\n    # 2. Check README\r\n    readme = REPO_ROOT / \"README.md\"\r\n    if not readme.exists():\r\n        print(\"[FAIL] README.md not found\")\r\n        return 2\r\n\r\n    text = readme.read_text(encoding=\"utf-8\")\r\n    lower = text.lower()\r\n    \r\n    # 3. Simple Content Check (Sovereign Context Engine)\r\n    # We relax the strict \"UTCP\" check as architecture evolves.\r\n    checks = [\r\n        (\"context engine\", \"Project name 'Context Engine' not found in README\"),\r\n    ]\r\n    \r\n    failed = []\r\n    for token, msg in checks:\r\n        if token not in lower:\r\n            failed.append(msg)\r\n            \r\n    if failed:\r\n        print(\"[FAIL] README checks failed:\")\r\n        for f in failed:\r\n            print(f\"  - {f}\")\r\n        return 2\r\n\r\n    print(\"[OK] Sovereign Doc Checks Passed\")\r\n    return 0\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n"
- role: system
  type: document
  source: scripts\download_models.py
  timestamp: 1767215198490
  content: "\r\nimport os\r\nimport json\r\nimport requests\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n# Configuration\r\nMODELS_DIR = Path(\"models\").resolve()\r\nHF_ENDPOINT = \"https://huggingface.co\"\r\n\r\ndef download_file(url, dest_path, progress_callback=None):\r\n    \"\"\"Download a file with progress indication\"\"\"\r\n    if dest_path.exists():\r\n        if progress_callback: progress_callback(f\"Skipping {dest_path.name} (exists)\", 1.0)\r\n        return\r\n\r\n    if progress_callback: progress_callback(f\"Downloading {dest_path.name}...\", 0.0)\r\n    \r\n    try:\r\n        response = requests.get(url, stream=True)\r\n        response.raise_for_status()\r\n        \r\n        total_size = int(response.headers.get('content-length', 0))\r\n        block_size = 8192\r\n        wrote = 0\r\n        \r\n        with open(dest_path, 'wb') as f:\r\n            for chunk in response.iter_content(chunk_size=block_size):\r\n                f.write(chunk)\r\n                wrote += len(chunk)\r\n                # Optional: detailed progress\r\n        \r\n        if progress_callback: progress_callback(f\"Saved {dest_path.name}\", 1.0)\r\n        \r\n    except Exception as e:\r\n        if progress_callback: progress_callback(f\"Error {dest_path.name}: {e}\", 0.0)\r\n        raise e\r\n\r\ndef download_model(model_id, repo_url=None, base_dir=None, progress_callback=None):\r\n    \"\"\"\r\n    Downloads an MLC model from Hugging Face.\r\n    \r\n    Args:\r\n        model_id (str): The ID of the model (e.g. \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\")\r\n        repo_url (str, optional): Full HF URL. Defaults to constructing from model_id.\r\n        base_dir (Path, optional): Directory to store models. Defaults to ./models\r\n        progress_callback (func, optional): Function(msg, progress_float)\r\n    \"\"\"\r\n    if base_dir is None:\r\n        base_dir = MODELS_DIR\r\n    \r\n    base_dir.mkdir(exist_ok=True)\r\n    \r\n    # Handle model_id / repo_url\r\n    if not repo_url:\r\n        repo_url = f\"{HF_ENDPOINT}/mlc-ai/{model_id}\"\r\n    \r\n    # Strip prefix from model_id for directory name\r\n    dir_name = model_id.split(\"/\")[-1]\r\n    model_dir = base_dir / dir_name\r\n    model_dir.mkdir(exist_ok=True)\r\n    \r\n    if progress_callback: progress_callback(f\"Starting download for {dir_name}\", 0.0)\r\n\r\n    # 1. Download ndarray-cache.json\r\n    cache_url = f\"{repo_url}/resolve/main/ndarray-cache.json\"\r\n    cache_path = model_dir / \"ndarray-cache.json\"\r\n    \r\n    try:\r\n        download_file(cache_url, cache_path, progress_callback)\r\n    except Exception as e:\r\n        print(f\"‚ùå Failed to fetch ndarray-cache.json: {e}\")\r\n        raise e\r\n\r\n    # 2. Parse cache\r\n    with open(cache_path, 'r') as f:\r\n        cache_data = json.load(f)\r\n        \r\n    records = cache_data.get(\"records\", [])\r\n    total_files = len(records) + 5\r\n    completed = 1\r\n\r\n    # 3. Download Shards\r\n    for record in records:\r\n        # Check both keys for safety (older MLC mappings used 'name')\r\n        file_name = record.get(\"dataPath\", record.get(\"name\"))\r\n        if not file_name:\r\n            continue\r\n            \r\n        url = f\"{repo_url}/resolve/main/{file_name}\"\r\n        dest = model_dir / file_name\r\n        \r\n        download_file(url, dest)\r\n        \r\n        completed += 1\r\n        if progress_callback: \r\n            progress_callback(f\"Downloading {file_name}\", completed/total_files)\r\n\r\n    # 4. Download Configs\r\n    config_files = [\"mlc-chat-config.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\", \"merges.txt\"]\r\n    for fname in config_files:\r\n        url = f\"{repo_url}/resolve/main/{fname}\"\r\n        dest = model_dir / fname\r\n        try:\r\n            download_file(url, dest)\r\n        except:\r\n            pass # Optional\r\n        \r\n        completed += 1\r\n        if progress_callback: \r\n            progress_callback(f\"Checked {fname}\", completed/total_files)\r\n\r\n    if progress_callback: progress_callback(\"Download Complete\", 1.0)\r\n    print(f\"Serve at: http://localhost:8080/models/{dir_name}\")\r\n\r\ndef main():\r\n    # Default behavior: Download Qwen2.5-Coder-1.5B\r\n    default_model = \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\"\r\n    \r\n    def print_progress(msg, p):\r\n        print(f\"[{int(p*100)}%] {msg}\")\r\n\r\n    download_model(default_model, progress_callback=print_progress)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
- role: system
  type: document
  source: scripts\gpu_manager.py
  timestamp: 1766591401831
  content: "#!/usr/bin/env python3\r\n\"\"\"\r\nGPU Resource Manager for ECE_Core\r\nProvides utilities to monitor and manage GPU locks in the WebGPU bridge\r\n\"\"\"\r\n\r\nimport requests\r\nimport json\r\nimport time\r\nimport argparse\r\nfrom typing import Dict, Any\r\n\r\nclass GPUResourceManager:\r\n    def __init__(self, bridge_url: str = \"http://localhost:8080\"):\r\n        self.bridge_url = bridge_url\r\n        self.headers = {\"Authorization\": \"Bearer sovereign-secret\"}\r\n    \r\n    def get_status(self) -> Dict[str, Any]:\r\n        \"\"\"Get current GPU status\"\"\"\r\n        try:\r\n            response = requests.get(f\"{self.bridge_url}/v1/gpu/status\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                return response.json()\r\n            else:\r\n                print(f\"Error getting status: {response.status_code} - {response.text}\")\r\n                return {}\r\n        except Exception as e:\r\n            print(f\"Error connecting to bridge: {e}\")\r\n            return {}\r\n    \r\n    def reset_lock(self) -> bool:\r\n        \"\"\"Reset the current GPU lock\"\"\"\r\n        try:\r\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/reset\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                print(\"‚úÖ GPU lock reset successfully\")\r\n                return True\r\n            else:\r\n                print(f\"‚ùå Failed to reset GPU lock: {response.status_code} - {response.text}\")\r\n                return False\r\n        except Exception as e:\r\n            print(f\"‚ùå Error resetting GPU lock: {e}\")\r\n            return False\r\n    \r\n    def force_release_all(self) -> bool:\r\n        \"\"\"Force release all GPU locks (emergency)\"\"\"\r\n        try:\r\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/force-release-all\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                print(\"‚úÖ All GPU locks force released successfully\")\r\n                return True\r\n            else:\r\n                print(f\"‚ùå Failed to force release GPU locks: {response.status_code} - {response.text}\")\r\n                return False\r\n        except Exception as e:\r\n            print(f\"‚ùå Error force releasing GPU locks: {e}\")\r\n            return False\r\n    \r\n    def monitor(self, interval: int = 5):\r\n        \"\"\"Monitor GPU status continuously\"\"\"\r\n        print(f\"üìä Monitoring GPU status every {interval}s (Ctrl+C to stop)\")\r\n        try:\r\n            while True:\r\n                status = self.get_status()\r\n                if status:\r\n                    locked = status.get('locked', False)\r\n                    owner = status.get('owner', 'None')\r\n                    queue_depth = status.get('queue_depth', 0)\r\n                    queued = status.get('queued', [])\r\n                    \r\n                    status_str = f\"GPU: {'LOCKED' if locked else 'FREE'}\"\r\n                    if locked:\r\n                        status_str += f\" by {owner}\"\r\n                    if queue_depth > 0:\r\n                        status_str += f\" | Queue: {queue_depth} | Queued: {', '.join(queued) if queued else 'None'}\"\r\n                    \r\n                    print(f\"[{time.strftime('%H:%M:%S')}] {status_str}\")\r\n                else:\r\n                    print(f\"[{time.strftime('%H:%M:%S')}] ‚ùå Unable to get GPU status\")\r\n                \r\n                time.sleep(interval)\r\n        except KeyboardInterrupt:\r\n            print(\"\\n‚èπÔ∏è  Monitoring stopped\")\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"GPU Resource Manager for ECE_Core\")\r\n    parser.add_argument(\"--bridge-url\", default=\"http://localhost:8080\", \r\n                       help=\"WebGPU bridge URL (default: http://localhost:8080)\")\r\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Get current GPU status\")\r\n    parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset GPU lock\")\r\n    parser.add_argument(\"--force-release\", action=\"store_true\", help=\"Force release all GPU locks\")\r\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Monitor GPU status continuously\")\r\n    parser.add_argument(\"--interval\", type=int, default=5, help=\"Monitor interval in seconds (default: 5)\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    manager = GPUResourceManager(args.bridge_url)\r\n    \r\n    if args.status:\r\n        status = manager.get_status()\r\n        if status:\r\n            print(json.dumps(status, indent=2))\r\n        else:\r\n            print(\"‚ùå Failed to get status\")\r\n    \r\n    elif args.reset:\r\n        manager.reset_lock()\r\n    \r\n    elif args.force_release:\r\n        manager.force_release_all()\r\n    \r\n    elif args.monitor:\r\n        manager.monitor(args.interval)\r\n    \r\n    else:\r\n        # Default: show status\r\n        status = manager.get_status()\r\n        if status:\r\n            locked = status.get('locked', False)\r\n            owner = status.get('owner', 'None')\r\n            queue_depth = status.get('queue_depth', 0)\r\n            queued = status.get('queued', [])\r\n            \r\n            print(f\"GPU Status: {'LOCKED' if locked else 'FREE'}\", end=\"\")\r\n            if locked:\r\n                print(f\" by {owner}\", end=\"\")\r\n            print(f\" | Queue: {queue_depth} items\")\r\n            \r\n            if queued:\r\n                print(f\"Queued: {', '.join(queued)}\")\r\n        else:\r\n            print(\"‚ùå Failed to get status\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
- role: system
  type: document
  source: server\run_context_read.js
  timestamp: 1767457815670
  content: |-
    const { createFullCorpusRecursive } = require('./src/read_all.js');

    // Run the function to aggregate content from the context directory
    // This is a wrapper to run the read_all functionality from the server directory
    // where all dependencies are properly installed

    console.log('Starting context aggregation from server directory...');
    createFullCorpusRecursive();
- role: system
  type: document
  source: server\src\index.js
  timestamp: 1767473486191
  content: |-
    const express = require('express');
    const cors = require('cors');
    const bodyParser = require('body-parser');
    const { CozoDb } = require('cozo-node');
    const chokidar = require('chokidar');
    const fs = require('fs');
    const path = require('path');
    const yaml = require('js-yaml');
    const { createReadStream } = require('fs');
    const { join } = require('path');

    // Initialize CozoDB with RocksDB backend
    const db = new CozoDb('rocksdb', './context.db');

    // Set up Express app
    const app = express();
    const PORT = 3000;

    // Serve static files from tools directory
    app.use(express.static(join(__dirname, '..', '..', 'tools')));

    // Middleware
    app.use(cors());
    app.use(bodyParser.json({ limit: '50mb' }));
    app.use(bodyParser.urlencoded({ extended: true }));

    // Initialize database schema
    async function initializeDb() {
      try {
        // Check if the memory relation already exists
        const checkQuery = '::relations';
        const relations = await db.run(checkQuery);

        // Only create the memory table if it doesn't already exist
        if (!relations.rows.some(row => row[0] === 'memory')) {
            const schemaQuery = ':create memory {id: String, timestamp: Int, content: String, source: String, type: String}';
            await db.run(schemaQuery);
            console.log('Database schema initialized');
        } else {
            console.log('Database schema already exists');
        }

        // Try to create FTS index (optional, may not be supported in all builds)
        try {
          const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;
          await db.run(ftsQuery);
          console.log('FTS index created');
        } catch (e) {
          console.log('FTS creation failed (optional feature):', e.message);
        }
      } catch (error) {
        console.error('Error initializing database:', error);
        throw error;
      }
    }

    // POST /v1/ingest endpoint
    app.post('/v1/ingest', async (req, res) => {
      try {
        const { content, filename, source, type = 'text' } = req.body;
        
        if (!content) {
          return res.status(400).json({ error: 'Content is required' });
        }
        
        const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
        const timestamp = Date.now();
        
        // Insert into CozoDB
        const query = `:insert memory {id, timestamp, content, source, type} <- $data`;
        const params = {
          data: [[
            id,
            timestamp,
            content,
            source || filename || 'unknown',
            type
          ]]
        };
        
        const result = await db.run(query, params);
        
        res.json({ 
          status: 'success', 
          id: id,
          message: 'Content ingested successfully'
        });
      } catch (error) {
        console.error('Ingest error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/query endpoint
    app.post('/v1/query', async (req, res) => {
      try {
        const { query, params = {} } = req.body;

        if (!query) {
          return res.status(400).json({ error: 'Query is required' });
        }

        const result = await db.run(query, params);

        res.json(result);
      } catch (error) {
        console.error('Query error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/memory/search endpoint (for context.html)
    app.post('/v1/memory/search', async (req, res) => {
      try {
        const { query, max_chars = 5000 } = req.body;

        if (!query) {
          return res.status(400).json({ error: 'Query is required' });
        }

        // Simple search implementation - retrieve all memory entries
        // We'll filter on the server side since CozoDB's text search functions may vary
        const searchQuery = `?[*] := *memory{id, timestamp, content, source, type}`;
        const params = {};

        const result = await db.run(searchQuery, params);

        if (result.ok) {
          let context = '';
          let charCount = 0;

          if (result.rows) {
            // Filter rows that contain the query term (case insensitive)
            const filteredRows = result.rows.filter(row => {
              const [id, timestamp, content, source, type] = row;
              return content.toLowerCase().includes(query.toLowerCase()) ||
                     source.toLowerCase().includes(query.toLowerCase());
            });

            // Sort by relevance (rows with query in content first, then in source)
            filteredRows.sort((a, b) => {
              const [a_id, a_timestamp, a_content, a_source, a_type] = a;
              const [b_id, b_timestamp, b_content, b_source, b_type] = b;

              const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());
              const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());

              // Prioritize content matches over source matches
              if (aContentMatch && !bContentMatch) return -1;
              if (!aContentMatch && bContentMatch) return 1;
              return 0;
            });

            for (const row of filteredRows) {
              const [id, timestamp, content, source, type] = row;
              const entryText = `### Source: ${source}\n${content}\n\n`;
              if (charCount + entryText.length > max_chars) {
                // Add partial content if we're near the limit
                const remainingChars = max_chars - charCount;
                context += entryText.substring(0, remainingChars);
                break;
              }
              context += entryText;
              charCount += entryText.length;
            }
          }

          res.json({ context: context || 'No results found.' });
        } else {
          res.status(500).json({ error: 'Search failed' });
        }
      } catch (error) {
        console.error('Memory search error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/system/spawn_shell endpoint (for index.html)
    app.post('/v1/system/spawn_shell', async (req, res) => {
      try {
        // For now, just return success - spawning a shell is complex and platform-dependent
        // In a real implementation, this would spawn a PowerShell terminal
        res.json({ success: true, message: "Shell spawned successfully" });
      } catch (error) {
        console.error('Spawn shell error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // GET /health endpoint
    app.get('/health', (req, res) => {
      res.json({ 
        status: 'Sovereign',
        timestamp: new Date().toISOString(),
        uptime: process.uptime()
      });
    });

    // Set up file watcher for context directory
    function setupFileWatcher() {
      const contextDir = path.join(__dirname, '..', 'context');
      
      // Ensure context directory exists
      if (!fs.existsSync(contextDir)) {
        fs.mkdirSync(contextDir, { recursive: true });
      }
      
      const watcher = chokidar.watch(contextDir, {
        ignored: /(^|[\/\\])\../, // ignore dotfiles
        persistent: true,
        ignoreInitial: true, // Don't trigger events for existing files
        awaitWriteFinish: {
          stabilityThreshold: 2000,
          pollInterval: 100
        }
      });

      watcher
        .on('add', filePath => handleFileChange(filePath))
        .on('change', filePath => handleFileChange(filePath))
        .on('error', error => console.error('Watcher error:', error));
        
      console.log('File watcher initialized for context directory');
    }

    async function handleFileChange(filePath) {
      console.log(`File changed: ${filePath}`);
      
      try {
        const content = fs.readFileSync(filePath, 'utf8');
        const relPath = path.relative(
          path.join(__dirname, '..', 'context'), 
          filePath
        );
        
        // Ingest the file content
        const query = `:insert memory {id, timestamp, content, source, type} <- $data`;
        const id = `file_${Date.now()}_${path.basename(filePath)}`;
        const params = {
          data: [[
            id,
            Date.now(),
            content,
            relPath,
            path.extname(filePath) || 'unknown'
          ]]
        };
        
        await db.run(query, params);
        console.log(`File ingested: ${relPath}`);
      } catch (error) {
        console.error(`Error processing file ${filePath}:`, error);
      }
    }

    // Initialize and start server
    async function startServer() {
      try {
        await initializeDb();
        setupFileWatcher();
        
        app.listen(PORT, () => {
          console.log(`Sovereign Context Engine listening on port ${PORT}`);
          console.log(`Health check: http://localhost:${PORT}/health`);
        });
      } catch (error) {
        console.error('Failed to start server:', error);
        process.exit(1);
      }
    }

    // Handle graceful shutdown
    process.on('SIGINT', async () => {
      console.log('Shutting down gracefully...');
      try {
        await db.close();
      } catch (e) {
        console.error('Error closing database:', e);
      }
      process.exit(0);
    });

    // Start the server
    startServer();

    module.exports = { db, app };
- role: system
  type: document
  source: server\src\migrate_history.js
  timestamp: 1767457063827
  content: |-
    const fs = require('fs');
    const path = require('path');
    const glob = require('glob');
    const yaml = require('js-yaml');

    // Migration script to consolidate legacy session files
    async function migrateHistory() {
      console.log('Starting legacy session migration...');

      // Find all session files
      const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');
      const pattern = path.join(sessionsDir, 'sessions_part_*.json');

      // Use glob to find all matching files
      const sessionFiles = glob.sync(pattern);

      if (sessionFiles.length === 0) {
        console.log('No session files found in the expected location.');
        // Try alternative path
        const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');
        const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');
        const altSessionFiles = glob.sync(altPattern);

        if (altSessionFiles.length === 0) {
          console.log('No session files found in alternative location either.');
          return;
        }

        console.log(`Found ${altSessionFiles.length} session files in alternative location.`);
        processSessionFiles(altSessionFiles);
        return;
      }

      console.log(`Found ${sessionFiles.length} session files`);
      processSessionFiles(sessionFiles);
    }

    function processSessionFiles(sessionFiles) {
      // Sort files numerically (part_1, part_2, ..., part_10, etc.)
      sessionFiles.sort((a, b) => {
        const matchA = a.match(/part_(\d+)/);
        const matchB = b.match(/part_(\d+)/);

        if (matchA && matchB) {
          return parseInt(matchA[1]) - parseInt(matchB[1]);
        }
        return a.localeCompare(b);
      });

      let allSessions = [];

      for (const file of sessionFiles) {
        console.log(`Processing: ${path.basename(file)}`);
        try {
          const content = fs.readFileSync(file, 'utf8');

          // Try to extract valid JSON from potentially corrupted files
          let data = extractValidJson(content);

          if (!data) {
            console.error(`Could not extract valid JSON from ${file}`);
            continue;
          }

          // Handle both list and object formats
          if (Array.isArray(data)) {
            allSessions = allSessions.concat(data);
          } else if (typeof data === 'object') {
            allSessions.push(data);
          } else {
            console.log(`Unexpected data format in ${file}, skipping...`);
          }
        } catch (error) {
          console.error(`Error reading ${file}:`, error.message);
        }
      }

      console.log(`Merged ${allSessions.length} total sessions`);

      // Save to YAML file
      const outputDir = path.join(__dirname, '..', '..', 'context');
      const outputFile = path.join(outputDir, 'full_history.yaml');

      // Custom YAML representer for multiline strings
      yaml.representer = {
        ...yaml.representer,
        string: (data) => {
          if (data.includes('\n')) {
            return new yaml.types.Str(data, { style: '|' });
          }
          return data;
        }
      };

      try {
        const yamlContent = yaml.dump(allSessions, {
          lineWidth: -1,
          noRefs: true,
          skipInvalid: true
        });

        fs.writeFileSync(outputFile, yamlContent, 'utf8');
        console.log(`YAML file created: ${outputFile}`);

        // Also save as JSON for compatibility
        const jsonOutputFile = path.join(outputDir, 'full_history.json');
        fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');
        console.log(`JSON file created: ${jsonOutputFile}`);

      } catch (error) {
        console.error('Error saving YAML file:', error.message);
        return;
      }

      console.log('Migration completed successfully!');
    }

    // Function to extract valid JSON from potentially corrupted files
    function extractValidJson(content) {
      try {
        // First, try to parse as regular JSON
        return JSON.parse(content);
      } catch (e) {
        // If that fails, clean the content and try again
        try {
          // Remove null bytes and other control characters that often corrupt JSON
          let cleanContent = content.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]/g, '');

          // Try to parse the cleaned content
          return JSON.parse(cleanContent);
        } catch (e2) {
          // If still failing, try to extract JSON array from the content
          try {
            // Find the main JSON array by looking for opening [ and closing ]
            const startIdx = cleanContent.indexOf('[');
            const endIdx = cleanContent.lastIndexOf(']');

            if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {
              const arrayContent = cleanContent.substring(startIdx, endIdx + 1);

              // Try to parse the extracted array
              return JSON.parse(arrayContent);
            }
          } catch (e3) {
            // If all attempts fail, return null
            return null;
          }
        }
      }

      return null;
    }

    // Run migration if this file is executed directly
    if (require.main === module) {
      migrateHistory().catch(console.error);
    }

    module.exports = { migrateHistory };
- role: system
  type: document
  source: specs\architecture-v2.md
  timestamp: 1767189341740
  content: |-
    # Sovereign Architecture V2: The Ghost & The Shell

    ## 1. Overview
    Architecture V2 decouples the **Interface** from the **Inference Engine**. 
    Instead of a monolithic "Chat UI" inside a browser, the system splits into a background service (Ghost) and a lightweight client (Shell).

    ## 2. Component A: The Ghost (Headless Engine)
    The Ghost is a background process responsible solely for loading the LLM into VRAM and exposing an API.

    * **Current Implementation:** Headless Chromium (`launch-ghost.ps1`).
    * **Future Implementation:** C++ Native Binary (`neural-ghost.exe`) using Dawn/WebGPU.
    * **Responsibility:**
        * Manage WebGPU Context.
        * Load Weights (MLC-LLM).
        * Serve `localhost:8080/v1/chat/completions`.
        * **Stealth Mode:** Uses `NoCacheStaticFiles` to treat models as RAM-only data, bypassing browser storage quotas.
        * **Search Engine:** Provides hybrid search (Vector + BM25 FTS) via CozoDB WASM.

    ## 3. Component B: The Shell (Native Client)
    The Shell is the user interface, residing in the user's native terminal environment (PowerShell, Bash, etc.).

    * **Implementation:** Python Client (`tools/sov.py`).
    * **Responsibility:**
        * Capture user input (`stdin`).
        * Send JSON payload to The Ghost.
        * Render streamed response to `stdout`.
        * Execute system commands (Agency).

    ## 4. The Data Flow
    1.  **User:** Types `sov "List large files"` in PowerShell.
    2.  **Shell:** Sends POST request to `localhost:8080`.
    3.  **Bridge:** Forwards request to Headless Browser (Ghost) via Websocket/Fetch.
    4.  **Ghost:** Runs inference on RTX 4090 via WebGPU.
    5.  **Ghost:** Returns tokens -> Bridge -> Shell.
    6.  **Shell:** Displays output or executes `Get-ChildItem` command.

    ## 5. Roadmap
    - [x] **Phase 1:** "Stealth Mode" Cache Bypass (Completed).
    - [x] **Phase 2:** Headless Browser Script (Completed).
    - [x] **Phase 3:** `sov.py` Native Client Implementation.
    - [x] **Phase 4:** Neural Shell Protocol (`/v1/shell/exec` endpoint).
    - [x] **Phase 4.5:** Ghost Auto-Ignition (Auto-start with ?headless=true flag).
    - [x] **Phase 5:** Native Shell Implementation (Anchor terminal with spawn endpoint).
    - [ ] **Phase 6:** Migration to C++ Native Runtime (Removing Chrome entirely).
- role: system
  type: document
  source: specs\architecture\anchor-core.spec.md
  timestamp: 1767327019386
  content: |-
    # Anchor Core Architecture Specification (v2.3.4)

    **Status:** Active (Text-Only + Watchdog + Context Persistence + Enhanced Resurrection) | **Component:** `tools/webgpu_bridge.py`

    ## Overview
    The Anchor Core unifies the Bridge, UI, and API into a single process running on port 8000, eliminating CORS issues and port conflicts. The system now uses an online-only model loading approach for reliable operation, with local model resolution as a fallback. Includes Watchdog service for passive text ingestion and enhanced auto-resurrection for browser process management.

    ## Architecture Diagram

    ```mermaid
    graph TD
        User["üë§ User"] --> Dashboard["üè† Dashboard (index.html)"]

        subgraph "ANCHOR CORE (Single Process)"
            Bridge["üîó WebGPU Bridge (8000)"]
            API["‚ö° API Endpoints"]
            UI["üåê UI Server"]
            WS["üì° WebSocket Layer"]
            ModelRedirect["üîÑ Model Redirect"]
            Resurrection["üîÑ Auto-Resurrection"]
        end

        subgraph "PASSIVE INGESTION"
            Watchdog["üêï Watchdog Service"]
            ContextFolder["üìÅ context/ folder"]
            FileEvents["üìù File System Events"]
        end

        Dashboard --> Bridge
        Bridge --> API
        Bridge --> UI
        Bridge --> WS
        Bridge --> ModelRedirect
        Bridge --> Resurrection

        subgraph "SHELL PROTOCOL"
            API --> ShellExec["Ïâò /v1/shell/exec"]
            API --> Spawn["üöÄ /v1/system/spawn_shell"]
        end

        subgraph "MEMORY INGESTION"
            API --> IngestAPI["üì• /v1/memory/ingest"]
            Watchdog --> IngestAPI
            ContextFolder --> Watchdog
            FileEvents --> Watchdog
        end

        ShellExec --> Host["üñ•Ô∏è Host System"]
        Spawn --> PowerShell["ü™ü PowerShell Window"]

        subgraph "MODEL LOADING"
            ModelRedirect --> Local["üíæ Local Models (if available)"]
            ModelRedirect --> Online["üåê Online Models (fallback)"]
        end
    ```

    ## Components

    ### 1. The Unified Core (`webgpu_bridge.py`)
    - **Role:** Single server for API, UI, and WebSockets
    - **Port:** 8000 (The One Port)
    - **Function:** Bridges browser WebGPU to system commands

    ### 2. Shell Protocol (The Hands)
    - **`/v1/shell/exec`**: Execute system commands via bridge

    ### 3. Context System
    - **Context UI**: Read-only interface for quick context retrieval and copy-paste.
    - **Memory Search**: Query the Ghost Engine's Graph (Vector + BM25) for relevant context.


    ### 4. Model Loading System
    - **Online-First**: Uses direct HuggingFace URLs for reliable loading (Standard 007)
    - **Local Fallback**: Redirects to local models when available, online when not
    - **Bridge Redirect**: `/models/{model}/resolve/main/{file}` endpoint handles resolution

    ### 5. Passive Ingestion System
    - **Watchdog Service**: Monitors `context/` folder for text file changes
    - **Debounce & Hash Check**: Prevents duplicate ingestion from autosave events
    - **File Ingestion Endpoint**: `/v1/memory/ingest` for text content ingestion
    - **Automatic Processing**: Processes .txt, .md, .markdown, .py, .js, .html, .json, .yaml, .yml, .sh, .bat and other code/text files automatically
    - **Code File Support**: Expanded to include common programming language extensions
    - **Session Recording**: Daily chat session files created in `context/sessions/` for persistence

    ### 6. Session Recording System
    - **Daily Files**: Creates `chat_YYYY-MM-DD.md` files for each day's conversations
    - **Text-File Source of Truth**: All chat history saved to markdown files for cross-machine sync
    - **Timestamped Entries**: Messages formatted with timestamps (`### ROLE [HH:MM:SS]`)
    - **Infinite Loop**: Chat -> File -> Ingestion -> Memory -> Next Chat
    - **Cross-Machine Sync**: Files automatically synced via Dropbox/Git for multi-device access

    ### 7. Process Management System
    - **Auto-Resurrection**: Automatically restarts Ghost Engine when connection drops
    - **Process Cleanup**: Kills existing browser processes before launching new ones
    - **Port Management**: Explicitly manages remote debugging port (9222) to prevent conflicts
    - **Retry Logic**: Implements retry mechanism with configurable attempts

    ## Endpoints

    ### `GET /sidecar`
    - **Function:** Serves the Context UI (consolidated interface).

    ### `GET /context`
    - **Function:** Serves the Context UI (consolidated interface).

    ### `POST /v1/memory/search`
    - **Function:** Queries the Ghost Engine's Graph (Vector + BM25).
    - **Input:** `{ "query": "string" }`
    - **Output:** `{ "context": "Formatted Ground Truth..." }`

    ### `POST /v1/chat/completions`
    - **Function:** Proxy to browser engine
    - **Auth:** Bearer token

    ### `POST /v1/shell/exec`
    - **Function:** Execute system commands
    - **Format:** `{ "cmd": "command" }`

    ### `POST /v1/system/spawn_shell`
    - **Function:** Launch native PowerShell client
    - **Result:** New `anchor.py` terminal window

    ### `POST /v1/memory/ingest`
    - **Function:** Ingest text content into memory graph
    - **Format:** `{ "content": "text", "source": "file_path", "timestamp": "ISO" }`
    - **Response:** `{ "status": "success", "message": "Memory ingested successfully" }`

    ### `POST /v1/gpu/lock`
    - **Function:** Acquire GPU resource with queuing
    - **Response:** `{ "status": "acquired", "token": "gpu_token_..." }`

    ### `POST /v1/gpu/unlock`
    - **Function:** Release GPU resource
    - **Response:** `{ "status": "released" }`

    ### `GET /v1/gpu/status`
    - **Function:** Get GPU lock status and queue depth
    - **Response:** `{ "locked": true/false, "owner": "agent_id", "queue_depth": number }`

    ## Search Architecture

    ### Hybrid Retrieval System

    * **BM25 FTS**: Lexical search using CozoDB Full Text Search with stemming
    * **Context Manager**: Intelligent retrieval in `ContextManager.findRelevantMemories()`
    * **Fallback Mechanism**: Regex-based search when FTS index unavailable

    ## Security
    - **Token Auth:** `Authorization: Bearer sovereign-secret`
    - **CORS Policy:** Open for internal use only
    - **System Access:** Restricted to authorized commands
- role: system
  type: document
  source: specs\architecture\api.spec.md
  timestamp: 1766733152737
  content: |
    # API Specification (WebGPU Bridge)

    **Status:** Production
    **Component:** `tools/webgpu_bridge.py`

    ## Overview
    The "Bridge" acts as a reverse-proxy, exposing the browser's WebLLM engine as an OpenAI-compatible API.

    ## Endpoints

    ### `POST /v1/chat/completions`
    - **Format:** OpenAI Standard.
    - **Flow:** 
      1. Client sends JSON to Python Bridge.
      2. Bridge forwards via WebSocket to `model-server-chat.html`.
      3. Browser computes response (WebGPU).
      4. Result streamed back to Bridge -> Client.

    ### `POST /v1/embeddings`
    - **Format:** OpenAI Standard.
    - **Flow:** Forwards to `webgpu-server-embed.html`.

    ### `POST /v1/shell/exec`
    - **Purpose:** Neural Shell Protocol (The Hands). Executes arbitrary shell commands on host.
    - **Format:** JSON `{ "cmd": "string" }`.
    - **Response:** JSON `{ "stdout": "...", "stderr": "...", "code": 0 }`.
    - **Security:** Strict Token Auth required. 30s timeout.

    ## WebSockets
    - `/ws/chat`: Connection for Chat Worker.
    - `/ws/embed`: Connection for Embedding Worker.

    ## Security
    - **Token Auth:** `Authorization: Bearer <BRIDGE_TOKEN>` required.
    - **Network:** Binds to random port (obfuscation) or `8000` (default).
- role: system
  type: document
  source: specs\architecture\sovereign-wasm.spec.md
  timestamp: 1767189568414
  content: |
    # Sovereign WASM Specification (Root Kernel)

    ## Architecture Overview
    The **Root Coda** system runs entirely in the browser using a unified Kernel (`sovereign.js`) that manages Compute (WebLLM) and Memory (CozoDB).

    ## 1. The Kernel (`tools/modules/sovereign.js`)
    The Kernel is the standard library for all Root Tools. It enforces consistency and safety.

    ### 1.1 Hardware Abstraction ("Snapdragon Fix")
    **Problem**: Adreno GPUs (Snapdragon X Elite) and some mobile chips crash if a WebGPU buffer >256MB is requested, or if context exceeds 4k tokens without specific driver flags.
    **Solution**: `getWebGPUConfig(profile)`
    - **Lite**: Clamps buffer to 256MB, Context to 2048.
    - **Mid**: Clamps buffer to 1GB, Context to 4096.
    - **High/Ultra**: Unlocked.

    ### 1.2 Unified Logging
    **Problem**: `console.log` is invisible on mobile or when running as a PWA.
    **Solution**: `SovereignLogger`
    - Broadcasts all logs to `BroadcastChannel('sovereign-logs')`.
    - Consumed by `log-viewer.html` for real-time remote debugging.

    ### 1.3 Reactive State
    **Problem**: Spaghetti code updating DOM elements manually.
    **Solution**: `createStore(initialState)`
    - Lightweight `Proxy`-based store.
    - Components subscribe to changes: `subscribe((key, val) => updateUI(key, val))`.

    ## 2. Memory Layer (CozoDB WASM)
    The Kernel provides a standardized loader: `initCozo(wasmPath)`.

    ### Data Portability
    - **Lossless Export**: The Root Builder features a "Lossless Export" button.
    - **Mechanism**: Dumps full Cozo relations (including vectors) to a JSON file.
    - **Use Case**: Transfer full "Brain" state between devices or backup.

    ### Search Enhancement (BM25 FTS)
    - **Hybrid Retrieval**: Combines vector search (semantic) with BM25 FTS (lexical) for optimal results.
    - **Index Creation**: FTS index created during initialization: `::fts create memory:content_fts`
    - **Stemming Support**: Uses English stemming for better word variation matching.
    - **Fallback Mechanism**: Maintains regex-based search when FTS index unavailable.

    ### Schema
    ```datalog
    :create memory {
        id: String
        =>
        timestamp: Int,
        role: String,
        content: String,
        source: String,
        embedding: <F32; 384>
    }
    ```

    ## 3. Tool Bridge (Legacy Support)
    The `webgpu_bridge.py` acts as a secure relay (websocket <-> http) for external tools (like VS Code extensions) to access the Browser's LLM.
    - **Input**: HTTP/REST (`/v1/chat/completions`)
    - **Output**: WebSocket (`ws://localhost:8080/ws/chat`)

    ### 3.1 Local Model Serving (Storage Quota Bypass)
    **Problem**: Browsers (especially in Incognito/Guest modes) strictly limit persistent storage (e.g., <300MB), preventing the caching of large LLM weights (~2GB+).
    **Solution**: The Bridge acts as a local HTTP File Server.
    - **Endpoint**: `http://localhost:8080/models/{model_id}/...`
    - **Mechanism**: 
        1. Frontend requests model from localhost.
        2. If 404, Frontend triggers `POST /v1/models/pull`.
        3. Bridge downloads artifacts from Hugging Face to `./models`.
        4. Frontend polls status and loads the model into RAM (bypassing IndexedDB quota).

    ## 4. Audio Input (Root Mic)
    **Goal**: Pure client-side speech-to-text without sending audio to a cloud.

    ### 4.1 Pipeline
    1. **Capture**: `MediaRecorder` (WebM) -> 48kHz decoding.
    2. **Preprocessing**:
       - Downsampling to 16kHz (Whisper Native).
       - **Noise Gate**: Discards audio if peak amplitude < 0.01 (Prevents transcribing silence).
       - **Amplification**: Smart gain (max 5x) for quiet voices, but capped to avoid boosting noise floor.
    3. **Inference (WASM)**: 
       - Model: `Xenova/whisper-tiny.en` (Quantized).
       - **Long-form Strategy**: Uses `chunk_length_s: 30` and `stride_length_s: 5` to process audio exceeding the model's native 30s window.
    4. **Post-Processing (Refinement)**:
       - **Hallucination Filter**: Regex removal of common Whisper artifacts (e.g., "[Music]", "Applause", "Amara.org").
       - **LLM Cleanup**: The raw transcript is passed to the local Qwen2.5 instance with a system prompt to fix grammar/punctuation without altering meaning.

    ### 4.2 Summarization Loop
    - **Trigger**: User clicks "Summarize & Clarify" after a successful transcription.
    - **Process**: The cleaned transcript is sent back to the Local Kernel (Qwen2.5) with a prompt to "summarize and clarify core meaning."
    - **Output**: The transcript is replaced by the summary, which is automatically copied to the clipboard.

    ## 5. Parallel Compute (The Worker)
    To prevent UI freezing during heavy inference, the LLM runs in a dedicated Web Worker.

    ### 5.1 `tools/modules/llm-worker.js`
    - **Role**: Hosts the `MLCEngine` instance.
    - **Communication**: Uses `WebWorkerMLCEngineHandler` to bridge messages between the main thread and the worker.
    - **Benefit**: Ensures the UI remains responsive (scrolling, typing) even while the GPU is crunching tokens.

    ## 6. Resource Management (Orchestrator)
    **Problem**: Multiple browser tabs (Mic, Console, Dreamer) competing for the single GPU resource led to deadlocks, timeouts, and "Device Lost" errors.
    **Solution**: A Priority-Queue based Locking System with enhanced timeout handling and emergency procedures.

    ### 6.1 GPU Controller (`tools/modules/sovereign.js`)
    - **Serialized Loading**: `withModelLoadLock()` ensures only one tab loads a model at a time, preventing GPU overload during initial loading.
    - **Access Priority**:
      - **Priority 0 (High)**: Root Mic (Voice Input - cannot wait)
      - **Priority 10 (Med)**: Root Console (Chat - user waiting)
      - **Priority 15 (Med)**: Default priority
      - **Priority 20 (Low)**: Root Dreamer (Background tasks)
    - **Timeouts**: Increased broken-lock timeout from 60s to **120s** (2 minutes) to accommodate large model loading.
    - **Retry Logic**: Added retry mechanism with proper error handling.
    - **Fallback Mechanism**: Direct WebGPU access when bridge unavailable.
    - **Status Checking**: Added GPU status check functionality.
    - **Emergency Release**: If a lock is held >120s, it is forcibly broken to prevent system deadlock.

    ### 6.2 Bridge Orchestration (`smart_gpu_bridge.py`)
    - **Queue Tracking**: Tracks request start times to prevent starvation.
    - **Enhanced Timeouts**: Increased timeout from 60s to 120s for lock acquisition.
    - **Request Tracking**: Added request_start_times to prevent queue starvation.
    - **Enhanced Status**: Detailed queue information in status endpoint.
    - **Endpoints**:
      - `GET /v1/gpu/status`: Monitor active locks and queue depth.
      - `POST /v1/gpu/lock`: Acquire lock (blocking).
      - `POST /v1/gpu/unlock`: Release GPU lock.
      - `POST /v1/gpu/reset`: Standard reset.
      - `POST /v1/gpu/force-release-all`: Nuclear option for stuck states.
      - `POST /v1/gpu/force-release`: Emergency release endpoint.
      - `POST /v1/hot-reload`: Hot reload endpoint for development.

    ### 6.3 GPU Management Utilities
    - **GPU Manager Script** (`scripts/gpu_manager.py`): Command-line tool to monitor and manage GPU resources.
    - **Test Script** (`scripts/test_gpu_fixes.py`): Comprehensive testing of GPU resource management.
    - **Monitoring Commands**:
      - `python scripts/gpu_manager.py --status`: Check GPU status
      - `python scripts/gpu_manager.py --monitor --interval 10`: Monitor continuously
      - `python scripts/gpu_manager.py --force-release`: Force release GPU locks
      - `python scripts/gpu_manager.py --reset`: Standard reset
      - `python scripts/gpu_manager.py --hot-reload`: Trigger hot reload

    ## 7. Development Infrastructure
    **Problem**: Restarting the Python bridge and refreshing 3 browser tabs for every small code change is slow.

    ### 7.1 Hot Reload System
    - **Backend**: `smart_gpu_bridge.py` monitors its own source code (and `download_models.py`) for changes. It automatically reloads the Python process while preserving active WebSocket connections if possible.
    - **Frontend**: `gpu-hot-reloader.js` connects to the bridge via WebSocket. When the bridge signals a reload (or detects an HTML update), the browser auto-refreshes.
    - **Safety**: Automatically releases all GPU locks during a reload event to prevent "Ghost Locks".
- role: system
  type: document
  source: specs\doc_policy.md
  timestamp: 1767473067273
  content: |
    # Documentation Policy (Root Coda)

    **Status:** Active | **Authority:** Human-Locked

    ## Core Philosophy
    1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.
    2. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.
    3. **Brevity:** Text sections must be <500 characters.
    4. **Pain into Patterns:** Every major bug must become a Standard.
    5. **LLM-First Documentation:** Documentation must be structured for LLM consumption and automated processing.

    ## Structure

    ### 1. The Blueprint (`specs/spec.md`)
    *   **Role:** The single architectural source of truth.
    *   **Format:** "Visual Monolith".
    *   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.

    ### 2. The Tracker (`specs/tasks.md`)
    *   **Role:** Current work queue.
    *   **Format:** Checklist.
    *   **Maintenance:** Updated by Agents after every major task.

    ### 3. The Roadmap (`specs/plan.md`)
    *   **Role:** Strategic vision.
    *   **Format:** Phased goals.

    ### 4. Standards (`specs/standards/*.md`)
    *   **Role:** Institutional Memory (The "Laws" of the codebase).
    *   **Trigger:** Created after any bug that took >1 hour to fix OR any systemic improvement that affects multiple components.
    *   **Format:** "The Triangle of Pain"
        1.  **What Happened:** The specific failure mode (e.g., "Bridge crashed on start").
        2.  **The Cost:** The impact (e.g., "3 hours debugging Unicode errors").
        3.  **The Rule:** The permanent constraint (e.g., "Force UTF-8 encoding on Windows stdout").

    ### 5. Local Context (`*/README.md`)
    *   **Role:** Directory-specific context.
    *   **Limit:** 1 sentence explaining the folder's purpose.

    ### 6. System-Wide Standards
    *   **Universal Logging:** All system components must route logs to the central log collection system (Standard 013)
    *   **Single Source of Truth:** The log viewer at `/log-viewer.html` is the single point for all system diagnostics
    *   **Async Best Practices:** All async/await operations must follow proper patterns for FastAPI integration (Standard 014)
    *   **Browser Control Center:** All primary operations must be accessible through unified browser interface (Standard 015)
    *   **Detached Script Execution:** All data processing scripts must run in detached mode with logging to `logs/` directory (Standard 025)
    *   **Never Attached Mode:** Long-running services and scripts must NEVER be run in attached mode to prevent command-line blocking (Standard 035 in 30-OPS)
    *   **Ghost Engine Connection Management:** All memory operations must handle Ghost Engine disconnections gracefully with proper error reporting and auto-reconnection (Standard 026)
    *   **No Resurrection Mode:** System must support manual Ghost Engine control via NO_RESURRECTION_MODE flag (Standard 027)
    *   **Default No Resurrection:** Ghost Engine resurrection is disabled by default, requiring manual activation (Standard 028)
    *   **Consolidated Data Aggregation:** Single authoritative script for data aggregation with multi-format output (Standard 029)
    *   **Multi-Format Output:** Project aggregation tools must generate JSON, YAML, and text outputs for maximum compatibility (Standard 030)
    *   **Ghost Engine Stability:** CozoDB schema creation must handle FTS failures gracefully to prevent browser crashes (Standard 031)
    *   **Ghost Engine Initialization Flow:** Database initialization must complete before processing ingestion requests to prevent race conditions (Standard 032)
    *   **CozoDB Syntax Compliance:** All CozoDB queries must use proper syntax to ensure successful execution (Standard 033)
    *   **Node.js Monolith Migration:** System must migrate from Python/Browser Bridge to Node.js Monolith architecture (Standard 034)

    ## LLM Protocol
    1. **Read-First:** Always read `specs/spec.md` AND `specs/standards/` before coding.
    2. **Drafting:** When asked to document, produce **Mermaid diagrams** and short summaries.
    3. **Editing:** Do not modify `specs/doc_policy.md` or `specs/spec.md` structure unless explicitly instructed.
    4. **Archival:** Move stale docs to `archive/` immediately.
    5. **Enforcement:** If a solution violates a Standard, reject it immediately.
    6. **Standards Evolution:** New standards should follow the "Triangle of Pain" format and be numbered sequentially (001, 002, etc.).
    7. **Cross-Reference:** When creating new standards, reference related existing standards to maintain consistency.
    8. **Detached Mode:** All LLM development scripts must run in detached mode (non-interactive) and log to files in the `logs/` directory with timestamped names (Standard 025).

    ---
    *Verified by Architecture Council. Edited by Humans Only.*
- role: system
  type: document
  source: specs\models.md
  timestamp: 1767076621300
  content: "# Verified MLC-LLM Model URLs\r\n\r\n**Status:** Registry of verified WebLLM-compatible model URLs.\r\n**Last Updated:** Dec 22, 2025\r\n**Source:** `mlc-ai/web-llm` config and verified HTTP checks.\r\n\r\n## Technology: WASM + WebGPU Inference\r\n\r\nThis project uses **WebLLM** (by MLC-AI) to run Large Language Models directly in the browser.\r\n\r\n1.  **Compilation:** Models (Llama 3, Qwen 2.5, etc.) are compiled into **WebAssembly (WASM)** modules (`.wasm`). These modules contain the model's architecture and logic, optimized for execution in a web environment.\r\n2.  **Acceleration:** The WASM module uses the **WebGPU API** to access the user's local GPU. This allows for massive parallelism, enabling 7B+ parameter models to run at interactive speeds (20-100+ tokens/sec) on consumer hardware.\r\n3.  **Zero-Server:** No data leaves the browser. The \"Backend\" is the user's own GPU.\r\n\r\n## Official Model Registry\r\n\r\n**CRITICAL REFERENCE:** The definitive list of supported models and their WASM binaries can be found at:\r\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293\r\n- This contains the official `prebuiltAppConfig` with verified model configurations\r\n\r\n**ADDITIONAL VERIFIED LINKS:**\r\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293 - Contains verified MLC model configurations and working WASM URLs\r\n\r\n---\r\n\r\n## 1. Verified Models (Ready for Production)\r\n\r\nThese models have been verified to exist in the `v0_2_80` library and are compatible with the current `web-llm` version.\r\n\r\n### üåü 7B - 8B Class (Recommended)\r\nBalanced performance for reasoning and chat. Requires 6GB+ VRAM.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2.5-7B-Instruct-q4f16_1-MLC` | **Best All-Rounder.** Fast, smart, 32k context. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.1-8B-Instruct-q4f32_1-MLC` | **Meta's Latest.** Strong reasoning. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3-8B-Instruct-q4f32_1-MLC` | Llama 3 Base. Reliable. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC` | **Reasoning Specialist.** | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` (Uses Qwen2 base) |\r\n\r\n### üöÄ High Performance (Small)\r\nFastest start times. Works on most laptops/integrated graphics.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Phi-3.5-mini-instruct-q4f16_1-MLC` | **Microsoft Phi.** 3.8B params. Very smart for size. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` | **Ultra-Lite.** 1.5B params. Blazing fast. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `SmolLM2-1.7B-Instruct-q4f16_1-MLC` | Efficient 1.7B model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üß† Code Specialist Models\r\nModels optimized for code generation and command translation.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` | **Qwen Coder 1.5B.** Specialized for code and command generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` | **Qwen Coder 7B.** Advanced code reasoning and command translation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üåü Gemma Models (Google)\r\nLightweight and efficient models from Google.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `gemma-2-9b-it-q4f16_1-MLC` | **Gemma 2 9B.** High performance text generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-9b-it-q4f32_1-MLC` | **Gemma 2 9B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-2b-it-q4f16_1-MLC` | **Gemma 2 2B.** Lightweight option. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-2b-it-q4f32_1-MLC` | **Gemma 2 2B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-3-1b-it-q4f16_1-MLC` | **Gemma 3 1B.** Latest generation small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-3-1b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üëÅÔ∏è Vision Models (Multimodal)\r\nModels that can process both text and images.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Phi-3.5-vision-instruct-q4f16_1-MLC` | **Microsoft Phi Vision.** 4.2B params, multimodal. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm` |\r\n| `Phi-3.5-vision-instruct-q4f32_1-MLC` | **Microsoft Phi Vision.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f32_1-ctx4k_cs2k-webgpu.wasm` |\r\n\r\n### üöÄ Larger Models (For High VRAM Systems)\r\nModels for systems with 12GB+ VRAM like RTX 4090.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Llama-2-13b-chat-hf-q4f16_1-MLC` | **Llama 2 13B.** For high-end systems. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-13b-chat-hf-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-8B-q4f16_1-MLC` | **Qwen 3 8B.** Latest generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-8B-q4f32_1-MLC` | **Qwen 3 8B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-4B-q4f16_1-MLC` | **Qwen 3 4B.** Balanced performance. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-4B-q4f32_1-MLC` | **Qwen 3 4B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üöÄ Smaller Models (For Low VRAM Systems)\r\nModels optimized for systems with limited VRAM like the XPS 13.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2-0.5B-Instruct-q4f16_1-MLC` | **Ultra-Lightweight.** 0.5B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2-0.5B-Instruct-q4f32_1-MLC` | **Ultra-Lightweight.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-0.6B-q4f16_1-MLC` | **Qwen 3 Tiny.** 0.6B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-0.6B-q4f32_1-MLC` | **Qwen 3 Tiny.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-1.7B-q4f16_1-MLC` | **Qwen 3 Small.** 1.7B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-1.7B-q4f32_1-MLC` | **Qwen 3 Small.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-1B-Instruct-q4f16_1-MLC` | **Llama 3.2 1B.** Efficient small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-1B-Instruct-q4f32_1-MLC` | **Llama 3.2 1B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-3B-Instruct-q4f16_1-MLC` | **Llama 3.2 3B.** Balanced small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-3B-Instruct-q4f32_1-MLC` | **Llama 3.2 3B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n---\r\n\r\n## 2. Known Issues / Missing Binaries\r\n\r\nThese models are listed in the official config but their WASM binaries are not currently hosted in the `v0_2_80` folder. **Avoid using these until binaries are available.**\r\n\r\n*   `Qwen2.5-14B-Instruct-q4f16_1-MLC` (404 Not Found) - **Known Issue**\r\n*   `DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC` (404 Not Found) - **Known Issue**\r\n*   `Qwen2-VL-7B-Instruct-q4f16_1-MLC` (404 Not Found)\r\n*   `gemma-3-2b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 2B available, only 1B exists**\r\n*   `gemma-3-12b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 12B available in current library**\r\n\r\n## 3. Verified Working Models (Recommended)\r\n\r\nBased on the official config at the GitHub link above, these models are confirmed to have working WASM binaries:\r\n\r\n### High Performance (7B-8B Range)\r\n*   `Qwen2.5-7B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Llama-3.1-8B-Instruct-q4f32_1-MLC` - Verified working\r\n*   `Phi-3.5-mini-instruct-q4f16_1-MLC` - Verified working\r\n*   `gemma-2-9b-it-q4f16_1-MLC` - Verified working\r\n*   `Qwen3-8B-q4f16_1-MLC` - Verified working\r\n\r\n### Lightweight Options (1.5B and below)\r\n*   `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Qwen2-0.5B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Llama-3.2-1B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `SmolLM2-1.7B-Instruct-q4f16_1-MLC` - Verified working\r\n\r\n### Vision Models\r\n*   `Phi-3.5-vision-instruct-q4f16_1-MLC` - Verified working\r\n\r\n### üíª Code Specialists\r\n*   `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` - **Best for Neural Terminal.** Fast & Smart.\r\n*   `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` - Verified working\r\n\r\n---\r\n\r\n## 3. URL Construction Logic\r\n\r\nIf you need to construct a URL manually:\r\n\r\n```javascript\r\nconst libBase = \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/\";\r\nconst version = \"v0_2_80\"; // Check src/config.ts for 'modelVersion'\r\nconst modelSpecificName = \"Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\";\r\n\r\nconst fullUrl = `${libBase}${version}/${modelSpecificName}`;\r\n```\r\n\r\n**Note:** The `modelSpecificName` often differs slightly from the Hugging Face repo name (e.g., `Llama-3_1` vs `Llama-3.1` or `Qwen2` base for `DeepSeek`). Always check `mlc_config.ts` mapping."
- role: system
  type: document
  source: specs\plan.md
  timestamp: 1767407686464
  content: |
    # Anchor Core Roadmap (V2.3)

    **Status:** Text-Only + Watchdog Deployed
    **Focus:** Stability & Passive Text Ingestion.

    ## Phase 1: Foundation (Completed)
    - [x] Pivot to WebLLM/WebGPU stack.
    - [x] Implement CozoDB (WASM) for memory.
    - [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).

    ## Phase 2: Stabilization (Completed)
    - [x] Fix Model Loading (Quota/VRAM config).
    - [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).
    - [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).

    ## Phase 2.5: Root Refactor (Completed)
    - [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).
    - [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.
    - [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.
    - [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).

    ## Phase 3: Expansion & Hardening (Completed)
    - [x] **Resource Hardening**: Implemented "Consciousness Semaphore" in `sovereign.js`.
    - [x] **Documentation Refactor**: Executed "Visual Monolith" strategy.
    - [x] **Memory Hygiene**: Implement "Forgetting Curve" in `root-dreamer.html`.
    - [x] **Active Memory Persistence**: Enable chat to write back to the Graph.
    - [x] **Temporal Awareness**: Ground the model in real-time.
    - [x] **Mobile Optimization**: Polish mobile UX for `model-server-chat.html`.

    ## Phase 4: Text-Only Architecture (Completed)
    - [x] **Vision Removal**: Remove brittle Vision/Ollama dependencies.
    - [x] **Watchdog Implementation**: Create passive text ingestion service.
    - [x] **Debounce & Hash Check**: Prevent duplicate file ingestion.
    - [x] **Auto-Resurrection**: Enhance browser process management.
    - [x] **Streaming CLI**: Improve terminal UX with streaming responses.

    ## Phase 5: Context Expansion & Persistence (Completed)
    - [x] **Code File Support**: Expand to monitor programming language extensions.
    - [x] **Browser Profile Management**: Implement temporary profile cleanup.
    - [x] **Chat Session Persistence**: Auto-save conversations to context directory.
    - [x] **Ingestion Loop Closure**: Ensure chat sessions become ingested context.

    ## Phase 6: Session Recorder & Text-File Source of Truth (Completed)
    - [x] **Daily Session Files**: Create `chat_YYYY-MM-DD.md` files for each day's conversations.
    - [x] **Text-File Source of Truth**: Implement "Database is Cache" philosophy.
    - [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access.
    - [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat.
    - [x] **Timestamped Entries**: Format messages with timestamps for better tracking.

    ## Phase 7: Model Loading Reliability (Completed)
    - [x] **URL Construction Fix**: Implemented `/models/{model}/resolve/main/{file}` redirect for MLC-LLM compatibility.
    - [x] **File Renaming**: Standardized component names (`anchor-mic.html`, `memory-builder.html`, `db_builder.html`).
    - [x] **Server Stability**: Fixed hanging issues with problematic path parameter syntax.
    - [x] **Endpoint Completeness**: Verified all documented endpoints are accessible.

    ## Phase 5.5: Search Enhancement (Completed)
    - [x] **BM25 Implementation**: Replaced regex-based search with CozoDB FTS using BM25 algorithm.
    - [x] **Hybrid Search**: Combined vector search (semantic) with BM25 (lexical) for better results.
    - [x] **Index Creation**: Added FTS index creation in memory initialization routines.
    - [x] **Stemming Support**: Enabled English stemming for improved word variation matching.

    ## Phase 6: GPU Resource Management (Completed)
    - [x] **GPU Queuing System**: Implemented automatic queuing for GPU resource requests to prevent conflicts
    - [x] **Resource Status Management**: Added GPU lock status tracking with owner identification
    - [x] **503 Error Resolution**: Fixed "Service Unavailable" errors by implementing proper resource queuing
    - [x] **Endpoint Integration**: Added `/v1/gpu/lock`, `/v1/gpu/unlock`, `/v1/gpu/status` endpoints
    - [x] **Log Integration**: Added GPU resource management to centralized logging system

    ## Phase 7: Async/Await Best Practices (Completed)
    - [x] **Coroutine Fixes**: Resolved "coroutine was never awaited" warnings in webgpu_bridge.py
    - [x] **Event Loop Integration**: Properly integrated async functions with FastAPI's event loop
    - [x] **Startup Sequence**: Ensured logging system initializes properly with application lifecycle
    - [x] **Resource Management**: Fixed resource cleanup in WebSocket handlers to prevent leaks
    - [x] **Error Handling**: Enhanced async error handling with proper cleanup procedures

    ## Phase 8: Browser-Based Control Center (Completed)
    - [x] **UI Integration**: Implemented browser-based sidecar with retrieval and vision tabs
    - [x] **Vision Engine**: Created Python-powered VLM integration for image analysis
    - [x] **Endpoint Expansion**: Added vision ingestion and enhanced logging endpoints
    - [x] **File Logging**: Implemented persistent file-based logging with truncation
    - [x] **UI Serving**: Extended bridge to serve HTML interfaces for unified workflow

    ## Phase 9: Context Ingestion Pipeline Fixes (Completed)
    - [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)
    - [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of "unknown"
    - [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)
    - [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging
    - [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization
    - [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable

    ## Phase 10: Federation
    - [ ] **Device Sync**: Sync IndexedDB across devices (Peer-to-Peer).
    - [ ] **Local-First Cloud**: Optional encrypted backup.
- role: system
  type: document
  source: specs\protocols\001-autonomous-execution.md
  timestamp: 1767456625741
  content: |-
    # Protocol 001: Autonomous Execution & Service Verification

    ## The Rule
    When executing any service, server, or long-running task, the Agent MUST:

    1.  **Isolate Logs:** Create a dedicated log file (e.g., `logs/startup_checks.log`).
    2.  **Detached Execution:** Launch the process in background/detached mode.
        - *Linux/Mac:* `nohup node src/index.js > ../logs/server.log 2>&1 &`
        - *Windows:* `Start-Process node -ArgumentList "src/index.js" -RedirectStandardOutput "../logs/server.log" -WindowStyle Hidden`
    3.  **The "Pulse Check":**
        - Wait 5 seconds.
        - Read the log file to check for "Error" or "Exception".
    4.  **Verification:**
        - If the log shows "Listening on port X", perform a HTTP GET to `/health` to confirm.
        - ONLY then declare success.
- role: system
  type: document
  source: specs\spec.md
  timestamp: 1767473090238
  content: |-
    # Anchor Core: The Visual Monolith (v3.6)

    **Status:** Text-Only + Watchdog + Session Recorder + Context Persistence + Ghost Engine Resilience Architecture | **Philosophy:** Visual Command Center, Resource-Queued.

    ## 1. The Anchor Architecture
    The **Anchor Core** (`webgpu_bridge.py`) is the unified server. The **Ghost Engine** is a headless browser window acting as the GPU Worker. **Watchdog** provides passive text ingestion.

    ```mermaid
    graph TD
        subgraph Anchor_Core [Localhost:8000]
            Bridge[WebGPU Bridge (Python)]

            subgraph Assets
                UI[chat.html]
                Context[context.html]
                Sidecar[sidecar.html]
                Watchdog[watchdog.py]
                Ghost_UI[ghost.html]
                Dreamer[memory-builder.html]
            end

            subgraph API_Endpoints
                ChatAPI["/v1/chat/completions"]
                SearchAPI["/v1/memory/search"]
                IngestAPI["/v1/memory/ingest"]
                GPUAPI["/v1/gpu/lock, /v1/gpu/unlock, /v1/gpu/status"]
                LogAPI["/logs/recent, /logs/collect"]
            end
        end

        subgraph Passive_Ingestion
            Watchdog_Service[Watchdog Service]
            Context_Folder["context/ folder"]
            File_Monitoring[File System Events]
        end

        subgraph Ghost_Engine [Headless Browser]
            Worker[WebLLM (WASM)]
            Memory[CozoDB (WASM)]
            Search[Hybrid Search]
            GPU[WebGPU Resources]
            Connection_Manager[Connection Manager]
        end

        User -->|HTTP| Sidecar
        User -->|HTTP| Context
        User -->|HTTP| Ghost_UI

        Watchdog_Service --> IngestAPI
        Context_Folder --> Watchdog_Service
        File_Monitoring --> Watchdog_Service

        Sidecar -->|Vision Ingest| VisionAPI
        Sidecar -->|Search| SearchAPI
        VisionAPI -->|VLM Analysis| Vision
        Vision -->|Memory Ingest| Ghost_Engine
        SearchAPI -->|Query| Ghost_Engine
        IngestAPI -->|Memory Ingest| Ghost_Engine
        Ghost_Engine -->|Ground Truth| Sidecar

        Sidecar -->|GPU Lock| GPUAPI
        GPUAPI -->|Queue Manager| Ghost_Engine
        Ghost_Engine -->|GPU| Worker

        Bridge -->|WebSocket| Connection_Manager
        Connection_Manager -->|API| Ghost_Engine
        Ghost_Engine -->|GPU| Worker
        ChatAPI -->|MLC-LLM| Worker
        SearchAPI -->|Memory Query| Memory
        IngestAPI -->|Memory Ingest| Memory
        Dreamer -->|Background Processing| Memory
        Resolver -->|File Redirect| Models[Local Model Files]
        UI -->|Context Retrieval| Search_Engine
        Search_Engine -->|Hybrid Results| UI
        Bridge -->|Log Collection| LogAPI
        LogAPI -->|Central Buffer| LogViewer[log-viewer.html]
    ```

    ## 2. Model Loading Strategy (Standard 007)
    The system now uses an online-first model loading approach for reliability:
    - **Primary**: Direct HuggingFace URLs for immediate availability
    - **Fallback**: Local model files when available
    - **Bridge Redirect**: `/models/{model}/resolve/main/{file}` handles resolution logic
    - **Simplified Configuration**: Online-only approach prevents loading hangs (Standard 007)

    ## 3. Port Map

    * **8000**: **The One Port.** Serves UI, API, Models, and WebSocket connections.

    ## 4. Search Architecture

    * **Hybrid Retrieval**: Combines Vector search (semantic) with BM25 (lexical) for optimal results.
    * **BM25 FTS**: CozoDB Full Text Search with stemming and relevance scoring.
    * **Context Manager**: Intelligent retrieval system in `ContextManager` class.

    ## 5. Ghost Engine Resilience (Standard 026)

    * **Connection Management**: Automatic WebSocket connection monitoring with reconnection attempts
    * **Graceful Degradation**: Proper error handling when Ghost Engine is disconnected (503 responses)
    * **Status Indicators**: Clear logging and UI indicators for connection status
    * **Auto-Resurrection**: Automatic recovery when Ghost Engine becomes available
    * **Queue Processing**: Pending operations are processed when connection resumes

    ## 6. No Resurrection Mode (Standard 027)

    * **Manual Control**: Option to disable automatic Ghost Engine launching via NO_RESURRECTION_MODE flag
    * **Resource Efficiency**: Reduces resource usage by avoiding automatic browser launches
    * **User Flexibility**: Allows users to connect Ghost Engine manually when needed
    * **Existing Browser**: Enables use of existing browser windows instead of launching headless instances
    * **Environment Variable**: Controlled via `set NO_RESURRECTION_MODE=true` before startup

    ## 7. Default No Resurrection Behavior (Standard 028)

    * **Default Setting**: Ghost Engine resurrection is now disabled by default for resource efficiency
    * **Manual Activation Required**: Users must explicitly open ghost.html to connect the Ghost Engine
    * **Environment Override**: Set `NO_RESURRECTION_MODE=false` to enable auto-launching
    * **Queued Processing**: Files and requests are queued until Ghost Engine connects
    * **User Control**: Provides maximum control over when computational resources are used

    ## 8. Consolidated Data Aggregation (Standard 029)

    * **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation
    * **Multi-Format Output**: Generates three formats - text corpus, JSON memory, and YAML memory
    * **YAML Formatting**: Uses proper multiline string formatting for readability
    * **Encoding Handling**: Robust encoding detection using chardet for reliable processing
    * **Recursive Processing**: Processes all subdirectories while respecting exclusions
    * **Metadata Preservation**: Maintains file metadata in structured outputs

    ## 9. Multi-Format Output for Project Aggregation (Standard 030)

    * **Multi-Format Support**: The `read_all.py` script in the root directory generates both JSON and YAML versions of memory records
    * **YAML Formatting**: Uses proper multiline string formatting (literal style with `|`) for readability
    * **Consistent Naming**: Output files follow consistent naming patterns (`combined_text.txt`, `combined_memory.json`, `combined_text.yaml`)
    * **Custom Representers**: Implements custom YAML representers to handle multiline content appropriately
    * **Maximum Compatibility**: Provides format flexibility for different downstream processing tools

    ## 10. Ghost Engine Stability Fix (Standard 031)

    * **Separate Schema Creation**: Basic schema and FTS index creation must be handled separately to prevent failures
    * **Graceful FTS Handling**: FTS creation failures should not prevent basic database functionality
    * **Error Prevention**: Proper error handling prevents "undefined" error messages
    * **Browser Stability**: Prevents browser crashes during database initialization
    * **Fallback Operations**: System continues to function even if advanced features fail

    ## 11. Ghost Engine Initialization Flow (Standard 032)

    * **Sequential Initialization**: Database must be fully initialized before signaling readiness to Bridge
    * **Database Readiness Checks**: All operations verify database is properly initialized before execution
    * **Proper Error Handling**: Return appropriate errors when database is not ready instead of failing silently
    * **Synchronous Connection Flow**: Connect ‚Üí Initialize Database ‚Üí Signal Ready ‚Üí Process Requests
    * **Graceful Degradation**: Report initialization failures and avoid processing requests when database fails
    * **Message Type Support**: Properly handle all message types including error responses

    ## 12. CozoDB Syntax Compliance (Standard 033)

    * **Schema Creation Syntax**: Use proper CozoDB syntax without line breaks in schema definitions
    * **FTS Creation Syntax**: Use correct FTS creation syntax for full-text search indexes
    * **Insert Query Syntax**: Use proper `:insert` or `:replace` syntax with correct parameter binding
    * **Parameter Formatting**: Format parameters correctly as nested arrays for bulk operations
    * **Schema Validation**: Properly propagate schema creation success/failure status
    * **Error Propagation**: Ensure all database operations properly handle and report errors

    ## 13. Node.js Monolith Migration (Standard 034)

    * **Node.js Runtime**: Use Node.js as the primary runtime environment for the Context Engine
    * **CozoDB Integration**: Integrate CozoDB directly using `cozo-node` for persistent storage
    * **Autonomous Execution**: Implement Protocol 001 for detached service execution with proper logging
    * **File Watchdog**: Use `chokidar` for efficient file system monitoring and automatic ingestion
    * **Standardized Endpoints**: Implement standardized API endpoints for ingestion, querying, and health checks
    * **Legacy Archival**: Archive all V2 Python infrastructure to preserve historical code
    * **JavaScript Conversion**: Convert Python utility scripts to JavaScript equivalents for consistency
    * **Platform Compatibility**: Ensure architecture works on Termux/Linux environments

    ## 14. Never Attached Mode (Standard 035)

    * **Detached Execution Only**: All long-running services must be started in detached mode using appropriate backgrounding techniques
    * **No Attached Mode**: Never run services like `npm start`, `python server.py` or similar long-running processes directly in an attached terminal session
    * **Proper Logging**: All detached processes must log to the designated `logs/` directory for monitoring and debugging
    * **Platform-Specific Detaching**: Use appropriate backgrounding techniques for each platform (nohup, start /min, etc.)
    * **Verification Method**: Verify detached services are running by checking logs or connecting to interfaces, not by waiting for terminal output
    * **Documentation Requirement**: All startup procedures must specify detached execution methods
    * **Domain Organization**: Standards are now organized by domain (CORE, ARCH, DATA, OPS, BRIDGE) for easier navigation
- role: system
  type: document
  source: specs\standards\00-CORE\012-context-utility-manifest.md
  timestamp: 1767254282424
  content: "<<<<<<< HEAD\r\n# Standard 012: Context Utility Manifest\r\n\r\n**Authority:** Active | **Philosophy:** Invisible Infrastructure\r\n\r\n## The Principle\r\nThe Anchor Core is not a \"Chat App\". It is a **Context Utility** (like electricity or WiFi).\r\n1.  **Headless First**: The system must provide value without a visible UI window.\r\n2.  **Passive Observation**: Data ingestion should happen automatically (Daemon Eyes) rather than requiring manual user input.\r\n3.  **Universal Availability**: Context must be accessible via standard HTTP endpoints (`/v1/memory/search`) to any client (Terminal, VS Code, Browser).\r\n\r\n## The Rules\r\n1.  **No UI Blocking**: Long-running tasks (like VLM analysis) MUST run in background threads/processes.\r\n2.  **Zero-Touch Ingestion**: Screen/Audio capture must require zero clicks after initial activation.\r\n3.  **Ground Truth**: All ingested context is immutable \"Ground Truth\" until proven otherwise.\r\n=======\r\n# Standard 012: Context Utility Manifest - The Invisible Infrastructure\r\n\r\n## What Happened?\r\nThe Anchor Core system was originally conceived as a chat application, but has evolved into a unified cognitive infrastructure. The system now needs to transition from \"active user input\" to \"passive observation\" to function as truly invisible infrastructure like electricity - always present but never demanding attention.\r\n\r\n## The Cost\r\n- UI bloat with multiple chat interfaces competing for user attention\r\n- Manual data entry required to populate context\r\n- Users having to copy/paste information instead of automatic capture\r\n- Architecture treating UI as primary rather than as debugging tool\r\n- Missing opportunity to create true \"ambient intelligence\"\r\n\r\n## The Rule\r\n1. **Headless by Default**: All core functionality must operate without user interface interaction\r\n   ```python\r\n   # Core services run as background daemons\r\n   daemon_services = [\r\n       \"memory_graph\",      # CozoDB persistence\r\n       \"gpu_engine\",        # WebLLM inference\r\n       \"context_capture\",   # Screen/Audio observation\r\n       \"data_ingestion\"     # Memory writing\r\n   ]\r\n   ```\r\n\r\n2. **Passive Observation**: System captures context automatically rather than waiting for user input\r\n   - **Eyes**: Automated screen sampling and OCR\r\n   - **Ears**: Continuous audio transcription (when enabled)\r\n   - **Memory**: Automatic ingestion without user intervention\r\n\r\n3. **Architecture Priority**: `webgpu_bridge.py` is the nervous system; UIs are merely debugging/interaction tools\r\n   - UIs are temporary visualization layers\r\n   - Core logic exists independently of any UI\r\n   - Background services operate without UI presence\r\n\r\n4. **Invisible Utility**: The system should function like electricity - always available, rarely noticed, essential infrastructure\r\n   - Zero user interaction required for core functions\r\n   - Automatic context capture and storage\r\n   - Seamless integration with user's workflow\r\n\r\n5. **Context First**: Prioritize capturing and understanding user context over responding to queries\r\n   - Short-term context populated automatically\r\n   - Long-term memory built passively\r\n   - Responses based on observed reality rather than explicit input\r\n\r\n## Implementation Requirements\r\n\r\n### Core Daemon Services\r\n- **Memory Daemon**: Continuous CozoDB operations in background\r\n- **Vision Daemon**: Automated screen capture and OCR (daemon_eyes.py)\r\n- **Audio Daemon**: Optional background audio processing\r\n- **Ingestion Daemon**: Automatic data flow to memory graph\r\n\r\n### API-First Design\r\n- All functionality accessible via API endpoints\r\n- UIs as thin clients consuming API services\r\n- Background services operating independently\r\n\r\n### Error Handling\r\n- Daemons must handle errors gracefully without user intervention\r\n- Automatic recovery from common failures\r\n- Silent operation with optional logging for debugging\r\n\r\n## Transition Protocol\r\n\r\nWhen implementing new features:\r\n1. Design for headless operation first\r\n2. Add UI as optional visualization layer\r\n3. Ensure all functionality available via API\r\n4. Test daemon operation independently of UI\r\n\r\nThis standard ensures that Anchor Core evolves into true invisible infrastructure rather than remaining a traditional application.\r\n>>>>>>> 3cd511631b7eaf7d033a1bacccff36325545fc78\r\n"
- role: system
  type: document
  source: specs\standards\00-CORE\028-default-no-resurrection-mode.md
  timestamp: 1767446980048
  content: |-
    # Standard 028: Configuration-Driven System with Default No Resurrection Mode

    ## What Happened?
    The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues with resource usage and prevented users from controlling when the Ghost Engine connects. The system now defaults to "No Resurrection Mode" where the Ghost Engine must be manually started by opening ghost.html in the browser. Additionally, ALL system variables are now abstracted to a central configuration file (config.json) to support future settings menu implementation.

    ## The Cost
    - Excessive resource usage from automatically launching headless browser
    - Browser processes that couldn't be controlled by the user
    - Confusion when multiple browser instances were running
    - Unnecessary complexity in the startup process
    - Users wanting more control over when the Ghost Engine connects
    - Hard-coded values throughout the codebase that made customization difficult

    ## The Rule
    1. **Default Behavior**: The system shall default to `NO_RESURRECTION_MODE=true`, meaning the Ghost Engine is not automatically launched.

    2. **Manual Connection**: Users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

    3. **Environment Override**: Users can set `NO_RESURRECTION_MODE=false` to return to auto-launching behavior.

    4. **Queued Operations**: When Ghost Engine is disconnected, operations shall be queued and processed when connection is established.

    5. **Clear Messaging**: The system shall provide clear instructions when Ghost Engine is not connected, indicating how to establish the connection.

    6. **Configurable Values**: All system parameters shall be configurable via the config.json file, including:
       - Server settings (port, host, CORS origins)
       - Ghost Engine settings (auto resurrection, browser paths, flags)
       - Logging configuration (max lines, directory, format)
       - Memory settings (max ingest size, default limits, char limits)
       - GPU management (enabled, concurrent ops, timeout)
       - Model loading (timeout, default model, base URL)
       - Watchdog settings (enabled, watch directory, allowed extensions, debounce time)

    7. **Detached Operation**: All scripts shall run in detached mode with logging to the logs/ directory as per Standard 025.

    ## Implementation
    - Default configuration sets `"ghost_engine.auto_resurrection_enabled": false`
    - The start-anchor.bat script defaults to NO_RESURRECTION_MODE=true
    - All system variables abstracted to config.json with config_manager.py
    - Watchdog logs appropriate messages when Ghost Engine is disconnected
    - API endpoints return 503 with clear messaging when Ghost Engine is disconnected
    - Files are queued for ingestion when Ghost Engine is not available
    - Created start_anchor_detached.py for proper detached operation with logging
- role: system
  type: document
  source: specs\standards\002-cache-api-security-policy.md
  timestamp: 1767076621302
  content: "# Standard 002: Cache API Security Policy\r\n\r\n## What Happened?\r\nBrowser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as \"data in RAM\" rather than \"persistent storage\", causing initialization failures.\r\n\r\n## The Cost\r\n- Multiple failed model loading attempts\r\n- Browser security errors preventing WebGPU initialization\r\n- \"Stealth Mode\" required to bypass strict policies\r\n- Significant debugging time to understand browser security model\r\n\r\n## The Rule\r\n1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:\r\n   ```python\r\n   class NoCacheStaticFiles(StaticFiles):\r\n       async def __call__(self, scope, receive, send):\r\n           async def send_wrapper(message):\r\n               if message['type'] == 'http.response.start':\r\n                   headers = message.get('headers', [])\r\n                   headers.extend([\r\n                       (b\"Cache-Control\", b\"no-store, no-cache, must-revalidate, proxy-revalidate\"),\r\n                       (b\"Pragma\", b\"no-cache\"),\r\n                       (b\"Expires\", b\"0\"),\r\n                   ])\r\n                   message['headers'] = headers\r\n               await send(message)\r\n           await super().__call__(scope, receive, send_wrapper)\r\n   ```\r\n\r\n2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.\r\n\r\n3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components."
- role: system
  type: document
  source: specs\standards\005-model-loading-configuration.md
  timestamp: 1767184554764
  content: |-
    # Standard 005: Model Loading Configuration & Endpoint Verification

    ## What Happened?
    Model loading failed due to various configuration issues including "Cannot find model record" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.

    ## The Cost
    - Failed model initialization preventing AI functionality
    - Multiple 404 errors for specific model types
    - 503 and 405 errors during embedding and model download requests
    - Hours spent debugging model configuration issues
    - Unreliable model loading across different model types
    - Significant time wasted discovering that documented endpoints didn't exist in the backend
    - Frontend-backend integration failures due to missing API endpoints

    ## The Rule
    1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:
       ```python
       # Example mapping for problematic models
       MODEL_MAPPINGS = {
           'OpenHermes': 'Mistral-v0.3',
           'NeuralHermes': 'Mistral-v0.3',
           # Add other mappings as needed
       }
       ```

    2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:
       ```python
       # In webgpu_bridge.py - ensure flexible model name handling
       # Don't validate model names strictly on the bridge side
       ```

    3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:
       ```javascript
       // In frontend code
       const internalModelId = getModelInternalId(userModelName);
       const modelUrl = getModelUrl(internalModelId);
       ```

    4. **Verification Registry:** Maintain `specs/mlc-urls.md` as a registry for verified WASM binaries to ensure compatibility.

    5. **Bridge-Based URLs:** Use bridge-based model URLs (`http://localhost:8000/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.

    6. **Endpoint Verification Protocol:** Always verify that documented endpoints exist in the backend implementation before deploying frontend code that depends on them:
       ```python
       # Example: Required endpoints for model management
       REQUIRED_ENDPOINTS = [
           "/v1/models/pull",
           "/v1/models/pull/status",
           "/v1/gpu/lock",
           "/v1/gpu/unlock",
           "/v1/gpu/status",
           "/v1/gpu/reset",
           "/v1/gpu/force-release-all",
           "/v1/system/spawn_shell",
           "/v1/shell/exec"
       ]
       ```

    7. **Documentation-Implementation Synchronization:** When documenting an endpoint, immediately implement it in the backend to prevent documentation-code drift.

    8. **Server Startup Verification:** After adding new endpoints, always verify that the server starts properly and doesn't hang due to problematic async operations or path parameter syntax:
       - Test import functionality: `python -c "import webgpu_bridge; print('Import successful')"`
       - Verify server startup and response to requests
       - Avoid problematic syntax like `:path` in route definitions that can cause server hangs
       - Use simple synchronous operations when possible to avoid blocking the event loop
- role: system
  type: document
  source: specs\standards\006-model-url-construction-fix.md
  timestamp: 1767188929845
  content: |-
    # Standard 006: Model URL Construction for MLC-LLM Compatibility

    ## What Happened?
    The Anchor Console (`chat.html`) failed to load models with the error "TypeError: Failed to construct 'URL': Invalid URL", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.

    ## The Cost
    - 4+ hours debugging model loading failures
    - Confusion between working and failing components
    - Inconsistent model loading across different UI components
    - User frustration with non-functional chat interface
    - Multiple failed attempts with different URL construction approaches

    ## The Rule
    1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path}` endpoint to redirect MLC-LLM requests to local model files:
       ```python
       @app.get("/models/{model_name}/resolve/main/{file_path:path}")
       async def model_resolve_redirect(model_name: str, file_path: str):
           import os
           from fastapi.responses import FileResponse, JSONResponse

           models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models")
           actual_path = os.path.join(models_base, model_name, file_path)

           if os.path.exists(actual_path) and os.path.isfile(actual_path):
               return FileResponse(actual_path)
           else:
               return JSONResponse(status_code=404, content={
                   "error": f"File {file_path} not found for model {model_name}"
               })
       ```

    2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.

    3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.

    4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.

    5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly.
- role: system
  type: document
  source: specs\standards\007-model-loading-transition-standard.md
  timestamp: 1767195216530
  content: |-
    # Standard 007: Model Loading Transition - Online-Only Implementation

    ## What Happened?
    The Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.

    The old implementation in `chat.html` was trying to:
    1. Check for local model files using the `/models/{model}/resolve/main/` pattern
    2. Download models through the bridge if not found locally
    3. Use a complex configuration with multiple model entries and local file resolution

    This approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.

    ## The Cost
    - Hours spent debugging model loading failures in `chat.html`
    - Confusion between working and failing components (anchor-mic.html vs chat.html)
    - Inconsistent model loading across different UI components
    - User frustration with non-functional chat interface
    - Time wasted on attempting to fix complex local model resolution logic
    - Delayed development due to complex debugging of the local file + bridge download approach

    ## The Rule
    1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:
       ```javascript
       // Use direct HuggingFace URLs like anchor-mic.html
       const appConfig = {
           model_list: [{
               model: "https://huggingface.co/" + selectedModelId + "/resolve/main/",
               model_id: selectedModelId,
               model_lib: modelLib,  // WASM library URL
               // ... other config
           }],
           useIndexedDBCache: false, // Disable caching to prevent issues
       };
       ```

    2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.

    3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:
       ```javascript
       // Archive the old function with a descriptive name
       async function loadModel_archived() {
           // Original complex implementation
       }
       
       // Implement the working online-only approach
       async function loadModel() {
           // Simplified online-only implementation
       }
       ```

    4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.

    5. **Progressive Enhancement**: Start with a working online-only solution, then add local model loading capabilities in a separate iteration after the basic functionality is stable.

    6. **Model Loading Verification**: Always test model loading with the same models across different UI components to ensure consistency.

    ## Implementation Pattern

    ### Working Online-Only Format (Recommended):
    ```javascript
    // Based on the working anchor-mic.html implementation
    async function loadModel() {
        // ... setup code ...
        
        const appConfig = {
            model_list: [{
                model: "https://huggingface.co/" + selectedModelId + "/resolve/main/",
                model_id: selectedModelId,
                model_lib: modelLib,  // WASM library URL from mapper
                vram_required_MB: 2000,
                low_resource_required: true,
                buffer_size_required_bytes: gpuConfig.maxBufferSize,
                overrides: {
                    context_window_size: gpuConfig.isConstrained ? 2048 : 4096
                }
            }],
            useIndexedDBCache: false, // Disable caching to prevent issues
        };

        engine = await CreateWebWorkerMLCEngine(
            new Worker('./modules/llm-worker.js', { type: 'module' }),
            selectedModelId,
            {
                initProgressCallback: (report) => {
                    // Progress reporting
                },
                appConfig: appConfig,
                logLevel: "INFO",
                useIndexedDBCache: false, // Force disable cache
            }
        );
    }
    ```

    ### Complex Local Resolution (Problematic - Avoid):
    ```javascript
    // DO NOT USE - This causes hangs after GPU configuration
    // Complex local file checking and bridge download logic
    const localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;
    const check = await fetch(localModelUrl, { method: 'HEAD' });
    // ... complex download and resolution logic that causes hangs
    ```

    ## Transition Protocol

    When transitioning model loading implementations:

    1. **Identify Working Component**: Find a UI component that successfully loads models (e.g., `anchor-mic.html`)
    2. **Analyze Working Pattern**: Study the model loading approach in the working component
    3. **Archive Complex Logic**: Preserve the old implementation for future reference
    4. **Implement Simple Approach**: Adopt the working pattern from the successful component
    5. **Test Thoroughly**: Verify the new implementation works with multiple models
    6. **Document Changes**: Record the transition in standards documentation

    ## Future Considerations

    The archived local model loading approach should be revisited after further prototyping and debugging. The online-only approach provides immediate functionality while the more complex local approach can be refined separately without blocking development progress.
- role: system
  type: document
  source: specs\standards\008-model-loading-online-only-approach.md
  timestamp: 1767215198490
  content: "# Standard 008: Model Loading - Online-Only Approach for Browser Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.\r\n\r\nThe issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.\r\n\r\n## The Cost\r\n- All models showing as unavailable in API tests\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with limited model availability\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: window.location.origin + \"/models/\" + selectedModelId, // This will redirect to online source\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Bridge Redirect Endpoint**: Ensure the `/models/{model}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist."
- role: system
  type: document
  source: specs\standards\009-model-loading-configuration-bridge-vs-direct.md
  timestamp: 1767215198491
  content: "# Standard 009: Model Loading Configuration - Bridge vs Direct Online\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:\r\n\r\n1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources\r\n2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser\r\n\r\nThe inconsistency occurred because:\r\n- Some components (like `anchor-mic.html`) work with direct online URLs\r\n- Other components (like `chat.html`) were configured for local file resolution\r\n- The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist\r\n\r\n## The Cost\r\n- Confusion about which model loading approach to use\r\n- Inconsistent behavior across different UI components\r\n- Models working in some components but not others\r\n- Debugging time spent on understanding different loading mechanisms\r\n- Users experiencing different model availability depending on which UI they use\r\n\r\n## The Rule\r\n1. **Consistent Model Configuration**: All UI components should use the same model loading approach:\r\n   ```javascript\r\n   // Recommended configuration pattern\r\n   const modelConfig = {\r\n       model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect\r\n       model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID\r\n       model_lib: modelLib,                                  // WASM library URL\r\n   };\r\n   ```\r\n\r\n2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:\r\n   - First check for local files in the models directory\r\n   - If local file doesn't exist, redirect to the corresponding HuggingFace URL:\r\n     `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`\r\n\r\n3. **Fallback Handling**: Implement proper fallback when local files are not available:\r\n   ```javascript\r\n   // In the UI, handle both local and online availability\r\n   async function loadModel(modelId) {\r\n       try {\r\n           // Try bridge-based loading first\r\n           await initializeEngine(bridgeConfig(modelId));\r\n       } catch (error) {\r\n           // Fallback to direct online loading if bridge fails\r\n           await initializeEngine(onlineConfig(modelId));\r\n       }\r\n   }\r\n   ```\r\n\r\n4. **Testing Protocol**: Test model loading through both pathways:\r\n   - Verify local file resolution works when files exist\r\n   - Verify online fallback works when local files don't exist\r\n   - Test both the redirect endpoint and direct access patterns\r\n\r\n5. **Documentation Consistency**: All UI components should follow the same documented approach to avoid confusion."
- role: system
  type: document
  source: specs\standards\011-comprehensive-testing-verification.md
  timestamp: 1767215198491
  content: "# Standard 011: Comprehensive Testing and Verification Protocol\r\n\r\n## What Happened?\r\nThe Anchor Core system required a comprehensive testing approach to prevent issues like missing endpoints, function syntax errors, model loading failures, and data pipeline problems. Previously, these issues were discovered reactively during development or deployment, causing delays and debugging overhead.\r\n\r\n## The Cost\r\n- Hours spent debugging missing endpoints after deployment\r\n- Time wasted on syntax errors in critical files\r\n- Model loading failures discovered during user testing\r\n- Data pipeline issues found late in the development cycle\r\n- Lack of systematic verification leading to inconsistent quality\r\n\r\n## The Rule\r\n1. **Dedicated Test Directory**: All test files must be organized in a dedicated `tests/` directory in the project root\r\n    ```bash\r\n    tests/\r\n    ‚îú‚îÄ‚îÄ comprehensive_test_suite.py\r\n    ‚îú‚îÄ‚îÄ endpoint_syntax_verification.py\r\n    ‚îú‚îÄ‚îÄ test_model_loading.py\r\n    ‚îú‚îÄ‚îÄ test_model_availability.py\r\n    ‚îú‚îÄ‚îÄ test_gpu_fixes.py\r\n    ‚îú‚îÄ‚îÄ test_orchestrator.py\r\n    ‚îú‚îÄ‚îÄ model_test.html\r\n    ‚îî‚îÄ‚îÄ README.md\r\n    ```\r\n\r\n2. **Comprehensive Test Coverage**: Tests must cover:\r\n   - Model loading functionality\r\n   - Data pipeline verification\r\n   - Endpoint accessibility\r\n   - Missing endpoint detection\r\n   - Function syntax error detection\r\n   - System health verification\r\n\r\n3. **Endpoint Verification Protocol**: All critical endpoints must be tested for accessibility:\r\n   ```python\r\n   # Example endpoint test pattern\r\n   critical_endpoints = [\r\n       (\"/health\", \"GET\", 200),\r\n       (\"/v1/chat/completions\", \"POST\", 400),  # Expected 400 due to missing body\r\n       (\"/v1/gpu/status\", \"GET\", 200),\r\n       # ... add all critical endpoints\r\n   ]\r\n   ```\r\n\r\n4. **Syntax Verification**: Critical Python files must be checked for syntax errors:\r\n   ```python\r\n   # Use AST parsing to verify syntax\r\n   import ast\r\n   with open(file_path, 'r') as f:\r\n       source_code = f.read()\r\n   ast.parse(source_code)  # Will raise SyntaxError if invalid\r\n   ```\r\n\r\n5. **Test Documentation**: All test files must be documented in `tests/README.md` with:\r\n   - Purpose of each test file\r\n   - How to run the tests\r\n   - Test coverage details\r\n   - Expected outputs\r\n\r\n6. **Pre-Deployment Verification**: Before any deployment, run the comprehensive test suite:\r\n   ```bash\r\n   python tests/comprehensive_test_suite.py\r\n   ```\r\n\r\n7. **Continuous Verification**: Implement automated testing in CI/CD pipelines to catch issues early\r\n\r\n## Implementation Example\r\n\r\n### Running the Comprehensive Test Suite:\r\n```bash\r\n# Basic test run\r\npython tests/comprehensive_test_suite.py\r\n\r\n# With custom parameters\r\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output report.json\r\n\r\n# Endpoint and syntax verification only\r\npython tests/endpoint_syntax_verification.py\r\n```\r\n\r\n### Expected Test Coverage:\r\n- Model loading: 100% coverage of model files and configurations\r\n- API endpoints: 100% verification of all documented endpoints\r\n- Syntax: 100% verification of critical Python files\r\n- Data pipeline: End-to-end verification of data flow\r\n- System health: Verification of all core services\r\n\r\n## Verification Checklist\r\n- [ ] All test files organized in `tests/` directory\r\n- [ ] Comprehensive test suite covers all major components\r\n- [ ] Endpoint verification tests all critical endpoints\r\n- [ ] Syntax verification tests all critical Python files\r\n- [ ] Tests are documented in `tests/README.md`\r\n- [ ] Test suite runs without errors\r\n- [ ] Test reports are generated and reviewed"
- role: system
  type: document
  source: specs\standards\013-universal-log-collection.md
  timestamp: 1767254282425
  content: "# Standard 013: Universal Log Collection System\r\n\r\n## What Happened?\r\nThe system had fragmented logging across multiple sources (browser console, Python stdout, WebSocket events) making debugging difficult. Users had to check multiple places to understand system behavior.\r\n\r\n## The Cost\r\n- 4+ hours spent debugging connection issues by checking browser console, Python terminal, and WebSocket messages separately\r\n- Inefficient troubleshooting workflow requiring multiple monitoring tools\r\n- Missed error correlations between different system components\r\n- Poor visibility into system-wide operation\r\n\r\n## The Rule\r\n1. **Universal Collection**: All system logs (Python, JavaScript, WebSocket, browser, model loading, GPU status) must be aggregated in a single location: `tools/log-viewer.html`\r\n2. **Broadcast Channel Protocol**: All components must use the `sovereign-logs` or `coda_logs` BroadcastChannel to send messages to the log viewer:\r\n   ```javascript\r\n   // From browser components\r\n   const logChannel = new BroadcastChannel('sovereign-logs');\r\n   logChannel.postMessage({\r\n       source: 'component-name',\r\n       type: 'info|success|error|warning|debug',\r\n       time: new Date().toISOString(),\r\n       msg: 'message content'\r\n   });\r\n   ```\r\n\r\n3. **Python Integration**: Python scripts must send log data via API endpoints that feed into the log viewer\r\n4. **Centralized Access**: The single point of truth for all system diagnostics is `http://localhost:8000/log-viewer.html`\r\n5. **File-based Logging**: Each component must also write to its own log file in the `logs/` directory for persistent storage\r\n6. **Log Truncation**: Individual log files must be truncated to last 1000 lines to prevent disk space issues\r\n7. **GPU Resource Queuing**: All GPU operations must use the queuing system (`/v1/gpu/lock`, `/v1/gpu/unlock`) to prevent resource conflicts\r\n8. **Source Tagging**: All log entries must be clearly tagged with their source for easy identification"
- role: system
  type: document
  source: specs\standards\014-async-best-practices.md
  timestamp: 1767254282425
  content: "# Standard 014: Async/Await Best Practices for FastAPI\r\n\r\n## What Happened?\r\nThe system had multiple \"coroutine was never awaited\" warnings due to improper async/await usage in the webgpu_bridge.py. These warnings occurred when async functions were called without being properly awaited or when they weren't integrated correctly with FastAPI's event loop system.\r\n\r\n## The Cost\r\n- Runtime warnings cluttering the console output\r\n- Potential resource leaks from improperly handled async operations\r\n- Unpredictable behavior in WebSocket connections and API endpoints\r\n- Difficulty debugging real issues due to noise from async warnings\r\n\r\n## The Rule\r\n1. **Proper Await Usage**: All async functions must be awaited when called within async contexts\r\n   ```python\r\n   # Correct\r\n   await add_log_entry(\"source\", \"type\", \"message\")\r\n   \r\n   # Incorrect\r\n   add_log_entry(\"source\", \"type\", \"message\")  # Creates unawaited coroutine\r\n   ```\r\n\r\n2. **Event Loop Integration**: When scheduling tasks at module level, ensure they run within an active event loop:\r\n   ```python\r\n   # Correct - in startup event\r\n   async def startup_event():\r\n       await add_log_entry(\"System\", \"info\", \"Service started\")\r\n   \r\n   # Incorrect - at module level before event loop starts\r\n   # asyncio.create_task(add_log_entry(...))  # Will cause warning\r\n   ```\r\n\r\n3. **FastAPI Event Handlers**: Use FastAPI's event system (`@app.on_event(\"startup\")`) for initialization tasks that require async operations\r\n\r\n4. **Background Tasks**: For fire-and-forget async operations, use FastAPI's BackgroundTasks or properly scheduled asyncio tasks within request handlers\r\n\r\n5. **WebSocket Cleanup**: Always ensure proper cleanup of async resources in WebSocket exception handlers to prevent resource leaks\r\n\r\n6. **Exception Handling**: Wrap async operations in try/catch blocks that properly handle async exceptions and clean up resources"
- role: system
  type: document
  source: specs\standards\015-browser-control-center.md
  timestamp: 1767254282425
  content: "# Standard 015: Browser-Based Control Center Architecture\r\n\r\n## What Happened?\r\nThe system had fragmented interfaces requiring users to interact with both browser UI and terminal commands. Context retrieval required switching between interfaces, and vision processing required separate Python scripts. This created inefficient workflows and poor user experience.\r\n\r\n## The Cost\r\n- 6+ hours spent switching between browser and terminal for different operations\r\n- Fragmented workflow requiring multiple interfaces for simple tasks\r\n- Poor visibility into system state across different components\r\n- Inefficient context retrieval and vision processing workflows\r\n- Users had to remember multiple endpoints and interfaces\r\n\r\n## The Rule\r\n1. **Unified Browser Interface**: All primary operations (context retrieval, vision processing, memory search) must be accessible through browser-based UI at `http://localhost:8000/sidecar`\r\n2. **Dual-Tab Architecture**: Interface must have separate tabs for \"Retrieve\" (context search) and \"Observe\" (vision processing) to prevent workflow interference\r\n3. **File-Based Logging**: All system components must write to individual log files in the `logs/` directory with automatic truncation to 1000 lines\r\n4. **Centralized Log Access**: All logs must be accessible via `/logs/recent` endpoint and consolidated in `log-viewer.html`\r\n5. **Python VLM Integration**: Vision processing must be handled by dedicated Python module (`vision_engine.py`) with Ollama backend support\r\n6. **Endpoint Consistency**: All UI components must use consistent endpoint patterns (`/v1/namespace/action`)\r\n7. **Error Handling**: All operations must provide clear, actionable error messages to the user interface\r\n8. **State Visibility**: System state (engine status, GPU availability, memory status) must be visible in the UI"
- role: system
  type: document
  source: specs\standards\018-streaming-cli-client-responsive-ux.md
  timestamp: 1767278652478
  content: |-
    # Standard 018: Streaming CLI Client for Responsive UX

    ## What Happened?
    The anchor.py CLI client was using `stream=False` for API requests, causing the terminal to hang during long responses from 7B models. This created a poor user experience where the terminal would freeze for 10-20 seconds during long answers, breaking the "fluid" feeling of a memory assistant.

    ## The Cost
    - Poor user experience with hanging terminal during long responses
    - 1+ hours spent updating anchor.py to use streaming for better UX
    - Users experiencing "frozen" terminal during model responses
    - Break in the conversational flow of the memory assistant

    ## The Rule
    1. **Enable Streaming**: Always use `stream=True` when making chat completion requests from CLI clients:
       ```python
       response = requests.post(
           f"{BRIDGE_URL}/v1/chat/completions",
           json={
               "messages": history,
               "stream": True  # Enable streaming for better UX
           },
           stream=True,  # Enable streaming at request level
           timeout=120
       )
       ```

    2. **Process Streaming Response**: Handle the streaming response line by line to provide immediate feedback:
       ```python
       for line in response.iter_lines(decode_unicode=True):
           if line and line.startswith("data: "):
               data_str = line[6:]  # Remove "data: " prefix
               if data_str.strip() == "[DONE]":
                   break
               # Process and display content as it arrives
       ```

    3. **Real-time Character Display**: Print characters as they arrive to provide immediate feedback:
       ```python
       print(content, end="", flush=True)
       ```

    4. **Maintain Conversation Context**: Properly accumulate streamed content to maintain conversation history:
       ```python
       ai_text += content  # Accumulate content for history
       history.append({"role": "assistant", "content": ai_text})
       ```

    This standard ensures CLI clients provide responsive feedback during long model responses while maintaining proper conversation context.
- role: system
  type: document
  source: specs\standards\019-code-file-ingestion-comprehensive-context.md
  timestamp: 1767281142803
  content: |-
    # Standard 019: Code File Ingestion for Comprehensive Context

    ## What Happened?
    The Watchdog service was only monitoring text files (.txt, .md, .markdown) but ignoring code files which represent a significant portion of developer context. This created an "Ingestion Blind Spot" where the system was blind to codebase context.

    ## The Cost
    - Limited context ingestion for developers
    - Missing important code-related information
    - 30 minutes spent updating watchdog.py to include code extensions

    ## The Rule
    1. **Expand File Extensions**: Include common programming language extensions in file monitoring:
       ```python
       enabled_extensions = {".txt", ".md", ".markdown", ".py", ".js", ".html", ".css", 
                             ".json", ".yaml", ".yml", ".sh", ".bat", ".ts", ".tsx", 
                             ".jsx", ".xml", ".sql", ".rs", ".go", ".cpp", ".c", ".h", ".hpp"}
       ```

    2. **Comprehensive Coverage**: Monitor all relevant text-based file types that contain context

    3. **Maintain Performance**: Ensure file size limits still apply to prevent performance issues with large code files

    This standard ensures that developer context is fully captured by including code files in passive ingestion.
- role: system
  type: document
  source: specs\standards\020-browser-profile-management-cleanup.md
  timestamp: 1767281189383
  content: |-
    # Standard 020: Browser Profile Management and Cleanup

    ## What Happened?
    Repeatedly launching and killing modern browsers (Edge/Chrome) could leave orphaned child processes or fill up the temporary directory with user data profiles. This created a "Memory Leak" risk where temporary browser profiles could accumulate over time and crash the host OS.

    ## The Cost
    - Potential disk space issues from accumulated temporary browser profiles
    - Risk of system instability from orphaned processes
    - 1 hour spent implementing proper browser profile management and cleanup

    ## The Rule
    1. **Unique Profile Directories**: Use unique temporary directories for each browser instance:
       ```python
       import tempfile
       temp_dir = tempfile.gettempdir()
       f"--user-data-dir={temp_dir}/anchor_ghost_{int(time.time())}"
       ```

    2. **Performance Optimization Flags**: Include performance optimization flags to reduce resource usage:
       ```python
       # Add these flags to browser launch command
       "--disable-background-timer-throttling",
       "--disable-backgrounding-occluded-windows", 
       "--disable-renderer-backgrounding",
       "--disable-ipc-flooding-protection",
       "--disable-background-media-suspend"
       ```

    3. **Cleanup Old Profiles**: Implement automatic cleanup of old temporary profiles:
       ```python
       async def _cleanup_old_profiles(self):
           # Remove directories older than 1 day
           cutoff_time = datetime.now() - timedelta(days=1)
           # Implementation to remove old temporary directories
       ```

    4. **Proper Process Termination**: Ensure browser processes are fully terminated before launching new ones

    This standard prevents disk space issues and system instability from temporary browser profiles.
- role: system
  type: document
  source: specs\standards\021-chat-session-persistence-context-continuity.md
  timestamp: 1767281203054
  content: |-
    # Standard 021: Chat Session Persistence for Context Continuity

    ## What Happened?
    The anchor.py CLI client maintained conversation history only in memory. If the terminal was closed or the CLI crashed, the entire conversation history was lost. This created a "Lost Context" risk where valuable conversation history was not preserved.

    ## The Cost
    - Loss of conversation history on CLI crashes or termination
    - Broken loop between active chatting and long-term memory
    - 45 minutes spent implementing chat session persistence to context folder

    ## The Rule
    1. **Auto-Save Sessions**: Automatically save each chat message to a session file:
       ```python
       def save_message_to_session(role, content):
           # Create timestamped session file in context/sessions/
           # Append each message as it occurs
       ```

    2. **Session Directory**: Create a dedicated `context/sessions/` directory for chat logs:
       ```python
       SESSIONS_DIR = os.path.join(CONTEXT_DIR, "sessions")
       os.makedirs(SESSIONS_DIR, exist_ok=True)
       ```

    3. **Markdown Format**: Save conversations in markdown format for easy reading and processing:
       ```python
       # Format: ## Role\nContent\n\n for each message
       ```

    4. **Loop Closure**: Ensure that chat sessions become text files that the Watchdog service can monitor and ingest into long-term memory automatically.

    This standard ensures conversation history survives CLI crashes and becomes part of the persistent context.
- role: system
  type: document
  source: specs\standards\023-anchor-lite-simplification.md
  timestamp: 1767329512223
  content: "# Standard 023: Anchor Lite Architecture\r\n\r\n## 1. What Happened\r\nThe system became overly complex with multiple database views (DB Builder, Memory Builder) and experimental chat interfaces, causing data synchronization issues and user confusion.\r\n\r\n## 2. The Cost\r\n- Loss of trust in retrieval (\"Jade\" not found).\r\n- High maintenance overhead.\r\n\r\n## 3. The Rule\r\n**Single Pipeline Architecture:**\r\n```mermaid\r\ngraph LR\r\n    BS[File System] -->|Watchdog| DB[Ghost Engine]\r\n    DB -->|WebSocket| UI[Context Console]\r\n    note[Single Source of Truth] -.-> BS\r\n```\r\n1. **Source:** File System (`context/` folder) is the Single Source of Truth.\r\n2. **Ingest:** `watchdog.py` monitors files and pushes to the Engine.\r\n3. **Index:** `ghost.html` (Headless CozoDB) maintains the index.\r\n4. **Retrieve:** `context.html` is the sole interface for search.\r\n"
- role: system
  type: document
  source: specs\standards\024-context-ingestion-pipeline-fix.md
  timestamp: 1767407613598
  content: |-
    # Standard 024: Context Ingestion Pipeline - Field Name Alignment Protocol

    ## What Happened?
    The context ingestion pipeline was failing silently due to field name mismatches between the watchdog service and the ghost engine. The watchdog was sending `filetype` but the memory ingestion endpoint expected `file_type`, and the ghost engine was looking for `msg.filetype` instead of `msg.file_type`. This caused the database to appear empty even though files were being processed, resulting in failed context searches for terms like "Dory" or "Coda".

    ## The Cost
    - 2+ hours spent debugging why context files weren't appearing in the database
    - Confusion from "Database appears empty!" messages in ghost engine logs
    - Failed context searches returning no results despite files existing in context directory
    - Misleading "Ingested" messages in watchdog logs that masked the actual field name mismatch
    - Users experiencing broken context retrieval functionality

    ## The Rule
    1. **Field Name Consistency**: All components in the ingestion pipeline must use consistent field names:
       - Watchdog sends: `file_type`, `source`, `content`, `filename`
       - Bridge expects: `file_type`, `source`, `content`, `filename`
       - Ghost engine receives: `file_type`, `source`, `content`, `filename`

    2. **Payload Validation**: Always validate that field names match across the entire pipeline:
       ```javascript
       // In ghost.html handleIngest function
       await runQuery(query, { 
           data: [[id, ts, msg.content, msg.source || msg.filename, msg.file_type || "text"]] 
       });
       ```

    3. **Source Identification**: The watchdog must send a proper source identifier instead of relying on default "unknown" values

    4. **Error Reporting**: Include detailed error messages when ingestion fails to help with debugging
- role: system
  type: document
  source: specs\standards\027-no-resurrection-mode.md
  timestamp: 1767421753814
  content: |-
    # Standard 027: No Resurrection Mode for Manual Ghost Engine Control

    ## What Happened?
    The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues for users who wanted to use an existing browser window or manually control when the Ghost Engine connects. Users needed an option to disable the automatic resurrection protocol and connect the Ghost Engine manually when needed.

    ## The Cost
    - Unnecessary browser processes launched automatically
    - Resource usage when Ghost Engine not needed
    - Inability to use existing browser windows for Ghost Engine operations
    - Confusion when multiple browser instances were running
    - Users wanting more control over when the Ghost Engine connects

    ## The Rule
    1. **Environment Variable Control**: The system must support a `NO_RESURRECTION_MODE=true` environment variable to disable automatic Ghost Engine launching.

    2. **Conditional Launch**: When `NO_RESURECTION_MODE=true`, the system shall NOT automatically launch the Ghost Engine during startup.

    3. **Manual Connection**: In no resurrection mode, users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

    4. **Clear Messaging**: The system shall provide clear instructions to users when no resurrection mode is enabled, indicating they need to open ghost.html manually.

    5. **Resource Management**: When no resurrection mode is enabled, the system shall not attempt to kill browser processes during shutdown.

    6. **API Behavior**: API endpoints that require the Ghost Engine shall return appropriate 503 errors with clear messaging when the Ghost Engine is disconnected, regardless of resurrection mode setting.

    ## Implementation
    - Set environment variable: `set NO_RESURRECTION_MODE=true` before running `start-anchor.bat`
    - The Bridge will log a message indicating manual connection is required
    - Users open `http://localhost:8000/ghost.html` in their browser to connect the Ghost Engine
    - All functionality remains the same, just with manual control over Ghost Engine connection
- role: system
  type: document
  source: specs\standards\030-multi-format-output.md
  timestamp: 1767450103565
  content: |-
    # Standard 030: Multi-Format Output for Project Aggregation

    ## What Happened?
    The `read_all.py` script in the root directory was only generating text and JSON outputs for project aggregation. To improve compatibility with various processing tools and follow the documentation policy of supporting YAML format, the script was updated to also generate a YAML version of the memory records.

    ## The Cost
    - Limited output format options for downstream processing
    - Inconsistency with the documentation policy that prefers YAML for configuration and data exchange
    - Missing opportunity to provide easily readable structured data in YAML format
    - Users had to convert JSON to YAML if they needed that format

    ## The Rule
    1. **Multi-Format Output**: The `read_all.py` script must generate both JSON and YAML versions of memory records.

    2. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.

    3. **Consistent Naming**: Output files should follow consistent naming patterns:
       - `combined_text.txt` - Aggregated text content
       - `combined_memory.json` - Structured JSON memory records
       - `combined_text.yaml` - Structured YAML memory records

    4. **Custom Representers**: Use custom YAML representers to handle multiline strings appropriately with the `|` indicator.

    5. **Encoding Handling**: Ensure proper UTF-8 encoding for both input and output operations.

    ## Implementation
    - Updated `read_all.py` to import and use the `yaml` module
    - Added custom string representer for multiline content
    - Created separate YAML output file with proper formatting
    - Maintained all existing functionality while adding YAML support
    - Used `yaml.dump()` with appropriate parameters for clean output
- role: system
  type: document
  source: specs\standards\031-ghost-engine-stability-fix.md
  timestamp: 1767451855084
  content: |-
    # Fix for Ghost Engine CozoDB Schema Issues

    ## Problem
    The Ghost Engine in ghost.html is experiencing crashes due to schema creation failures:
    - "Schema creation failed: undefined"
    - "Test query failed: undefined"
    - Browser crashes when reloading database

    ## Root Cause
    The issue is in the `ensureSchema()` function in ghost.html. The schema creation query attempts to create both the main table and the FTS (Full Text Search) index in a single operation:

    ```javascript
    const schemaQuery = `
    :create memory {
        id: String =>
        timestamp: Int,
        content: String,
        source: String,
        type: String
    } if not exists;

    ::fts create memory:content_fts {
        extractor: content,
        tokenizer: Simple,
        filters: [Lowercase]
    } if not exists;
    `;
    ```

    If the FTS creation fails (which can happen with certain CozoDB WASM builds), the entire schema creation fails.

    ## Solution
    Modify the `ensureSchema()` function to separate schema and FTS creation:

    ```javascript
    // First, create the basic schema
    async function ensureSchema() {
        // Create basic table first
        const basicSchemaQuery = `
        :create memory {
            id: String =>
            timestamp: Int,
            content: String,
            source: String,
            type: String
        } if not 
