[
  {
    "id": "CHANGELOG.md",
    "timestamp": 1767282468,
    "role": "file",
    "content": "# Context-Engine Changelog\n\n## [2.3.0] - 2026-01-01 \"Stability & Passive Text Ingestion\"\n\n### Added\n- **Auto-Resurrection System**: Ghost Engine (headless browser) automatically restarts on WebSocket disconnect\n  - `ResurrectionManager` class handles browser lifecycle\n  - Retry logic: 3 attempts with 2-second delays\n  - Comprehensive logging of resurrection events\n  - Works on Windows (Edge/Chrome) and Linux/Mac (Chrome)\n- **Passive Context Ingestion**: New `watchdog.py` service for continuous context growth\n  - Monitors `context/` folder for `.txt`, `.md`, `.markdown` files\n  - Batch processing with configurable interval (default 5s)\n  - Real-time mode (watchdog library) and polling mode (fallback)\n  - `/v1/memory/ingest` endpoint for receiving ingested files\n- **Model Server Improvements**: Enhanced `/models/{file_path:path}` endpoint\n  - Proper streaming for large binary files\n  - Correct MIME type handling (`.wasm`, `.safetensors`, `.json`, etc.)\n  - Path traversal protection\n  - Range request support with proper headers\n  - 1-week cache TTL for immutable model files\n\n### Fixed\n- **Model Loading**: All `chat.html` appConfig instances now use local bridge URLs\n  - Ensures reliable local model loading\n  - Fallback to HuggingFace only when files not available locally\n  - Prevents 404 errors from hardcoded external URLs\n- **Connection Stability**: WebSocket handler properly triggers auto-resurrection\n  - Cleans up pending requests before restart\n  - Prevents orphaned request queues\n- **File Size Validation**: Watchdog service enforces max 1MB file size (configurable)\n- **Security**: Added path validation to prevent directory traversal attacks\n\n### Changed\n- **Architecture**: Pivot from Vision/Ollama to Text-Only + Watchdog model\n  - Removed heavy Vision processing pipeline\n  - Simplified to pure text ingestion for stability\n  - Focus on reliable model serving and auto-recovery\n- **Model Configuration**: Unified model URL scheme across all UI components\n  - All models now served via local bridge redirect\n  - Consistent URL format: `window.location.origin + \"/models/{model_id}/resolve/main/\"`\n- **Logging**: Enhanced logging system with per-component log files\n  - `logs/webgpu_bridge.log`\n  - `logs/resurrection.log`\n  - `logs/memory_api.log`\n  - `logs/model_server.log`\n\n### Removed\n- Vision processing pipeline (Ollama integration)\n\n## [2.3.1] - 2026-01-01 \"Process Management & Streaming UX\"\n\n### Added\n- **Enhanced Process Management**: Improved `ResurrectionManager` with process cleanup\n  - `kill_existing_browsers()` method to terminate existing browser processes before launching new ones\n  - Explicit `--remote-debugging-port=9222` flag to prevent port conflicts\n  - Increased initialization wait time from 3 to 5 seconds\n- **File Ingestion Improvements**: Enhanced `watchdog.py` with debounce and hash checking\n  - Debounce functionality to wait for silence before processing file changes\n  - MD5 hash checking to prevent duplicate ingestion of unchanged files\n  - Proper cleanup of debounce timers\n- **Streaming CLI Client**: Updated `anchor.py` to use streaming for better UX\n  - Changed from `stream=False` to `stream=True` for real-time character display\n  - Added line-by-line processing of streaming responses\n  - Maintained conversation history accumulation while providing immediate feedback\n- **New Standards**: Added Standards 016-018 for process management, file ingestion, and streaming UX\n\n### Fixed\n- **Zombie Process Risk**: Fixed issue where browser resurrection would fail due to port conflicts\n- **Autosave Flood**: Fixed excessive database writes from editor autosave features\n- **CLI Latency**: Fixed hanging terminal during long model responses\n\n### Changed\n- **Process Management**: Enhanced browser process management with proper cleanup\n- **File Ingestion**: Improved file monitoring with debounce and hash checking\n- **CLI UX**: Enhanced terminal experience with streaming responses\n\n## [2.3.2] - 2026-01-01 \"Ingestion Expansion & Memory Persistence\"\n\n### Added\n- **Code File Support**: Extended `watchdog.py` to monitor code extensions\n  - Added `.py`, `.js`, `.html`, `.css`, `.json`, `.yaml`, `.yml`, `.sh`, `.bat`, `.ts`, `.tsx`, `.jsx`, `.xml`, `.sql`, `.rs`, `.go`, `.cpp`, `.c`, `.h`, `.hpp` to monitored extensions\n- **Browser Profile Cleanup**: Enhanced `ResurrectionManager` with temporary profile management\n  - Added `--user-data-dir` with unique timestamp to prevent profile conflicts\n  - Added `_cleanup_old_profiles()` method to remove old temporary directories\n  - Added performance optimization flags for headless browser\n- **Chat Session Persistence**: Updated `anchor.py` to auto-save conversations\n  - Created `context/sessions/` directory for chat logs\n  - Added `save_message_to_session()` to persist each message to markdown files\n  - Ensures conversation history survives CLI crashes and becomes ingested context\n\n### Fixed\n- **Ingestion Blind Spot**: Fixed system being blind to code files in user's context\n- **Memory Leak Risk**: Fixed potential disk space issues from temporary browser profiles\n- **Lost Context Risk**: Fixed loss of conversation history on CLI crashes\n\n## [2.3.3] - 2026-01-01 \"Session Recorder & Text-File Source of Truth\"\n\n### Added\n- **Daily Session Files**: Updated `anchor.py` to create daily markdown files (`chat_YYYY-MM-DD.md`)\n  - Uses `ensure_session_file()` to create dated session files\n  - Formats messages with timestamps: `### ROLE [HH:MM:SS]`\n  - Stores in `../context/sessions/` directory relative to tools/\n- **Text-File Source of Truth**: Implemented \"Database is Cache\" philosophy\n  - All chat history saved to text files for cross-machine sync\n  - Files automatically ingested by watchdog service\n  - Creates infinite feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat\n- **Session Tracking**: Added session file path display in CLI startup\n\n### Changed\n- **File Structure**: Changed from timestamped individual files to daily consolidated files\n- **Relative Pathing**: Uses relative path (`../context/sessions/`) for proper directory structure\n- **Markdown Format**: Improved formatting with headers and timestamps for better readability\n\n## [2.2.12] - 2025-12-31 \"UI Endpoint Fixes & Log File Verification\"\n\n### Fixed\n- **UI Endpoints**: Fixed `/context` and `/sidecar` endpoints to properly serve HTML files with correct paths\n- **File Response**: Corrected FileResponse paths to use absolute paths for reliable file serving\n- **Endpoint Accessibility**: Resolved 500 errors for UI endpoints by fixing file path resolution\n- **Static File Serving**: Enhanced static file serving for HTML interfaces\n\n### Changed\n- **Path Resolution**: Updated endpoint handlers to use proper absolute path resolution for static files\n- **UI Serving**: Improved reliability of UI file serving from the bridge\n- **Error Handling**: Enhanced error handling for file serving operations\n\n## [2.2.11] - 2025-12-31 \"Coroutines and Async Fixes\"\n\n### Fixed\n- **Async Warnings**: Fixed \"coroutine was never awaited\" warnings by properly implementing startup event handlers\n- **Event Loop Integration**: Corrected async function calls to work properly with FastAPI's event loop\n\n- **Resource Management**: Fixed resource cleanup in WebSocket handlers to prevent leaks\n- **Startup Sequence**: Ensured logging system initializes properly with the application lifecycle\n\n### Changed\n- **Async Handling**: Improved async/await patterns throughout the bridge for better stability\n- **Error Handling**: Enhanced error handling for async operations with proper cleanup\n- **Initialization**: Refined startup sequence to ensure all components initialize correctly\n\n## [2.2.10] - 2025-12-31 \"Log File System Implementation\"\n\n### Added\n- **Logs Directory**: Created `logs/` directory to store individual component logs\n- **File-based Logging**: Each system component now writes to its own log file (e.g., `chat-api.log`, `memory-api.log`, `websocket-bridge.log`)\n- **Log Truncation**: Implemented automatic log truncation to keep only last 1000 lines per file\n- **Individual Log Files**: Separate log files for each component for easier debugging\n\n### Changed\n- **Log Storage**: Moved from in-memory only to file-based persistent logging\n- **Log Management**: Added automatic log rotation and truncation to prevent disk space issues\n- **API Endpoints**: All API endpoints now write to both central buffer and individual log files\n\n## [2.2.9] - 2025-12-31 \"Complete Process Log Capture\"\n\n### Added\n- **Process Logging**: Added logging for all major system processes (chat, memory search, WebSocket connections)\n- **Error Tracking**: Enhanced error logging with detailed context and request IDs\n- **Status Monitoring**: Added detailed status messages for connection states and process flow\n\n### Changed\n- **Log Collection**: Enhanced centralized log collection with comprehensive process monitoring\n- **API Endpoints**: All API endpoints now log detailed request/response information\n- **WebSocket Handler**: Improved WebSocket connection logging with detailed status updates\n- **Error Handling**: Enhanced error messages with better context and correlation IDs\n\n## [2.2.8] - 2025-12-31 \"Universal Log Collection System\"\n\n### Added\n- **Log Collection**: Added centralized log collection system in `webgpu_bridge.py` with global log buffer\n- **API Endpoints**: Added `/logs/recent` and `/logs/collect` endpoints for log aggregation\n- **Standard 013**: Created universal log collection standard for all system components\n- **Cross-Platform Logging**: Implemented logging from all system components (Python, JavaScript, WebSocket)\n\n### Changed\n- **Log Viewer**: Updated `log-viewer.html` to consume logs from the new centralized endpoint\n- **WebSocket Logging**: Enhanced WebSocket connection to send detailed status messages to log viewer\n- **System Integration**: All components now route logs through the central collection system\n\n## [2.2.7] - 2025-12-31 \"Ghost Engine Startup Improvements\"\n\n### Fixed\n- **Connection Issues**: Fixed Ghost Engine startup to ensure proper WebSocket connection establishment\n- **Process Launch**: Improved startup scripts to properly launch Ghost Engine with correct parameters\n- **CPU-Only Mode**: Enhanced CPU-only mode startup with appropriate browser flags\n- **Low-Resource Mode**: Fixed low-resource mode startup with conservative GPU settings\n\n### Changed\n- **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` with better Ghost Engine launch parameters\n- **Connection Timing**: Improved timing between server and Ghost Engine startup\n- **User Feedback**: Added clearer status messages during startup process\n\n## [2.2.6] - 2025-12-31 \"WebGPU Adapter Error Handling\"\n\n### Fixed\n- **WebGPU Errors**: Added specific handling for \"No WebGPU Adapter found\" errors on Snapdragon/limited GPU devices\n- **Error Messages**: Improved error messages to guide users when WebGPU is unavailable\n- **Graceful Degradation**: System now provides helpful guidance instead of failing silently\n\n### Changed\n- **Chat Endpoint**: Enhanced `/v1/chat/completions` to handle WebGPU adapter errors gracefully\n- **Error Response**: More informative error messages for GPU-related issues\n- **User Guidance**: Clear instructions for users with unsupported GPU configurations\n\n## [2.2.5] - 2025-12-31 \"Log Viewer Consolidation\"\n\n### Added\n- **Single Panel**: Consolidated all logs into one unified panel for easier monitoring\n- **Stream Collection**: All app processes now stream to single consolidated view\n- **Efficiency**: Removed unused chat and context panels that were empty\n\n### Changed\n- **Log Viewer**: `tools/log-viewer.html` now shows all logs in single panel\n- **UI Simplification**: Streamlined interface for better usability\n- **Copy Functionality**: Simplified copy to clipboard for all logs at once\n\n## [2.2.4] - 2025-12-31 \"Chat Client & Bridge Reorientation\"\n\n### Added\n- **Chat Client**: Converted `tools/anchor.py` from Shell Executor to Chat Client interface\n- **Stream Accumulation**: Enhanced bridge to properly accumulate chat stream responses\n- **Terminal Chat**: Added conversation history and context management to CLI\n\n### Fixed\n- **Chat Response**: Fixed issue where chat responses were cut off due to stream handling\n- **Bridge Protocol**: Improved WebSocket message handling for complete response delivery\n- **Conversation Flow**: Added proper conversation history management in CLI\n\n### Changed\n- **Architecture**: Shifted from command execution to chat interface in terminal client\n- **API Handling**: Bridge now accumulates streaming responses for non-streaming API compatibility\n- **User Experience**: Terminal client now provides full chat experience with context\n\n## [2.2.3] - 2025-12-31 \"Ghost Engine Startup Fix\"\n\n### Fixed\n- **JavaScript Disabled**: Removed `--disable-javascript` flag that was preventing Ghost Engine from starting\n- **WASM Engine**: Fixed issue where WebAssembly AI engine couldn't load with JavaScript disabled\n- **WebSocket Connection**: Resolved \"Failed to fetch\" errors by enabling JavaScript in headless browser\n- **Memory Search**: Fixed context search functionality by ensuring Ghost Engine starts properly\n\n### Changed\n- **Startup Scripts**: Updated `start-anchor.bat` and `start-low-resource.bat` to enable JavaScript\n- **Ghost Engine**: Headless browser now properly loads WASM AI engine and connects to bridge\n\n## [2.2.2] - 2025-12-31 \"Context Search Fix\"\n\n### Fixed\n- **Memory Search**: Fixed 503 errors when Ghost Engine is disconnected by providing helpful error messages\n- **WebSocket Handling**: Improved handling of search result responses from Ghost Engine\n- **Timeout Management**: Added proper timeout handling for search requests\n- **Error Messages**: Enhanced error messages to guide users when Ghost Engine is not connected\n\n### Changed\n- **Search Endpoint**: Improved `/v1/memory/search` to handle disconnected Ghost Engine gracefully\n- **Chat Endpoint**: Enhanced error handling for chat completions when Ghost Engine is unavailable\n\n## [2.2.1] - 2025-12-31 \"UI Consolidation\"\n\n### Removed\n- **Sidecar Interface**: Removed duplicate sidecar.html interface to consolidate to single Context UI\n- **Redundant Endpoints**: Streamlined UI access to focus on single interface\n\n### Added\n- **Context UI**: Single, focused interface for retrieval and search functionality\n- **Endpoint Consolidation**: Both `/sidecar` and `/context` now serve the same Context UI\n\n### Changed\n- **UI Strategy**: Shifted from multiple similar interfaces to single, focused Context UI\n- **User Experience**: Simplified navigation with single interface for context retrieval\n\n## [2.2.0] - 2025-12-31 \"Text-Only Architecture Pivot\"\n\n### Removed\n- **Vision Engine**: Removed Python-based vision_engine.py and Ollama dependency\n- **Image Processing**: Removed all /v1/vision/* endpoints and image-related functionality\n- **External Dependencies**: Eliminated heavy Python/Ollama dependencies for lightweight operation\n\n### Added\n- **Text-Only Focus**: Streamlined architecture focusing purely on text context and memory\n- **Simplified Bridge**: Cleaned webgpu_bridge.py with only essential context relay functionality\n- **Memory Builder**: Reinforced tools/memory-builder.html as the primary background processor\n- **Browser-Native Processing**: Leverage Ghost Engine (WebGPU) for all processing needs\n\n### Changed\n- **Architecture**: Shifted from multi-component system to lightweight, browser-native approach\n- **Processing Model**: Memory processing now handled by Qwen 1.5B in WebGPU (memory-builder.html)\n- **Dependency Management**: Eliminated external inference servers (Ollama) in favor of browser-native models\n- **Sidecar Interface**: Simplified to focus solely on retrieval and search functionality\n\n## [2.1.0] - 2025-12-31 \"Daemon Eyes & Passive Observation\"\n\n### Added\n- **Daemon Eyes**: Implemented \"Digital Proprioception\". System now observes user screen activity via `sidecar.html` toggle.\n- **Vision Pipeline**: Integrated `vision_engine.py` to convert images/screenshots into semantic text memories.\n- **Live Context Loop**: Added `POST /v1/vision/screenshot` for non-blocking background context ingestion.\n- **Unified Sidecar**: Merged Retrieval and Vision tools into `tools/sidecar.html`.\n- **Context UI**: Added `tools/context.html` for simplified read-only context retrieval with scrollable display and one-click copy\n- **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:\n    - `GET /sidecar` - Serves the unified control center\n    - `GET /context` - Serves the read-only context retrieval UI\n    - `POST /v1/vision/ingest` - Handles image upload and VLM processing\n    - `POST /v1/vision/screenshot` - Handles background screenshot processing\n    - `POST /v1/memory/search` - Implements memory graph search functionality\n\n### Changed\n- **Context Strategy**: Shifted from \"Manual Copy-Paste\" to \"Passive Accumulation + Manual Retrieval\".\n- **Bridge Architecture**: `webgpu_bridge.py` now manages background tasks (FastAPI `BackgroundTasks`) for image processing to prevent UI freezing.\n- **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs\n\n## [2.0.3] - 2025-12-31 \"Browser-Based Control Center & VLM Integration\"\n\n### Added\n- **Vision Engine**: Created `tools/vision_engine.py` for Python-powered image analysis using Ollama backend\n- **Browser Control Center**: Implemented `tools/sidecar.html` with dual tabs for context retrieval and vision ingestion\n- **Context UI**: Added `tools/context.html` for manual context retrieval with scrollable display and one-click copy\n- **New Endpoints**: Added bridge endpoints for serving UI and processing vision requests:\n    - `GET /sidecar` - Serves the sidecar dashboard\n    - `GET /context` - Serves the context retrieval UI\n    - `POST /v1/vision/ingest` - Handles image upload and VLM processing\n    - `POST /v1/memory/search` - Implements memory graph search functionality\n- **VLM Integration**: Full integration pipeline from image upload ‚Üí Python VLM ‚Üí memory graph ingestion\n\n### Changed\n- **Bridge Enhancement**: Extended `webgpu_bridge.py` to serve UI files and orchestrate vision processing\n- **Memory Search**: Implemented placeholder search functionality with realistic response structure\n- **UI Workflow**: Unified workflow to browser-based control center, reducing terminal interaction needs\n\n## [2.0.2] - 2025-12-31 \"Test Suite Organization & Pipeline Verification\"\n\n### Added\n- **Test Directory Structure**: Created dedicated `tests/` directory in project root for all test files\n- **Test File Migration**: Moved all test files from `tools/` and `scripts/` to new `tests/` directory:\n    - `test_model_loading.py` ‚Üí `tests/test_model_loading.py`\n    - `test_model_availability.py` ‚Üí `tests/test_model_availability.py`\n    - `test_orchestrator.py` ‚Üí `tests/test_orchestrator.py`\n    - `model_test.html` ‚Üí `tests/model_test.html`\n    - `test_gpu_fixes.py` ‚Üí `tests/test_gpu_fixes.py`\n- **Test Configuration Updates**: Updated test files to use correct port (8000 instead of 8080) for current architecture\n- **Comprehensive Test Suite**: Enhanced test coverage for model loading, endpoint accessibility, and data pipeline verification\n\n### Changed\n- **Project Organization**: Consolidated all test assets into dedicated directory for better maintainability\n- **Test Architecture**: Updated test configurations to match current Anchor Core unified architecture (port 8000)\n\n## [2.0.1] - 2025-12-30 \"Server Stability & Endpoint Fixes\"\n\n### Fixed\n- **Server Startup Issues**: Resolved server hanging issues caused by problematic path parameter syntax (`:path`) in route definitions that prevented proper server startup\n- **Missing Endpoints**: Added critical missing endpoints (`/v1/models/pull`, `/v1/models/pull/status`, `/v1/gpu/lock`, `/v1/gpu/status`, etc.) that were documented but missing from implementation\n- **Endpoint Accessibility**: Verified all documented endpoints are now accessible and responding properly\n- **Model Availability**: Improved model availability testing showing that `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` and `Qwen2.5-7B-Instruct-q4f16_1-MLC` have most files available (missing only `params.json`)\n\n### Architecture Shift\n- **Unified Anchor Core**: Consolidated Bridge, File Server, and UI into a single process (`webgpu_bridge.py`) running on **Port 8000**.\n- **Single Origin**: Eliminated CORS issues and port confusion. UI, API, and Models are served from the same origin.\n- **Protocol**:\n    - Brain: `http://localhost:8000/chat.html`\n    - Terminal: `http://localhost:8000/terminal.html`\n    - API: `http://localhost:8000/v1/...`\n\n### Removed / Archived\n- **CLI Bloat**: Deleted `anchor.py` and `sov.py` in favor of web-based `terminal.html` or direct API calls.\n- **Legacy Scripts**: Archived `start-bridge.bat`, `start-ghost-shell.bat`, `launch-ghost.ps1`, `hot_reload_gpu.py`, and others to `archive/v2_ghost_shell/`.\n\n### Added\n- **start-anchor.bat**: Single-click launcher that starts the Core and the Ghost Engine (Minimized Browser).\n\n## [1.2.4] - 2025-12-29 \"Ghost & Shell Architecture\"\n\n### Added\n- **Ghost & Shell Architecture**: Implemented headless Ghost engine with native Anchor shell for OS integration\n- **Auto-Ignition Protocol**: Added auto-start sequence for headless browser with `?headless=true` parameter\n- **Anchor Terminal**: Created `tools/anchor.py` for native PowerShell interface with natural language processing\n- **Spawn Endpoint**: Added `/v1/system/spawn_shell` to launch native terminals from dashboard\n- **Neural Shell Protocol**: Enhanced `/v1/shell/exec` to process natural language to PowerShell commands\n- **UTF-8 Encoding Fix**: Added Windows encoding enforcement to prevent Unicode crashes in bridge\n- **Minimized Window Approach**: Updated `scripts/launch-ghost.ps1` to use `--start-minimized` for proper GPU access\n- **Unified Startup**: Consolidated to single `start-ghost-shell.bat` script launching complete architecture\n\n### Changed\n- **Renamed Kernel**: Migrated from `sovereign.js` to `anchor.js` with updated imports across all components\n- **Simplified Bridge**: Streamlined `webgpu_bridge.py` with essential functionality only\n- **Updated Neural Terminal**: Modified `tools/neural-terminal.html` to use new shell protocol\n- **Dashboard Integration**: Added Anchor Shell button to `tools/index.html`\n- **Startup Scripts**: Consolidated multiple startup scripts to single unified approach\n\n### Fixed\n- **Windows Encoding**: Resolved Unicode encoding crashes with UTF-8 enforcement\n- **Bridge Authorization**: Fixed authentication token consistency across components\n- **Headless GPU Access**: Resolved WebGPU initialization issues with minimized window approach\n- **Model Loading**: Fixed auto-load sequence for Ghost engine with proper model selection\n- **Cache Bypass Protocol**: Implemented \"Stealth Mode\" for browser AI engine by overriding Cache API and modifying static file headers to force browser to treat models as \"data in RAM\" rather than \"persistent storage\", bypassing strict security policies.\n- **NoCacheStaticFiles**: Custom StaticFiles class with `Cache-Control: no-store` headers to prevent browser cache API usage when serving models through the bridge.\n- **Neural Shell Protocol**: Activated \"The Hands\" (Layer 3) in `webgpu_bridge.py` (`/v1/shell/exec`), allowing the browser to execute system commands on the host.\n- **Neural Terminal**: `tools/neural-terminal.html` provides a matrix-style command interface for direct shell access from the browser.\n- **Hot Reload Improvement**: Fixed `start-sovereign-console-hotreload.bat` port conflicts and added `/file-mod-time` endpoint to `smart_gpu_bridge.py` for correct frontend reloading.\n- **Root Mic (Audio Input)**: Renamed `sovereign-mic.html` to `root-mic.html` and added \"Summarize & Clarify\" feature using the local Qwen2.5 model.\n- **Long-Form Transcription**: Fixed Whisper pipeline to support recordings >30s using chunking and striding.\n- **CozoDB Corruption Recovery**: Enhanced error handling for IndexedDB corruption with automatic fallback to in-memory database, manual recovery button, and timeout protection against hanging WASM calls.\n- **Bulk CozoDB Import Tool**: Added `tools/prepare_cozo_import.py` to transform `combined_memory.json` into the canonical `relations` payload (`cozo_import_memory.json`) for atomic bulk imports into CozoDB.\n- **Import Safety & Verification**: Added recommended import procedure and a post-import verification + backup step to avoid Schema Detachment.\n- **WebGPU Bridge**: `webgpu_bridge.py` for proxying OpenAI API requests to browser workers.\n- **Chat Worker**: `webgpu-server-chat.html` for running LLMs in the browser.\n- **Embed Worker**: `webgpu-server-embed.html` for running embedding models in the browser.\n- **Mobile Chat**: `mobile-chat.html` for a lightweight, mobile-friendly UI.\n- **Log Viewer**: `log-viewer.html` for real-time server log monitoring.\n\n### Changed\n- **Model Loading**: Updated `model-server-chat.html` to use bridge-based model URLs (`http://localhost:8080/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.\n- **Ingestion Defaults**: Recommended batch size increased to 100 to prevent long-running slow writes that can desync CozoDB's in-memory metadata.\n- **Git Configuration**: Added `models/` directory to `.gitignore` to prevent committing large binary model files.\n\n---\n\n## [1.2.3] - 2025-12-19 \"Snapdragon Optimization\"\n\n### Added\n- **Qwen3 Support**: Added `Qwen3-4B-Instruct` to the verified model list.\n- **Llama 3.2 Support**: Added `Llama-3.2-1B-Instruct` as the recommended lightweight model.\n- **Buffer Override**: Implemented `appConfig` overrides to force high-end performance on 256MB GPUs (fixing Adreno throttling).\n\n### Changed\n- **Portable Launchers**: All scripts now use `--user-data-dir=\"%~dp0browser_data\"` for fully portable, clean-running instances.\n- **Model Config**: Refactored `CreateMLCEngine` initialization to handle both URL-based and ID-based model definitions reliably.\n\n---\n\n## [1.2.2] - 2025-12-18 \"Hermes & CozoDB Fixes\"\n\n### Fixed\n- **Hermes Model Support**: Fixed 404 errors for OpenHermes and NeuralHermes by mapping them to the verified `Mistral-v0.3` WASM library.\n- **CozoDB Date Formatting**: Removed `strftime` dependency from WASM queries (causing `no_implementation` errors) and moved date formatting to client-side JavaScript.\n- **Drag-and-Drop Import**: Fixed handling of CozoDB `relations` export format in drag-and-drop ingestion.\n- **Documentation**: Established `specs/mlc-urls.md` as a registry for verified WASM binaries.\n\n---\n\n## [1.2.1] - 2025-12-15 \"DeepSeek & CozoDB Stabilization\"\n\n### Fixed\n- **CozoDB Initialization**: Resolved `CozoDb.new_from_path is not a function` error by switching to `CozoDb.new_from_indexed_db` for persistent browser storage (IndexedDB backend).\n- **WASM Memory Access**: Fixed \"memory access out of bounds\" error in `sovereign-db-builder.html` and `unified-coda.html` by correctly stringifying JSON parameters passed to `db.run()`.\n- **DeepSeek Configuration**: Fixed \"Cannot find model record\" error in `unified-coda.html` by decoupling the internal model ID from the HuggingFace URL.\n\n### Added\n- **Sovereign Hub**: Created `tools/index.html` as a central dashboard for the Console, Builder, and Log Viewer.\n- **Log Viewer Upgrade**: Refactored `tools/log-viewer.html` to use `BroadcastChannel` for real-time, polling-free log updates from the console.\n- **Expanded File Support**: Updated `sovereign-db-builder.html` to support ingestion of a wider range of code and config files (ts, rs, go, sql, ini, xml, etc.).\n\n## [1.2.0] - 2025-12-15 \"Sovereign Architecture\"\n\n### Added\n- **Sovereign Console**: Created `tools/unified-coda.html`, a standalone WASM-based chat console with local CozoDB (OPFS) and Transformers.js.\n- **Sovereign DB Builder**: Created `tools/sovereign-db-builder.html` for ingesting JSON logs into the browser-based database.\n- **Model Support**: Expanded `unified-coda.html` to support the full range of MLC-compatible models (Llama 3.2, Qwen 2.5, Gemma 2, etc.).\n\n### Changed\n- **Log Management**: Updated backend logging to truncate files at 500KB to prevent disk bloat.\n\n## [1.1.0] - 2025-12-14 \"Browser Stability & Bridge Fixes\"\n\n### Fixed\n- **WebGPU Bridge**: Patched `tools/webgpu_bridge.py` to accept any model name, resolving 503 errors during embedding requests.\n- **LLM Client**: Updated `backend/src/llm.py` to correctly identify and use the configured embedding model (`nomic-embed-text-v1.5`).\n- **Coda Chat**: Modified `backend/src/recipes/coda_chat.py` to sanitize and truncate `retrieve_memory` outputs. Large JSON payloads were causing `Maximum call stack size exceeded` errors in the browser-based LLM worker.\n\n## [1.0.0] - 2025-12-08 \"Infinite Context Pipeline\"\n\n### Added\n- **Phase 1: Hardware Foundation**: All LLM servers now boot with 65,536 context window and Flash Attention enabled\n- **Phase 2: Context Rotation Protocol**: ContextManager automatically rotates context when exceeding 55k tokens\n- **Phase 3: Graph-R1 Enhancement**: GraphReasoner now retrieves ContextGist memories for historical continuity\n- **ContextGist Nodes**: Neo4j storage for compressed historical context summaries with chronological links\n- **Context Shifting Logic**: Intelligent distillation of old content using Distiller agent with gist creation\n- **Documentation Structure**: Organized specs/ directories at root, backend, and anchor levels with spec.md, plan.md, tasks.md\n- **Infinite Context Pipeline**: Complete end-to-end implementation enabling unlimited context window management\n\n### Changed\n- **Upgraded Context Windows**: All start scripts now default to 64k context for infinite work capability\n- **Enhanced Memory Architecture**: Neo4j now stores both active memories and ContextGist historical summaries\n- **Improved ContextManager**: Added check_and_rotate_context() logic with automatic gist creation and storage\n- **Extended GraphReasoner**: Updated retrieval queries to include ContextGist nodes alongside regular memories\n- **Optimized Distiller Integration**: Enhanced _chunk_and_distill functionality for context rotation use cases\n- **Refined Archivist Agent**: Now coordinates context rotation and gist management operations\n\n### Fixed\n- **Context Limit Elimination**: Fixed issue where systems would crash when reaching context limits\n- **Memory Continuity**: Resolved problems with historical context access across conversation boundaries\n- **Performance Optimization**: Fixed inefficiencies in large context handling with 64k window support\n- **Rotation Logic**: Fixed issues with context preservation during rotation cycles\n\n---\n\n## [0.9.0] - 2025-12-07 \"Reka & Local Proxy\"\n\n### Added\n- **Reka Configuration**: Full support for Reka-Flash-3-21B (Q4_K_S) with 16k context, stop tokens, and optimized LLaMa server flags.\n- **Local API Proxy**: Added `scripts/local_api_proxy.py` to enforce static API keys for local LLaMa instances (fixes Cline extension \"OpenAI API Key\" requirement).\n- **VS Code Integration**: Added `.vscode/settings.json` template and `VSCODE_CLINE_SETUP.md` for seamless local development.\n- **MCP Health**: Added `/health` endpoint to Unified Launcher for better compatibility.\n\n### Fixed\n- **MCP Routing**: Resolved duplicate `/mcp` prefix in Unified Launcher routes (`/mcp/tools` is now accessible).\n- **LLM Client**: Added `stop` token support to API payloads and local GGUF generation.\n\n## [0.8.0] - 2025-12-06 \"Archivist Protocol\"\n\n### Added\n- **Archivist Ingestion**: Implemented `POST /archivist/ingest` endpoint to accept live data from the browser.\n- **Memory Schema**: Enforced **Directive INJ-A1** (`PlaintextMemory`) for immutable \"Page-Store\" records.\n- **Modular DOM Adapters**:\n    - `GeminiAdapter`: Clean extraction for Google Gemini.\n    - `ChatGPTAdapter`: Clean extraction for ChatGPT.\n    - `ClaudeAdapter`: Clean extraction for Claude.ai.\n    - `GenericAdapter`: Universal fallback for any webpage.\n- **Extension UI**: Added **[Save to Memory]** button to the Side Panel for manual ingestion.\n\n### Fixed\n- **Encoding Crash**: Resolved Windows `charmap` error by enforcing `PYTHONIOENCODING='utf-8'`.\n- **Server Stability**: Fixed startup crashes caused by `MemoryWeaver` resource contention.\n\n## [0.7.0] - 2025-12-06 \"Operation Concrete\"\n\n### Added\n- **Browser Bridge**: A Chrome Extension (MV3) capable of:\n    - **Voice**: Streaming chat interface via Side Panel.\n    - **Sight**: Context injection (reading active tab).\n    - **Hands**: JavaScript execution on active pages (User-ratified).\n- **Backend Architecture**: Migrated from monolithic scripts to **Modular Recipes** (MAX Agentic Cookbook standard).\n    - `CodaChatRecipe`: Handles orchestration, context, and tool execution.\n- **Persistence**: Side panel now saves chat history to local storage.\n- **Markdown Support**: Chat interface renders code blocks and syntax highlighting.\n\n### Changed\n- **Identity**: System formally renamed from \"Sybil\" to **\"Coda\"**.\n- **Documentation**: Adopted `specs/` based documentation policy.\n\n### Fixed\n- **Audit Logger**: Patched critical `NameError` in streaming endpoints.\n- **Security**: Hardened extension execution via `world: \"MAIN\"` to bypass strict CSP on some sites.\n\n---\n\n## [0.6.0] - 2025-11-30 \"Operation MCP Integrated\"\n\n### Added\n- **MCP Integration**: Complete integration of MCP server into main ECE Core server\n- **Unified Endpoint**: All MCP functionality now available at `/mcp` on main server (port 8000)\n- **Memory Tools**: Enhanced MCP tools for memory operations:\n    - `add_memory` - Add to Neo4j memory graph\n    - `search_memories` - Search memory graph with relationships\n    - `get_summaries` - Get session summaries\n- **Configuration**: New `mcp_enabled` setting in config.yaml to toggle integration\n- **Authentication**: MCP endpoints now inherit main server authentication settings\n\n### Changed\n- **Architecture**: MCP server no longer runs as separate process, now integrated into main ECE server\n- **Endpoints**: MCP tools now accessed via `/mcp/tools` and `/mcp/call` instead of separate server\n- **Deployment**: Simplified deployment - no need to start separate MCP service\n- **Resources**: Reduced memory footprint by eliminating duplicate server processes\n\n### Fixed\n- **Connection Issues**: Resolved intermittent connection failures between ECE and external MCP server\n- **Latency**: Reduced tool call latency by eliminating inter-service communication overhead\n- **Synchronization**: Fixed race conditions in concurrent tool executions\n\n---\n\n## [0.5.1] - 2025-11-29 \"Memory Weaver Security Audit\"\n\n### Added\n- **Security Hardening**: Added input validation for all GraphReasoner queries\n- **Audit Trail**: Enhanced logging for all automated relationship repairs\n- **Circuit Breakers**: Added fail safes for Weaver operations\n\n### Changed\n- **Weaver Engine**: Refactored to use parameterized queries, preventing Cypher injection\n- **Permission Model**: Strengthened access controls for relationship modification operations\n\n### Fixed\n- **Cypher Injection**: Patched vulnerability in Neo4j relationship queries\n- **Race Conditions**: Fixed concurrency issues in automated repair operations\n- **Resource Exhaustion**: Added limits to prevent DoS via excessive repair requests\n\n---\n\n## [0.5.0] - 2025-11-28 \"Memory Weaver (Automated Repair)\"\n\n### Added\n- **Memory Weaver Engine**: Automated system for detecting and repairing broken relationships in Neo4j\n- **Similarity Detection**: Embedding-based relationship discovery for linking related memories\n- **Audit System**: Complete traceability for all automated repairs with `auto_commit_run_id`\n- **Rollback Capability**: Deterministic reversal of automated changes via `rollback_commits_by_run.py`\n- **Scheduler**: Background maintenance tasks for continuous graph integrity\n\n### Changed\n- **Graph Maintenance**: Automated relationship repair now runs as background process\n- **Quality Assurance**: Enhanced relationship validation with similarity scoring\n- **Traceability**: All automated changes now logged with unique run identifiers\n\n### Fixed\n- **Orphaned Nodes**: Automatically discovers and connects isolated memories\n- **Broken Links**: Repairs missing relationships between related concepts\n- **Data Drift**: Corrects inconsistent metadata across related nodes\n\n---\n\n## [0.4.0] - 2025-11-25 \"Graph-R1 Implementation\"\n\n### Added\n- **Graph Reasoner**: Iterative \"Think ‚Üí Query ‚Üí Retrieve ‚Üí Rethink\" reasoning engine\n- **Q-Learning Retrieval**: Reinforcement learning for optimized memory access patterns\n- **Markovian Reasoning**: Chunked thinking with state preservation across context shifts\n- **Multi-Hop Queries**: Complex graph traversal for answering compound questions\n- **Cognitive Agents**: Plugin architecture for specialized reasoning tasks\n\n### Changed\n- **Retrieval Method**: Replaced simple vector search with Graph-R1 retrieval\n- **Memory Access**: Graph-based traversal now primary method for context assembly\n- **Agent Architecture**: Modular cognitive agents for specialized tasks\n- **Context Building**: Enhanced context with relationship-aware retrieval\n\n### Fixed\n- **Context Relevance**: Improved precision of memory retrieval\n- **Chain of Thought**: Better preservation of reasoning pathways\n- **Memory Decay**: Reduced loss of historical context in long conversations\n\n---\n\n## [0.3.1] - 2025-11-20 \"Security Hardening\"\n\n### Added\n- **API Authentication**: Token-based authentication for all endpoints\n- **Rate Limiting**: Request throttling to prevent abuse\n- **Input Sanitization**: Enhanced validation for all user inputs\n- **Audit Logging**: Comprehensive logging of all sensitive operations\n- **Secure Defaults**: Safe configuration presets for common deployment scenarios\n\n### Changed\n- **Security Model**: Implemented zero-trust architecture\n- **Credential Handling**: Secure storage and transmission of API keys\n- **Access Controls**: Granular permissions for different API endpoints\n\n### Fixed\n- **Authentication Bypass**: Patched critical vulnerability in API access\n- **Data Exposure**: Resolved information disclosure in error messages\n- **Injection Attacks**: Fixed potential SQL injection in Neo4j queries\n\n---\n\n## [0.3.0] - 2025-11-15 \"Neo4j Migration Complete\"\n\n### Added\n- **Neo4j Integration**: Complete migration from SQLite to Neo4j graph database\n- **Redis Cache**: Hot cache layer for active session management\n- **Graph Schema**: Formal schema definition for memory relationships\n- **Migration Tools**: Scripts to migrate existing SQLite data to Neo4j\n- **Backup System**: Automated graph backup and restoration procedures\n\n### Changed\n- **Storage Architecture**: Tiered storage (Redis hot cache + Neo4j persistent)\n- **Query Language**: Cypher queries for graph operations\n- **Relationship Modeling**: Graph-based connections between memories\n- **Indexing Strategy**: Graph-based indices for faster retrieval\n\n### Fixed\n- **Performance**: Significantly improved query performance for complex relationships\n- **Scalability**: Better handling of large-scale memory graphs\n- **Consistency**: Stronger data integrity with ACID-compliant transactions\n\n---\n\n## [0.2.0] - 2025-10-30 \"Cognitive Agents\"\n\n### Added\n- **Verifier Agent**: Fact-checking via empirical distrust protocol\n- **Archivist Agent**: Memory maintenance and staleness detection\n- **Distiller Agent**: Content summarization and extraction\n- **Agent Framework**: Plugin system for extensible cognitive capabilities\n- **Truth Scoring**: Provenance-aware fact-checking with primary source priority\n\n### Changed\n- **Memory Hygiene**: Automated maintenance of memory quality\n- **Verification Process**: Evidence-based fact-checking system\n- **Quality Assurance**: Continuous assessment of memory reliability\n- **Maintenance Schedule**: Regular memory grooming operations\n\n### Fixed\n- **Hallucinations**: Reduced false information in responses\n- **Stale Information**: Automatic detection and updating of outdated memories\n- **Data Quality**: Improved content validation and cleaning procedures\n\n---\n\n## [0.1.0] - 2025-09-15 \"Initial Architecture\"\n\n### Added\n- **Core Backend**: Initial ECE_Core with SQLite memory system\n- **Anchor Interface**: Terminal interface for user interaction\n- **Basic Memory**: Text-based memory storage and retrieval\n- **LLM Integration**: Support for various local LLM servers\n- **Plugin System**: Extensible tool architecture (UTCP)\n\n### Changed\n- **Foundation**: Established core architecture patterns\n- **API Design**: Defined RESTful API structure for components\n\n### Fixed\n- **Basic Functionality**: Initial implementation of core features",
    "source": "CHANGELOG.md"
  },
  {
    "id": "migration_complete.md",
    "timestamp": 1767458395,
    "role": "file",
    "content": "# Migration Complete: Python/Browser Bridge ‚Üí Node.js Monolith\n\n## Summary\nThe migration from the Python/Browser Bridge (V2) to the Node.js Monolith (V3) has been successfully completed.\n\n## Completed Tasks\n‚úì **Phase 1**: Established Safety Protocol 001 for autonomous execution\n‚úì **Phase 2**: Archived legacy V2 artifacts to `archive/v2_python_bridge/`:\n  - `webgpu_bridge.py`\n  - `anchor_watchdog.py` \n  - `start-anchor.bat`\n  - `kill-edge.bat`\n  - `launch-chromium-*.bat`\n  - `backend/` folder\n  - Other V2 artifacts\n\n‚úì **Phase 3**: Bootstrapped Node.js monolith in `server/`:\n  - Created `package.json` with all required dependencies\n  - Implemented CozoDB integration with RocksDB backend\n  - Created main server (`src/index.js`) with all required endpoints\n\n‚úì **Phase 4**: Implemented data migration:\n  - Created `migrate_history.js` for legacy session consolidation\n  - Generated `context/full_history.yaml` and `context/full_history.json`\n  - Successfully processed 827 legacy sessions\n\n‚úì **Phase 5**: Deployed new context collection:\n  - Created JavaScript version `context/Coding-Notes/Notebook/read_all.js`\n  - Replaced Python version with JavaScript equivalent\n  - Generated comprehensive context files:\n    - `combined_text.txt` (89.7MB)\n    - `combined_memory.json` (108MB) \n    - `combined_memory.yaml` (96.1MB)\n\n## Current Status\n- ‚úÖ Server running on `http://localhost:3000`\n- ‚úÖ Health check: `http://localhost:3000/health` responding\n- ‚úÖ CozoDB schema initialized\n- ‚úÖ File watcher monitoring `context/` directory\n- ‚úÖ All API endpoints operational\n- ‚úÖ Legacy data successfully migrated\n- ‚úÖ Context collection operational\n\n## API Endpoints Available\n- `POST /v1/ingest` - Content ingestion\n- `POST /v1/query` - CozoDB query execution  \n- `GET /health` - Service health verification\n\n## Architecture Benefits\n- Eliminated fragile headless browser dependency\n- Reduced resource consumption\n- Improved platform compatibility (works on Termux/Linux)\n- Simplified deployment and maintenance\n- Enhanced stability and reliability",
    "source": "migration_complete.md"
  },
  {
    "id": "README.md",
    "timestamp": 1767460951,
    "role": "file",
    "content": "# Context Engine (Sovereign Edition)\n\n> **Philosophy:** Your mind, augmented. Your data, sovereign. Your tools, open.\n\nA **Headless Node.js** cognitive extraction system. No browser dependencies. No cloud. No installation.\nJust you, Node.js, and your infinite context.\n\n---\n\n## ‚ö° Quick Start\n\n1.  **Download** this repository.\n2.  **Install** Node.js dependencies: `cd server && npm install`\n3.  **Launch** the unified system: `cd server && npm start`\n4.  **Access** the API at `http://localhost:3000`\n\n*That's it. You are running a headless context engine with persistent Graph Memory.*\n\n---\n\n## üèóÔ∏è Architecture\n\nThe system now runs in `server/` using Node.js with direct CozoDB integration.\n\n### 1. The Sovereign Loop\n```mermaid\ngraph TD\n    User -->|Input| API[\"HTTP API\"]\n\n    subgraph \"SOVEREIGN ENGINE (Port 3000)\"\n        Server[Express Server]\n        API[\"API Endpoints\"]\n        Cozo[\"CozoDB Node\"]\n    end\n\n    API -->|REST| Server\n    Server -->|Direct| Cozo[\"CozoDB (RocksDB)\"]\n\n    subgraph File_Watcher [\"Context Watcher\"]\n        Watcher[chokidar] -->|Monitor| Context[\"context/ directory\"]\n        Watcher -->|Ingest| Cozo\n    end\n\n    subgraph Cognitive_Engine\n        Server -->|Query| Cozo\n        Cozo -->|Results| Server\n    end\n```\n\n### 2. Core Components\n*   **Engine**: `src/index.js` - Node.js server with Express, CORS, and body-parser.\n*   **Memory**: `CozoDB (Node)` - Stores relations (`*memory`) with RocksDB persistence.\n*   **Ingestion**: `chokidar` - Watches `context/` directory for file changes and auto-ingests.\n*   **Core**: `server/` - Node.js monolith handling API, ingestion, and database operations.\n*   **Data**: `context/` - Directory for storing context files that are automatically monitored.\n\n---\n\n## üèõÔ∏è Node.js Monolith Architecture\n\nThe system now features the unified Node.js monolith architecture for simplified deployment:\n\n*   **Sovereign Engine**: Single-process Node.js server handling API, ingestion, and database on port 3000\n*   **CozoDB Integration**: Direct integration with `cozo-node` using RocksDB backend\n*   **File Watcher**: `chokidar` monitors `context/` directory for automatic ingestion\n*   **API Endpoints**: Standardized endpoints for ingestion, querying, and health checks\n\n### Getting Started with Node.js Monolith\n1. Install dependencies: `cd server && npm install`\n2. Start the unified system: `cd server && npm start`\n3. Access the health check: `http://localhost:3000/health`\n4. Use the API endpoints for ingestion and querying\n\n### API Endpoints\n*   `POST /v1/ingest` - Content ingestion endpoint\n*   `POST /v1/query` - CozoDB query execution endpoint\n*   `GET /health` - Service health verification endpoint\n\n### Migration Benefits\n* **Eliminated Browser Dependencies**: No more fragile headless browser architecture\n* **Reduced Resource Consumption**: Lower memory and CPU usage\n* **Improved Platform Compatibility**: Works on Termux/Linux environments\n* **Enhanced Stability**: More reliable operation without browser quirks\n* **Simplified Deployment**: Single Node.js process instead of complex browser bridge\n\n---\n\n## üîÑ Context Collection & Migration\n\nThe system now includes comprehensive context collection and legacy migration:\n\n*   **Legacy Migration**: `migrate_history.js` consolidates legacy session files into YAML/JSON\n*   **Context Collection**: `read_all.js` aggregates content from project directories\n*   **File Monitoring**: Automatic ingestion of new files in `context/` directory\n*   **Multi-Format Output**: Generates text, JSON, and YAML formats for maximum compatibility\n*   **Archive Strategy**: Legacy V2 artifacts archived to `archive/v2_python_bridge/`\n\n### Data Migration Process\n1. Legacy session files consolidated from `context/Coding-Notes/Notebook/history/important-context/sessions/`\n2. Converted to YAML format in `context/full_history.yaml` and `context/full_history.json`\n3. Auto-ingested into CozoDB for persistent storage\n4. Legacy Python infrastructure archived for historical reference\n\n### Context Collection Strategy\n*   **File Discovery**: Recursive scanning of `context/` directory\n*   **Format Support**: Handles .json, .md, .yaml, .txt, .py, .js, .html, .css, .sh, .ps1, .bat\n*   **Exclusion Rules**: Skips common build directories, logs, and combined outputs\n*   **Encoding Detection**: Robust encoding handling for various file types\n*   **Structured Output**: Generates both JSON and YAML memory files\n\n---\n\n## üìö Documentation\n\n*   **Architecture**: [specs/spec.md](specs/spec.md)\n*   **Roadmap**: [specs/plan.md](specs/plan.md)\n*   **Migration Guide**: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)\n*   **Autonomous Execution**: [specs/protocols/001-autonomous-execution.md](specs/protocols/001-autonomous-execution.md)\n\n---\n\n## üßπ Legacy Support\nThe old Python/Browser Bridge (V2) has been **archived**.\n*   Legacy artifacts: `archive/v2_python_bridge/`\n*   Legacy code: `webgpu_bridge.py`, `anchor_watchdog.py`, `start-anchor.bat`, etc.\n*   Migration guide: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)",
    "source": "README.md"
  },
  {
    "id": "read_all.py",
    "timestamp": 1767462495,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nRoot Reader: Aggregates content from relevant project files for orchestration.\n\nThis script scans the project directory and combines the content of source code,\nconfiguration, and documentation files into a single text file (combined_text.txt).\n\nIt ignores files with more than 2000 lines, and skips common combined outputs and package manifest files.\n\nIt respects the current project structure:\n- Root level scripts and docs\n- tools/: Sovereign Core (JS/HTML/CSS)\n- specs/: System Specifications\n- scripts/: CI/Utility scripts\n\nIt also respects .gitignore files to determine which files should be excluded.\n\nLIMIT: Content is limited to under 100k tokens for orchestrator model consumption.\n\"\"\"\n\nimport argparse\nimport fnmatch\nimport json\nimport os\nimport re\nfrom typing import Any, List, Tuple\n\nimport yaml\n\n\ndef find_project_root(start_path: str | None = None) -> str:\n    \"\"\"\n    Locate project root by looking for indicators like .git, package.json, README.md\n    \"\"\"\n    if start_path is None:\n        start_path = os.path.abspath(__file__)\n\n    path = os.path.abspath(start_path)\n    if os.path.isfile(path):\n        path = os.path.dirname(path)\n\n    root_indicators = (\".git\", \"package.json\", \"README.md\")\n    while True:\n        if any(os.path.exists(os.path.join(path, ind)) for ind in root_indicators):\n            return path\n        parent = os.path.dirname(path)\n        if parent == path:\n            return os.getcwd()\n        path = parent\n\n\ndef parse_gitignore(gitignore_path: str) -> List[str]:\n    \"\"\"\n    Parse a .gitignore file and return a list of patterns.\n    \"\"\"\n    patterns = []\n    try:\n        with open(gitignore_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                # Skip empty lines and comments\n                if line and not line.startswith(\"#\"):\n                    patterns.append(line)\n    except Exception:\n        # If we can't read the file, return empty patterns\n        pass\n    return patterns\n\n\ndef is_ignored(\n    file_path: str, gitignore_patterns: List[str], project_root: str\n) -> bool:\n    \"\"\"\n    Check if a file should be ignored based on .gitignore patterns.\n    \"\"\"\n    rel_path = os.path.relpath(file_path, project_root).replace(os.sep, \"/\")\n\n    for pattern in gitignore_patterns:\n        # Handle negation patterns (starting with !)\n        if pattern.startswith(\"!\"):\n            negated_pattern = pattern[1:]\n            if fnmatch.fnmatch(rel_path, negated_pattern) or fnmatch.fnmatch(\n                os.path.basename(rel_path), negated_pattern\n            ):\n                return False  # Don't ignore if it matches a negated pattern\n        else:\n            # Check if the pattern matches the file path\n            if fnmatch.fnmatch(rel_path, pattern) or fnmatch.fnmatch(\n                os.path.basename(rel_path), pattern\n            ):\n                # If pattern starts with /, it only matches from the gitignore's directory\n                if pattern.startswith(\"/\"):\n                    # Check if the relative path from gitignore directory matches\n                    gitignore_dir = os.path.dirname(\n                        os.path.relpath(gitignore_path, project_root)\n                    ).replace(os.sep, \"/\")\n                    if gitignore_dir == \".\":\n                        gitignore_dir = \"\"\n                    else:\n                        gitignore_dir += \"/\"\n                    if rel_path.startswith(gitignore_dir):\n                        relative_to_gitignore = rel_path[len(gitignore_dir) :]\n                        if fnmatch.fnmatch(\n                            relative_to_gitignore, pattern[1:]\n                        ) or fnmatch.fnmatch(\n                            os.path.basename(relative_to_gitignore), pattern[1:]\n                        ):\n                            return True\n                else:\n                    # For non-absolute patterns, match against the full path\n                    if fnmatch.fnmatch(rel_path, pattern) or fnmatch.fnmatch(\n                        os.path.basename(rel_path), pattern\n                    ):\n                        return True\n    return False\n\n\ndef count_tokens(text: str) -> int:\n    \"\"\"\n    Count approximate tokens in text using a simple heuristic.\n    This is a basic approximation - 1 token is roughly 4 characters or 1 word.\n    \"\"\"\n    # Simple tokenization: split on whitespace and punctuation\n    # This is a rough approximation; for more accuracy, we could use tiktoken or similar\n    if not text:\n        return 0\n\n    # Split on whitespace and common punctuation\n    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", text)\n    return len(tokens)\n\n\ndef file_has_too_many_lines(file_path: str, max_lines: int = 2000) -> bool:\n    \"\"\"\n    Return True if file has more than max_lines lines.\n    Counted efficiently by reading in binary and summing newline bytes.\n    \"\"\"\n    try:\n        with open(file_path, \"rb\") as f:\n            lines = 0\n            for chunk in iter(lambda: f.read(8192), b\"\"):\n                lines += chunk.count(b\"\\n\")\n                if lines > max_lines:\n                    return True\n    except Exception:\n        # If we can't read the file, be conservative and skip it\n        return True\n    return False\n\n\ndef get_allowed_files(project_root: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    Returns list of (file_path, section_name) for all relevant project files.\n    \"\"\"\n    allowed_files = []\n\n    # Extensions we care about\n    code_exts = {\n        \".py\",\n        \".js\",\n        \".ts\",\n        \".html\",\n        \".css\",\n        \".json\",\n        \".md\",\n        \".bat\",\n        \".ps1\",\n        \".sh\",\n        \".yaml\",\n        \".yml\",\n    }\n\n    # Directories to completely ignore\n    ignored_dirs = {\n        \"context\",\n        \".git\",\n        \".venv\",\n        \"browser_data\",\n        \"archive\",\n        \"__pycache__\",\n        \"node_modules\",\n        \".github\",\n        \"tests\",\n        \"standards\",\n    }\n\n    # Files to ignore (add common package manifests and combined outputs)\n    ignored_files = {\n        \"package-lock.json\",\n        \"package.json\",\n        \"yarn.lock\",\n        \"pnpm-lock.yaml\",\n        \"combined_text.txt\",\n        \"combined_text.json\",\n        \"cozo_lib_wasm_bg.wasm\",\n        \"combined_memory.json\",\n        \"cozo_import_memory.json\",\n    }\n\n    # Collect all .gitignore patterns from the project\n    gitignore_patterns = []\n    for root, dirs, files in os.walk(project_root):\n        # Filter directories in-place to avoid walking into ignored ones\n        dirs[:] = [d for d in dirs if d not in ignored_dirs and not d.startswith(\".\")]\n\n        if \".gitignore\" in files:\n            gitignore_path = os.path.join(root, \".gitignore\")\n            patterns = parse_gitignore(gitignore_path)\n            # Add directory context to relative patterns\n            for pattern in patterns:\n                gitignore_patterns.append((root, pattern))\n\n    for root, dirs, files in os.walk(project_root):\n        # Filter directories in-place to avoid walking into ignored ones\n        dirs[:] = [d for d in dirs if d not in ignored_dirs and not d.startswith(\".\")]\n\n        rel_root = os.path.relpath(root, project_root)\n        section = \"ROOT\" if rel_root == \".\" else rel_root.replace(os.sep, \"_\").upper()\n\n        for f in files:\n            fname_lower = f.lower()\n            # Skip explicitly ignored files and common combined outputs\n            if (\n                fname_lower in ignored_files\n                or \"combined_text\" in fname_lower\n                or \"combined_memory\" in fname_lower\n            ):\n                continue\n\n            ext = os.path.splitext(f)[1].lower()\n            if ext in code_exts:\n                full_path = os.path.join(root, f)\n\n                # Check if file should be ignored based on .gitignore patterns\n                rel_path = os.path.relpath(full_path, project_root).replace(os.sep, \"/\")\n                should_ignore = False\n\n                for gitignore_dir, pattern in gitignore_patterns:\n                    # Calculate relative path from the gitignore directory\n                    rel_to_gitignore = os.path.relpath(\n                        full_path, gitignore_dir\n                    ).replace(os.sep, \"/\")\n\n                    # Handle negation patterns (starting with !)\n                    if pattern.startswith(\"!\"):\n                        negated_pattern = pattern[1:]\n                        if fnmatch.fnmatch(\n                            rel_to_gitignore, negated_pattern\n                        ) or fnmatch.fnmatch(f, negated_pattern):\n                            should_ignore = (\n                                False  # Don't ignore if it matches a negated pattern\n                            )\n                    else:\n                        # Check if the pattern matches the file path\n                        if pattern.startswith(\"/\"):\n                            # Pattern is anchored to the directory containing the .gitignore\n                            if fnmatch.fnmatch(\n                                rel_to_gitignore, pattern[1:]\n                            ) or fnmatch.fnmatch(f, pattern[1:]):\n                                should_ignore = True\n                        else:\n                            # Pattern applies recursively\n                            if fnmatch.fnmatch(\n                                rel_to_gitignore, pattern\n                            ) or fnmatch.fnmatch(f, pattern):\n                                should_ignore = True\n\n                if should_ignore:\n                    print(f\"Skipping '{rel_path}' ‚Äî ignored by .gitignore\")\n                    continue\n\n                # Skip files that are too large (more than 2000 lines)\n                if file_has_too_many_lines(full_path, 2000):\n                    print(\n                        f\"Skipping '{os.path.relpath(full_path, project_root)}' ‚Äî exceeds 2000 lines.\"\n                    )\n                    continue\n                allowed_files.append((full_path, section))\n\n    return allowed_files\n\n\ndef to_yaml_style(obj: Any, indent: int = 0) -> str:\n    \"\"\"\n    Recursively converts a JSON-compatible object to a YAML-like string.\n    \"\"\"\n    lines = []\n    prefix = \"  \" * indent\n\n    if isinstance(obj, dict):\n        for k, v in obj.items():\n            if isinstance(v, (dict, list)):\n                lines.append(f\"{prefix}{k}:\")\n                lines.append(to_yaml_style(v, indent + 1))\n            else:\n                # Handle multiline strings safely\n                v_str = str(v)\n                if \"\\n\" in v_str:\n                    lines.append(f\"{prefix}{k}: |\")\n                    for line in v_str.split(\"\\n\"):\n                        lines.append(f\"{prefix}  {line}\")\n                else:\n                    lines.append(f\"{prefix}{k}: {v}\")\n    elif isinstance(obj, list):\n        for item in obj:\n            if isinstance(item, (dict, list)):\n                lines.append(f\"{prefix}-\")\n                # For list items that are objects, we want the properties to align slightly differently\n                # But for simplicity in this custom dumper:\n                sub = to_yaml_style(item, indent + 1)\n                lines.append(sub)\n            else:\n                lines.append(f\"{prefix}- {item}\")\n    else:\n        return f\"{prefix}{obj}\"\n\n    return \"\\n\".join(lines)\n\n\ndef create_project_corpus(\n    output_file: str | None = None,\n    dry_run: bool = False,\n    max_tokens: int = 200000,  # Limit to under 200k tokens for orchestrator\n):\n    \"\"\"\n    Aggregates content from project files into a single corpus.\n    Limits content to under max_tokens for orchestrator model consumption.\n    Also creates a YAML version of the memory records for easier processing.\n    \"\"\"\n    project_root = find_project_root()\n    output_file = output_file or os.path.join(project_root, \"combined_text.txt\")\n    yaml_output_file = (\n        output_file.replace(\".txt\", \".yaml\")\n        if output_file.endswith(\".txt\")\n        else os.path.join(project_root, \"combined_memory.yaml\")\n    )\n\n    print(f\"Project Root Detected: {project_root}\")\n    allowed_files = get_allowed_files(project_root)\n\n    if not allowed_files:\n        print(f\"No relevant files found in '{project_root}'.\")\n        return\n\n    print(f\"Found {len(allowed_files)} files to process.\")\n\n    if dry_run:\n        print(f\"Dry run enabled ‚Äî would process {len(allowed_files)} files:\")\n        for file_path, section in allowed_files:\n            print(f\"  - {os.path.relpath(file_path, project_root)} ({section})\")\n        return\n\n    memory_records = []\n    total_tokens = 0\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n        # Add a file map at the very top for the orchestrator\n        outfile.write(\"=== PROJECT FILE MAP ===\\n\")\n        for file_path, section in allowed_files:\n            rel_path = os.path.relpath(file_path, project_root)\n            outfile.write(f\"- {rel_path} ({section})\\n\")\n        outfile.write(\"========================\\n\\n\")\n\n        # Write file map to token count\n        file_map_content = \"=== PROJECT FILE MAP ===\\n\"\n        for file_path, section in allowed_files:\n            rel_path = os.path.relpath(file_path, project_root)\n            file_map_content += f\"- {rel_path} ({section})\\n\"\n        file_map_content += \"========================\\n\\n\"\n        total_tokens += count_tokens(file_map_content)\n        print(f\"Token count after file map: {total_tokens}\")\n\n        for file_path, section in allowed_files:\n            rel_path = os.path.relpath(file_path, project_root)\n\n            # Check if we're approaching the token limit\n            if total_tokens >= max_tokens:\n                print(f\"Token limit ({max_tokens}) reached. Stopping file processing.\")\n                break\n\n            print(f\"Processing '{rel_path}'...\")\n            try:\n                with open(file_path, \"rb\") as raw_file:\n                    raw_data = raw_file.read()\n                if not raw_data:\n                    continue\n\n                try:\n                    decoded_content = raw_data.decode(\"utf-8\")\n                except UnicodeDecodeError:\n                    decoded_content = raw_data.decode(\"utf-8\", errors=\"replace\")\n\n                ext = os.path.splitext(file_path)[1].lower()\n                final_content = decoded_content\n\n                # Upgrade: Convert JSON to YAML-like text\n                if ext == \".json\":\n                    try:\n                        json_obj = json.loads(decoded_content)\n                        # Use pretty print json as a reliable fallback or strict yaml style\n                        # The user asked for \"YAML-like string (key: value) or pretty-printed JSON (indent=2)\"\n                        # Let's try our YAML converter first, it's cleaner for reading.\n                        final_content = to_yaml_style(json_obj)\n                    except Exception:\n                        # Fallback to original content if parsing fails\n                        pass\n\n                # Check if adding this file would exceed the token limit\n                file_start_marker = f\"--- START OF FILE: {rel_path} ---\\n\"\n                file_end_marker = f\"--- END OF FILE: {rel_path} ---\\n\\n\"\n\n                # Calculate tokens for this file's content\n                file_tokens = (\n                    count_tokens(file_start_marker)\n                    + count_tokens(final_content)\n                    + count_tokens(file_end_marker)\n                )\n\n                if total_tokens + file_tokens > max_tokens:\n                    print(\n                        f\"Skipping '{rel_path}' - would exceed token limit ({total_tokens + file_tokens} > {max_tokens})\"\n                    )\n                    continue\n\n                outfile.write(file_start_marker)\n                outfile.write(final_content)\n                if not final_content.endswith(\"\\n\"):\n                    outfile.write(\"\\n\")\n                outfile.write(file_end_marker)\n\n                # Update token count\n                total_tokens += file_tokens\n                print(f\"Tokens after '{rel_path}': {total_tokens}/{max_tokens}\")\n\n                # Store for JSON memory export (Node structure)\n                memory_records.append(\n                    {\n                        \"id\": rel_path,\n                        \"timestamp\": int(os.path.getmtime(file_path)),\n                        \"role\": \"file\",\n                        \"content\": final_content,\n                        \"source\": rel_path,\n                    }\n                )\n\n            except Exception as e:\n                print(f\"Error processing '{rel_path}': {e}\")\n\n    # Save the combined memory records for Builder ingestion\n    memory_file = os.path.join(project_root, \"combined_memory.json\")\n    with open(memory_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(memory_records, f, indent=2, ensure_ascii=False)\n    print(f\"Memory records saved to '{memory_file}'.\")\n\n    # Save the combined memory records as YAML for easier processing\n    with open(yaml_output_file, \"w\", encoding=\"utf-8\") as f:\n        # Custom YAML representer for multiline strings\n        def represent_str(dumper, data):\n            if \"\\n\" in data:\n                return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n            return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\n\n        yaml.add_representer(str, represent_str)\n        yaml.dump(\n            memory_records, f, default_flow_style=False, allow_unicode=True, indent=2\n        )\n    print(f\"Memory records saved to '{yaml_output_file}' (YAML format).\")\n\n    print(f\"\\nAggregation complete. Corpus saved to '{output_file}'.\")\n    print(f\"Total tokens in output: {total_tokens}\")\n\n\ndef _parse_cli() -> argparse.Namespace:\n    p = argparse.ArgumentParser(\n        description=\"Aggregate project code and docs for orchestration.\"\n    )\n    p.add_argument(\n        \"--out\",\n        \"-o\",\n        default=None,\n        help=\"Output file path (defaults to combined_text.txt in project root)\",\n    )\n    p.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Show what would be processed without writing the combined file\",\n    )\n    p.add_argument(\n        \"--max-tokens\",\n        \"-t\",\n        type=int,\n        default=100000,\n        help=\"Maximum number of tokens to include in output (default: 100000)\",\n    )\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = _parse_cli()\n    create_project_corpus(\n        output_file=args.out,\n        dry_run=args.dry_run,\n        max_tokens=args.max_tokens,\n    )\n",
    "source": "read_all.py"
  },
  {
    "id": "start-low-resource.bat",
    "timestamp": 1767254282,
    "role": "file",
    "content": "@echo off\r\necho üì± STARTING ANCHOR SYSTEM IN LOW-RESOURCE MODE...\r\n\r\necho üìã Configuration:\r\necho   - GPU Buffer: 64MB (conservative)\r\necho   - Single-threaded operations\r\necho   - Small model defaults (Phi-3.5-mini)\r\necho   - Reduced cache sizes\r\necho   - Longer timeouts for stability\r\n\r\nREM Set environment variables for low-resource mode\r\nset LOW_RESOURCE_MODE=true\r\n\r\nREM 1. Start the Unified Server with low-resource settings\r\necho Starting Anchor Core...\r\nstart \"Anchor Core\" /min cmd /c \"cd tools && set LOW_RESOURCE_MODE=true && python webgpu_bridge.py\"\r\n\r\nREM 2. Wait for Server to initialize\r\necho Waiting for server to initialize...\r\ntimeout /t 5 /nobreak >nul\r\n\r\nREM 3. Launch the Ghost Engine with conservative GPU settings (FIXED: JavaScript Enabled)\r\necho üëª Launching Ghost Engine...\r\nstart \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost --max-active-webgl-contexts=1 --max-webgl-contexts-per-group=1 --disable-gpu-memory-buffer-compositor-resources --force-gpu-mem-available-mb=64 --force-low-power-gpu --disable-gpu-driver-workarounds --disable-gpu-sandbox --disable-features=VizDisplayCompositor --disable-gpu-memory-buffer-video-frames --disable-gpu-memory-buffer-compositor-resources\"\r\n\r\necho.\r\necho ‚úÖ Anchor System Started in Low-Resource Mode\r\necho    Open http://localhost:8000 in your browser when ready\r\necho    Services will start automatically when you access the UI\r\necho.\r\necho üí° Tips for low-resource devices:\r\necho   - Use Phi-3.5-mini or smaller models\r\necho   - Expect slower response times\r\necho   - Close other GPU-intensive applications\r\necho   - Consider using CPU-only mode if GPU crashes persist",
    "source": "start-low-resource.bat"
  },
  {
    "id": "docs\\low_resource_mode.md",
    "timestamp": 1767254282,
    "role": "file",
    "content": "# Low-Resource Mode for Anchor Core\r\n\r\nThis guide explains how to optimize the Anchor Core for phones, small laptops, and other low-resource devices.\r\n\r\n## Environment Variables\r\n\r\nThe system supports two environment variables for optimization:\r\n\r\n### `LOW_RESOURCE_MODE`\r\n- Set to `true` to enable conservative settings for low-resource devices\r\n- Reduces GPU buffer size to 64MB\r\n- Limits concurrent operations to 1\r\n- Uses smaller models and reduced context windows\r\n\r\n### `CPU_ONLY_MODE`\r\n- Set to `true` to force CPU-only processing (no GPU)\r\n- Useful when GPU is unavailable or causing crashes\r\n- Slower but more stable on constrained hardware\r\n\r\n## Setting Environment Variables\r\n\r\n### Windows Command Prompt:\r\n```cmd\r\nset LOW_RESOURCE_MODE=true\r\nstart-anchor.bat\r\n```\r\n\r\n### Windows PowerShell:\r\n```powershell\r\n$env:LOW_RESOURCE_MODE=\"true\"\r\n.\\start-anchor.bat\r\n```\r\n\r\n### Linux/Mac:\r\n```bash\r\nexport LOW_RESOURCE_MODE=true\r\n./start-anchor.sh\r\n```\r\n\r\n## Conservative Settings Applied\r\n\r\nWhen `LOW_RESOURCE_MODE` is enabled, the system applies:\r\n\r\n- **GPU Buffer**: 64MB (vs 256MB default)\r\n- **Model**: Phi-3.5-mini (smallest recommended)\r\n- **Context Window**: 2048 tokens (vs 4096+ default)\r\n- **Batch Size**: 1 (vs 4+ default)\r\n- **WebGL Contexts**: 1 max (vs 16+ default)\r\n- **Cache Size**: 128MB (vs 1GB+ default)\r\n- **Timeouts**: 120 seconds (vs 30s default)\r\n\r\n## For Phones and Tablets\r\n\r\nFor mobile devices, use both settings:\r\n```cmd\r\nset LOW_RESOURCE_MODE=true\r\nset CPU_ONLY_MODE=true\r\nstart-anchor.bat\r\n```\r\n\r\n## Model Recommendations for Low-Resource Devices\r\n\r\n- `Phi-3.5-mini-instruct-q4f16_1-MLC` - Smallest recommended model\r\n- `Qwen2-0.5B-Instruct-q4f16_1-MLC` - If available, even smaller\r\n- Avoid models > 1.5B parameters on devices with < 1GB VRAM\r\n\r\n## Troubleshooting\r\n\r\n### GPU Crashes\r\n- Enable `LOW_RESOURCE_MODE=true`\r\n- Consider `CPU_ONLY_MODE=true` for stability\r\n\r\n### Slow Performance\r\n- Use the smallest available models\r\n- Reduce context window size\r\n- Close other GPU-intensive applications\r\n\r\n### Memory Issues\r\n- Enable conservative memory settings\r\n- Clear browser cache regularly\r\n- Use single-threaded mode",
    "source": "docs\\low_resource_mode.md"
  },
  {
    "id": "docs\\sidecar_vision_guide.md",
    "timestamp": 1767254282,
    "role": "file",
    "content": "# Anchor Core: Browser-Based Control Center & Vision Integration\r\n\r\n## Overview\r\n\r\nThe Anchor Core now includes a browser-based control center that provides unified access to system functionality through two main interfaces:\r\n\r\n1. **Sidecar Dashboard** (`http://localhost:8000/sidecar`) - Dual-tab interface for context retrieval and vision processing\r\n2. **Context UI** (`http://localhost:8000/context`) - Manual context retrieval with scrollable display and copy functionality\r\n\r\n## Components\r\n\r\n### Vision Engine (`tools/vision_engine.py`)\r\n- Python-powered Vision Language Model (VLM) integration\r\n- Currently configured for Ollama backend with LLaVA model\r\n- Handles image analysis and converts to text descriptions for memory storage\r\n\r\n### Sidecar Dashboard (`tools/sidecar.html`)\r\n- **Retrieve Tab**: Query the memory graph and retrieve context\r\n- **Vision Tab**: Drag-and-drop image processing with VLM analysis\r\n- Real-time processing logs\r\n\r\n### Context UI (`tools/context.html`)\r\n- Manual context retrieval interface\r\n- Scrollable text display for reviewing context\r\n- One-click copy functionality for pasting into other tools\r\n\r\n## Setup\r\n\r\n### Prerequisites\r\n1. Ensure Ollama is installed and running:\r\n   ```bash\r\n   ollama serve\r\n   ```\r\n\r\n2. Pull a vision model (e.g., LLaVA):\r\n   ```bash\r\n   ollama pull llava\r\n   ```\r\n\r\n### Launch\r\n1. Start the Anchor Core:\r\n   ```bash\r\n   start-anchor.bat\r\n   ```\r\n\r\n2. Access the interfaces:\r\n   - Sidecar: `http://localhost:8000/sidecar`\r\n   - Context UI: `http://localhost:8000/context`\r\n\r\n## Usage\r\n\r\n### Context Retrieval\r\n1. Open `http://localhost:8000/context`\r\n2. Enter a query in the search field (e.g., \"Project Specs\")\r\n3. Click \"Fetch Context\" to retrieve relevant information\r\n4. Review the context in the scrollable text area\r\n5. Click \"üìã Copy to Clipboard\" to copy the context for use elsewhere\r\n\r\n### Vision Processing\r\n1. Open `http://localhost:8000/sidecar`\r\n2. Go to the \"Vision\" tab\r\n3. Drag and drop an image or click to upload\r\n4. The image will be processed by the VLM\r\n5. Results will be stored in the memory graph automatically\r\n\r\n### Memory Search\r\n1. Use the \"Retrieve\" tab in the sidecar\r\n2. Enter your query and click \"Fetch Context\"\r\n3. Copy the results to use in other applications\r\n\r\n## Endpoints\r\n\r\n- `GET /sidecar` - Serve the sidecar dashboard\r\n- `GET /context` - Serve the context UI\r\n- `POST /v1/vision/ingest` - Process uploaded images with VLM\r\n- `POST /v1/memory/search` - Search the memory graph\r\n\r\n## Architecture\r\n\r\nThe system follows a unified architecture where:\r\n- The WebGPU Bridge (`webgpu_bridge.py`) serves UI files and orchestrates components\r\n- Vision processing happens in Python via the Vision Engine\r\n- Memory storage uses the graph database\r\n- Communication between components happens via WebSockets",
    "source": "docs\\sidecar_vision_guide.md"
  },
  {
    "id": "extension\\background.js",
    "timestamp": 1767462987,
    "role": "file",
    "content": "chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n  if (request.action === 'queryMemories') {\n    queryMemoriesFromCozoDB(request.query)\n      .then(memories => ({\n        success: true,\n        memories: memories,\n        summary: generateSummary(memories)\n      }))\n      .then(result => sendResponse(result))\n      .catch(err => {\n        console.error('[Sovereign] Error querying memories:', err);\n        sendResponse({\n          success: false,\n          error: err.message,\n          summary: null\n        });\n      });\n    return true; // Keep channel open for async response\n  }\n});\n\nasync function queryMemoriesFromCozoDB(userInput) {\n  try {\n    // Attempt to hit the Local Bridge (Node.js Sovereign Engine)\n    // The bridge now runs on port 3000 as part of the Node.js monolith\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), 10000); // 10 second timeout\n\n    // Try the new Node.js server on port 3000\n    const port = 3000;\n    let response = null;\n    let lastError = null;\n\n    try {\n      // First, try to ping the health endpoint to see if there's a bridge running\n      const healthUrl = `http://localhost:${port}/health`;\n      let healthResponse;\n\n      try {\n        healthResponse = await fetch(healthUrl, {\n          method: 'GET',\n          signal: controller.signal\n        });\n      } catch (healthError) {\n        console.log(`[Sovereign] Health check failed for port ${port}:`, healthError.message);\n        throw healthError;\n      }\n\n      if (!healthResponse.ok) {\n        console.log(`[Sovereign] Health check failed for port ${port}, status: ${healthResponse.status}`);\n        throw new Error(`Health check failed with status: ${healthResponse.status}`);\n      }\n\n      console.log(`[Sovereign] Bridge detected on port ${port}, testing query...`);\n\n      // Now try the query endpoint (the new Node.js endpoint)\n      const queryUrl = `http://localhost:${port}/v1/query`;\n\n      // Try the query without authentication (Node.js server is open)\n      response = await fetch(queryUrl, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          query: userInput, // This should be a CozoDB query string\n          params: {}\n        }),\n        signal: controller.signal\n      });\n\n      if (response.ok) {\n        console.log(`[Sovereign] Successfully connected to bridge at port ${port}`);\n        clearTimeout(timeoutId);\n        return response.json();\n      } else {\n        console.log(`[Sovereign] Query failed with status: ${response.status}`);\n        throw new Error(`Query failed with status: ${response.status}`);\n      }\n    } catch (e) {\n      lastError = e;\n      console.log(`[Sovereign] Trying bridge port ${port} failed:`, e.message);\n    }\n\n    // If all ports failed, use the last error\n    clearTimeout(timeoutId);\n    throw lastError || new Error(`Node.js bridge on port 3000 unavailable. Is the Sovereign Engine running?`);\n  } catch (e) {\n    console.warn('[Sovereign] Backend unavailable, falling back to simulated response...', e.message);\n    // In the future, this can connect directly to IndexedDB via shared worker\n    // For now, return simulated data\n    return [\n      {\n        content: \"This is a simulated memory based on your input: \" + userInput.substring(0, 100) + \"...\",\n        timestamp: new Date().toISOString(),\n        relevance: 0.8\n      }\n    ];\n  }\n}\n\nfunction generateSummary(memories) {\n  if (!memories || memories.length === 0) return null;\n\n  const maxMemories = 3;\n  const relevant = memories.slice(0, maxMemories);\n\n  return relevant\n    .map((m, idx) => `[Memory ${idx + 1}] ${m.content.substring(0, 150)}...`)\n    .join('\\n');\n}",
    "source": "extension\\background.js"
  },
  {
    "id": "extension\\content.js",
    "timestamp": 1767463066,
    "role": "file",
    "content": "// Platform-specific DOM selectors\nconst SELECTORS = {\n    'gemini.google.com': 'div[contenteditable=\"true\"], textarea',\n    'chatgpt.openai.com': 'textarea, div[contenteditable=\"true\"]'\n};\n\nlet textArea = null;\nlet inputTimeout = null;\nconst PAUSE_THRESHOLD = 3000; // 3 seconds\n\n// 1. Detect the active text input\nfunction detectTextArea() {\n    const domain = window.location.hostname;\n    const selector = SELECTORS[domain];\n    if (!selector) return null;\n    return document.querySelector(selector);\n}\n\n// 2. Extract text from the input\nfunction getVisibleText() {\n    if (!textArea) return \"\";\n    return textArea.value || textArea.textContent || \"\";\n}\n\n// 3. Monitor for user pauses\nfunction setupPauseDetector() {\n    if (!textArea) return;\n\n    textArea.addEventListener('input', () => {\n        clearTimeout(inputTimeout);\n        inputTimeout = setTimeout(() => {\n            const text = getVisibleText();\n            if (text.length > 10) { // Only query if meaningful text exists\n                console.log('[Sovereign] 3-second pause detected, querying memories...');\n                chrome.runtime.sendMessage(\n                    { action: 'queryMemories', query: text },\n                    (response) => {\n                        if (response && response.success) injectContext(response);\n                    }\n                );\n            }\n        }, PAUSE_THRESHOLD);\n    });\n}\n\n// 4. Inject the retrieved context\nasync function injectContext(contextData) {\n    if (!contextData.summary) return;\n\n    const timestamp = new Date().toLocaleTimeString();\n    const summary = `\\n\\n[Sovereign Context Injection at ${timestamp}]\\n${contextData.summary}\\n---\\n`;\n\n    // For contenteditable (Gemini/modern apps)\n    if (textArea.isContentEditable || textArea.getAttribute('contenteditable') === 'true') {\n        // Simple append - in production this might need Range/Selection manipulation for cursors\n        textArea.textContent = textArea.textContent + summary;\n    }\n    // For standard textarea (ChatGPT legacy)\n    else {\n        textArea.value += summary;\n    }\n\n    // Also send the current text content to the server for ingestion\n    try {\n        const currentText = getVisibleText();\n        if (currentText && currentText.length > 10) { // Only send if meaningful content exists\n            const response = await fetch('http://localhost:3000/v1/ingest', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json'\n                },\n                body: JSON.stringify({\n                    content: currentText,\n                    filename: `extension_capture_${Date.now()}.txt`,\n                    source: `extension://${window.location.hostname}`\n                })\n            });\n\n            if (response.ok) {\n                console.log('[Sovereign] Content successfully sent for ingestion');\n            } else {\n                console.warn('[Sovereign] Ingestion request failed:', response.status);\n            }\n        }\n    } catch (e) {\n        console.warn('[Sovereign] Error sending content for ingestion:', e.message);\n    }\n\n    // Notify user\n    displayIndicator('\\u2713 Context injected', 'success'); // Using Unicode checkmark\n}\n\n// 5. UI Feedback\nfunction displayIndicator(message, type) {\n    let indicator = document.getElementById('sovereign-indicator');\n    if (!indicator) {\n        indicator = document.createElement('div');\n        indicator.id = 'sovereign-indicator';\n        indicator.style.position = 'fixed';\n        indicator.style.bottom = '20px';\n        indicator.style.right = '20px';\n        indicator.style.padding = '10px 15px';\n        indicator.style.borderRadius = '5px';\n        indicator.style.zIndex = '9999';\n        indicator.style.fontFamily = 'monospace';\n        document.body.appendChild(indicator);\n    }\n\n    indicator.textContent = message;\n    indicator.style.background = type === 'success' ? '#238636' : '#da3633';\n    indicator.style.color = '#ffffff';\n\n    setTimeout(() => indicator.remove(), 5000);\n}\n\n// Handle messages from popup\nchrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n    if (request.action === 'testInjection') {\n        // For testing purposes, inject a sample context\n        const testData = {\n            summary: \"This is a test injection from Sovereign Context Bridge.\\n\\n[Sample Memory] Example context for testing purposes...\"\n        };\n        injectContext(testData);\n        sendResponse({ success: true });\n        return true; // Keep channel open for async response\n    }\n});\n\n// --- 6. Robust Initialization ---\nfunction startSovereignObserver() {\n    // Safety Check: If body isn't ready, wait for next frame\n    if (!document.body) {\n        console.warn(\"[Sovereign] document.body not ready, retrying...\");\n        requestAnimationFrame(startSovereignObserver);\n        return;\n    }\n\n    console.log(\"[Sovereign] Body detected. Eyes opening...\");\n\n    // Main Logic\n    if (!textArea) {\n        textArea = detectTextArea();\n        if (textArea) {\n            console.log(\"[Sovereign] Input Found on Init.\");\n            setupPauseDetector();\n        }\n    }\n\n    // Watch for dynamic changes\n    const observer = new MutationObserver(() => {\n        if (!textArea) {\n            textArea = detectTextArea();\n            if (textArea) console.log(\"[Sovereign] Input Found via Mutation.\");\n        }\n    });\n\n    observer.observe(document.body, { childList: true, subtree: true });\n}\n\n// Start only when DOM is ready\nif (document.readyState === 'loading') {\n    document.addEventListener('DOMContentLoaded', startSovereignObserver);\n} else {\n    startSovereignObserver();\n}\n\n// --- DEBUG: CLICK-TO-LOG REFLEX ---\n// This allows us to manually verify what the extension sees when we touch the UI.\ndocument.addEventListener('click', (event) => {\n    const target = event.target;\n    const detected = detectTextArea();\n\n    console.group(\"üëÅÔ∏è [Sovereign Debug] Retina Scan\");\n    console.log(\"üñ±Ô∏è Clicked Element:\", target);\n    console.log(\"üè∑Ô∏è Clicked Class:\", target.className);\n\n    if (detected) {\n        console.log(\"%c‚úÖ Active Input Detected:\", \"color:green;font-weight:bold\", detected);\n        console.log(\"üìù Current Value:\", detected.value || detected.innerText || detected.textContent);\n    } else {\n        console.log(\"%c‚ùå No Input Detected via Selector\", \"color:red;font-weight:bold\");\n        console.log(\"üîç Current Selector for Domain:\", SELECTORS[window.location.hostname] || \"NONE\");\n    }\n    console.groupEnd();\n});",
    "source": "extension\\content.js"
  },
  {
    "id": "extension\\manifest.json",
    "timestamp": 1767463114,
    "role": "file",
    "content": "manifest_version: 3\nname: Sovereign Context Bridge\nversion: 1.0.2\ndescription: Silent context injection for LLM conversations\npermissions:\n  - activeTab\n  - scripting\n  - storage\n  - webRequest\nhost_permissions:\n  - *://gemini.google.com/*\n  - *://chatgpt.com/*\n  - http://localhost/*\n  - http://127.0.0.1/*\n  - http://localhost:3000/*\ncontent_scripts:\n  -\n    matches:\n      - *://gemini.google.com/*\n      - *://chatgpt.com/*\n    js:\n      - content.js\n    run_at: document_idle\nbackground:\n  service_worker: background.js\naction:\n  default_popup: popup.html\n  default_title: Sovereign Context Bridge",
    "source": "extension\\manifest.json"
  },
  {
    "id": "extension\\popup.html",
    "timestamp": 1766591401,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <meta charset=\"UTF-8\">\r\n  <style>\r\n    body { background: #0f1115; color: #e2e8f0; font-family: sans-serif; width: 250px; padding: 15px; }\r\n    h3 { margin-top: 0; color: #00ff88; font-weight: 300; border-bottom: 1px solid #333; padding-bottom: 10px; }\r\n    .stat-row { display: flex; justify-content: space-between; margin-bottom: 8px; font-size: 0.9rem; }\r\n    .val { font-weight: bold; color: #58a6ff; }\r\n    button { width: 100%; padding: 8px; margin-top: 10px; background: #2d2d2d; border: 1px solid #444; color: #fff; cursor: pointer; border-radius: 4px; }\r\n    button:hover { background: #333; border-color: #00ff88; }\r\n    .status-ok { color: #00ff88; }\r\n    .status-err { color: #ff4444; }\r\n  </style>\r\n</head>\r\n<body>\r\n  <h3>&#129300 Sovereign Bridge</h3>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Status:</span>\r\n    <span id=\"status-badge\" class=\"status-err\">&#9679; Offline</span>\r\n  </div>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Memories:</span>\r\n    <span id=\"mem-count\" class=\"val\">0</span>\r\n  </div>\r\n\r\n  <div class=\"stat-row\">\r\n    <span>Last Inject:</span>\r\n    <span id=\"last-inject\" class=\"val\">-</span>\r\n  </div>\r\n\r\n  <button id=\"settings-btn\">&#9881;&#65039 Settings</button>\r\n  <button id=\"test-inject-btn\">&#129512 Test Injection</button>\r\n\r\n  <script src=\"popup.js\"></script>\r\n</body>\r\n</html>",
    "source": "extension\\popup.html"
  },
  {
    "id": "extension\\popup.js",
    "timestamp": 1766591401,
    "role": "file",
    "content": "document.addEventListener('DOMContentLoaded', async () => {\r\n    const statusBadge = document.getElementById('status-badge');\r\n\r\n    // Check connection to Local Bridge\r\n    try {\r\n        // Using a more robust approach to handle CORS issues\r\n        const controller = new AbortController();\r\n        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout\r\n\r\n        const res = await fetch('http://localhost:8080/health', {\r\n            signal: controller.signal,\r\n            mode: 'cors', // Explicitly set CORS mode\r\n            credentials: 'omit' // Don't send credentials\r\n        });\r\n\r\n        clearTimeout(timeoutId);\r\n\r\n        if (res.ok) {\r\n            statusBadge.textContent = \"‚óè Online\";\r\n            statusBadge.className = \"status-ok\";\r\n        } else {\r\n            statusBadge.textContent = \"‚óè Offline\";\r\n            statusBadge.className = \"status-err\";\r\n        }\r\n    } catch (e) {\r\n        // Handle network errors, CORS errors, and timeouts\r\n        console.warn('[Sovereign] Backend connection failed:', e.message);\r\n        statusBadge.textContent = \"‚óè Offline\";\r\n        statusBadge.className = \"status-err\";\r\n    }\r\n\r\n    document.getElementById('test-inject-btn').addEventListener('click', () => {\r\n        // Trigger manual test injection\r\n        chrome.tabs.query({active: true, currentWindow: true}, (tabs) => {\r\n            chrome.tabs.sendMessage(tabs[0].id, { action: 'testInjection' }, (response) => {\r\n                if (chrome.runtime.lastError) {\r\n                    console.log('[Sovereign] Test injection not available on this page');\r\n                } else {\r\n                    console.log('[Sovereign] Test injection triggered');\r\n                }\r\n            });\r\n        });\r\n    });\r\n\r\n    // Add settings button functionality\r\n    document.getElementById('settings-btn').addEventListener('click', () => {\r\n        // For now, just show a message - in the future this could open options page\r\n        alert('Sovereign Context Bridge Settings\\n\\nConfigure extension preferences here.');\r\n    });\r\n});",
    "source": "extension\\popup.js"
  },
  {
    "id": "extension\\test_connection.html",
    "timestamp": 1767463024,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Extension Server Connection Test</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n            max-width: 600px;\n            margin: 50px auto;\n            padding: 20px;\n            background-color: #f5f5f5;\n        }\n        .container {\n            background: white;\n            padding: 30px;\n            border-radius: 8px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        button {\n            background-color: #007bff;\n            color: white;\n            border: none;\n            padding: 12px 24px;\n            border-radius: 4px;\n            cursor: pointer;\n            font-size: 16px;\n        }\n        button:hover {\n            background-color: #0056b3;\n        }\n        .result {\n            margin-top: 20px;\n            padding: 15px;\n            border-radius: 4px;\n            display: none;\n        }\n        .success {\n            background-color: #d4edda;\n            color: #155724;\n            border: 1px solid #c3e6cb;\n        }\n        .error {\n            background-color: #f8d7da;\n            color: #721c24;\n            border: 1px solid #f5c6cb;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Extension Server Connection Test</h1>\n        <p>Click the button below to test if the extension can connect to the Sovereign Engine server.</p>\n        \n        <button id=\"ping\">Ping Server</button>\n        \n        <div id=\"result\" class=\"result\"></div>\n    </div>\n\n    <script>\n        document.getElementById('ping').onclick = async () => {\n            const resultDiv = document.getElementById('result');\n            resultDiv.style.display = 'none';\n            \n            try {\n                const res = await fetch('http://localhost:3000/health');\n                const data = await res.json();\n                \n                resultDiv.className = 'result success';\n                resultDiv.innerHTML = `<strong>Success!</strong><br>Server Status: ${JSON.stringify(data, null, 2)}`;\n                resultDiv.style.display = 'block';\n            } catch (e) {\n                resultDiv.className = 'result error';\n                resultDiv.innerHTML = `<strong>Connection Failed:</strong><br>${e.message}`;\n                resultDiv.style.display = 'block';\n            }\n        };\n    </script>\n</body>\n</html>",
    "source": "extension\\test_connection.html"
  },
  {
    "id": "extension\\images\\README.md",
    "timestamp": 1766591401,
    "role": "file",
    "content": "# Extension Icons\r\n\r\nThis directory contains the following icon files:\r\n\r\n- `icon-16.png` - 16x16 pixel icon (minimal valid PNG)\r\n- `icon-32.png` - 32x32 pixel icon (minimal valid PNG)\r\n- `icon-128.png` - 128x128 pixel icon (minimal valid PNG)\r\n\r\nThese icons represent the Sovereign Context Bridge extension.\r\n\r\nNote: The current files are minimal valid PNGs for extension loading. Replace with actual designed icons for production use.",
    "source": "extension\\images\\README.md"
  },
  {
    "id": "logs\\README.md",
    "timestamp": 1767254282,
    "role": "file",
    "content": "# Logs Directory\r\n\r\nThis directory contains individual log files for each system component to facilitate debugging and monitoring.\r\n\r\n## Log File Naming Convention\r\n\r\nEach component writes to its own log file named after the source:\r\n\r\n- `system.log` - System startup and general operations\r\n- `chat_api.log` - Chat API requests and responses\r\n- `memory_api.log` - Memory search API operations\r\n- `websocket_bridge.log` - WebSocket connection events\r\n- `python_stdout.log` - Python standard output\r\n- `python_stderr.log` - Python standard error\r\n\r\n## Log Rotation\r\n\r\nEach log file is automatically truncated to keep only the last 1000 lines to prevent excessive disk usage.\r\n\r\n## Log Format\r\n\r\nEach log entry follows this format:\r\n```\r\n[YYYY-MM-DD HH:MM:SS] [LEVEL] Message content\r\n```\r\n\r\nWhere LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG\r\n\r\n## Accessing Logs\r\n\r\n- **Real-time viewing**: Use the log viewer at `http://localhost:8000/log-viewer.html`\r\n- **File access**: Individual log files are available in this directory\r\n- **API access**: Recent logs available via `/logs/recent` endpoint",
    "source": "logs\\README.md"
  },
  {
    "id": "scripts\\CHANGELOG.md",
    "timestamp": 1766774826,
    "role": "file",
    "content": "# Changelog\r\n\r\nAll notable changes to the `scripts/` module will be documented in this file.\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **Smart GPU Bridge**: Added `StaticFiles` mount at `/models` to serve local model artifacts.\r\n- **On-Demand Downloads**: Added `POST /v1/models/pull` and `GET /v1/models/pull/status` to handle server-side model downloading from Hugging Face.\r\n- **Shared Module**: Integrated `scripts.download_models` for reusable download logic.\r\n\r\n### Fixed\r\n- **CORS/Auth Collision**: Reordered `CORSMiddleware` to wrap the entire application (including Auth middleware) to ensure CORS headers are sent even on 401 Unauthorized responses.\r\n- **Authentication**: Exempted `/models` path from Token Verification to allow browser-side fetching of artifacts without credentials.\r\n- **Import Error**: Fixed `ModuleNotFoundError` by changing import to `from download_models import ...` for direct script execution.\r\n",
    "source": "scripts\\CHANGELOG.md"
  },
  {
    "id": "scripts\\download_models.py",
    "timestamp": 1767215198,
    "role": "file",
    "content": "\r\nimport os\r\nimport json\r\nimport requests\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n# Configuration\r\nMODELS_DIR = Path(\"models\").resolve()\r\nHF_ENDPOINT = \"https://huggingface.co\"\r\n\r\ndef download_file(url, dest_path, progress_callback=None):\r\n    \"\"\"Download a file with progress indication\"\"\"\r\n    if dest_path.exists():\r\n        if progress_callback: progress_callback(f\"Skipping {dest_path.name} (exists)\", 1.0)\r\n        return\r\n\r\n    if progress_callback: progress_callback(f\"Downloading {dest_path.name}...\", 0.0)\r\n    \r\n    try:\r\n        response = requests.get(url, stream=True)\r\n        response.raise_for_status()\r\n        \r\n        total_size = int(response.headers.get('content-length', 0))\r\n        block_size = 8192\r\n        wrote = 0\r\n        \r\n        with open(dest_path, 'wb') as f:\r\n            for chunk in response.iter_content(chunk_size=block_size):\r\n                f.write(chunk)\r\n                wrote += len(chunk)\r\n                # Optional: detailed progress\r\n        \r\n        if progress_callback: progress_callback(f\"Saved {dest_path.name}\", 1.0)\r\n        \r\n    except Exception as e:\r\n        if progress_callback: progress_callback(f\"Error {dest_path.name}: {e}\", 0.0)\r\n        raise e\r\n\r\ndef download_model(model_id, repo_url=None, base_dir=None, progress_callback=None):\r\n    \"\"\"\r\n    Downloads an MLC model from Hugging Face.\r\n    \r\n    Args:\r\n        model_id (str): The ID of the model (e.g. \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\")\r\n        repo_url (str, optional): Full HF URL. Defaults to constructing from model_id.\r\n        base_dir (Path, optional): Directory to store models. Defaults to ./models\r\n        progress_callback (func, optional): Function(msg, progress_float)\r\n    \"\"\"\r\n    if base_dir is None:\r\n        base_dir = MODELS_DIR\r\n    \r\n    base_dir.mkdir(exist_ok=True)\r\n    \r\n    # Handle model_id / repo_url\r\n    if not repo_url:\r\n        repo_url = f\"{HF_ENDPOINT}/mlc-ai/{model_id}\"\r\n    \r\n    # Strip prefix from model_id for directory name\r\n    dir_name = model_id.split(\"/\")[-1]\r\n    model_dir = base_dir / dir_name\r\n    model_dir.mkdir(exist_ok=True)\r\n    \r\n    if progress_callback: progress_callback(f\"Starting download for {dir_name}\", 0.0)\r\n\r\n    # 1. Download ndarray-cache.json\r\n    cache_url = f\"{repo_url}/resolve/main/ndarray-cache.json\"\r\n    cache_path = model_dir / \"ndarray-cache.json\"\r\n    \r\n    try:\r\n        download_file(cache_url, cache_path, progress_callback)\r\n    except Exception as e:\r\n        print(f\"‚ùå Failed to fetch ndarray-cache.json: {e}\")\r\n        raise e\r\n\r\n    # 2. Parse cache\r\n    with open(cache_path, 'r') as f:\r\n        cache_data = json.load(f)\r\n        \r\n    records = cache_data.get(\"records\", [])\r\n    total_files = len(records) + 5\r\n    completed = 1\r\n\r\n    # 3. Download Shards\r\n    for record in records:\r\n        # Check both keys for safety (older MLC mappings used 'name')\r\n        file_name = record.get(\"dataPath\", record.get(\"name\"))\r\n        if not file_name:\r\n            continue\r\n            \r\n        url = f\"{repo_url}/resolve/main/{file_name}\"\r\n        dest = model_dir / file_name\r\n        \r\n        download_file(url, dest)\r\n        \r\n        completed += 1\r\n        if progress_callback: \r\n            progress_callback(f\"Downloading {file_name}\", completed/total_files)\r\n\r\n    # 4. Download Configs\r\n    config_files = [\"mlc-chat-config.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\", \"merges.txt\"]\r\n    for fname in config_files:\r\n        url = f\"{repo_url}/resolve/main/{fname}\"\r\n        dest = model_dir / fname\r\n        try:\r\n            download_file(url, dest)\r\n        except:\r\n            pass # Optional\r\n        \r\n        completed += 1\r\n        if progress_callback: \r\n            progress_callback(f\"Checked {fname}\", completed/total_files)\r\n\r\n    if progress_callback: progress_callback(\"Download Complete\", 1.0)\r\n    print(f\"Serve at: http://localhost:8080/models/{dir_name}\")\r\n\r\ndef main():\r\n    # Default behavior: Download Qwen2.5-Coder-1.5B\r\n    default_model = \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\"\r\n    \r\n    def print_progress(msg, p):\r\n        print(f\"[{int(p*100)}%] {msg}\")\r\n\r\n    download_model(default_model, progress_callback=print_progress)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "source": "scripts\\download_models.py"
  },
  {
    "id": "scripts\\gpu_manager.py",
    "timestamp": 1766591401,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nGPU Resource Manager for ECE_Core\r\nProvides utilities to monitor and manage GPU locks in the WebGPU bridge\r\n\"\"\"\r\n\r\nimport requests\r\nimport json\r\nimport time\r\nimport argparse\r\nfrom typing import Dict, Any\r\n\r\nclass GPUResourceManager:\r\n    def __init__(self, bridge_url: str = \"http://localhost:8080\"):\r\n        self.bridge_url = bridge_url\r\n        self.headers = {\"Authorization\": \"Bearer sovereign-secret\"}\r\n    \r\n    def get_status(self) -> Dict[str, Any]:\r\n        \"\"\"Get current GPU status\"\"\"\r\n        try:\r\n            response = requests.get(f\"{self.bridge_url}/v1/gpu/status\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                return response.json()\r\n            else:\r\n                print(f\"Error getting status: {response.status_code} - {response.text}\")\r\n                return {}\r\n        except Exception as e:\r\n            print(f\"Error connecting to bridge: {e}\")\r\n            return {}\r\n    \r\n    def reset_lock(self) -> bool:\r\n        \"\"\"Reset the current GPU lock\"\"\"\r\n        try:\r\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/reset\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                print(\"‚úÖ GPU lock reset successfully\")\r\n                return True\r\n            else:\r\n                print(f\"‚ùå Failed to reset GPU lock: {response.status_code} - {response.text}\")\r\n                return False\r\n        except Exception as e:\r\n            print(f\"‚ùå Error resetting GPU lock: {e}\")\r\n            return False\r\n    \r\n    def force_release_all(self) -> bool:\r\n        \"\"\"Force release all GPU locks (emergency)\"\"\"\r\n        try:\r\n            response = requests.post(f\"{self.bridge_url}/v1/gpu/force-release-all\", headers=self.headers)\r\n            if response.status_code == 200:\r\n                print(\"‚úÖ All GPU locks force released successfully\")\r\n                return True\r\n            else:\r\n                print(f\"‚ùå Failed to force release GPU locks: {response.status_code} - {response.text}\")\r\n                return False\r\n        except Exception as e:\r\n            print(f\"‚ùå Error force releasing GPU locks: {e}\")\r\n            return False\r\n    \r\n    def monitor(self, interval: int = 5):\r\n        \"\"\"Monitor GPU status continuously\"\"\"\r\n        print(f\"üìä Monitoring GPU status every {interval}s (Ctrl+C to stop)\")\r\n        try:\r\n            while True:\r\n                status = self.get_status()\r\n                if status:\r\n                    locked = status.get('locked', False)\r\n                    owner = status.get('owner', 'None')\r\n                    queue_depth = status.get('queue_depth', 0)\r\n                    queued = status.get('queued', [])\r\n                    \r\n                    status_str = f\"GPU: {'LOCKED' if locked else 'FREE'}\"\r\n                    if locked:\r\n                        status_str += f\" by {owner}\"\r\n                    if queue_depth > 0:\r\n                        status_str += f\" | Queue: {queue_depth} | Queued: {', '.join(queued) if queued else 'None'}\"\r\n                    \r\n                    print(f\"[{time.strftime('%H:%M:%S')}] {status_str}\")\r\n                else:\r\n                    print(f\"[{time.strftime('%H:%M:%S')}] ‚ùå Unable to get GPU status\")\r\n                \r\n                time.sleep(interval)\r\n        except KeyboardInterrupt:\r\n            print(\"\\n‚èπÔ∏è  Monitoring stopped\")\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"GPU Resource Manager for ECE_Core\")\r\n    parser.add_argument(\"--bridge-url\", default=\"http://localhost:8080\", \r\n                       help=\"WebGPU bridge URL (default: http://localhost:8080)\")\r\n    parser.add_argument(\"--status\", action=\"store_true\", help=\"Get current GPU status\")\r\n    parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset GPU lock\")\r\n    parser.add_argument(\"--force-release\", action=\"store_true\", help=\"Force release all GPU locks\")\r\n    parser.add_argument(\"--monitor\", action=\"store_true\", help=\"Monitor GPU status continuously\")\r\n    parser.add_argument(\"--interval\", type=int, default=5, help=\"Monitor interval in seconds (default: 5)\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    manager = GPUResourceManager(args.bridge_url)\r\n    \r\n    if args.status:\r\n        status = manager.get_status()\r\n        if status:\r\n            print(json.dumps(status, indent=2))\r\n        else:\r\n            print(\"‚ùå Failed to get status\")\r\n    \r\n    elif args.reset:\r\n        manager.reset_lock()\r\n    \r\n    elif args.force_release:\r\n        manager.force_release_all()\r\n    \r\n    elif args.monitor:\r\n        manager.monitor(args.interval)\r\n    \r\n    else:\r\n        # Default: show status\r\n        status = manager.get_status()\r\n        if status:\r\n            locked = status.get('locked', False)\r\n            owner = status.get('owner', 'None')\r\n            queue_depth = status.get('queue_depth', 0)\r\n            queued = status.get('queued', [])\r\n            \r\n            print(f\"GPU Status: {'LOCKED' if locked else 'FREE'}\", end=\"\")\r\n            if locked:\r\n                print(f\" by {owner}\", end=\"\")\r\n            print(f\" | Queue: {queue_depth} items\")\r\n            \r\n            if queued:\r\n                print(f\"Queued: {', '.join(queued)}\")\r\n        else:\r\n            print(\"‚ùå Failed to get status\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
    "source": "scripts\\gpu_manager.py"
  },
  {
    "id": "scripts\\README.md",
    "timestamp": 1766448510,
    "role": "file",
    "content": "# Scripts Directory\n\nContains utility scripts for continuous integration and local environment setup.\n",
    "source": "scripts\\README.md"
  },
  {
    "id": "scripts\\ci\\check_docs.py",
    "timestamp": 1766241700,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"CI doc check script (Sovereign Era).\r\n\r\nVerifies the presence of critical spec files defined in specs/doc_policy.md.\r\n\"\"\"\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n\r\nREPO_ROOT = Path(__file__).resolve().parents[2]\r\n\r\n\r\ndef main() -> int:\r\n    # 1. Check Core Specs (per specs/doc_policy.md Rule 3)\r\n    expected = [\r\n        REPO_ROOT / \"specs\" / \"spec.md\",\r\n        REPO_ROOT / \"specs\" / \"plan.md\",\r\n        REPO_ROOT / \"specs\" / \"tasks.md\",\r\n        REPO_ROOT / \"specs\" / \"doc_policy.md\",\r\n    ]\r\n    \r\n    missing = [str(p) for p in expected if not p.exists()]\r\n    if missing:\r\n        print(\"[FAIL] Missing core specification files:\")\r\n        for m in missing:\r\n            print(f\"  - {m}\")\r\n        return 2\r\n\r\n    # 2. Check README\r\n    readme = REPO_ROOT / \"README.md\"\r\n    if not readme.exists():\r\n        print(\"[FAIL] README.md not found\")\r\n        return 2\r\n\r\n    text = readme.read_text(encoding=\"utf-8\")\r\n    lower = text.lower()\r\n    \r\n    # 3. Simple Content Check (Sovereign Context Engine)\r\n    # We relax the strict \"UTCP\" check as architecture evolves.\r\n    checks = [\r\n        (\"context engine\", \"Project name 'Context Engine' not found in README\"),\r\n    ]\r\n    \r\n    failed = []\r\n    for token, msg in checks:\r\n        if token not in lower:\r\n            failed.append(msg)\r\n            \r\n    if failed:\r\n        print(\"[FAIL] README checks failed:\")\r\n        for f in failed:\r\n            print(f\"  - {f}\")\r\n        return 2\r\n\r\n    print(\"[OK] Sovereign Doc Checks Passed\")\r\n    return 0\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n",
    "source": "scripts\\ci\\check_docs.py"
  },
  {
    "id": "server\\run_context_read.js",
    "timestamp": 1767457815,
    "role": "file",
    "content": "const { createFullCorpusRecursive } = require('./src/read_all.js');\n\n// Run the function to aggregate content from the context directory\n// This is a wrapper to run the read_all functionality from the server directory\n// where all dependencies are properly installed\n\nconsole.log('Starting context aggregation from server directory...');\ncreateFullCorpusRecursive();",
    "source": "server\\run_context_read.js"
  },
  {
    "id": "server\\src\\index.js",
    "timestamp": 1767458175,
    "role": "file",
    "content": "const express = require('express');\nconst cors = require('cors');\nconst bodyParser = require('body-parser');\nconst { CozoDb } = require('cozo-node');\nconst chokidar = require('chokidar');\nconst fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\n\n// Initialize CozoDB with RocksDB backend\nconst db = new CozoDb('rocksdb', './context.db');\n\n// Set up Express app\nconst app = express();\nconst PORT = 3000;\n\n// Middleware\napp.use(cors());\napp.use(bodyParser.json({ limit: '50mb' }));\napp.use(bodyParser.urlencoded({ extended: true }));\n\n// Initialize database schema\nasync function initializeDb() {\n  try {\n    // Create the memory table\n    const schemaQuery = ':create memory {id: String, timestamp: Int, content: String, source: String, type: String}';\n    \n    await db.run(schemaQuery);\n    console.log('Database schema initialized');\n    \n    // Try to create FTS index (optional, may not be supported in all builds)\n    try {\n      const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;\n      await db.run(ftsQuery);\n      console.log('FTS index created');\n    } catch (e) {\n      console.log('FTS creation failed (optional feature):', e.message);\n    }\n  } catch (error) {\n    console.error('Error initializing database:', error);\n    throw error;\n  }\n}\n\n// POST /v1/ingest endpoint\napp.post('/v1/ingest', async (req, res) => {\n  try {\n    const { content, filename, source, type = 'text' } = req.body;\n    \n    if (!content) {\n      return res.status(400).json({ error: 'Content is required' });\n    }\n    \n    const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;\n    const timestamp = Date.now();\n    \n    // Insert into CozoDB\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const params = {\n      data: [[\n        id,\n        timestamp,\n        content,\n        source || filename || 'unknown',\n        type\n      ]]\n    };\n    \n    const result = await db.run(query, params);\n    \n    res.json({ \n      status: 'success', \n      id: id,\n      message: 'Content ingested successfully'\n    });\n  } catch (error) {\n    console.error('Ingest error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/query endpoint\napp.post('/v1/query', async (req, res) => {\n  try {\n    const { query, params = {} } = req.body;\n    \n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n    \n    const result = await db.run(query, params);\n    \n    res.json(result);\n  } catch (error) {\n    console.error('Query error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// GET /health endpoint\napp.get('/health', (req, res) => {\n  res.json({ \n    status: 'Sovereign',\n    timestamp: new Date().toISOString(),\n    uptime: process.uptime()\n  });\n});\n\n// Set up file watcher for context directory\nfunction setupFileWatcher() {\n  const contextDir = path.join(__dirname, '..', 'context');\n  \n  // Ensure context directory exists\n  if (!fs.existsSync(contextDir)) {\n    fs.mkdirSync(contextDir, { recursive: true });\n  }\n  \n  const watcher = chokidar.watch(contextDir, {\n    ignored: /(^|[\\/\\\\])\\../, // ignore dotfiles\n    persistent: true,\n    ignoreInitial: true, // Don't trigger events for existing files\n    awaitWriteFinish: {\n      stabilityThreshold: 2000,\n      pollInterval: 100\n    }\n  });\n\n  watcher\n    .on('add', filePath => handleFileChange(filePath))\n    .on('change', filePath => handleFileChange(filePath))\n    .on('error', error => console.error('Watcher error:', error));\n    \n  console.log('File watcher initialized for context directory');\n}\n\nasync function handleFileChange(filePath) {\n  console.log(`File changed: ${filePath}`);\n  \n  try {\n    const content = fs.readFileSync(filePath, 'utf8');\n    const relPath = path.relative(\n      path.join(__dirname, '..', 'context'), \n      filePath\n    );\n    \n    // Ingest the file content\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const id = `file_${Date.now()}_${path.basename(filePath)}`;\n    const params = {\n      data: [[\n        id,\n        Date.now(),\n        content,\n        relPath,\n        path.extname(filePath) || 'unknown'\n      ]]\n    };\n    \n    await db.run(query, params);\n    console.log(`File ingested: ${relPath}`);\n  } catch (error) {\n    console.error(`Error processing file ${filePath}:`, error);\n  }\n}\n\n// Initialize and start server\nasync function startServer() {\n  try {\n    await initializeDb();\n    setupFileWatcher();\n    \n    app.listen(PORT, () => {\n      console.log(`Sovereign Context Engine listening on port ${PORT}`);\n      console.log(`Health check: http://localhost:${PORT}/health`);\n    });\n  } catch (error) {\n    console.error('Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Handle graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('Shutting down gracefully...');\n  try {\n    await db.close();\n  } catch (e) {\n    console.error('Error closing database:', e);\n  }\n  process.exit(0);\n});\n\n// Start the server\nstartServer();\n\nmodule.exports = { db, app };",
    "source": "server\\src\\index.js"
  },
  {
    "id": "server\\src\\migrate_history.js",
    "timestamp": 1767457063,
    "role": "file",
    "content": "const fs = require('fs');\nconst path = require('path');\nconst glob = require('glob');\nconst yaml = require('js-yaml');\n\n// Migration script to consolidate legacy session files\nasync function migrateHistory() {\n  console.log('Starting legacy session migration...');\n\n  // Find all session files\n  const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');\n  const pattern = path.join(sessionsDir, 'sessions_part_*.json');\n\n  // Use glob to find all matching files\n  const sessionFiles = glob.sync(pattern);\n\n  if (sessionFiles.length === 0) {\n    console.log('No session files found in the expected location.');\n    // Try alternative path\n    const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');\n    const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');\n    const altSessionFiles = glob.sync(altPattern);\n\n    if (altSessionFiles.length === 0) {\n      console.log('No session files found in alternative location either.');\n      return;\n    }\n\n    console.log(`Found ${altSessionFiles.length} session files in alternative location.`);\n    processSessionFiles(altSessionFiles);\n    return;\n  }\n\n  console.log(`Found ${sessionFiles.length} session files`);\n  processSessionFiles(sessionFiles);\n}\n\nfunction processSessionFiles(sessionFiles) {\n  // Sort files numerically (part_1, part_2, ..., part_10, etc.)\n  sessionFiles.sort((a, b) => {\n    const matchA = a.match(/part_(\\d+)/);\n    const matchB = b.match(/part_(\\d+)/);\n\n    if (matchA && matchB) {\n      return parseInt(matchA[1]) - parseInt(matchB[1]);\n    }\n    return a.localeCompare(b);\n  });\n\n  let allSessions = [];\n\n  for (const file of sessionFiles) {\n    console.log(`Processing: ${path.basename(file)}`);\n    try {\n      const content = fs.readFileSync(file, 'utf8');\n\n      // Try to extract valid JSON from potentially corrupted files\n      let data = extractValidJson(content);\n\n      if (!data) {\n        console.error(`Could not extract valid JSON from ${file}`);\n        continue;\n      }\n\n      // Handle both list and object formats\n      if (Array.isArray(data)) {\n        allSessions = allSessions.concat(data);\n      } else if (typeof data === 'object') {\n        allSessions.push(data);\n      } else {\n        console.log(`Unexpected data format in ${file}, skipping...`);\n      }\n    } catch (error) {\n      console.error(`Error reading ${file}:`, error.message);\n    }\n  }\n\n  console.log(`Merged ${allSessions.length} total sessions`);\n\n  // Save to YAML file\n  const outputDir = path.join(__dirname, '..', '..', 'context');\n  const outputFile = path.join(outputDir, 'full_history.yaml');\n\n  // Custom YAML representer for multiline strings\n  yaml.representer = {\n    ...yaml.representer,\n    string: (data) => {\n      if (data.includes('\\n')) {\n        return new yaml.types.Str(data, { style: '|' });\n      }\n      return data;\n    }\n  };\n\n  try {\n    const yamlContent = yaml.dump(allSessions, {\n      lineWidth: -1,\n      noRefs: true,\n      skipInvalid: true\n    });\n\n    fs.writeFileSync(outputFile, yamlContent, 'utf8');\n    console.log(`YAML file created: ${outputFile}`);\n\n    // Also save as JSON for compatibility\n    const jsonOutputFile = path.join(outputDir, 'full_history.json');\n    fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');\n    console.log(`JSON file created: ${jsonOutputFile}`);\n\n  } catch (error) {\n    console.error('Error saving YAML file:', error.message);\n    return;\n  }\n\n  console.log('Migration completed successfully!');\n}\n\n// Function to extract valid JSON from potentially corrupted files\nfunction extractValidJson(content) {\n  try {\n    // First, try to parse as regular JSON\n    return JSON.parse(content);\n  } catch (e) {\n    // If that fails, clean the content and try again\n    try {\n      // Remove null bytes and other control characters that often corrupt JSON\n      let cleanContent = content.replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]/g, '');\n\n      // Try to parse the cleaned content\n      return JSON.parse(cleanContent);\n    } catch (e2) {\n      // If still failing, try to extract JSON array from the content\n      try {\n        // Find the main JSON array by looking for opening [ and closing ]\n        const startIdx = cleanContent.indexOf('[');\n        const endIdx = cleanContent.lastIndexOf(']');\n\n        if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {\n          const arrayContent = cleanContent.substring(startIdx, endIdx + 1);\n\n          // Try to parse the extracted array\n          return JSON.parse(arrayContent);\n        }\n      } catch (e3) {\n        // If all attempts fail, return null\n        return null;\n      }\n    }\n  }\n\n  return null;\n}\n\n// Run migration if this file is executed directly\nif (require.main === module) {\n  migrateHistory().catch(console.error);\n}\n\nmodule.exports = { migrateHistory };",
    "source": "server\\src\\migrate_history.js"
  },
  {
    "id": "server\\src\\read_all.js",
    "timestamp": 1767457667,
    "role": "file",
    "content": "const fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\n\n/**\n * Aggregates all readable text content from a directory and its subdirectories\n * into:\n * 1. A single text corpus (combined_text.txt) for human reading.\n * 2. A structured JSON memory file (combined_memory.json) for Sovereign DB ingestion.\n * 3. A structured YAML memory file (combined_memory.yaml) for easier processing and migration.\n */\nfunction createFullCorpusRecursive() {\n  // Set the root directory to scan as the directory containing this script\n  const rootDirToScan = path.dirname(__filename);\n\n  const outputTextFile = path.join(rootDirToScan, 'combined_text.txt');\n  const outputJsonFile = path.join(rootDirToScan, 'combined_memory.json');\n  const outputYamlFile = path.join(rootDirToScan, 'combined_memory.yaml');\n\n  console.log(`Scanning Target Directory: ${rootDirToScan}`);\n\n  const textExtensions = new Set([\n    '.json', '.md', '.poml', '.yaml', '.yml', '.txt', \n    '.py', '.js', '.ts', '.css', '.sh', '.ps1', '.html', '.bat'\n  ]);\n\n  const excludeDirs = new Set([\n    '.venv', '.git', '.vscode', '__pycache__', \n    'node_modules', '.obsidian', 'random', 'archive', \n    'build', 'dist', 'logs'\n  ]);\n\n  // Files to exclude from the corpus itself to avoid recursion\n  const excludeFiles = new Set([\n    path.basename(outputTextFile),\n    path.basename(outputJsonFile),\n    path.basename(outputYamlFile),\n    'package-lock.json',\n    'yarn.lock'\n  ]);\n\n  const filesToProcess = [];\n\n  function walkDirectory(currentPath) {\n    const items = fs.readdirSync(currentPath);\n\n    for (const item of items) {\n      const itemPath = path.join(currentPath, item);\n      const stat = fs.statSync(itemPath);\n\n      if (stat.isDirectory()) {\n        if (!excludeDirs.has(item)) {\n          walkDirectory(itemPath);\n        }\n      } else if (stat.isFile()) {\n        const ext = path.extname(item).toLowerCase();\n        if (textExtensions.has(ext) && !excludeFiles.has(item)) {\n          filesToProcess.push(itemPath);\n        }\n      }\n    }\n  }\n\n  walkDirectory(rootDirToScan);\n  filesToProcess.sort();\n\n  if (filesToProcess.length === 0) {\n    console.log(`No processable files found in '${rootDirToScan}'.`);\n    return;\n  }\n\n  console.log(`Found ${filesToProcess.length} files to process.`);\n\n  const memoryRecords = [];\n\n  // 1. Generate Text Corpus\n  const textStream = fs.createWriteStream(outputTextFile, { encoding: 'utf-8' });\n\n  for (const filePath of filesToProcess) {\n    console.log(`Processing '${filePath}'...`);\n    try {\n      // Get file metadata\n      const fileStats = fs.statSync(filePath);\n      const modTime = fileStats.mtimeMs; // milliseconds timestamp\n      const relPath = path.relative(rootDirToScan, filePath);\n\n      // Read file content\n      const rawContent = fs.readFileSync(filePath);\n      // For simplicity in JS, we'll assume UTF-8, but could implement encoding detection\n      const decodedContent = rawContent.toString('utf-8');\n\n      // Write to Text File\n      textStream.write(`--- START OF FILE: ${relPath} ---\\n`);\n      textStream.write(decodedContent + \"\\n\");\n      textStream.write(`--- END OF FILE: ${relPath} ---\\n\\n`);\n\n      // Add to Memory Records\n      memoryRecords.push({\n        role: 'system',\n        type: 'document',\n        source: relPath,\n        timestamp: Math.floor(modTime), // Convert to integer\n        content: decodedContent\n      });\n\n    } catch (e) {\n      console.log(`An unexpected error occurred with file '${filePath}': ${e.message}`);\n    }\n  }\n\n  textStream.end();\n\n  // 2. Generate JSON Memory File\n  console.log(`Generating Structured Memory: ${outputJsonFile}`);\n  fs.writeFileSync(outputJsonFile, JSON.stringify(memoryRecords, null, 2), 'utf-8');\n\n  // 3. Generate YAML Memory File\n  console.log(`Generating YAML Memory: ${outputYamlFile}`);\n\n  // Custom YAML representer for multiline strings\n  const schema = yaml.DEFAULT_SCHEMA.extend([\n    new yaml.Type('!long-string', {\n      kind: 'scalar',\n      predicate: (data) => typeof data === 'string' && data.includes('\\n'),\n      represent: (data) => ({ value: data, style: '|' })\n    })\n  ]);\n\n  // Use default representer with multiline string style\n  const yamlContent = yaml.dump(memoryRecords, {\n    lineWidth: -1, // Don't wrap lines\n    noRefs: true,\n    quotingType: '\"', // Use double quotes when needed\n    forceQuotes: false\n  });\n\n  fs.writeFileSync(outputYamlFile, yamlContent, 'utf-8');\n\n  console.log('\\nCorpus aggregation complete.');\n  console.log(`1. Text Corpus: '${outputTextFile}'`);\n  console.log(`2. JSON Memory: '${outputJsonFile}' (Drop this into Coda Console)`);\n  console.log(`3. YAML Memory: '${outputYamlFile}' (Alternative format for easier processing)`);\n}\n\n// Run the function if this script is executed directly\nif (require.main === module) {\n  createFullCorpusRecursive();\n}\n\nmodule.exports = { createFullCorpusRecursive };",
    "source": "server\\src\\read_all.js"
  },
  {
    "id": "specs\\architecture-v2.md",
    "timestamp": 1767189341,
    "role": "file",
    "content": "# Sovereign Architecture V2: The Ghost & The Shell\n\n## 1. Overview\nArchitecture V2 decouples the **Interface** from the **Inference Engine**. \nInstead of a monolithic \"Chat UI\" inside a browser, the system splits into a background service (Ghost) and a lightweight client (Shell).\n\n## 2. Component A: The Ghost (Headless Engine)\nThe Ghost is a background process responsible solely for loading the LLM into VRAM and exposing an API.\n\n* **Current Implementation:** Headless Chromium (`launch-ghost.ps1`).\n* **Future Implementation:** C++ Native Binary (`neural-ghost.exe`) using Dawn/WebGPU.\n* **Responsibility:**\n    * Manage WebGPU Context.\n    * Load Weights (MLC-LLM).\n    * Serve `localhost:8080/v1/chat/completions`.\n    * **Stealth Mode:** Uses `NoCacheStaticFiles` to treat models as RAM-only data, bypassing browser storage quotas.\n    * **Search Engine:** Provides hybrid search (Vector + BM25 FTS) via CozoDB WASM.\n\n## 3. Component B: The Shell (Native Client)\nThe Shell is the user interface, residing in the user's native terminal environment (PowerShell, Bash, etc.).\n\n* **Implementation:** Python Client (`tools/sov.py`).\n* **Responsibility:**\n    * Capture user input (`stdin`).\n    * Send JSON payload to The Ghost.\n    * Render streamed response to `stdout`.\n    * Execute system commands (Agency).\n\n## 4. The Data Flow\n1.  **User:** Types `sov \"List large files\"` in PowerShell.\n2.  **Shell:** Sends POST request to `localhost:8080`.\n3.  **Bridge:** Forwards request to Headless Browser (Ghost) via Websocket/Fetch.\n4.  **Ghost:** Runs inference on RTX 4090 via WebGPU.\n5.  **Ghost:** Returns tokens -> Bridge -> Shell.\n6.  **Shell:** Displays output or executes `Get-ChildItem` command.\n\n## 5. Roadmap\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\n- [x] **Phase 2:** Headless Browser Script (Completed).\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\n- [x] **Phase 4:** Neural Shell Protocol (`/v1/shell/exec` endpoint).\n- [x] **Phase 4.5:** Ghost Auto-Ignition (Auto-start with ?headless=true flag).\n- [x] **Phase 5:** Native Shell Implementation (Anchor terminal with spawn endpoint).\n- [ ] **Phase 6:** Migration to C++ Native Runtime (Removing Chrome entirely).",
    "source": "specs\\architecture-v2.md"
  },
  {
    "id": "specs\\doc_policy.md",
    "timestamp": 1767457745,
    "role": "file",
    "content": "# Documentation Policy (Root Coda)\n\n**Status:** Active | **Authority:** Human-Locked\n\n## Core Philosophy\n1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.\n2. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.\n3. **Brevity:** Text sections must be <500 characters.\n4. **Pain into Patterns:** Every major bug must become a Standard.\n5. **LLM-First Documentation:** Documentation must be structured for LLM consumption and automated processing.\n\n## Structure\n\n### 1. The Blueprint (`specs/spec.md`)\n*   **Role:** The single architectural source of truth.\n*   **Format:** \"Visual Monolith\".\n*   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.\n\n### 2. The Tracker (`specs/tasks.md`)\n*   **Role:** Current work queue.\n*   **Format:** Checklist.\n*   **Maintenance:** Updated by Agents after every major task.\n\n### 3. The Roadmap (`specs/plan.md`)\n*   **Role:** Strategic vision.\n*   **Format:** Phased goals.\n\n### 4. Standards (`specs/standards/*.md`)\n*   **Role:** Institutional Memory (The \"Laws\" of the codebase).\n*   **Trigger:** Created after any bug that took >1 hour to fix OR any systemic improvement that affects multiple components.\n*   **Format:** \"The Triangle of Pain\"\n    1.  **What Happened:** The specific failure mode (e.g., \"Bridge crashed on start\").\n    2.  **The Cost:** The impact (e.g., \"3 hours debugging Unicode errors\").\n    3.  **The Rule:** The permanent constraint (e.g., \"Force UTF-8 encoding on Windows stdout\").\n\n### 5. Local Context (`*/README.md`)\n*   **Role:** Directory-specific context.\n*   **Limit:** 1 sentence explaining the folder's purpose.\n\n### 6. System-Wide Standards\n*   **Universal Logging:** All system components must route logs to the central log collection system (Standard 013)\n*   **Single Source of Truth:** The log viewer at `/log-viewer.html` is the single point for all system diagnostics\n*   **Async Best Practices:** All async/await operations must follow proper patterns for FastAPI integration (Standard 014)\n*   **Browser Control Center:** All primary operations must be accessible through unified browser interface (Standard 015)\n*   **Detached Script Execution:** All data processing scripts must run in detached mode with logging to `logs/` directory (Standard 025)\n*   **Ghost Engine Connection Management:** All memory operations must handle Ghost Engine disconnections gracefully with proper error reporting and auto-reconnection (Standard 026)\n*   **No Resurrection Mode:** System must support manual Ghost Engine control via NO_RESURRECTION_MODE flag (Standard 027)\n*   **Default No Resurrection:** Ghost Engine resurrection is disabled by default, requiring manual activation (Standard 028)\n*   **Consolidated Data Aggregation:** Single authoritative script for data aggregation with multi-format output (Standard 029)\n*   **Multi-Format Output:** Project aggregation tools must generate JSON, YAML, and text outputs for maximum compatibility (Standard 030)\n*   **Ghost Engine Stability:** CozoDB schema creation must handle FTS failures gracefully to prevent browser crashes (Standard 031)\n*   **Ghost Engine Initialization Flow:** Database initialization must complete before processing ingestion requests to prevent race conditions (Standard 032)\n*   **CozoDB Syntax Compliance:** All CozoDB queries must use proper syntax to ensure successful execution (Standard 033)\n*   **Node.js Monolith Migration:** System must migrate from Python/Browser Bridge to Node.js Monolith architecture (Standard 034)\n\n## LLM Protocol\n1. **Read-First:** Always read `specs/spec.md` AND `specs/standards/` before coding.\n2. **Drafting:** When asked to document, produce **Mermaid diagrams** and short summaries.\n3. **Editing:** Do not modify `specs/doc_policy.md` or `specs/spec.md` structure unless explicitly instructed.\n4. **Archival:** Move stale docs to `archive/` immediately.\n5. **Enforcement:** If a solution violates a Standard, reject it immediately.\n6. **Standards Evolution:** New standards should follow the \"Triangle of Pain\" format and be numbered sequentially (001, 002, etc.).\n7. **Cross-Reference:** When creating new standards, reference related existing standards to maintain consistency.\n8. **Detached Mode:** All LLM development scripts must run in detached mode (non-interactive) and log to files in the `logs/` directory with timestamped names (Standard 025).\n\n---\n*Verified by Architecture Council. Edited by Humans Only.*\n",
    "source": "specs\\doc_policy.md"
  },
  {
    "id": "specs\\models.md",
    "timestamp": 1767076621,
    "role": "file",
    "content": "# Verified MLC-LLM Model URLs\r\n\r\n**Status:** Registry of verified WebLLM-compatible model URLs.\r\n**Last Updated:** Dec 22, 2025\r\n**Source:** `mlc-ai/web-llm` config and verified HTTP checks.\r\n\r\n## Technology: WASM + WebGPU Inference\r\n\r\nThis project uses **WebLLM** (by MLC-AI) to run Large Language Models directly in the browser.\r\n\r\n1.  **Compilation:** Models (Llama 3, Qwen 2.5, etc.) are compiled into **WebAssembly (WASM)** modules (`.wasm`). These modules contain the model's architecture and logic, optimized for execution in a web environment.\r\n2.  **Acceleration:** The WASM module uses the **WebGPU API** to access the user's local GPU. This allows for massive parallelism, enabling 7B+ parameter models to run at interactive speeds (20-100+ tokens/sec) on consumer hardware.\r\n3.  **Zero-Server:** No data leaves the browser. The \"Backend\" is the user's own GPU.\r\n\r\n## Official Model Registry\r\n\r\n**CRITICAL REFERENCE:** The definitive list of supported models and their WASM binaries can be found at:\r\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293\r\n- This contains the official `prebuiltAppConfig` with verified model configurations\r\n\r\n**ADDITIONAL VERIFIED LINKS:**\r\n- https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293 - Contains verified MLC model configurations and working WASM URLs\r\n\r\n---\r\n\r\n## 1. Verified Models (Ready for Production)\r\n\r\nThese models have been verified to exist in the `v0_2_80` library and are compatible with the current `web-llm` version.\r\n\r\n### üåü 7B - 8B Class (Recommended)\r\nBalanced performance for reasoning and chat. Requires 6GB+ VRAM.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2.5-7B-Instruct-q4f16_1-MLC` | **Best All-Rounder.** Fast, smart, 32k context. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.1-8B-Instruct-q4f32_1-MLC` | **Meta's Latest.** Strong reasoning. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3_1-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3-8B-Instruct-q4f32_1-MLC` | Llama 3 Base. Reliable. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3-8B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC` | **Reasoning Specialist.** | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` (Uses Qwen2 base) |\r\n\r\n### üöÄ High Performance (Small)\r\nFastest start times. Works on most laptops/integrated graphics.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Phi-3.5-mini-instruct-q4f16_1-MLC` | **Microsoft Phi.** 3.8B params. Very smart for size. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-mini-instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` | **Ultra-Lite.** 1.5B params. Blazing fast. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `SmolLM2-1.7B-Instruct-q4f16_1-MLC` | Efficient 1.7B model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/SmolLM2-1.7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üß† Code Specialist Models\r\nModels optimized for code generation and command translation.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` | **Qwen Coder 1.5B.** Specialized for code and command generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` | **Qwen Coder 7B.** Advanced code reasoning and command translation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üåü Gemma Models (Google)\r\nLightweight and efficient models from Google.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `gemma-2-9b-it-q4f16_1-MLC` | **Gemma 2 9B.** High performance text generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-9b-it-q4f32_1-MLC` | **Gemma 2 9B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-9b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-2b-it-q4f16_1-MLC` | **Gemma 2 2B.** Lightweight option. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-2-2b-it-q4f32_1-MLC` | **Gemma 2 2B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-2-2b-it-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `gemma-3-1b-it-q4f16_1-MLC` | **Gemma 3 1B.** Latest generation small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/gemma-3-1b-it-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üëÅÔ∏è Vision Models (Multimodal)\r\nModels that can process both text and images.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Phi-3.5-vision-instruct-q4f16_1-MLC` | **Microsoft Phi Vision.** 4.2B params, multimodal. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f16_1-ctx4k_cs2k-webgpu.wasm` |\r\n| `Phi-3.5-vision-instruct-q4f32_1-MLC` | **Microsoft Phi Vision.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Phi-3.5-vision-instruct-q4f32_1-ctx4k_cs2k-webgpu.wasm` |\r\n\r\n### üöÄ Larger Models (For High VRAM Systems)\r\nModels for systems with 12GB+ VRAM like RTX 4090.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Llama-2-13b-chat-hf-q4f16_1-MLC` | **Llama 2 13B.** For high-end systems. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-2-13b-chat-hf-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-8B-q4f16_1-MLC` | **Qwen 3 8B.** Latest generation. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-8B-q4f32_1-MLC` | **Qwen 3 8B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-8B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-4B-q4f16_1-MLC` | **Qwen 3 4B.** Balanced performance. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-4B-q4f32_1-MLC` | **Qwen 3 4B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-4B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n### üöÄ Smaller Models (For Low VRAM Systems)\r\nModels optimized for systems with limited VRAM like the XPS 13.\r\n\r\n| Model ID | Details | WASM URL |\r\n| :--- | :--- | :--- |\r\n| `Qwen2-0.5B-Instruct-q4f16_1-MLC` | **Ultra-Lightweight.** 0.5B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen2-0.5B-Instruct-q4f32_1-MLC` | **Ultra-Lightweight.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-0.5B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-0.6B-q4f16_1-MLC` | **Qwen 3 Tiny.** 0.6B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-0.6B-q4f32_1-MLC` | **Qwen 3 Tiny.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-0.6B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-1.7B-q4f16_1-MLC` | **Qwen 3 Small.** 1.7B params. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Qwen3-1.7B-q4f32_1-MLC` | **Qwen 3 Small.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen3-1.7B-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-1B-Instruct-q4f16_1-MLC` | **Llama 3.2 1B.** Efficient small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-1B-Instruct-q4f32_1-MLC` | **Llama 3.2 1B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-1B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-3B-Instruct-q4f16_1-MLC` | **Llama 3.2 3B.** Balanced small model. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm` |\r\n| `Llama-3.2-3B-Instruct-q4f32_1-MLC` | **Llama 3.2 3B.** Higher precision variant. | `https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Llama-3.2-3B-Instruct-q4f32_1-ctx4k_cs1k-webgpu.wasm` |\r\n\r\n---\r\n\r\n## 2. Known Issues / Missing Binaries\r\n\r\nThese models are listed in the official config but their WASM binaries are not currently hosted in the `v0_2_80` folder. **Avoid using these until binaries are available.**\r\n\r\n*   `Qwen2.5-14B-Instruct-q4f16_1-MLC` (404 Not Found) - **Known Issue**\r\n*   `DeepSeek-R1-Distill-Qwen-14B-q4f16_1-MLC` (404 Not Found) - **Known Issue**\r\n*   `Qwen2-VL-7B-Instruct-q4f16_1-MLC` (404 Not Found)\r\n*   `gemma-3-2b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 2B available, only 1B exists**\r\n*   `gemma-3-12b-it-q4f16_1-MLC` (404 Not Found) - **Note: No Gemma 3 12B available in current library**\r\n\r\n## 3. Verified Working Models (Recommended)\r\n\r\nBased on the official config at the GitHub link above, these models are confirmed to have working WASM binaries:\r\n\r\n### High Performance (7B-8B Range)\r\n*   `Qwen2.5-7B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Llama-3.1-8B-Instruct-q4f32_1-MLC` - Verified working\r\n*   `Phi-3.5-mini-instruct-q4f16_1-MLC` - Verified working\r\n*   `gemma-2-9b-it-q4f16_1-MLC` - Verified working\r\n*   `Qwen3-8B-q4f16_1-MLC` - Verified working\r\n\r\n### Lightweight Options (1.5B and below)\r\n*   `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Qwen2-0.5B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `Llama-3.2-1B-Instruct-q4f16_1-MLC` - Verified working\r\n*   `SmolLM2-1.7B-Instruct-q4f16_1-MLC` - Verified working\r\n\r\n### Vision Models\r\n*   `Phi-3.5-vision-instruct-q4f16_1-MLC` - Verified working\r\n\r\n### üíª Code Specialists\r\n*   `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` - **Best for Neural Terminal.** Fast & Smart.\r\n*   `Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC` - Verified working\r\n\r\n---\r\n\r\n## 3. URL Construction Logic\r\n\r\nIf you need to construct a URL manually:\r\n\r\n```javascript\r\nconst libBase = \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/\";\r\nconst version = \"v0_2_80\"; // Check src/config.ts for 'modelVersion'\r\nconst modelSpecificName = \"Qwen2.5-7B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\";\r\n\r\nconst fullUrl = `${libBase}${version}/${modelSpecificName}`;\r\n```\r\n\r\n**Note:** The `modelSpecificName` often differs slightly from the Hugging Face repo name (e.g., `Llama-3_1` vs `Llama-3.1` or `Qwen2` base for `DeepSeek`). Always check `mlc_config.ts` mapping.",
    "source": "specs\\models.md"
  },
  {
    "id": "specs\\plan.md",
    "timestamp": 1767407686,
    "role": "file",
    "content": "# Anchor Core Roadmap (V2.3)\n\n**Status:** Text-Only + Watchdog Deployed\n**Focus:** Stability & Passive Text Ingestion.\n\n## Phase 1: Foundation (Completed)\n- [x] Pivot to WebLLM/WebGPU stack.\n- [x] Implement CozoDB (WASM) for memory.\n- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).\n\n## Phase 2: Stabilization (Completed)\n- [x] Fix Model Loading (Quota/VRAM config).\n- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).\n- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).\n\n## Phase 2.5: Root Refactor (Completed)\n- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).\n- [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.\n- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.\n- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).\n\n## Phase 3: Expansion & Hardening (Completed)\n- [x] **Resource Hardening**: Implemented \"Consciousness Semaphore\" in `sovereign.js`.\n- [x] **Documentation Refactor**: Executed \"Visual Monolith\" strategy.\n- [x] **Memory Hygiene**: Implement \"Forgetting Curve\" in `root-dreamer.html`.\n- [x] **Active Memory Persistence**: Enable chat to write back to the Graph.\n- [x] **Temporal Awareness**: Ground the model in real-time.\n- [x] **Mobile Optimization**: Polish mobile UX for `model-server-chat.html`.\n\n## Phase 4: Text-Only Architecture (Completed)\n- [x] **Vision Removal**: Remove brittle Vision/Ollama dependencies.\n- [x] **Watchdog Implementation**: Create passive text ingestion service.\n- [x] **Debounce & Hash Check**: Prevent duplicate file ingestion.\n- [x] **Auto-Resurrection**: Enhance browser process management.\n- [x] **Streaming CLI**: Improve terminal UX with streaming responses.\n\n## Phase 5: Context Expansion & Persistence (Completed)\n- [x] **Code File Support**: Expand to monitor programming language extensions.\n- [x] **Browser Profile Management**: Implement temporary profile cleanup.\n- [x] **Chat Session Persistence**: Auto-save conversations to context directory.\n- [x] **Ingestion Loop Closure**: Ensure chat sessions become ingested context.\n\n## Phase 6: Session Recorder & Text-File Source of Truth (Completed)\n- [x] **Daily Session Files**: Create `chat_YYYY-MM-DD.md` files for each day's conversations.\n- [x] **Text-File Source of Truth**: Implement \"Database is Cache\" philosophy.\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access.\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat.\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking.\n\n## Phase 7: Model Loading Reliability (Completed)\n- [x] **URL Construction Fix**: Implemented `/models/{model}/resolve/main/{file}` redirect for MLC-LLM compatibility.\n- [x] **File Renaming**: Standardized component names (`anchor-mic.html`, `memory-builder.html`, `db_builder.html`).\n- [x] **Server Stability**: Fixed hanging issues with problematic path parameter syntax.\n- [x] **Endpoint Completeness**: Verified all documented endpoints are accessible.\n\n## Phase 5.5: Search Enhancement (Completed)\n- [x] **BM25 Implementation**: Replaced regex-based search with CozoDB FTS using BM25 algorithm.\n- [x] **Hybrid Search**: Combined vector search (semantic) with BM25 (lexical) for better results.\n- [x] **Index Creation**: Added FTS index creation in memory initialization routines.\n- [x] **Stemming Support**: Enabled English stemming for improved word variation matching.\n\n## Phase 6: GPU Resource Management (Completed)\n- [x] **GPU Queuing System**: Implemented automatic queuing for GPU resource requests to prevent conflicts\n- [x] **Resource Status Management**: Added GPU lock status tracking with owner identification\n- [x] **503 Error Resolution**: Fixed \"Service Unavailable\" errors by implementing proper resource queuing\n- [x] **Endpoint Integration**: Added `/v1/gpu/lock`, `/v1/gpu/unlock`, `/v1/gpu/status` endpoints\n- [x] **Log Integration**: Added GPU resource management to centralized logging system\n\n## Phase 7: Async/Await Best Practices (Completed)\n- [x] **Coroutine Fixes**: Resolved \"coroutine was never awaited\" warnings in webgpu_bridge.py\n- [x] **Event Loop Integration**: Properly integrated async functions with FastAPI's event loop\n- [x] **Startup Sequence**: Ensured logging system initializes properly with application lifecycle\n- [x] **Resource Management**: Fixed resource cleanup in WebSocket handlers to prevent leaks\n- [x] **Error Handling**: Enhanced async error handling with proper cleanup procedures\n\n## Phase 8: Browser-Based Control Center (Completed)\n- [x] **UI Integration**: Implemented browser-based sidecar with retrieval and vision tabs\n- [x] **Vision Engine**: Created Python-powered VLM integration for image analysis\n- [x] **Endpoint Expansion**: Added vision ingestion and enhanced logging endpoints\n- [x] **File Logging**: Implemented persistent file-based logging with truncation\n- [x] **UI Serving**: Extended bridge to serve HTML interfaces for unified workflow\n\n## Phase 9: Context Ingestion Pipeline Fixes (Completed)\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\n\n## Phase 10: Federation\n- [ ] **Device Sync**: Sync IndexedDB across devices (Peer-to-Peer).\n- [ ] **Local-First Cloud**: Optional encrypted backup.\n",
    "source": "specs\\plan.md"
  },
  {
    "id": "specs\\spec.md",
    "timestamp": 1767457758,
    "role": "file",
    "content": "# Anchor Core: The Visual Monolith (v3.6)\n\n**Status:** Text-Only + Watchdog + Session Recorder + Context Persistence + Ghost Engine Resilience Architecture | **Philosophy:** Visual Command Center, Resource-Queued.\n\n## 1. The Anchor Architecture\nThe **Anchor Core** (`webgpu_bridge.py`) is the unified server. The **Ghost Engine** is a headless browser window acting as the GPU Worker. **Watchdog** provides passive text ingestion.\n\n```mermaid\ngraph TD\n    subgraph Anchor_Core [Localhost:8000]\n        Bridge[WebGPU Bridge (Python)]\n\n        subgraph Assets\n            UI[chat.html]\n            Context[context.html]\n            Sidecar[sidecar.html]\n            Watchdog[watchdog.py]\n            Ghost_UI[ghost.html]\n            Dreamer[memory-builder.html]\n        end\n\n        subgraph API_Endpoints\n            ChatAPI[\"/v1/chat/completions\"]\n            SearchAPI[\"/v1/memory/search\"]\n            IngestAPI[\"/v1/memory/ingest\"]\n            GPUAPI[\"/v1/gpu/lock, /v1/gpu/unlock, /v1/gpu/status\"]\n            LogAPI[\"/logs/recent, /logs/collect\"]\n        end\n    end\n\n    subgraph Passive_Ingestion\n        Watchdog_Service[Watchdog Service]\n        Context_Folder[\"context/ folder\"]\n        File_Monitoring[File System Events]\n    end\n\n    subgraph Ghost_Engine [Headless Browser]\n        Worker[WebLLM (WASM)]\n        Memory[CozoDB (WASM)]\n        Search[Hybrid Search]\n        GPU[WebGPU Resources]\n        Connection_Manager[Connection Manager]\n    end\n\n    User -->|HTTP| Sidecar\n    User -->|HTTP| Context\n    User -->|HTTP| Ghost_UI\n\n    Watchdog_Service --> IngestAPI\n    Context_Folder --> Watchdog_Service\n    File_Monitoring --> Watchdog_Service\n\n    Sidecar -->|Vision Ingest| VisionAPI\n    Sidecar -->|Search| SearchAPI\n    VisionAPI -->|VLM Analysis| Vision\n    Vision -->|Memory Ingest| Ghost_Engine\n    SearchAPI -->|Query| Ghost_Engine\n    IngestAPI -->|Memory Ingest| Ghost_Engine\n    Ghost_Engine -->|Ground Truth| Sidecar\n\n    Sidecar -->|GPU Lock| GPUAPI\n    GPUAPI -->|Queue Manager| Ghost_Engine\n    Ghost_Engine -->|GPU| Worker\n\n    Bridge -->|WebSocket| Connection_Manager\n    Connection_Manager -->|API| Ghost_Engine\n    Ghost_Engine -->|GPU| Worker\n    ChatAPI -->|MLC-LLM| Worker\n    SearchAPI -->|Memory Query| Memory\n    IngestAPI -->|Memory Ingest| Memory\n    Dreamer -->|Background Processing| Memory\n    Resolver -->|File Redirect| Models[Local Model Files]\n    UI -->|Context Retrieval| Search_Engine\n    Search_Engine -->|Hybrid Results| UI\n    Bridge -->|Log Collection| LogAPI\n    LogAPI -->|Central Buffer| LogViewer[log-viewer.html]\n```\n\n## 2. Model Loading Strategy (Standard 007)\nThe system now uses an online-first model loading approach for reliability:\n- **Primary**: Direct HuggingFace URLs for immediate availability\n- **Fallback**: Local model files when available\n- **Bridge Redirect**: `/models/{model}/resolve/main/{file}` handles resolution logic\n- **Simplified Configuration**: Online-only approach prevents loading hangs (Standard 007)\n\n## 3. Port Map\n\n* **8000**: **The One Port.** Serves UI, API, Models, and WebSocket connections.\n\n## 4. Search Architecture\n\n* **Hybrid Retrieval**: Combines Vector search (semantic) with BM25 (lexical) for optimal results.\n* **BM25 FTS**: CozoDB Full Text Search with stemming and relevance scoring.\n* **Context Manager**: Intelligent retrieval system in `ContextManager` class.\n\n## 5. Ghost Engine Resilience (Standard 026)\n\n* **Connection Management**: Automatic WebSocket connection monitoring with reconnection attempts\n* **Graceful Degradation**: Proper error handling when Ghost Engine is disconnected (503 responses)\n* **Status Indicators**: Clear logging and UI indicators for connection status\n* **Auto-Resurrection**: Automatic recovery when Ghost Engine becomes available\n* **Queue Processing**: Pending operations are processed when connection resumes\n\n## 6. No Resurrection Mode (Standard 027)\n\n* **Manual Control**: Option to disable automatic Ghost Engine launching via NO_RESURRECTION_MODE flag\n* **Resource Efficiency**: Reduces resource usage by avoiding automatic browser launches\n* **User Flexibility**: Allows users to connect Ghost Engine manually when needed\n* **Existing Browser**: Enables use of existing browser windows instead of launching headless instances\n* **Environment Variable**: Controlled via `set NO_RESURRECTION_MODE=true` before startup\n\n## 7. Default No Resurrection Behavior (Standard 028)\n\n* **Default Setting**: Ghost Engine resurrection is now disabled by default for resource efficiency\n* **Manual Activation Required**: Users must explicitly open ghost.html to connect the Ghost Engine\n* **Environment Override**: Set `NO_RESURRECTION_MODE=false` to enable auto-launching\n* **Queued Processing**: Files and requests are queued until Ghost Engine connects\n* **User Control**: Provides maximum control over when computational resources are used\n\n## 8. Consolidated Data Aggregation (Standard 029)\n\n* **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation\n* **Multi-Format Output**: Generates three formats - text corpus, JSON memory, and YAML memory\n* **YAML Formatting**: Uses proper multiline string formatting for readability\n* **Encoding Handling**: Robust encoding detection using chardet for reliable processing\n* **Recursive Processing**: Processes all subdirectories while respecting exclusions\n* **Metadata Preservation**: Maintains file metadata in structured outputs\n\n## 9. Multi-Format Output for Project Aggregation (Standard 030)\n\n* **Multi-Format Support**: The `read_all.py` script in the root directory generates both JSON and YAML versions of memory records\n* **YAML Formatting**: Uses proper multiline string formatting (literal style with `|`) for readability\n* **Consistent Naming**: Output files follow consistent naming patterns (`combined_text.txt`, `combined_memory.json`, `combined_text.yaml`)\n* **Custom Representers**: Implements custom YAML representers to handle multiline content appropriately\n* **Maximum Compatibility**: Provides format flexibility for different downstream processing tools\n\n## 10. Ghost Engine Stability Fix (Standard 031)\n\n* **Separate Schema Creation**: Basic schema and FTS index creation must be handled separately to prevent failures\n* **Graceful FTS Handling**: FTS creation failures should not prevent basic database functionality\n* **Error Prevention**: Proper error handling prevents \"undefined\" error messages\n* **Browser Stability**: Prevents browser crashes during database initialization\n* **Fallback Operations**: System continues to function even if advanced features fail\n\n## 11. Ghost Engine Initialization Flow (Standard 032)\n\n* **Sequential Initialization**: Database must be fully initialized before signaling readiness to Bridge\n* **Database Readiness Checks**: All operations verify database is properly initialized before execution\n* **Proper Error Handling**: Return appropriate errors when database is not ready instead of failing silently\n* **Synchronous Connection Flow**: Connect ‚Üí Initialize Database ‚Üí Signal Ready ‚Üí Process Requests\n* **Graceful Degradation**: Report initialization failures and avoid processing requests when database fails\n* **Message Type Support**: Properly handle all message types including error responses\n\n## 12. CozoDB Syntax Compliance (Standard 033)\n\n* **Schema Creation Syntax**: Use proper CozoDB syntax without line breaks in schema definitions\n* **FTS Creation Syntax**: Use correct FTS creation syntax for full-text search indexes\n* **Insert Query Syntax**: Use proper `:insert` or `:replace` syntax with correct parameter binding\n* **Parameter Formatting**: Format parameters correctly as nested arrays for bulk operations\n* **Schema Validation**: Properly propagate schema creation success/failure status\n* **Error Propagation**: Ensure all database operations properly handle and report errors\n\n## 13. Node.js Monolith Migration (Standard 034)\n\n* **Node.js Runtime**: Use Node.js as the primary runtime environment for the Context Engine\n* **CozoDB Integration**: Integrate CozoDB directly using `cozo-node` for persistent storage\n* **Autonomous Execution**: Implement Protocol 001 for detached service execution with proper logging\n* **File Watchdog**: Use `chokidar` for efficient file system monitoring and automatic ingestion\n* **Standardized Endpoints**: Implement standardized API endpoints for ingestion, querying, and health checks\n* **Legacy Archival**: Archive all V2 Python infrastructure to preserve historical code\n* **JavaScript Conversion**: Convert Python utility scripts to JavaScript equivalents for consistency\n* **Platform Compatibility**: Ensure architecture works on Termux/Linux environments",
    "source": "specs\\spec.md"
  },
  {
    "id": "specs\\tasks.md",
    "timestamp": 1767407577,
    "role": "file",
    "content": "# Context-Engine Implementation Tasks\n\n## Current Work Queue (Unified Anchor Architecture)\n\n### Phase 5: Unified Anchor (Completed)\n- [x] **Consolidation**: Merge File Server and Bridge into `webgpu_bridge.py` (Port 8000).\n- [x] **Renaming**: `model-server-chat` -> `chat.html`, `neural-terminal` -> `terminal.html`.\n- [x] **Cleanup**: Archive legacy startup scripts.\n- [x] **Launcher**: Create `start-anchor.bat`.\n- [x] **Native Shell Spawning**: Implement `/v1/system/spawn_shell` endpoint.\n- [x] **Dashboard Integration**: Add Anchor Shell spawn button to `index.html`.\n- [x] **Native Client**: Create `anchor.py` for PowerShell terminal spawning.\n- [x] **Architecture Documentation**: Create Anchor Core specification.\n- [x] **Testing Suite**: Create `test_model_loading.py` and `model_test.html` for endpoint verification.\n- [x] **Troubleshooting Documentation**: Add model loading troubleshooting standard.\n\n### Phase 5.1: Model Loading Fixes (Completed)\n- [x] **URL Construction Fix**: Implement `/models/{model}/resolve/main/{file}` redirect endpoint for MLC-LLM compatibility.\n- [x] **File Renaming**: Rename `root-mic.html` -> `anchor-mic.html`, `root-dreamer.html` -> `memory-builder.html`, `sovereign-db-builder.html` -> `db_builder.html`.\n- [x] **UI Layout Fix**: Add proper margins to prevent elements from being cut off at top of browser window.\n- [x] **Server Stability**: Fix server hanging issues caused by problematic path parameter syntax.\n- [x] **Endpoint Verification**: Ensure all documented endpoints are accessible and responding properly.\n\n### Phase 5.2: Search Enhancement (Completed)\n- [x] **BM25 Implementation**: Replace regex-based search with CozoDB FTS using BM25 algorithm in `tools/chat.html`.\n- [x] **Index Creation**: Add FTS index creation in `memory-builder.html`, `db_builder.html`, and `chat.html` initialization.\n- [x] **Hybrid Search**: Maintain vector search alongside BM25 for semantic + lexical retrieval.\n- [x] **Fallback Mechanism**: Implement regex fallback if FTS index is unavailable.\n- [x] **Stemming Support**: Enable English stemming for better word variation matching.\n\n### Phase 6: Text-Only + Watchdog Architecture (Completed)\n- [x] **Vision Removal**: Remove brittle Vision/Ollama dependencies to increase survival rate.\n- [x] **Watchdog Service**: Create `tools/watchdog.py` for passive text file monitoring.\n- [x] **File Ingestion**: Implement `/v1/memory/ingest` endpoint in bridge for text ingestion.\n- [x] **Debounce & Hash Check**: Add debounce and content hash checking to prevent duplicate ingestion.\n- [x] **Auto-Resurrection**: Enhance `ResurrectionManager` to kill existing browser processes before launching new ones.\n- [x] **Streaming CLI**: Update `anchor.py` to use streaming for better UX.\n- [x] **Documentation Update**: Update architecture specs and standards to reflect new approach.\n\n### Phase 7: Context Expansion & Persistence (Completed)\n- [x] **Code File Support**: Expand watchdog to monitor code extensions (.py, .js, .html, etc.)\n- [x] **Browser Profile Cleanup**: Add unique temp profiles and cleanup for browser processes\n- [x] **Chat Session Persistence**: Auto-save conversations to context/sessions/ directory\n- [x] **Ingestion Loop Closure**: Ensure chat sessions become ingested context automatically\n- [x] **Memory Leak Prevention**: Implement profile cleanup to prevent disk space issues\n- [x] **Documentation Update**: Create new standards 019-021 for new features\n\n### Phase 8: Session Recorder & Text-File Source of Truth (Completed)\n- [x] **Daily Session Files**: Create `chat_YYYY-MM-DD.md` files for each day's conversations\n- [x] **Text-File Source of Truth**: Implement \"Database is Cache\" philosophy\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking\n- [x] **Session Tracking**: Add session file path display in CLI startup\n\n### Completed - Root Refactor ‚úÖ\n- [x] **Kernel**: Implement `tools/modules/sovereign.js`.\n- [x] **Mic**: Refactor `root-mic.html` to use Kernel.\n- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.\n- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).\n- [x] **Docs**: Update all specs to reflect Root Architecture.\n\n### Completed - Hardware Optimization üêâ\n- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.\n- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.\n- [x] **Crash Prevention**: Context clamping for constrained drivers.\n- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.\n- [x] **Consciousness Semaphore**: Implemented resource arbitration in `sovereign.js`.\n\n### Completed - The Subconscious ‚úÖ\n- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.\n- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.\n- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).\n- [x] **Memory Hygiene**: Implemented \"Forgetting Curve\" in `root-dreamer.html`.\n\n### Completed - Active Cognition ‚úÖ\n- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.\n- [x] **User Control**: Add \"Auto-Save\" toggle to System Controls.\n- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.\n- [x] **Multimodal**: Add Drag-and-Drop Image support to Console.\n\n### Phase 4.1: The Neural Shell (Completed) üöß\n**Objective:** Decouple Intelligence (Chat) from Agency (Terminal).\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\n- [x] **Phase 3.5:** Ghost Auto-Ignition (Headless auto-start with ?headless=true flag).\n- [x] **Phase 4:** Migration to C++ Native Runtime (Removing Chrome entirely).\n- [x] **Bridge Repair**: Debug and stabilize `extension-bridge` connectivity.\n- [x] **Neural Shell Protocol**: Implement `/v1/shell/exec` in `webgpu_bridge.py`.\n- [x] **The \"Coder\" Model**: Add `Qwen2.5-Coder-1.5B` to Model Registry.\n- [x] **Terminal UI**: Create `tools/neural-terminal.html` for natural language command execution.\n\n### Phase 4.2: Agentic Expansion (Deferred)\n- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.\n- [ ] **Voice Output**: Add TTS to Console.\n\n## Phase 5: The Specialist Array\n- [ ] **Dataset Generation**: Samsung TRM / Distillation.\n- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.\n- [ ] **Model Merging**: FrankenMoE construction.\n\n## Phase 6: GPU Resource Management (Completed)\n- [x] **GPU Queuing System**: Implement `/v1/gpu/lock`, `/v1/gpu/unlock`, and `/v1/gpu/status` endpoints with automatic queuing\n- [x] **Resource Conflict Resolution**: Eliminate GPU lock conflicts with proper queue management\n- [x] **503 Error Resolution**: Fix \"Service Unavailable\" errors by implementing proper resource queuing\n- [x] **Sidecar Integration**: Add GPU status monitoring to sidecar interface\n- [x] **Log Integration**: Add GPU resource management logs to centralized logging system\n- [x] **Documentation**: Update specs and standards to reflect GPU queuing system\n\n## Phase 7: Async/Await Best Practices (Completed)\n- [x] **Coroutine Fixes**: Resolve \"coroutine was never awaited\" warnings in webgpu_bridge.py\n- [x] **Event Loop Integration**: Properly integrate async functions with FastAPI's event loop\n- [x] **Startup Sequence**: Ensure logging system initializes properly with application lifecycle\n- [x] **Resource Management**: Fix resource cleanup in WebSocket handlers to prevent leaks\n- [x] **Error Handling**: Enhance async error handling with proper cleanup procedures\n- [x] **Documentation**: Create Standard 014 for async/await best practices\n\n## Phase 8: Browser-Based Control Center (Completed)\n- [x] **Sidecar UI**: Implement `tools/sidecar.html` with dual tabs for retrieval and vision\n- [x] **Context UI**: Implement `tools/context.html` for manual context retrieval\n- [x] **Vision Engine**: Create `tools/vision_engine.py` for Python-powered image analysis\n- [x] **Bridge Integration**: Update `webgpu_bridge.py` to serve UI and handle vision endpoints\n- [x] **Endpoint Implementation**: Add `/v1/vision/ingest`, `/v1/memory/search`, `/logs/recent` endpoints\n- [x] **File-based Logging**: Implement persistent logging to `logs/` directory with truncation\n- [x] **Documentation**: Update specs and standards to reflect new architecture\n\n### Phase 9: Anchor Lite Refactor (Completed)\n- [x] **Consolidation**: Simplified system to Single Source of Truth (`context/`) -> Single Index (CozoDB) -> Single UI (`context.html`).\n- [x] **Cleanup**: Archived unused tools (`db_builder`, `memory-builder`, `sidecar`, `mobile-chat`).\n- [x] **Engine Refactor**: Created headless `ghost.html` engine with WebSocket bridge.\n- [x] **Launch Logic**: Unified startup in `start-anchor.bat` and `webgpu_bridge.py`.\n- [x] **Standard 023**: Documented \"Anchor Lite\" architecture and \"Triangle of Pain\".\n\n### Phase 10: Context Ingestion Pipeline Fixes (Completed)\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\n\n## Backlog\n- [ ] **Federation Protocol**: P2P sync.\n- [ ] **Android App**: Wrapper for Root Coda.",
    "source": "specs\\tasks.md"
  },
  {
    "id": "specs\\test-suite-model-verification.md",
    "timestamp": 1767215198,
    "role": "file",
    "content": "# Model Loading Test Suite Documentation\r\n\r\n## Overview\r\nThis document describes how the updated test suite has helped debug model loading issues in the Anchor Core system, particularly the dual loading system problems that caused inconsistent behavior across different UI components.\r\n\r\n## The Dual Loading System Problem\r\n\r\n### Background\r\nThe Anchor Core system had two different model loading pathways:\r\n1. **Bridge-based loading**: Uses local file resolution via `/models/{model}/resolve/main/{file}` endpoint\r\n2. **Direct online loading**: Uses direct HuggingFace URLs in the browser\r\n\r\n### Issues Identified\r\n- Models worked in some components (like `anchor-mic.html`) but not others (like `chat.html`)\r\n- Confusion about which approach to use\r\n- Inconsistent model availability across UI components\r\n- Debugging time wasted trying to fix local file resolution when online loading worked\r\n\r\n## Test Suite Updates and Their Debugging Impact\r\n\r\n### 1. `verify_hf_models.py` - Hugging Face Verification\r\n**Purpose**: Verify models are available on Hugging Face before local testing\r\n\r\n**Debugging Impact**:\r\n- Identified which models actually exist on Hugging Face\r\n- Prevented wasted time on models that don't exist online\r\n- Clarified the source of truth for model availability\r\n- Revealed that some models listed in documentation don't have available binaries\r\n\r\n### 2. `verify_local_models.py` - Local File Verification  \r\n**Purpose**: Check if required model files exist locally in the models directory\r\n\r\n**Debugging Impact**:\r\n- Identified which models are properly downloaded and available locally\r\n- Revealed that the models directory was empty in many cases\r\n- Showed the difference between online availability and local presence\r\n- Helped understand why local file resolution was failing\r\n\r\n### 3. `verify_model_complete.py` - Complete Pipeline Verification\r\n**Purpose**: End-to-end verification including Hugging Face ‚Üí Local ‚Üí Bridge availability\r\n\r\n**Debugging Impact**:\r\n- Revealed the complete pathway for model loading\r\n- Identified exactly where in the chain models were failing\r\n- Showed that some models were available via bridge redirects but not local files\r\n- Provided clear categorization of model status (locally available, bridge available, needs download, unavailable)\r\n\r\n## How the Test Suite Resolved the Issues\r\n\r\n### 1. Separation of Concerns\r\n**Before**: Single test tried to check both online and local availability, causing confusion\r\n**After**: Separate tests for each verification type, allowing clear diagnosis\r\n\r\n### 2. Online-First Verification with Redirect Handling\r\n**Before**: Tests assumed local files existed without verifying online availability first\r\n**After**: Tests first verify models exist on Hugging Face, then check local availability\r\n**Key Discovery**: 307/302 redirect status codes indicate files exist on Hugging Face (not missing)\r\n\r\n### 3. Clear Status Categorization\r\n**Before**: Models were just \"available\" or \"not available\" with unclear reasons\r\n**After**: Models categorized as:\r\n- Available locally (ready to use immediately)\r\n- Available via bridge (redirects to online sources)\r\n- Need download (via `/v1/models/pull` endpoint)\r\n- Completely unavailable (not on Hugging Face)\r\n\r\n### 4. Bridge Redirect Validation\r\n**Before**: No verification that the bridge redirect endpoint was working properly\r\n**After**: Explicit testing of bridge redirect functionality to ensure it properly serves files\r\n\r\n### 5. Key Finding - Models Are Available Online\r\n**Critical Discovery**: All tested models are available on Hugging Face with 307 redirects, indicating they exist and can be downloaded. The issue was not with online availability but with local presence and download status.\r\n\r\n## Resolution of Dual Loading System Issues\r\n\r\n### Problem: Inconsistent Behavior\r\n- `anchor-mic.html` worked with online loading\r\n- `chat.html` failed with local file resolution\r\n- Confusion about which approach to use\r\n\r\n### Solution: Test Suite Insights\r\nThe test suite revealed:\r\n1. Online loading (like in `anchor-mic.html`) works when models are available on Hugging Face\r\n2. Local file resolution (like in `chat.html`) fails when files aren't properly downloaded\r\n3. Bridge redirects can serve as a fallback when local files don't exist\r\n4. The system needs to handle both pathways gracefully\r\n\r\n### Recommended Approach\r\nBased on test results and Standard 008 (Online-Only Approach):\r\n1. First attempt to load via bridge redirect (which can serve local or redirect to online)\r\n2. Fallback to direct online loading if bridge fails\r\n3. Use consistent configuration patterns across all UI components\r\n4. Test both pathways during development to ensure compatibility\r\n\r\n### Actual Solution: Download Required Models (Enhanced Bridge Redirect)\r\nBased on the verification results, all models are available on Hugging Face but need to be downloaded to the local models directory:\r\n\r\n```bash\r\n# Example: Download the Qwen2.5-Coder-1.5B model using the API\r\ncurl -X POST http://localhost:8000/v1/models/pull \\\r\n  -H \"Authorization: Bearer sovereign-secret\" \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"model_id\": \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n    \"url\": \"https://huggingface.co/mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\"\r\n  }'\r\n```\r\n\r\n**Enhanced Bridge Redirect Implementation**: The system now implements Standard 009's bridge redirect logic:\r\n- **Check Local First**: When requesting `/models/{file_path}`, the bridge first checks for the file locally\r\n- **Serve Local**: If found locally, serves with proper no-cache headers\r\n- **Redirect Online**: If missing locally, redirects to HuggingFace with HTTP 302\r\n- **Fallback Resilience**: This ensures models work even if not downloaded locally\r\n\r\nAfter downloading models, they will be available for both local loading and bridge redirects. However, the enhanced bridge now provides automatic fallback to online sources when local files are missing.\r\n\r\n## Verification Workflow\r\n\r\n### For New Models:\r\n1. Run `verify_hf_models.py` to ensure model exists on Hugging Face\r\n2. Download model if needed using `/v1/models/pull` endpoint\r\n3. Run `verify_local_models.py` to confirm local availability\r\n4. Run `verify_model_complete.py` for end-to-end verification\r\n5. Test in browser components\r\n\r\n### For Troubleshooting:\r\n1. If model fails to load in UI, run complete verification to identify where it fails\r\n2. Check Hugging Face availability first\r\n3. Verify local file presence\r\n4. Test bridge redirect functionality\r\n5. Apply appropriate fix based on verification results\r\n\r\n## Conclusion\r\n\r\nThe updated test suite has successfully resolved the dual loading system confusion by:\r\n- Separating online and local verification concerns\r\n- Providing clear status categorization\r\n- Enabling systematic debugging of model loading issues\r\n- Supporting both loading pathways with clear fallback strategies\r\n- Following the \"online-first\" approach documented in standards\r\n\r\nThis systematic approach prevents the \"groundhog day\" effect where the same model loading issues are debugged repeatedly without understanding the root cause.",
    "source": "specs\\test-suite-model-verification.md"
  },
  {
    "id": "specs\\architecture\\anchor-core.spec.md",
    "timestamp": 1767327019,
    "role": "file",
    "content": "# Anchor Core Architecture Specification (v2.3.4)\n\n**Status:** Active (Text-Only + Watchdog + Context Persistence + Enhanced Resurrection) | **Component:** `tools/webgpu_bridge.py`\n\n## Overview\nThe Anchor Core unifies the Bridge, UI, and API into a single process running on port 8000, eliminating CORS issues and port conflicts. The system now uses an online-only model loading approach for reliable operation, with local model resolution as a fallback. Includes Watchdog service for passive text ingestion and enhanced auto-resurrection for browser process management.\n\n## Architecture Diagram\n\n```mermaid\ngraph TD\n    User[\"üë§ User\"] --> Dashboard[\"üè† Dashboard (index.html)\"]\n\n    subgraph \"ANCHOR CORE (Single Process)\"\n        Bridge[\"üîó WebGPU Bridge (8000)\"]\n        API[\"‚ö° API Endpoints\"]\n        UI[\"üåê UI Server\"]\n        WS[\"üì° WebSocket Layer\"]\n        ModelRedirect[\"üîÑ Model Redirect\"]\n        Resurrection[\"üîÑ Auto-Resurrection\"]\n    end\n\n    subgraph \"PASSIVE INGESTION\"\n        Watchdog[\"üêï Watchdog Service\"]\n        ContextFolder[\"üìÅ context/ folder\"]\n        FileEvents[\"üìù File System Events\"]\n    end\n\n    Dashboard --> Bridge\n    Bridge --> API\n    Bridge --> UI\n    Bridge --> WS\n    Bridge --> ModelRedirect\n    Bridge --> Resurrection\n\n    subgraph \"SHELL PROTOCOL\"\n        API --> ShellExec[\"Ïâò /v1/shell/exec\"]\n        API --> Spawn[\"üöÄ /v1/system/spawn_shell\"]\n    end\n\n    subgraph \"MEMORY INGESTION\"\n        API --> IngestAPI[\"üì• /v1/memory/ingest\"]\n        Watchdog --> IngestAPI\n        ContextFolder --> Watchdog\n        FileEvents --> Watchdog\n    end\n\n    ShellExec --> Host[\"üñ•Ô∏è Host System\"]\n    Spawn --> PowerShell[\"ü™ü PowerShell Window\"]\n\n    subgraph \"MODEL LOADING\"\n        ModelRedirect --> Local[\"üíæ Local Models (if available)\"]\n        ModelRedirect --> Online[\"üåê Online Models (fallback)\"]\n    end\n```\n\n## Components\n\n### 1. The Unified Core (`webgpu_bridge.py`)\n- **Role:** Single server for API, UI, and WebSockets\n- **Port:** 8000 (The One Port)\n- **Function:** Bridges browser WebGPU to system commands\n\n### 2. Shell Protocol (The Hands)\n- **`/v1/shell/exec`**: Execute system commands via bridge\n\n### 3. Context System\n- **Context UI**: Read-only interface for quick context retrieval and copy-paste.\n- **Memory Search**: Query the Ghost Engine's Graph (Vector + BM25) for relevant context.\n\n\n### 4. Model Loading System\n- **Online-First**: Uses direct HuggingFace URLs for reliable loading (Standard 007)\n- **Local Fallback**: Redirects to local models when available, online when not\n- **Bridge Redirect**: `/models/{model}/resolve/main/{file}` endpoint handles resolution\n\n### 5. Passive Ingestion System\n- **Watchdog Service**: Monitors `context/` folder for text file changes\n- **Debounce & Hash Check**: Prevents duplicate ingestion from autosave events\n- **File Ingestion Endpoint**: `/v1/memory/ingest` for text content ingestion\n- **Automatic Processing**: Processes .txt, .md, .markdown, .py, .js, .html, .json, .yaml, .yml, .sh, .bat and other code/text files automatically\n- **Code File Support**: Expanded to include common programming language extensions\n- **Session Recording**: Daily chat session files created in `context/sessions/` for persistence\n\n### 6. Session Recording System\n- **Daily Files**: Creates `chat_YYYY-MM-DD.md` files for each day's conversations\n- **Text-File Source of Truth**: All chat history saved to markdown files for cross-machine sync\n- **Timestamped Entries**: Messages formatted with timestamps (`### ROLE [HH:MM:SS]`)\n- **Infinite Loop**: Chat -> File -> Ingestion -> Memory -> Next Chat\n- **Cross-Machine Sync**: Files automatically synced via Dropbox/Git for multi-device access\n\n### 7. Process Management System\n- **Auto-Resurrection**: Automatically restarts Ghost Engine when connection drops\n- **Process Cleanup**: Kills existing browser processes before launching new ones\n- **Port Management**: Explicitly manages remote debugging port (9222) to prevent conflicts\n- **Retry Logic**: Implements retry mechanism with configurable attempts\n\n## Endpoints\n\n### `GET /sidecar`\n- **Function:** Serves the Context UI (consolidated interface).\n\n### `GET /context`\n- **Function:** Serves the Context UI (consolidated interface).\n\n### `POST /v1/memory/search`\n- **Function:** Queries the Ghost Engine's Graph (Vector + BM25).\n- **Input:** `{ \"query\": \"string\" }`\n- **Output:** `{ \"context\": \"Formatted Ground Truth...\" }`\n\n### `POST /v1/chat/completions`\n- **Function:** Proxy to browser engine\n- **Auth:** Bearer token\n\n### `POST /v1/shell/exec`\n- **Function:** Execute system commands\n- **Format:** `{ \"cmd\": \"command\" }`\n\n### `POST /v1/system/spawn_shell`\n- **Function:** Launch native PowerShell client\n- **Result:** New `anchor.py` terminal window\n\n### `POST /v1/memory/ingest`\n- **Function:** Ingest text content into memory graph\n- **Format:** `{ \"content\": \"text\", \"source\": \"file_path\", \"timestamp\": \"ISO\" }`\n- **Response:** `{ \"status\": \"success\", \"message\": \"Memory ingested successfully\" }`\n\n### `POST /v1/gpu/lock`\n- **Function:** Acquire GPU resource with queuing\n- **Response:** `{ \"status\": \"acquired\", \"token\": \"gpu_token_...\" }`\n\n### `POST /v1/gpu/unlock`\n- **Function:** Release GPU resource\n- **Response:** `{ \"status\": \"released\" }`\n\n### `GET /v1/gpu/status`\n- **Function:** Get GPU lock status and queue depth\n- **Response:** `{ \"locked\": true/false, \"owner\": \"agent_id\", \"queue_depth\": number }`\n\n## Search Architecture\n\n### Hybrid Retrieval System\n\n* **BM25 FTS**: Lexical search using CozoDB Full Text Search with stemming\n* **Context Manager**: Intelligent retrieval in `ContextManager.findRelevantMemories()`\n* **Fallback Mechanism**: Regex-based search when FTS index unavailable\n\n## Security\n- **Token Auth:** `Authorization: Bearer sovereign-secret`\n- **CORS Policy:** Open for internal use only\n- **System Access:** Restricted to authorized commands",
    "source": "specs\\architecture\\anchor-core.spec.md"
  },
  {
    "id": "specs\\architecture\\api.spec.md",
    "timestamp": 1766733152,
    "role": "file",
    "content": "# API Specification (WebGPU Bridge)\n\n**Status:** Production\n**Component:** `tools/webgpu_bridge.py`\n\n## Overview\nThe \"Bridge\" acts as a reverse-proxy, exposing the browser's WebLLM engine as an OpenAI-compatible API.\n\n## Endpoints\n\n### `POST /v1/chat/completions`\n- **Format:** OpenAI Standard.\n- **Flow:** \n  1. Client sends JSON to Python Bridge.\n  2. Bridge forwards via WebSocket to `model-server-chat.html`.\n  3. Browser computes response (WebGPU).\n  4. Result streamed back to Bridge -> Client.\n\n### `POST /v1/embeddings`\n- **Format:** OpenAI Standard.\n- **Flow:** Forwards to `webgpu-server-embed.html`.\n\n### `POST /v1/shell/exec`\n- **Purpose:** Neural Shell Protocol (The Hands). Executes arbitrary shell commands on host.\n- **Format:** JSON `{ \"cmd\": \"string\" }`.\n- **Response:** JSON `{ \"stdout\": \"...\", \"stderr\": \"...\", \"code\": 0 }`.\n- **Security:** Strict Token Auth required. 30s timeout.\n\n## WebSockets\n- `/ws/chat`: Connection for Chat Worker.\n- `/ws/embed`: Connection for Embedding Worker.\n\n## Security\n- **Token Auth:** `Authorization: Bearer <BRIDGE_TOKEN>` required.\n- **Network:** Binds to random port (obfuscation) or `8000` (default).\n",
    "source": "specs\\architecture\\api.spec.md"
  },
  {
    "id": "specs\\architecture\\sovereign-wasm.spec.md",
    "timestamp": 1767189568,
    "role": "file",
    "content": "# Sovereign WASM Specification (Root Kernel)\n\n## Architecture Overview\nThe **Root Coda** system runs entirely in the browser using a unified Kernel (`sovereign.js`) that manages Compute (WebLLM) and Memory (CozoDB).\n\n## 1. The Kernel (`tools/modules/sovereign.js`)\nThe Kernel is the standard library for all Root Tools. It enforces consistency and safety.\n\n### 1.1 Hardware Abstraction (\"Snapdragon Fix\")\n**Problem**: Adreno GPUs (Snapdragon X Elite) and some mobile chips crash if a WebGPU buffer >256MB is requested, or if context exceeds 4k tokens without specific driver flags.\n**Solution**: `getWebGPUConfig(profile)`\n- **Lite**: Clamps buffer to 256MB, Context to 2048.\n- **Mid**: Clamps buffer to 1GB, Context to 4096.\n- **High/Ultra**: Unlocked.\n\n### 1.2 Unified Logging\n**Problem**: `console.log` is invisible on mobile or when running as a PWA.\n**Solution**: `SovereignLogger`\n- Broadcasts all logs to `BroadcastChannel('sovereign-logs')`.\n- Consumed by `log-viewer.html` for real-time remote debugging.\n\n### 1.3 Reactive State\n**Problem**: Spaghetti code updating DOM elements manually.\n**Solution**: `createStore(initialState)`\n- Lightweight `Proxy`-based store.\n- Components subscribe to changes: `subscribe((key, val) => updateUI(key, val))`.\n\n## 2. Memory Layer (CozoDB WASM)\nThe Kernel provides a standardized loader: `initCozo(wasmPath)`.\n\n### Data Portability\n- **Lossless Export**: The Root Builder features a \"Lossless Export\" button.\n- **Mechanism**: Dumps full Cozo relations (including vectors) to a JSON file.\n- **Use Case**: Transfer full \"Brain\" state between devices or backup.\n\n### Search Enhancement (BM25 FTS)\n- **Hybrid Retrieval**: Combines vector search (semantic) with BM25 FTS (lexical) for optimal results.\n- **Index Creation**: FTS index created during initialization: `::fts create memory:content_fts`\n- **Stemming Support**: Uses English stemming for better word variation matching.\n- **Fallback Mechanism**: Maintains regex-based search when FTS index unavailable.\n\n### Schema\n```datalog\n:create memory {\n    id: String\n    =>\n    timestamp: Int,\n    role: String,\n    content: String,\n    source: String,\n    embedding: <F32; 384>\n}\n```\n\n## 3. Tool Bridge (Legacy Support)\nThe `webgpu_bridge.py` acts as a secure relay (websocket <-> http) for external tools (like VS Code extensions) to access the Browser's LLM.\n- **Input**: HTTP/REST (`/v1/chat/completions`)\n- **Output**: WebSocket (`ws://localhost:8080/ws/chat`)\n\n### 3.1 Local Model Serving (Storage Quota Bypass)\n**Problem**: Browsers (especially in Incognito/Guest modes) strictly limit persistent storage (e.g., <300MB), preventing the caching of large LLM weights (~2GB+).\n**Solution**: The Bridge acts as a local HTTP File Server.\n- **Endpoint**: `http://localhost:8080/models/{model_id}/...`\n- **Mechanism**: \n    1. Frontend requests model from localhost.\n    2. If 404, Frontend triggers `POST /v1/models/pull`.\n    3. Bridge downloads artifacts from Hugging Face to `./models`.\n    4. Frontend polls status and loads the model into RAM (bypassing IndexedDB quota).\n\n## 4. Audio Input (Root Mic)\n**Goal**: Pure client-side speech-to-text without sending audio to a cloud.\n\n### 4.1 Pipeline\n1. **Capture**: `MediaRecorder` (WebM) -> 48kHz decoding.\n2. **Preprocessing**:\n   - Downsampling to 16kHz (Whisper Native).\n   - **Noise Gate**: Discards audio if peak amplitude < 0.01 (Prevents transcribing silence).\n   - **Amplification**: Smart gain (max 5x) for quiet voices, but capped to avoid boosting noise floor.\n3. **Inference (WASM)**: \n   - Model: `Xenova/whisper-tiny.en` (Quantized).\n   - **Long-form Strategy**: Uses `chunk_length_s: 30` and `stride_length_s: 5` to process audio exceeding the model's native 30s window.\n4. **Post-Processing (Refinement)**:\n   - **Hallucination Filter**: Regex removal of common Whisper artifacts (e.g., \"[Music]\", \"Applause\", \"Amara.org\").\n   - **LLM Cleanup**: The raw transcript is passed to the local Qwen2.5 instance with a system prompt to fix grammar/punctuation without altering meaning.\n\n### 4.2 Summarization Loop\n- **Trigger**: User clicks \"Summarize & Clarify\" after a successful transcription.\n- **Process**: The cleaned transcript is sent back to the Local Kernel (Qwen2.5) with a prompt to \"summarize and clarify core meaning.\"\n- **Output**: The transcript is replaced by the summary, which is automatically copied to the clipboard.\n\n## 5. Parallel Compute (The Worker)\nTo prevent UI freezing during heavy inference, the LLM runs in a dedicated Web Worker.\n\n### 5.1 `tools/modules/llm-worker.js`\n- **Role**: Hosts the `MLCEngine` instance.\n- **Communication**: Uses `WebWorkerMLCEngineHandler` to bridge messages between the main thread and the worker.\n- **Benefit**: Ensures the UI remains responsive (scrolling, typing) even while the GPU is crunching tokens.\n\n## 6. Resource Management (Orchestrator)\n**Problem**: Multiple browser tabs (Mic, Console, Dreamer) competing for the single GPU resource led to deadlocks, timeouts, and \"Device Lost\" errors.\n**Solution**: A Priority-Queue based Locking System with enhanced timeout handling and emergency procedures.\n\n### 6.1 GPU Controller (`tools/modules/sovereign.js`)\n- **Serialized Loading**: `withModelLoadLock()` ensures only one tab loads a model at a time, preventing GPU overload during initial loading.\n- **Access Priority**:\n  - **Priority 0 (High)**: Root Mic (Voice Input - cannot wait)\n  - **Priority 10 (Med)**: Root Console (Chat - user waiting)\n  - **Priority 15 (Med)**: Default priority\n  - **Priority 20 (Low)**: Root Dreamer (Background tasks)\n- **Timeouts**: Increased broken-lock timeout from 60s to **120s** (2 minutes) to accommodate large model loading.\n- **Retry Logic**: Added retry mechanism with proper error handling.\n- **Fallback Mechanism**: Direct WebGPU access when bridge unavailable.\n- **Status Checking**: Added GPU status check functionality.\n- **Emergency Release**: If a lock is held >120s, it is forcibly broken to prevent system deadlock.\n\n### 6.2 Bridge Orchestration (`smart_gpu_bridge.py`)\n- **Queue Tracking**: Tracks request start times to prevent starvation.\n- **Enhanced Timeouts**: Increased timeout from 60s to 120s for lock acquisition.\n- **Request Tracking**: Added request_start_times to prevent queue starvation.\n- **Enhanced Status**: Detailed queue information in status endpoint.\n- **Endpoints**:\n  - `GET /v1/gpu/status`: Monitor active locks and queue depth.\n  - `POST /v1/gpu/lock`: Acquire lock (blocking).\n  - `POST /v1/gpu/unlock`: Release GPU lock.\n  - `POST /v1/gpu/reset`: Standard reset.\n  - `POST /v1/gpu/force-release-all`: Nuclear option for stuck states.\n  - `POST /v1/gpu/force-release`: Emergency release endpoint.\n  - `POST /v1/hot-reload`: Hot reload endpoint for development.\n\n### 6.3 GPU Management Utilities\n- **GPU Manager Script** (`scripts/gpu_manager.py`): Command-line tool to monitor and manage GPU resources.\n- **Test Script** (`scripts/test_gpu_fixes.py`): Comprehensive testing of GPU resource management.\n- **Monitoring Commands**:\n  - `python scripts/gpu_manager.py --status`: Check GPU status\n  - `python scripts/gpu_manager.py --monitor --interval 10`: Monitor continuously\n  - `python scripts/gpu_manager.py --force-release`: Force release GPU locks\n  - `python scripts/gpu_manager.py --reset`: Standard reset\n  - `python scripts/gpu_manager.py --hot-reload`: Trigger hot reload\n\n## 7. Development Infrastructure\n**Problem**: Restarting the Python bridge and refreshing 3 browser tabs for every small code change is slow.\n\n### 7.1 Hot Reload System\n- **Backend**: `smart_gpu_bridge.py` monitors its own source code (and `download_models.py`) for changes. It automatically reloads the Python process while preserving active WebSocket connections if possible.\n- **Frontend**: `gpu-hot-reloader.js` connects to the bridge via WebSocket. When the bridge signals a reload (or detects an HTML update), the browser auto-refreshes.\n- **Safety**: Automatically releases all GPU locks during a reload event to prevent \"Ghost Locks\".\n",
    "source": "specs\\architecture\\sovereign-wasm.spec.md"
  },
  {
    "id": "specs\\protocols\\001-autonomous-execution.md",
    "timestamp": 1767456625,
    "role": "file",
    "content": "# Protocol 001: Autonomous Execution & Service Verification\n\n## The Rule\nWhen executing any service, server, or long-running task, the Agent MUST:\n\n1.  **Isolate Logs:** Create a dedicated log file (e.g., `logs/startup_checks.log`).\n2.  **Detached Execution:** Launch the process in background/detached mode.\n    - *Linux/Mac:* `nohup node src/index.js > ../logs/server.log 2>&1 &`\n    - *Windows:* `Start-Process node -ArgumentList \"src/index.js\" -RedirectStandardOutput \"../logs/server.log\" -WindowStyle Hidden`\n3.  **The \"Pulse Check\":**\n    - Wait 5 seconds.\n    - Read the log file to check for \"Error\" or \"Exception\".\n4.  **Verification:**\n    - If the log shows \"Listening on port X\", perform a HTTP GET to `/health` to confirm.\n    - ONLY then declare success.",
    "source": "specs\\protocols\\001-autonomous-execution.md"
  },
  {
    "id": "templates\\waveai_ece.json",
    "timestamp": 1766171656,
    "role": "file",
    "content": "ece-local:\n  display:name: Sovereign Console (ECE)\n  display:order: 1\n  display:icon: microchip\n  display:description: Local WebGPU Model via ECE Bridge\n  ai:provider: custom\n  ai:apitype: openai-chat\n  ai:model: webgpu-chat\n  ai:thinkinglevel: medium\n  ai:endpoint: http://127.0.0.1:8080/v1/chat/completions\n  ai:apitoken: not-needed\n  ai:capabilities:\n    - tools",
    "source": "templates\\waveai_ece.json"
  },
  {
    "id": "tools\\anchor-mic.html",
    "timestamp": 1767142174,
    "role": "file",
    "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Root Mic üéôÔ∏è</title>\n    <style>\n        :root {\n            --bg-color: #0f0f11;\n            --surface-color: #1a1a1d;\n            --primary-color: #00ff88;\n            --accent-color: #00ccff;\n            --text-color: #eeeeee;\n            --danger-color: #ff4444;\n        }\n\n        body {\n            background-color: var(--bg-color);\n            color: var(--text-color);\n            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n            margin: 0;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            justify-content: center;\n            min-height: 100vh;\n            padding: 8px; /* Add padding to prevent cutoff */\n            overflow: auto; /* Allow scrolling if needed */\n        }\n\n        .container {\n            text-align: center;\n            width: 100%;\n            max-width: 500px;\n            padding: 20px;\n        }\n\n        h1 {\n            font-weight: 300;\n            letter-spacing: 2px;\n            margin-bottom: 30px;\n            text-transform: uppercase;\n            font-size: 1.5rem;\n            color: var(--accent-color);\n            text-shadow: 0 0 10px rgba(0, 204, 255, 0.3);\n        }\n\n        #mic-btn {\n            width: 150px;\n            height: 150px;\n            border-radius: 50%;\n            background: radial-gradient(circle at 30% 30%, #444, #222);\n            border: 4px solid #333;\n            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5), 0 0 0 4px var(--bg-color);\n            cursor: pointer;\n            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            margin: 0 auto 30px;\n            position: relative;\n            outline: none;\n            -webkit-tap-highlight-color: transparent;\n        }\n\n        #mic-btn::after {\n            content: '';\n            position: absolute;\n            top: -10px;\n            left: -10px;\n            right: -10px;\n            bottom: -10px;\n            border-radius: 50%;\n            border: 2px solid var(--primary-color);\n            opacity: 0;\n            transform: scale(0.8);\n            transition: all 0.3s;\n        }\n\n        #mic-btn:hover {\n            transform: scale(1.05);\n            background: radial-gradient(circle at 30% 30%, #555, #333);\n        }\n\n        #mic-btn.active {\n            background: radial-gradient(circle at 30% 30%, #ff5555, #aa0000);\n            box-shadow: 0 0 30px rgba(255, 68, 68, 0.6);\n            border-color: #ff4444;\n            transform: scale(0.95);\n        }\n\n        #mic-btn.active::after {\n            animation: pulse 1.5s infinite;\n            opacity: 1;\n        }\n\n        #mic-icon {\n            font-size: 64px;\n            color: #888;\n            transition: color 0.3s;\n        }\n\n        #mic-btn.active #mic-icon {\n            color: white;\n        }\n\n        #clarify-btn {\n            background: transparent;\n            color: var(--accent-color);\n            border: 1px solid var(--accent-color);\n            padding: 8px 16px;\n            border-radius: 20px;\n            cursor: pointer;\n            font-size: 0.9rem;\n            margin-bottom: 20px;\n            transition: all 0.3s;\n            text-transform: uppercase;\n            letter-spacing: 1px;\n        }\n\n        #clarify-btn:hover:not(:disabled) {\n            background: rgba(0, 204, 255, 0.1);\n            transform: translateY(-2px);\n        }\n\n        #clarify-btn:disabled {\n            opacity: 0.3;\n            cursor: not-allowed;\n            border-color: #555;\n            color: #555;\n        }\n\n        #status {\n            font-size: 1.2rem;\n            margin-bottom: 20px;\n            height: 1.5em;\n            color: #888;\n        }\n\n        .visualizer {\n            width: 100%;\n            height: 60px;\n            background: var(--surface-color);\n            border-radius: 12px;\n            margin-bottom: 20px;\n            position: relative;\n            overflow: hidden;\n            border: 1px solid #333;\n        }\n\n        .bar-container {\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            height: 100%;\n            gap: 2px;\n        }\n\n        .bar {\n            width: 4px;\n            background: var(--primary-color);\n            border-radius: 2px;\n            height: 4px;\n            transition: height 0.1s ease;\n        }\n\n        #output {\n            background: var(--surface-color);\n            padding: 20px;\n            border-radius: 12px;\n            border: 1px solid #333;\n            min-height: 100px;\n            text-align: left;\n            font-family: 'Courier New', monospace;\n            font-size: 0.9rem;\n            color: #ccc;\n            position: relative;\n            overflow-y: auto;\n            max-height: 200px;\n        }\n\n        #copy-toast {\n            position: absolute;\n            top: 20px;\n            right: 20px;\n            background: var(--primary-color);\n            color: #000;\n            padding: 8px 16px;\n            border-radius: 20px;\n            font-weight: bold;\n            opacity: 0;\n            transform: translateY(-20px);\n            transition: all 0.3s;\n            pointer-events: none;\n        }\n\n        #copy-toast.show {\n            opacity: 1;\n            transform: translateY(0);\n        }\n\n        footer {\n            margin-top: 40px;\n            font-size: 0.7rem;\n            color: #444;\n        }\n\n        @keyframes pulse {\n            0% {\n                transform: scale(1);\n                opacity: 1;\n                border-color: var(--danger-color);\n            }\n\n            100% {\n                transform: scale(1.5);\n                opacity: 0;\n                border-color: var(--danger-color);\n            }\n        }\n\n        #loading-overlay {\n            position: fixed;\n            top: 0;\n            left: 0;\n            width: 100%;\n            height: 100%;\n            background: rgba(0, 0, 0, 0.9);\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            justify-content: center;\n            z-index: 1000;\n            transition: opacity 0.5s;\n        }\n\n        .loader {\n            width: 48px;\n            height: 48px;\n            border: 5px solid #FFF;\n            border-bottom-color: var(--primary-color);\n            border-radius: 50%;\n            display: inline-block;\n            box-sizing: border-box;\n            animation: rotation 1s linear infinite;\n        }\n\n        .progress-text {\n            margin-top: 20px;\n            font-family: monospace;\n            color: var(--primary-color);\n        }\n\n        @keyframes rotation {\n            0% {\n                transform: rotate(0deg);\n            }\n\n            100% {\n                transform: rotate(360deg);\n            }\n        }\n    </style>\n</head>\n<body>\n    <div id=\"loading-overlay\">\n        <span class=\"loader\"></span>\n        <div id=\"loading-text\" class=\"progress-text\">Initializing Neural Engines...</div>\n    </div>\n    <div id=\"copy-toast\">Copied to Clipboard!</div>\n    <div class=\"container\">\n        <h1>Root Mic üéôÔ∏è</h1>\n        <div class=\"visualizer\"><div class=\"bar-container\" id=\"bars\"></div></div>\n        <button id=\"mic-btn\"><div id=\"mic-icon\">üéôÔ∏è</div></button>\n        <button id=\"clarify-btn\" disabled>Refine Text</button>\n        <div id=\"status\">Ready</div>\n        <div id=\"output\">...</div>\n        <footer>Running Locally: Whisper-Tiny (Audio) + Qwen2.5-1.5B (Text)</footer>\n    </div>\n\n    <script type=\"module\">\n        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\n        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\n\n        // THE NEW KERNEL\n        import { AnchorLogger, createStore, getWebGPUConfig, GPUController } from './modules/anchor.js';\n\n        // Load hot reload functionality in development\n        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {\n            import('./modules/gpu-hot-reloader.js').then(() => {\n                console.log('üîÑ GPU Hot Reloader loaded for development');\n            }).catch(err => {\n                console.warn('‚ö†Ô∏è GPU Hot Reloader not available:', err);\n            });\n        }\n\n        const logger = new AnchorLogger('Root-Mic');\n        \n        // Reactive Store\n        const { state, subscribe } = createStore({\n            status: 'Ready',\n            output: '...',\n            isLoading: true,\n            loadingText: 'Initializing Neural Engines...',\n            isRecording: false\n        });\n\n        // UI Bindings\n        subscribe((prop, val) => {\n            if (prop === 'status') document.getElementById('status').innerText = val;\n            if (prop === 'output') {\n                document.getElementById('output').innerText = val;\n                // Enable clarify button if there is text (and not just placeholder/loading)\n                const btn = document.getElementById('clarify-btn');\n                if (val && val !== '...' && val.length > 10) {\n                    btn.disabled = false;\n                } else {\n                    btn.disabled = true;\n                }\n            }\n            if (prop === 'loadingText') document.getElementById('loading-text').innerText = val;\n            if (prop === 'isLoading') {\n                const ol = document.getElementById('loading-overlay');\n                ol.style.opacity = val ? '1' : '0';\n                setTimeout(() => ol.style.display = val ? 'flex' : 'none', 500);\n            }\n            if (prop === 'isRecording') {\n                const btn = document.getElementById('mic-btn');\n                if (val) btn.classList.add('active'); else btn.classList.remove('active');\n            }\n        });\n\n        let whisperWorker = null;\n        let llmEngine = null;\n        let mediaRecorder = null;\n        let audioChunks = [];\n        let audioContext = null;\n        let analyser = null;\n        let dataArray = null;\n        let animationId = null;\n        let silenceStart = 0;\n\n        async function init() {\n            try {\n                // 1. Whisper Init (Worker)\n                state.loadingText = \"Step 1/2: Initializing Whisper Worker...\";\n                whisperWorker = new Worker('./modules/whisper-worker.js', { type: 'module' });\n                \n                // Wait for worker init\n                await new Promise((resolve, reject) => {\n                    whisperWorker.onmessage = (e) => {\n                        if (e.data.type === 'init_done') resolve();\n                        if (e.data.type === 'error') reject(new Error(e.data.error));\n                    };\n                    whisperWorker.postMessage({ type: 'init' });\n                });\n                logger.success(\"Whisper Worker Ready\");\n\n                // 2. LLM Init (Using Kernel)\n                state.loadingText = \"Step 2/2: Config Qwen2.5 (Brain)...\";\n                await initLLM();\n\n                state.isLoading = false;\n                initVisualizer();\n                logger.success(\"Root Mic Online\");\n            } catch (e) {\n                logger.error(e.message);\n                state.loadingText = `Error: ${e.message}`;\n            }\n        }\n\n        // Clarify Logic\n        document.getElementById('clarify-btn').addEventListener('click', async () => {\n            if (!state.output || state.output === '...' || state.output.length < 5) return;\n            \n            const originalText = state.output;\n            state.status = \"Refining...\";\n            state.output = originalText + \"\\n\\n[Refining...]\";\n\n            try {\n                const reply = await llmEngine.chat.completions.create({\n                    messages: [\n                        { role: \"system\", content: \"You are a professional text editor. Your task is to refine the provided speech-to-text output into clear, coherent, and polished text suitable for reading or pasting. Fix grammar and flow, but keep the original meaning intact. Output ONLY the refined text.\" },\n                        { role: \"user\", content: `Refine this text:\\n\\n\"${originalText}\"` }\n                    ],\n                    temperature: 0.5,\n                    max_tokens: 512,\n                });\n\n                const summary = reply.choices[0].message.content;\n                state.output = summary; // Replace output with summary\n                state.status = \"Clarified\";\n                \n                if (document.hasFocus()) {\n                    navigator.clipboard.writeText(summary);\n                    const t = document.getElementById('copy-toast');\n                    t.innerText = \"Refined Text Copied!\";\n                    t.classList.add('show');\n                    setTimeout(() => { \n                        t.classList.remove('show');\n                        t.innerText = \"Copied to Clipboard!\";\n                    }, 2000);\n                }\n\n            } catch (e) {\n                logger.error(\"Clarify failed: \" + e.message);\n                state.status = \"Error\";\n                state.output = originalText; // Revert\n            }\n        });\n\n        async function initLLM() {\n            const modelId = \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\";\n            const snapdragonId = \"snapdragon-mic-qwen\";\n\n            // 0. THE BLOCKER (Model Load Lock) - Serialize model loading\n            logger.info(\"Requesting Model Load Lock...\");\n\n            try {\n                await GPUController.withModelLoadLock(\"Root-Mic\", async () => {\n                    logger.success(\"Model Load Lock Acquired.\");\n\n                    // KERNEL CALL: Get safe GPU config\n                    const gpuConfig = await getWebGPUConfig('lite');\n\n                    if (gpuConfig.isConstrained) {\n                        logger.warn(`Clamping Buffer to ${Math.round(gpuConfig.maxBufferSize/1024/1024)}MB for Mobile/XPS compatibility.`);\n                    }\n\n                    // Create device explicitly with limits\n                    const device = await gpuConfig.adapter.requestDevice(gpuConfig.deviceConfig);\n\n                    const appConfig = {\n                        model_list: [{\n                            model: \"https://huggingface.co/\" + modelId + \"/resolve/main/\",\n                            model_id: snapdragonId,\n                            model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\n                            vram_required_MB: 2000,\n                            low_resource_required: true,\n                            buffer_size_required_bytes: gpuConfig.maxBufferSize\n                        }]\n                    };\n\n                    llmEngine = await webllm.CreateMLCEngine(snapdragonId, {\n                        appConfig,\n                        device,\n                        initProgressCallback: (report) => {\n                            state.loadingText = `Loading Brain: ${Math.round(report.progress * 100)}%`;\n                        }\n                    });\n                });\n            } catch (error) {\n                state.loadingText = `Model Load Error: ${error.message}`;\n\n                // Try to check GPU status for more information\n                try {\n                    const status = await GPUController.checkStatus();\n                    if (status && status.locked) {\n                        logger.warn(`GPU currently locked by: ${status.owner || 'unknown'}`);\n                        if (status.queued && status.queued.length > 0) {\n                            logger.warn(`Queue: ${status.queued.join(', ')}`);\n                        }\n                    }\n                } catch (statusErr) {\n                    logger.warn(`Could not get GPU status: ${statusErr.message}`);\n                }\n\n                throw error;\n            }\n        }\n\n        function initVisualizer() {\n            const container = document.getElementById('bars');\n            for (let i = 0; i < 30; i++) {\n                const bar = document.createElement('div');\n                bar.className = 'bar';\n                container.appendChild(bar);\n            }\n        }\n\n        // Recording Logic\n        document.getElementById('mic-btn').addEventListener('click', async () => {\n            if (!state.isRecording) startRecording(); else stopRecording();\n        });\n\n        async function startRecording() {\n            try {\n                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n                mediaRecorder = new MediaRecorder(stream);\n                audioChunks = [];\n                mediaRecorder.ondataavailable = e => audioChunks.push(e.data);\n                mediaRecorder.onstop = processAudio;\n\n                // Visualizer\n                audioContext = new (window.AudioContext || window.webkitAudioContext)();\n                const source = audioContext.createMediaStreamSource(stream);\n                analyser = audioContext.createAnalyser();\n                analyser.fftSize = 64;\n                source.connect(analyser);\n                dataArray = new Uint8Array(analyser.frequencyBinCount);\n                animateVisualizer();\n\n                mediaRecorder.start();\n                state.isRecording = true;\n                silenceStart = Date.now();\n                state.status = \"Listening...\";\n                state.output = \"...\";\n            } catch (e) { alert(e.message); }\n        }\n\n        function stopRecording() {\n            mediaRecorder.stop();\n            state.isRecording = false;\n            state.status = \"Processing...\";\n            cancelAnimationFrame(animationId);\n            if (audioContext) audioContext.close();\n            document.querySelectorAll('.bar').forEach(b => b.style.height = '4px');\n        }\n\n        function animateVisualizer() {\n            if (!state.isRecording) return;\n            animationId = requestAnimationFrame(animateVisualizer);\n            analyser.getByteFrequencyData(dataArray);\n            const bars = document.querySelectorAll('.bar');\n            let maxVol = 0;\n            for (let i = 0; i < bars.length; i++) {\n                const val = dataArray[i];\n                if (val > maxVol) maxVol = val;\n                bars[i].style.height = `${Math.max(4, (val / 255) * 50)}px`;\n            }\n            if (maxVol > 10) silenceStart = Date.now();\n            else if (Date.now() - silenceStart > 3000) state.status = \"‚ö†Ô∏è No Audio?\";\n        }\n\n        async function processAudio() {\n            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); // Default browser format\n\n            // 1. Decode to System Sample Rate (e.g., 48000Hz)\n            const audioCtx = new AudioContext();\n            const arrayBuffer = await audioBlob.arrayBuffer();\n            const decodedBuffer = await audioCtx.decodeAudioData(arrayBuffer);\n\n            // 2. Resample to 16000Hz (Required by Whisper)\n            const targetRate = 16000;\n            const offlineCtx = new OfflineAudioContext(1, decodedBuffer.duration * targetRate, targetRate);\n            const source = offlineCtx.createBufferSource();\n            source.buffer = decodedBuffer;\n            source.connect(offlineCtx.destination);\n            source.start(0);\n            \n            const resampledBuffer = await offlineCtx.startRendering();\n            let audioData = resampledBuffer.getChannelData(0);\n\n            // NORMALIZE / AMPLIFY with Noise Gate\n            let peak = 0;\n            for (let i = 0; i < audioData.length; i++) {\n                const val = Math.abs(audioData[i]);\n                if (val > peak) peak = val;\n            }\n\n            // Cap amplification to avoid boosting silence/hiss into \"Applause\"\n            // If peak is TOO low (silence), don't amplify at all.\n            let ampFactor = 1.0;\n            if (peak > 0.01 && peak < 0.5) {\n                ampFactor = Math.min(0.5 / peak, 5.0); // Max 5x boost\n                for (let i = 0; i < audioData.length; i++) {\n                    audioData[i] = audioData[i] * ampFactor;\n                }\n            } else if (peak <= 0.01) {\n                // Too quiet, likely silence. Don't send to Whisper or send silence.\n                state.status = \"Too quiet (Ignored)\";\n                return;\n            }\n\n            state.status = \"Transcribing...\";\n            \n            // 0. THE BLOCKER (GPU Lock)\n            await GPUController.withLock(\"Root-Mic-Process\", async () => {\n                // Offload to Worker\n                const rawText = await new Promise((resolve, reject) => {\n                    const reqId = Date.now();\n                    const handler = (e) => {\n                        if (e.data.id === reqId) {\n                            whisperWorker.removeEventListener('message', handler);\n                            if (e.data.type === 'transcribe_result') resolve(e.data.text);\n                            else reject(new Error(e.data.error));\n                        }\n                    };\n                    whisperWorker.addEventListener('message', handler);\n                    whisperWorker.postMessage({ type: 'transcribe', data: audioData, id: reqId });\n                });\n\n                // Hallucination Filter (Aggressive)\n                let cleanedText = rawText.trim();\n                const hallucinations = [\n                    '[Music]', '[BLANK_AUDIO]', 'Computed', '*sigh*', '*breathing*', \n                    'Applause', 'Thank you', 'Subtitles', 'Amara.org', 'Copyright', \n                    '¬©', 'Caption', 'Sovereign' \n                ];\n                \n                hallucinations.forEach(h => { \n                    const regex = new RegExp(h.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'), 'gi');\n                    cleanedText = cleanedText.replace(regex, '').trim();\n                });\n\n                // Filter single punctuation or very short junk\n                if (/^[.,?!;:]+$/.test(cleanedText) || cleanedText.length < 2) cleanedText = \"\";\n\n                if (!cleanedText || cleanedText.length < 1) {\n                    state.status = \"Heard nothing\";\n                    return;\n                }\n\n                state.output = `Raw: \"${cleanedText}\"\\n\\nCleaning...`;\n                state.status = \"Refining...\";\n\n                const reply = await llmEngine.chat.completions.create({\n                    messages: [\n                        { role: \"system\", content: \"You are a verbatim transcription corrector. Your ONLY task is to fix grammar, spelling, and punctuation. Do NOT answer questions. Do NOT add commentary. Output ONLY the corrected text.\" },\n                        { role: \"user\", content: `Correct this text: \"${cleanedText}\"` }\n                    ],\n                    temperature: 0.3,\n                    max_tokens: 512,\n                });\n\n                const finalText = reply.choices[0].message.content;\n                state.output = finalText;\n            }); // End Lock\n\n            state.status = \"Ready\";\n            \n            // Auto Copy (Handle focus requirement)\n            if (document.hasFocus()) {\n                navigator.clipboard.writeText(cleanText).then(() => {\n                    const t = document.getElementById('copy-toast');\n                    t.classList.add('show');\n                    setTimeout(() => t.classList.remove('show'), 2000);\n                }).catch(err => {\n                    console.warn(\"Clipboard write failed (focus lost?):\", err);\n                });\n            } else {\n                console.warn(\"Clipboard write skipped: Document not focused.\");\n                state.output += \"\\n(Copy skipped - Click to copy)\";\n            }\n        }\n\n        init();\n    </script>\n</body>\n</html>\n",
    "source": "tools\\anchor-mic.html"
  },
  {
    "id": "tools\\anchor.py",
    "timestamp": 1767282403,
    "role": "file",
    "content": "import requests\nimport sys\nimport json\nimport os\nfrom datetime import datetime\n\n# Configuration\nBRIDGE_URL = \"http://localhost:8000\"\nCONTEXT_DIR = \"../context/sessions\"  # Path relative to tools/\n\ndef ensure_session_file():\n    \"\"\"Creates a daily session file if it doesn't exist\"\"\"\n    if not os.path.exists(CONTEXT_DIR):\n        os.makedirs(CONTEXT_DIR)\n\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    filename = f\"{CONTEXT_DIR}/chat_{date_str}.md\"\n\n    if not os.path.exists(filename):\n        with open(filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"# Chat Session: {date_str}\\n\\n\")\n\n    return filename\n\ndef append_to_log(role, text):\n    \"\"\"Writes the message to the daily markdown file\"\"\"\n    filename = ensure_session_file()\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n\n    with open(filename, \"a\", encoding=\"utf-8\") as f:\n        # Format as Markdown\n        f.write(f\"### {role.upper()} [{timestamp}]\\n\")\n        f.write(f\"{text}\\n\\n\")\n\ndef check_connection():\n    try:\n        requests.get(f\"{BRIDGE_URL}/health\", timeout=1)\n        return True\n    except:\n        return False\n\ndef chat_loop():\n    print(\"\\n‚öì Anchor Terminal (Chat Mode)\")\n    print(\"--------------------------------\")\n    print(f\"üìÅ Session Log: {os.path.abspath(CONTEXT_DIR)}\")\n    print(\"Connecting to Ghost Engine...\")\n\n    if not check_connection():\n        print(f\"‚ùå Could not connect to {BRIDGE_URL}\")\n        print(\"   -> Run 'start-anchor.bat' first.\")\n        return\n\n    # Conversation History\n    history = [\n        {\"role\": \"system\", \"content\": \"You are Anchor, a helpful AI assistant running locally.\"}\n    ]\n\n    print(\"‚úÖ Connected. Type 'exit' to quit, 'clear' to reset.\\n\")\n\n    while True:\n        try:\n            user_input = input(\"You: \").strip()\n            if not user_input: continue\n\n            if user_input.lower() in ['exit', 'quit']:\n                print(\"üëã Disconnecting.\")\n                break\n\n            if user_input.lower() == 'clear':\n                history = [history[0]]\n                print(\"--- Context Cleared ---\")\n                continue\n\n            # 1. Update History & Log\n            history.append({\"role\": \"user\", \"content\": user_input})\n            append_to_log(\"user\", user_input)\n\n            print(\"Anchor: \", end=\"\", flush=True)\n\n            # 2. Send to Bridge\n            try:\n                response = requests.post(\n                    f\"{BRIDGE_URL}/v1/chat/completions\",\n                    json={\n                        \"messages\": history,\n                        \"stream\": True\n                    },\n                    stream=True,\n                    timeout=120\n                )\n\n                if response.status_code == 200:\n                    ai_text = \"\"\n                    for line in response.iter_lines(decode_unicode=True):\n                        if line and line.startswith(\"data: \"):\n                            data_str = line[6:]\n                            if data_str.strip() == \"[DONE]\": break\n                            try:\n                                chunk = json.loads(data_str)\n                                if 'choices' in chunk and len(chunk['choices']) > 0:\n                                    content = chunk['choices'][0].get('delta', {}).get('content', '')\n                                    if content:\n                                        print(content, end=\"\", flush=True)\n                                        ai_text += content\n                            except: continue\n\n                    print(\"\\n\")\n                    # 3. Update History & Log Assistant Response\n                    history.append({\"role\": \"assistant\", \"content\": ai_text})\n                    append_to_log(\"assistant\", ai_text)\n\n                else:\n                    print(f\"‚ùå Error {response.status_code}: {response.text}\")\n\n            except Exception as e:\n                print(f\"‚ùå Request Failed: {e}\")\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted.\")\n            break\n\nif __name__ == \"__main__\":\n    chat_loop()",
    "source": "tools\\anchor.py"
  },
  {
    "id": "tools\\ask_memory.py",
    "timestamp": 1767218550,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nAnchor Memory Retriever\nUsage: python ask_memory.py \"What did I work on last week?\"\n\"\"\"\nimport sys\nimport requests\nimport argparse\nimport json\n\n# Configuration\nBRIDGE_URL = \"http://localhost:8000\"\n\ndef search_memory(query):\n    print(f\"‚öì Querying Anchor Memory for: '{query}'...\")\n    try:\n        response = requests.post(\n            f\"{BRIDGE_URL}/v1/memory/search\",\n            json={\"query\": query},\n            timeout=15\n        )\n        \n        if response.status_code == 200:\n            data = response.json()\n            if data.get('status') == 'success':\n                return data.get('result', '')\n            else:\n                print(f\"‚ùå Error from Engine: {data.get('error')}\")\n        elif response.status_code == 503:\n            print(f\"‚ùå Service unavailable: Ghost Engine not connected. Is chat.html open?\")\n        elif response.status_code == 400:\n            print(f\"‚ùå Bad request: {response.json().get('error', 'Invalid query')}\")\n        elif response.status_code == 504:\n            print(f\"‚ùå Gateway timeout: Search took too long\")\n        else:\n            print(f\"‚ùå Bridge Error ({response.status_code}): {response.text}\")\n            \n    except requests.exceptions.ConnectionError:\n        print(f\"‚ùå Could not connect to Anchor Bridge at {BRIDGE_URL}\")\n        print(\"   Is 'start-anchor.bat' running?\")\n    except Exception as e:\n        print(f\"‚ùå Unexpected error: {e}\")\n    \n    return None\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Query your local Anchor Memory system.\")\n    parser.add_argument(\"query\", nargs=\"+\", help=\"The question or topic to search for.\")\n    args = parser.parse_args()\n    \n    full_query = \" \".join(args.query)\n    \n    context = search_memory(full_query)\n    \n    if context:\n        print(\"\\n\" + \"=\"*60)\n        print(\"üìã COPY THE TEXT BELOW TO WEBLLM CHAT\")\n        print(\"=\"*60 + \"\\n\")\n        print(context)\n        print(\"\\n\" + \"=\"*60)\n        \n        # Optional: Auto-copy to clipboard if user has pyperclip\n        try:\n            import pyperclip\n            pyperclip.copy(context)\n            print(\"‚úÖ Copied to clipboard automatically!\")\n        except ImportError:\n            print(\"(Tip: Install 'pyperclip' to auto-copy: pip install pyperclip)\")\n    else:\n        print(\"\\n‚ùå Failed to retrieve memory context.\")\n        print(\"Make sure:\")\n        print(\"1. Anchor Core is running (start-anchor.bat)\")\n        print(\"2. The chat.html page is open and connected\")\n        print(\"3. The database has been initialized\")\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\ask_memory.py"
  },
  {
    "id": "tools\\CHANGELOG.md",
    "timestamp": 1767326899,
    "role": "file",
    "content": "# Tools Changelog\r\n\r\n## [2.4.0] - 2026-01-01 \"Anchor Lite Refactor\"\r\n### Changed\r\n- Renamed `chat.html` to `ghost.html` (Headless Engine).\r\n- Refactored `context.html` to be the primary Search Dashboard.\r\n### Removed\r\n- Archived `db_builder.html`, `memory-builder.html`, `mobile-chat.html`.\r\n- Removed auto-loading of WebLLM to save resources.\r\n\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **On-Demand Model Serving**: `model-server-chat.html` now checks `http://localhost:8080/models/{id}` before loading. If missing, it triggers a download via the Bridge.\r\n- **Quota Bypass**: Using `useIndexedDBCache: false` for large models to bypass browser storage limits, relying on the Bridge's local file server instead.\r\n\r\n### Fixed\r\n- **Model ID Mismatch**: Fixed logic in `loadModel` where the `mlc-ai/` prefix was being aggressively stripped, causing config lookups to fail.\r\n- **UI Progress**: Added real-time progress bars for server-side model downloads.\r\n\r\n - 2025-12-23\r\n\r\n### Added\r\n- **Orchestrator Model:** New `orchestrator.py` tool to programmatically interact with the MLC Bridge from Python.\r\n- **Health Endpoint:** Added `/health` to `webgpu_bridge.py` for extension connectivity checks.\r\n- **Audit Whitelist:** Added `/audit/server-logs` to auth whitelist in Bridge to fix Log Viewer 401 errors.\r\n\r\n### Changed\r\n- **Bridge Port:** Moved standard bridge port from `8000` to `8080` to avoid conflicts with `http.server`.\r\n- **Launch Scripts:** Updated `start-sovereign-console.bat` and `launch-chromium-d3d12.bat` to respect new port `8080` and correct paths.\r\n- **Root Dreamer:**\r\n  - Robustified `init()` to handle non-Error objects during crash.\r\n  - Improved JSON parsing to strip markdown code blocks before parsing.\r\n  - Added strict engine readiness check to `dreamLoop` to prevent race conditions.\r\n- **Root Console:**\r\n  - Added \"High Performance (Small)\" models (Qwen 2.5 1.5B, TinyLlama) to the dropdown.\r\n  - Updated JS mapper to handle new small model paths.\r\n  - Fixed crash in `executeR1Loop` where `genErr.message` could be undefined.\r\n\r\n### Fixed\r\n- **WebGPU Crash:** Mitigated `DXGI_ERROR_DEVICE_REMOVED` by advising single-tab usage and providing smaller model options for constrained profiles.\r\n- **Extension Connection:** Fixed CORS and Port mismatch preventing the Chrome Extension from connecting to the Bridge.\r\n",
    "source": "tools\\CHANGELOG.md"
  },
  {
    "id": "tools\\code_tools.py",
    "timestamp": 1764687908,
    "role": "file",
    "content": "\"\"\"Top-level shim to re-export `anchor.tools.code_tools` for test imports that expect `tools.code_tools`.\r\n\"\"\"\r\ntry:\r\n    from anchor.tools.code_tools import *  # noqa: F401, F403\r\nexcept Exception:\r\n    # Minimal fallback implementations\r\n    def code_search(root: str, query: str, **kwargs):\r\n        return {\"root\": root, \"query\": query, \"count\": 0, \"results\": []}\r\n\r\n    def code_grep(root: str, query: str, **kwargs):\r\n        return {\"root\": root, \"query\": query, \"files\": 0, \"total_matches\": 0, \"results\": []}\r\n\r\n__all__ = [\"code_search\", \"code_grep\"]\r\n",
    "source": "tools\\code_tools.py"
  },
  {
    "id": "tools\\config.json",
    "timestamp": 1767423588,
    "role": "file",
    "content": "server:\n  port: 8000\n  host: 0.0.0.0\n  cors_origins:\n    - *\nghost_engine:\n  auto_resurrection_enabled: False\n  browser_executables:\n    - C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\n    - C:\\Program Files\\Microsoft\\Edge\\Application\\msedge.exe\n    - msedge\n    - chrome\n    - google-chrome\n  browser_flags:\n    - --headless=new\n    - --no-first-run\n    - --no-default-browser-check\n    - --disable-background-networking\n    - --disable-extensions\n    - --disable-background-timer-throttling\n    - --disable-backgrounding-occluded-windows\n    - --disable-renderer-backgrounding\n    - --disable-ipc-flooding-protection\n    - --disable-background-media-suspend\n    - --remote-debugging-port=9222\n  low_resource_flags:\n    - --max-active-webgl-contexts=1\n    - --max-webgl-contexts-per-group=1\n    - --disable-gpu-memory-buffer-compositor-resources\n    - --force-gpu-mem-available-mb=64\n    - --force-low-power-gpu\n  cpu_only_flags:\n    - --disable-gpu\n    - --disable-software-rasterizer\n    - --disable-gpu-sandbox\n    - --disable-features=VizDisplayCompositor\nlogging:\n  max_lines: 1000\n  log_directory: ../logs\n  log_format: [%Y-%m-%d %H:%M:%S] [%level] %message\nmemory:\n  max_ingest_size: 1000000\n  default_limit: 10\n  max_chars: 10000\n  ingest_timeout: 10.0\ngpu_management:\n  enabled: True\n  max_concurrent: 1\n  queue_timeout: 60\nmodel_loading:\n  timeout_seconds: 120\n  default_model: Qwen2.5-7B-Instruct-q4f16_1-MLC\n  model_base_url: http://localhost:8000/models\nwatchdog:\n  enabled: True\n  watch_directory: ../context\n  allowed_extensions:\n    - .md\n    - .txt\n    - .json\n    - .yaml\n    - .py\n    - .js\n    - .html\n    - .css\n    - .ts\n    - .tsx\n    - .jsx\n  debounce_time: 2.0",
    "source": "tools\\config.json"
  },
  {
    "id": "tools\\config_manager.py",
    "timestamp": 1767423379,
    "role": "file",
    "content": "\"\"\"\nConfiguration Manager for Anchor Core\n\nThis module provides a centralized configuration system that loads settings from config.json\nand allows runtime modification. It serves as the basis for a future settings menu.\n\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\n\nclass ConfigManager:\n    \"\"\"Centralized configuration manager for Anchor Core settings.\"\"\"\n    \n    def __init__(self, config_path: str = \"config.json\"):\n        self.config_path = Path(config_path)\n        self.config = self._load_config()\n    \n    def _load_config(self) -> Dict[str, Any]:\n        \"\"\"Load configuration from JSON file, with fallback to defaults.\"\"\"\n        if self.config_path.exists():\n            try:\n                with open(self.config_path, 'r', encoding='utf-8') as f:\n                    return json.load(f)\n            except Exception as e:\n                print(f\"‚ö†Ô∏è  Error loading config from {self.config_path}: {e}\")\n                print(\"Using default configuration...\")\n        \n        # Default configuration\n        return {\n            \"server\": {\n                \"port\": 8000,\n                \"host\": \"0.0.0.0\",\n                \"cors_origins\": [\"*\"]\n            },\n            \"ghost_engine\": {\n                \"auto_resurrection_enabled\": True,\n                \"browser_executables\": [\n                    \"C:\\\\Program Files (x86)\\\\Microsoft\\\\Edge\\\\Application\\\\msedge.exe\",\n                    \"C:\\\\Program Files\\\\Microsoft\\\\Edge\\\\Application\\\\msedge.exe\",\n                    \"msedge\",\n                    \"chrome\",\n                    \"google-chrome\"\n                ],\n                \"browser_flags\": [\n                    \"--headless=new\",\n                    \"--no-first-run\",\n                    \"--no-default-browser-check\",\n                    \"--disable-background-networking\",\n                    \"--disable-extensions\",\n                    \"--disable-background-timer-throttling\",\n                    \"--disable-backgrounding-occluded-windows\",\n                    \"--disable-renderer-backgrounding\",\n                    \"--disable-ipc-flooding-protection\",\n                    \"--disable-background-media-suspend\",\n                    \"--remote-debugging-port=9222\"\n                ],\n                \"low_resource_flags\": [\n                    \"--max-active-webgl-contexts=1\",\n                    \"--max-webgl-contexts-per-group=1\",\n                    \"--disable-gpu-memory-buffer-compositor-resources\",\n                    \"--force-gpu-mem-available-mb=64\",\n                    \"--force-low-power-gpu\"\n                ],\n                \"cpu_only_flags\": [\n                    \"--disable-gpu\",\n                    \"--disable-software-rasterizer\",\n                    \"--disable-gpu-sandbox\",\n                    \"--disable-features=VizDisplayCompositor\"\n                ]\n            },\n            \"logging\": {\n                \"max_lines\": 1000,\n                \"log_directory\": \"../logs\",\n                \"log_format\": \"[%Y-%m-%d %H:%M:%S] [%level] %message\"\n            },\n            \"memory\": {\n                \"max_ingest_size\": 1000000,\n                \"default_limit\": 10,\n                \"max_chars\": 10000\n            },\n            \"gpu_management\": {\n                \"enabled\": True,\n                \"max_concurrent\": 1,\n                \"queue_timeout\": 60\n            },\n            \"model_loading\": {\n                \"timeout_seconds\": 120,\n                \"default_model\": \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n                \"model_base_url\": \"http://localhost:8000/models\"\n            },\n            \"watchdog\": {\n                \"enabled\": True,\n                \"watch_directory\": \"../context\",\n                \"allowed_extensions\": [\".md\", \".txt\", \".json\", \".yaml\", \".py\", \".js\", \".html\", \".css\", \".ts\", \".tsx\", \".jsx\"],\n                \"debounce_time\": 2.0\n            }\n        }\n    \n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"\n        Get a configuration value using dot notation (e.g., 'server.port').\n        \n        Args:\n            key: Configuration key in dot notation\n            default: Default value if key is not found\n            \n        Returns:\n            Configuration value or default\n        \"\"\"\n        keys = key.split('.')\n        value = self.config\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return default\n        \n        return value\n    \n    def set(self, key: str, value: Any) -> None:\n        \"\"\"\n        Set a configuration value using dot notation (e.g., 'server.port').\n        \n        Args:\n            key: Configuration key in dot notation\n            value: Value to set\n        \"\"\"\n        keys = key.split('.')\n        config_ref = self.config\n        \n        for k in keys[:-1]:\n            if k not in config_ref or not isinstance(config_ref[k], dict):\n                config_ref[k] = {}\n            config_ref = config_ref[k]\n        \n        config_ref[keys[-1]] = value\n    \n    def save(self) -> bool:\n        \"\"\"Save current configuration to the config file.\"\"\"\n        try:\n            # Create directory if it doesn't exist\n            self.config_path.parent.mkdir(parents=True, exist_ok=True)\n            \n            with open(self.config_path, 'w', encoding='utf-8') as f:\n                json.dump(self.config, f, indent=2, ensure_ascii=False)\n            return True\n        except Exception as e:\n            print(f\"‚ùå Error saving config to {self.config_path}: {e}\")\n            return False\n    \n    def update_from_env(self) -> None:\n        \"\"\"Update configuration based on environment variables.\"\"\"\n        # Server settings\n        if os.environ.get(\"BRIDGE_PORT\"):\n            self.set(\"server.port\", int(os.environ[\"BRIDGE_PORT\"]))\n\n        if os.environ.get(\"LOW_RESOURCE_MODE\") == \"true\":\n            self.set(\"ghost_engine.auto_resurrection_enabled\", True)  # Still enabled but with different flags\n            # Apply low resource flags\n            current_flags = self.get(\"ghost_engine.browser_flags\", [])\n            low_resource_flags = self.get(\"ghost_engine.low_resource_flags\", [])\n            self.set(\"ghost_engine.browser_flags\", current_flags + low_resource_flags)\n\n        if os.environ.get(\"CPU_ONLY_MODE\") == \"true\":\n            self.set(\"ghost_engine.auto_resurrection_enabled\", True)  # Still enabled but with different flags\n            # Apply CPU-only flags\n            current_flags = self.get(\"ghost_engine.browser_flags\", [])\n            cpu_only_flags = self.get(\"ghost_engine.cpu_only_flags\", [])\n            self.set(\"ghost_engine.browser_flags\", current_flags + cpu_only_flags)\n\n        if os.environ.get(\"NO_RESURRECTION_MODE\") == \"true\":\n            self.set(\"ghost_engine.auto_resurrection_enabled\", False)\n\n        # Update browser executable paths if needed\n        if os.environ.get(\"BROWSER_PATH\"):\n            self.set(\"ghost_engine.browser_executables\", [os.environ[\"BROWSER_PATH\"]])\n\n    def get_server_config(self) -> Dict[str, Any]:\n        \"\"\"Get server-specific configuration.\"\"\"\n        return self.get(\"server\", {})\n    \n    def get_ghost_engine_config(self) -> Dict[str, Any]:\n        \"\"\"Get Ghost Engine-specific configuration.\"\"\"\n        return self.get(\"ghost_engine\", {})\n    \n    def get_logging_config(self) -> Dict[str, Any]:\n        \"\"\"Get logging-specific configuration.\"\"\"\n        return self.get(\"logging\", {})\n    \n    def get_memory_config(self) -> Dict[str, Any]:\n        \"\"\"Get memory-specific configuration.\"\"\"\n        return self.get(\"memory\", {})\n    \n    def get_gpu_config(self) -> Dict[str, Any]:\n        \"\"\"Get GPU management configuration.\"\"\"\n        return self.get(\"gpu_management\", {})\n    \n    def get_model_config(self) -> Dict[str, Any]:\n        \"\"\"Get model loading configuration.\"\"\"\n        return self.get(\"model_loading\", {})\n    \n    def get_watchdog_config(self) -> Dict[str, Any]:\n        \"\"\"Get watchdog configuration.\"\"\"\n        return self.get(\"watchdog\", {})\n\n\n# Global configuration instance\nconfig_manager = ConfigManager()\n\n\ndef get_config() -> ConfigManager:\n    \"\"\"Get the global configuration manager instance.\"\"\"\n    return config_manager",
    "source": "tools\\config_manager.py"
  },
  {
    "id": "tools\\context.html",
    "timestamp": 1767326881,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Anchor Context Console</title>\r\n    <style>\r\n        :root {\r\n            --bg: #0f172a;\r\n            --panel: #1e293b;\r\n            --text: #e2e8f0;\r\n            --accent: #38bdf8;\r\n        }\r\n\r\n        body {\r\n            background: var(--bg);\r\n            color: var(--text);\r\n            font-family: system-ui;\r\n            margin: 0;\r\n            padding: 20px;\r\n            height: 100vh;\r\n            display: flex;\r\n            gap: 20px;\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        .sidebar {\r\n            width: 300px;\r\n            display: flex;\r\n            flex-direction: column;\r\n            gap: 20px;\r\n        }\r\n\r\n        .main {\r\n            flex: 1;\r\n            display: flex;\r\n            flex-direction: column;\r\n            background: var(--panel);\r\n            border-radius: 12px;\r\n            border: 1px solid #334155;\r\n            padding: 20px;\r\n        }\r\n\r\n        input,\r\n        button {\r\n            width: 100%;\r\n            padding: 12px;\r\n            border-radius: 8px;\r\n            border: 1px solid #334155;\r\n            background: #000;\r\n            color: #fff;\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        button {\r\n            background: var(--accent);\r\n            color: #000;\r\n            font-weight: bold;\r\n            cursor: pointer;\r\n            border: none;\r\n        }\r\n\r\n        button:hover {\r\n            opacity: 0.9;\r\n        }\r\n\r\n        textarea {\r\n            flex: 1;\r\n            background: #000;\r\n            color: #a5f3fc;\r\n            border: none;\r\n            padding: 15px;\r\n            font-family: monospace;\r\n            resize: none;\r\n            outline: none;\r\n            border-radius: 8px;\r\n        }\r\n\r\n        .slider-group {\r\n            background: var(--panel);\r\n            padding: 15px;\r\n            border-radius: 8px;\r\n            border: 1px solid #334155;\r\n        }\r\n\r\n        label {\r\n            display: block;\r\n            margin-bottom: 10px;\r\n            font-size: 0.9rem;\r\n            color: #94a3b8;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <div class=\"sidebar\">\r\n        <div>\r\n            <label>üîé Search Memory</label>\r\n            <input type=\"text\" id=\"query\" placeholder=\"Type keyword...\" onkeyup=\"if(event.key==='Enter') search()\">\r\n        </div>\r\n        <div class=\"slider-group\">\r\n            <label>Volume: <span id=\"vol-val\">5000</span> chars</label>\r\n            <input type=\"range\" id=\"vol\" min=\"1000\" max=\"50000\" step=\"1000\" value=\"5000\"\r\n                oninput=\"document.getElementById('vol-val').innerText=this.value\">\r\n        </div>\r\n        <button onclick=\"search()\">Fetch Context</button>\r\n        <button onclick=\"copy()\" style=\"background: #334155; color: #fff; border: 1px solid #475569\">üìã Copy to\r\n            Clipboard</button>\r\n    </div>\r\n    <div class=\"main\">\r\n        <textarea id=\"output\" readonly placeholder=\"Context results will appear here...\"></textarea>\r\n    </div>\r\n    <script>\r\n        async function search() {\r\n            const query = document.getElementById('query').value;\r\n            const limit = document.getElementById('vol').value;\r\n            const out = document.getElementById('output');\r\n            if (!query) return;\r\n            out.value = \"Searching...\";\r\n            try {\r\n                const res = await fetch('/v1/memory/search', {\r\n                    method: 'POST',\r\n                    body: JSON.stringify({ query, max_chars: parseInt(limit) })\r\n                });\r\n                const data = await res.json();\r\n                out.value = data.context || \"No results.\";\r\n            } catch (e) { out.value = \"Error: \" + e; }\r\n        }\r\n        function copy() {\r\n            const el = document.getElementById('output');\r\n            el.select();\r\n            document.execCommand('copy');\r\n        }\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\context.html"
  },
  {
    "id": "tools\\cozo_lib_wasm.js",
    "timestamp": 1767331845,
    "role": "file",
    "content": "import { loadAllFromIndexedDb, flushPendingWrites, writeToIndexedDb, setWriteCounter } from './indexeddb.js';\n\nlet wasm;\n\nconst heap = new Array(128).fill(undefined);\n\nheap.push(undefined, null, true, false);\n\nfunction getObject(idx) { return heap[idx]; }\n\nlet heap_next = heap.length;\n\nfunction dropObject(idx) {\n    if (idx < 132) return;\n    heap[idx] = heap_next;\n    heap_next = idx;\n}\n\nfunction takeObject(idx) {\n    const ret = getObject(idx);\n    dropObject(idx);\n    return ret;\n}\n\nconst cachedTextDecoder = (typeof TextDecoder !== 'undefined' ? new TextDecoder('utf-8', { ignoreBOM: true, fatal: true }) : { decode: () => { throw Error('TextDecoder not available') } } );\n\nif (typeof TextDecoder !== 'undefined') { cachedTextDecoder.decode(); };\n\nlet cachedUint8Memory0 = null;\n\nfunction getUint8Memory0() {\n    if (cachedUint8Memory0 === null || cachedUint8Memory0.byteLength === 0) {\n        cachedUint8Memory0 = new Uint8Array(wasm.memory.buffer);\n    }\n    return cachedUint8Memory0;\n}\n\nfunction getStringFromWasm0(ptr, len) {\n    ptr = ptr >>> 0;\n    return cachedTextDecoder.decode(getUint8Memory0().subarray(ptr, ptr + len));\n}\n\nfunction addHeapObject(obj) {\n    if (heap_next === heap.length) heap.push(heap.length + 1);\n    const idx = heap_next;\n    heap_next = heap[idx];\n\n    heap[idx] = obj;\n    return idx;\n}\n\nfunction makeMutClosure(arg0, arg1, dtor, f) {\n    const state = { a: arg0, b: arg1, cnt: 1, dtor };\n    const real = (...args) => {\n        // First up with a closure we increment the internal reference\n        // count. This ensures that the Rust closure environment won't\n        // be deallocated while we're invoking it.\n        state.cnt++;\n        const a = state.a;\n        state.a = 0;\n        try {\n            return f(a, state.b, ...args);\n        } finally {\n            if (--state.cnt === 0) {\n                wasm.__wbindgen_export_0.get(state.dtor)(a, state.b);\n\n            } else {\n                state.a = a;\n            }\n        }\n    };\n    real.original = state;\n\n    return real;\n}\nfunction __wbg_adapter_22(arg0, arg1, arg2) {\n    wasm.wasm_bindgen__convert__closures__invoke1_mut__hd17e34166836fd48(arg0, arg1, addHeapObject(arg2));\n}\n\nlet WASM_VECTOR_LEN = 0;\n\nconst cachedTextEncoder = (typeof TextEncoder !== 'undefined' ? new TextEncoder('utf-8') : { encode: () => { throw Error('TextEncoder not available') } } );\n\nconst encodeString = (typeof cachedTextEncoder.encodeInto === 'function'\n    ? function (arg, view) {\n    return cachedTextEncoder.encodeInto(arg, view);\n}\n    : function (arg, view) {\n    const buf = cachedTextEncoder.encode(arg);\n    view.set(buf);\n    return {\n        read: arg.length,\n        written: buf.length\n    };\n});\n\nfunction passStringToWasm0(arg, malloc, realloc) {\n    if (typeof arg !== 'string') arg = arg ? String(arg) : \"\";\n    if (realloc === undefined) {\n        const buf = cachedTextEncoder.encode(arg);\n        const ptr = malloc(buf.length, 1) >>> 0;\n        getUint8Memory0().subarray(ptr, ptr + buf.length).set(buf);\n        WASM_VECTOR_LEN = buf.length;\n        return ptr;\n    }\n\n    let len = arg.length;\n    let ptr = malloc(len, 1) >>> 0;\n\n    const mem = getUint8Memory0();\n\n    let offset = 0;\n\n    for (; offset < len; offset++) {\n        const code = arg.charCodeAt(offset);\n        if (code > 0x7F) break;\n        mem[ptr + offset] = code;\n    }\n\n    if (offset !== len) {\n        if (offset !== 0) {\n            arg = arg.slice(offset);\n        }\n        ptr = realloc(ptr, len, len = offset + arg.length * 3, 1) >>> 0;\n        const view = getUint8Memory0().subarray(ptr + offset, ptr + len);\n        const ret = encodeString(arg, view);\n\n        offset += ret.written;\n    }\n\n    WASM_VECTOR_LEN = offset;\n    return ptr;\n}\n\nlet cachedInt32Memory0 = null;\n\nfunction getInt32Memory0() {\n    if (cachedInt32Memory0 === null || cachedInt32Memory0.byteLength === 0) {\n        cachedInt32Memory0 = new Int32Array(wasm.memory.buffer);\n    }\n    return cachedInt32Memory0;\n}\n\nfunction handleError(f, args) {\n    try {\n        return f.apply(this, args);\n    } catch (e) {\n        wasm.__wbindgen_exn_store(addHeapObject(e));\n    }\n}\nfunction __wbg_adapter_76(arg0, arg1, arg2, arg3) {\n    wasm.wasm_bindgen__convert__closures__invoke2_mut__h62fdc46dd4e23c5d(arg0, arg1, addHeapObject(arg2), addHeapObject(arg3));\n}\n\n/**\n*/\nexport class CozoDb {\n\n    static __wrap(ptr) {\n        ptr = ptr >>> 0;\n        const obj = Object.create(CozoDb.prototype);\n        obj.__wbg_ptr = ptr;\n\n        return obj;\n    }\n\n    __destroy_into_raw() {\n        const ptr = this.__wbg_ptr;\n        this.__wbg_ptr = 0;\n\n        return ptr;\n    }\n\n    free() {\n        const ptr = this.__destroy_into_raw();\n        wasm.__wbg_cozodb_free(ptr);\n    }\n    /**\n    * @returns {CozoDb}\n    */\n    static new() {\n        const ret = wasm.cozodb_new();\n        return CozoDb.__wrap(ret);\n    }\n    /**\n    * Create CozoDb from IndexedDB\n    * @param {string} db_name\n    * @param {string} store_name\n    * @param {any} on_write_callback\n    * @returns {Promise<CozoDb>}\n    */\n    static new_from_indexed_db(db_name, store_name, on_write_callback) {\n        const ptr0 = passStringToWasm0(db_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n        const len0 = WASM_VECTOR_LEN;\n        const ptr1 = passStringToWasm0(store_name, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n        const len1 = WASM_VECTOR_LEN;\n        const ret = wasm.cozodb_new_from_indexed_db(ptr0, len0, ptr1, len1, addHeapObject(on_write_callback));\n        return takeObject(ret);\n    }\n    /**\n    * @param {string} script\n    * @param {string} params\n    * @param {boolean} immutable\n    * @returns {Promise<string>}\n    */\n    run(script, params, immutable) {\n        if (typeof params === 'undefined' || params === null) params = \"{}\";\n        const ptr0 = passStringToWasm0(script, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n        const len0 = WASM_VECTOR_LEN;\n        const ptr1 = passStringToWasm0(params, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n        const len1 = WASM_VECTOR_LEN;\n        const ret = wasm.cozodb_run(this.__wbg_ptr, ptr0, len0, ptr1, len1, immutable);\n        return takeObject(ret);\n    }\n    /**\n    * @param {string} data\n    * @returns {string}\n    */\n    export_relations(data) {\n        let deferred2_0;\n        let deferred2_1;\n        try {\n            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);\n            const ptr0 = passStringToWasm0(data, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n            const len0 = WASM_VECTOR_LEN;\n            wasm.cozodb_export_relations(retptr, this.__wbg_ptr, ptr0, len0);\n            var r0 = getInt32Memory0()[retptr / 4 + 0];\n            var r1 = getInt32Memory0()[retptr / 4 + 1];\n            deferred2_0 = r0;\n            deferred2_1 = r1;\n            return getStringFromWasm0(r0, r1);\n        } finally {\n            wasm.__wbindgen_add_to_stack_pointer(16);\n            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\n        }\n    }\n    /**\n    * @param {string} data\n    * @returns {string}\n    */\n    import_relations(data) {\n        // Normalize common JSON payload shapes so the WASM importer receives\n        // a canonical `{\"relations\": [{ name, headers, rows }]}` shape.\n        let normalized = null;\n        try {\n            const parsed = JSON.parse(data);\n            // Top-level 'memory' -> convert\n            if (parsed && parsed.memory) {\n                const mem = parsed.memory;\n                const headers = mem.headers || (mem.named_rows && mem.named_rows.headers) || null;\n                const rows = mem.rows || (mem.named_rows && mem.named_rows.rows) || [];\n                normalized = { relations: [{ name: 'memory', headers: headers || [], rows }] };\n            }\n            // Already a relations wrapper\n            else if (parsed && parsed.relations && Array.isArray(parsed.relations)) {\n                const rels = parsed.relations.map(r => {\n                    const headers = r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'];\n                    const rawRows = r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [];\n                    const rows = rawRows.map(row => {\n                        // Normalize object rows into arrays using headers order\n                        if (row && typeof row === 'object' && !Array.isArray(row)) {\n                            return headers.map(h => (row[h] === undefined ? null : row[h]));\n                        }\n                        // Ensure array rows have proper length and default embedding\n                        const arr = Array.isArray(row) ? row.slice(0, headers.length) : [];\n                        while (arr.length < headers.length) arr.push(null);\n                        const embIdx = headers.indexOf('embedding');\n                        if (embIdx >= 0 && (arr[embIdx] === null || arr[embIdx] === undefined)) arr[embIdx] = [];\n                        return arr;\n                    });\n                    // Provide multiple shapes to satisfy various importer variants\n                    return {\n                        name: r.name || 'memory',\n                        headers,\n                        rows,\n                        named_rows: { headers, rows },\n                        NamedRows: { headers, rows }\n                    };\n                });\n                normalized = { relations: rels };\n            }\n            // Else, could be raw array of records -> convert to rows\n            else if (Array.isArray(parsed)) {\n                const rows = parsed.map(rec => {\n                    const ts = rec.timestamp ? (isNaN(Number(rec.timestamp)) ? new Date(rec.timestamp).getTime() : Number(rec.timestamp)) : Date.now();\n                    return [\n                        `${ts}-${Math.random().toString(36).substr(2,9)}`,\n                        ts,\n                        rec.role || rec.type || 'unknown',\n                        (rec.content || rec.response_content || rec.message || '').substring(0, 20000),\n                        rec.source || 'combined_memory.json',\n                        null\n                    ];\n                });\n                normalized = { relations: [{ name: 'memory', headers: ['id','timestamp','role','content','source','embedding'], rows }] };\n            }\n        } catch (e) {\n            // Not valid JSON or normalization failed; fall back to raw string\n        }\n\n        const payload = normalized ? JSON.stringify(normalized) : data;\n\n        let deferred2_0;\n        let deferred2_1;\n        try {\n            // Debug: show the exact payload being passed into WASM importer (trimmed)\n            try {\n                console.log('CozoDb.import_relations - payload preview (trimmed):', payload.slice(0, 2000));\n                const parsedPreview = JSON.parse(payload);\n                console.log('CozoDb.import_relations - parsed relations preview:', parsedPreview.relations && parsedPreview.relations[0] ? Object.keys(parsedPreview.relations[0]) : null);\n            } catch (pe) {\n                console.warn('CozoDb.import_relations - payload not parseable as JSON preview.');\n            }\n            const retptr = wasm.__wbindgen_add_to_stack_pointer(-16);\n            const ptr0 = passStringToWasm0(payload, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n            const len0 = WASM_VECTOR_LEN;\n            try {\n                wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptr0, len0);\n            } catch (e) {\n                console.error('CozoDb.import_relations wasm call failed:', e, 'payload_preview:', payload.slice(0,2000));\n                throw e;\n            }\n            var r0 = getInt32Memory0()[retptr / 4 + 0];\n            var r1 = getInt32Memory0()[retptr / 4 + 1];\n            deferred2_0 = r0;\n            deferred2_1 = r1;\n            const resultStr = getStringFromWasm0(r0, r1);\n\n            // If WASM returned a NamedRows header error, try a fallback shaped payload where relation includes NamedRows explicitly.\n            try {\n                const parsedResult = JSON.parse(resultStr);\n                if (parsedResult && parsedResult.ok === false && typeof parsedResult.message === 'string' && parsedResult.message.includes('NamedRows requires')) {\n                    try {\n                        const parsedPayload = JSON.parse(payload);\n                        if (parsedPayload && parsedPayload.relations && Array.isArray(parsedPayload.relations)) {\n                            const fallback = { relations: parsedPayload.relations.map(r => ({ name: r.name || 'memory', NamedRows: { headers: r.headers || (r.named_rows && r.named_rows.headers) || (r.NamedRows && r.NamedRows.headers) || ['id','timestamp','role','content','source','embedding'], rows: r.rows || (r.named_rows && r.named_rows.rows) || (r.NamedRows && r.NamedRows.rows) || [] } })) };\n                            const fallbackStr = JSON.stringify(fallback);\n                            console.log('CozoDb.import_relations - retrying with fallback NamedRows payload (trimmed):', fallbackStr.slice(0,2000));\n                            const ptrF = passStringToWasm0(fallbackStr, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n                            const lenF = WASM_VECTOR_LEN;\n                            wasm.cozodb_import_relations(retptr, this.__wbg_ptr, ptrF, lenF);\n                            var r0b = getInt32Memory0()[retptr / 4 + 0];\n                            var r1b = getInt32Memory0()[retptr / 4 + 1];\n                            // free previous\n                            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\n                            deferred2_0 = r0b;\n                            deferred2_1 = r1b;\n                            return getStringFromWasm0(r0b, r1b);\n                        }\n                    } catch (retryErr) {\n                        console.warn('CozoDb.import_relations - fallback retry failed to build payload or call wasm:', retryErr);\n                    }\n                }\n            } catch (e) {\n                // ignore parse errors\n            }\n\n            return resultStr;\n        } finally {\n            wasm.__wbindgen_add_to_stack_pointer(16);\n            wasm.__wbindgen_free(deferred2_0, deferred2_1, 1);\n        }\n    }\n}\n\nasync function __wbg_load(module, imports) {\n    if (typeof Response === 'function' && module instanceof Response) {\n        if (typeof WebAssembly.instantiateStreaming === 'function') {\n            try {\n                return await WebAssembly.instantiateStreaming(module, imports);\n\n            } catch (e) {\n                if (module.headers.get('Content-Type') != 'application/wasm') {\n                    console.warn(\"`WebAssembly.instantiateStreaming` failed because your server does not serve wasm with `application/wasm` MIME type. Falling back to `WebAssembly.instantiate` which is slower. Original error:\\n\", e);\n\n                } else {\n                    throw e;\n                }\n            }\n        }\n\n        const bytes = await module.arrayBuffer();\n        return await WebAssembly.instantiate(bytes, imports);\n\n    } else {\n        const instance = await WebAssembly.instantiate(module, imports);\n\n        if (instance instanceof WebAssembly.Instance) {\n            return { instance, module };\n\n        } else {\n            return instance;\n        }\n    }\n}\n\nfunction __wbg_get_imports() {\n    const imports = {};\n    imports.wbg = {};\n    imports.wbg.__wbindgen_object_drop_ref = function(arg0) {\n        takeObject(arg0);\n    };\n    imports.wbg.__wbg_log_b78d654f19d681e0 = function(arg0, arg1) {\n        console.log(getStringFromWasm0(arg0, arg1));\n    };\n    imports.wbg.__wbg_loadAllFromIndexedDb_05f8df9a19c8d344 = function(arg0, arg1, arg2, arg3, arg4) {\n        const ret = loadAllFromIndexedDb(getStringFromWasm0(arg0, arg1), getStringFromWasm0(arg2, arg3), getObject(arg4));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_flushPendingWrites_dd4341a0dafcf428 = function() {\n        const ret = flushPendingWrites();\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_string_new = function(arg0, arg1) {\n        const ret = getStringFromWasm0(arg0, arg1);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_cozodb_new = function(arg0) {\n        const ret = CozoDb.__wrap(arg0);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_cb_drop = function(arg0) {\n        const obj = takeObject(arg0).original;\n        if (obj.cnt-- == 1) {\n            obj.a = 0;\n            return true;\n        }\n        const ret = false;\n        return ret;\n    };\n    imports.wbg.__wbg_new_abda76e883ba8a5f = function() {\n        const ret = new Error();\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_stack_658279fe44541cf6 = function(arg0, arg1) {\n        const ret = getObject(arg1).stack;\n        const ptr1 = passStringToWasm0(ret, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc);\n        const len1 = WASM_VECTOR_LEN;\n        getInt32Memory0()[arg0 / 4 + 1] = len1;\n        getInt32Memory0()[arg0 / 4 + 0] = ptr1;\n    };\n    imports.wbg.__wbg_error_f851667af71bcfc6 = function(arg0, arg1) {\n        let deferred0_0;\n        let deferred0_1;\n        try {\n            deferred0_0 = arg0;\n            deferred0_1 = arg1;\n            console.error(getStringFromWasm0(arg0, arg1));\n        } finally {\n            wasm.__wbindgen_free(deferred0_0, deferred0_1, 1);\n        }\n    };\n    imports.wbg.__wbg_setWriteCounter_295838a9805b3542 = function(arg0) {\n        setWriteCounter(arg0 >>> 0);\n    };\n    imports.wbg.__wbg_writeToIndexedDb_ec8ba47108ce4d3a = function(arg0, arg1) {\n        const ret = writeToIndexedDb(getObject(arg0), getObject(arg1));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_crypto_c48a774b022d20ac = function(arg0) {\n        const ret = getObject(arg0).crypto;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_is_object = function(arg0) {\n        const val = getObject(arg0);\n        const ret = typeof(val) === 'object' && val !== null;\n        return ret;\n    };\n    imports.wbg.__wbg_process_298734cf255a885d = function(arg0) {\n        const ret = getObject(arg0).process;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_versions_e2e78e134e3e5d01 = function(arg0) {\n        const ret = getObject(arg0).versions;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_node_1cd7a5d853dbea79 = function(arg0) {\n        const ret = getObject(arg0).node;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_is_string = function(arg0) {\n        const ret = typeof(getObject(arg0)) === 'string';\n        return ret;\n    };\n    imports.wbg.__wbg_msCrypto_bcb970640f50a1e8 = function(arg0) {\n        const ret = getObject(arg0).msCrypto;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_require_8f08ceecec0f4fee = function() { return handleError(function () {\n        const ret = module.require;\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbindgen_is_function = function(arg0) {\n        const ret = typeof(getObject(arg0)) === 'function';\n        return ret;\n    };\n    imports.wbg.__wbg_randomFillSync_dc1e9a60c158336d = function() { return handleError(function (arg0, arg1) {\n        getObject(arg0).randomFillSync(takeObject(arg1));\n    }, arguments) };\n    imports.wbg.__wbg_getRandomValues_37fa2ca9e4e07fab = function() { return handleError(function (arg0, arg1) {\n        getObject(arg0).getRandomValues(getObject(arg1));\n    }, arguments) };\n    imports.wbg.__wbg_get_44be0491f933a435 = function(arg0, arg1) {\n        const ret = getObject(arg0)[arg1 >>> 0];\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_length_fff51ee6522a1a18 = function(arg0) {\n        const ret = getObject(arg0).length;\n        return ret;\n    };\n    imports.wbg.__wbg_newnoargs_581967eacc0e2604 = function(arg0, arg1) {\n        const ret = new Function(getStringFromWasm0(arg0, arg1));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_call_cb65541d95d71282 = function() { return handleError(function (arg0, arg1) {\n        const ret = getObject(arg0).call(getObject(arg1));\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbindgen_object_clone_ref = function(arg0) {\n        const ret = getObject(arg0);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_self_1ff1d729e9aae938 = function() { return handleError(function () {\n        const ret = self.self;\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbg_window_5f4faef6c12b79ec = function() { return handleError(function () {\n        const ret = window.window;\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbg_globalThis_1d39714405582d3c = function() { return handleError(function () {\n        const ret = globalThis.globalThis;\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbg_global_651f05c6a0944d1c = function() { return handleError(function () {\n        const ret = global.global;\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbindgen_is_undefined = function(arg0) {\n        const ret = getObject(arg0) === undefined;\n        return ret;\n    };\n    imports.wbg.__wbg_isArray_4c24b343cb13cfb1 = function(arg0) {\n        const ret = Array.isArray(getObject(arg0));\n        return ret;\n    };\n    imports.wbg.__wbg_call_01734de55d61e11d = function() { return handleError(function (arg0, arg1, arg2) {\n        const ret = getObject(arg0).call(getObject(arg1), getObject(arg2));\n        return addHeapObject(ret);\n    }, arguments) };\n    imports.wbg.__wbg_now_9c5990bda04c7e53 = function() {\n        const ret = Date.now();\n        return ret;\n    };\n    imports.wbg.__wbg_new_43f1b47c28813cbd = function(arg0, arg1) {\n        try {\n            var state0 = {a: arg0, b: arg1};\n            var cb0 = (arg0, arg1) => {\n                const a = state0.a;\n                state0.a = 0;\n                try {\n                    return __wbg_adapter_76(a, state0.b, arg0, arg1);\n                } finally {\n                    state0.a = a;\n                }\n            };\n            const ret = new Promise(cb0);\n            return addHeapObject(ret);\n        } finally {\n            state0.a = state0.b = 0;\n        }\n    };\n    imports.wbg.__wbg_resolve_53698b95aaf7fcf8 = function(arg0) {\n        const ret = Promise.resolve(getObject(arg0));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_then_f7e06ee3c11698eb = function(arg0, arg1) {\n        const ret = getObject(arg0).then(getObject(arg1));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_then_b2267541e2a73865 = function(arg0, arg1, arg2) {\n        const ret = getObject(arg0).then(getObject(arg1), getObject(arg2));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_buffer_085ec1f694018c4f = function(arg0) {\n        const ret = getObject(arg0).buffer;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_newwithbyteoffsetandlength_6da8e527659b86aa = function(arg0, arg1, arg2) {\n        const ret = new Uint8Array(getObject(arg0), arg1 >>> 0, arg2 >>> 0);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_new_8125e318e6245eed = function(arg0) {\n        const ret = new Uint8Array(getObject(arg0));\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_set_5cf90238115182c3 = function(arg0, arg1, arg2) {\n        getObject(arg0).set(getObject(arg1), arg2 >>> 0);\n    };\n    imports.wbg.__wbg_length_72e2208bbc0efc61 = function(arg0) {\n        const ret = getObject(arg0).length;\n        return ret;\n    };\n    imports.wbg.__wbg_instanceof_Uint8Array_d8d9cb2b8e8ac1d4 = function(arg0) {\n        let result;\n        try {\n            result = getObject(arg0) instanceof Uint8Array;\n        } catch {\n            result = false;\n        }\n        const ret = result;\n        return ret;\n    };\n    imports.wbg.__wbg_newwithlength_e5d69174d6984cd7 = function(arg0) {\n        const ret = new Uint8Array(arg0 >>> 0);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbg_subarray_13db269f57aa838d = function(arg0, arg1, arg2) {\n        const ret = getObject(arg0).subarray(arg1 >>> 0, arg2 >>> 0);\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_throw = function(arg0, arg1) {\n        throw new Error(getStringFromWasm0(arg0, arg1));\n    };\n    imports.wbg.__wbindgen_memory = function() {\n        const ret = wasm.memory;\n        return addHeapObject(ret);\n    };\n    imports.wbg.__wbindgen_closure_wrapper186 = function(arg0, arg1, arg2) {\n        const ret = makeMutClosure(arg0, arg1, 87, __wbg_adapter_22);\n        return addHeapObject(ret);\n    };\n\n    return imports;\n}\n\nfunction __wbg_init_memory(imports, maybe_memory) {\n\n}\n\nfunction __wbg_finalize_init(instance, module) {\n    wasm = instance.exports;\n    __wbg_init.__wbindgen_wasm_module = module;\n    cachedInt32Memory0 = null;\n    cachedUint8Memory0 = null;\n\n\n    return wasm;\n}\n\nfunction initSync(module) {\n    if (wasm !== undefined) return wasm;\n\n    const imports = __wbg_get_imports();\n\n    __wbg_init_memory(imports);\n\n    if (!(module instanceof WebAssembly.Module)) {\n        module = new WebAssembly.Module(module);\n    }\n\n    const instance = new WebAssembly.Instance(module, imports);\n\n    return __wbg_finalize_init(instance, module);\n}\n\nasync function __wbg_init(input) {\n    if (wasm !== undefined) return wasm;\n\n    if (typeof input === 'undefined') {\n        input = new URL('cozo_lib_wasm_bg.wasm', import.meta.url);\n    }\n    const imports = __wbg_get_imports();\n\n    if (typeof input === 'string' || (typeof Request === 'function' && input instanceof Request) || (typeof URL === 'function' && input instanceof URL)) {\n        input = fetch(input);\n    }\n\n    __wbg_init_memory(imports);\n\n    const { instance, module } = await __wbg_load(await input, imports);\n\n    return __wbg_finalize_init(instance, module);\n}\n\nexport { initSync }\nexport default __wbg_init;\n",
    "source": "tools\\cozo_lib_wasm.js"
  },
  {
    "id": "tools\\daemon_eyes.py",
    "timestamp": 1767217505,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nDaemon Eyes - Background Vision Service for Anchor Core\n\nThis script provides automated screen observation for the Anchor Core system.\nIt captures screenshots at regular intervals, performs OCR, and sends the\nextracted text to the Anchor Core bridge for automatic memory ingestion.\n\nThe goal is to provide \"digital proprioception\" - automatic awareness of\nwhat the user is looking at without requiring manual input.\n\"\"\"\n\nimport time\nimport requests\nimport pyautogui\nimport pytesseract\nimport json\nimport logging\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\n\nclass DaemonEyes:\n    def __init__(self, bridge_url=\"http://localhost:8000\", \n                 interval=5, \n                 token=\"sovereign-secret\",\n                 enabled=True):\n        \"\"\"\n        Initialize the Daemon Eyes service\n        \n        Args:\n            bridge_url (str): URL of the Anchor Core bridge\n            interval (int): Time between screenshots in seconds\n            token (str): Authentication token for the bridge\n            enabled (bool): Whether the daemon is initially enabled\n        \"\"\"\n        self.bridge_url = bridge_url\n        self.interval = interval\n        self.token = token\n        self.enabled = enabled\n        self.headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        # Setup logging\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - DaemonEyes - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('daemon_eyes.log'),\n                logging.StreamHandler(sys.stdout)\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n        \n        # Validate dependencies\n        self._validate_dependencies()\n        \n        # Track previous content to avoid duplicates\n        self.last_content = \"\"\n        self.last_content_hash = \"\"\n\n    def _validate_dependencies(self):\n        \"\"\"Validate that required dependencies are available\"\"\"\n        try:\n            # Test pyautogui\n            pyautogui.size()  # Get screen size\n        except Exception as e:\n            self.logger.error(f\"pyautogui not available: {e}\")\n            raise RuntimeError(\"pyautogui is required but not available\")\n        \n        try:\n            # Test pytesseract\n            pytesseract.get_tesseract_version()\n        except Exception as e:\n            self.logger.error(f\"pytesseract not available: {e}\")\n            raise RuntimeError(\"pytesseract is required but not available\")\n        \n        self.logger.info(\"Dependencies validated successfully\")\n\n    def take_screenshot(self):\n        \"\"\"Take a screenshot of the current screen\"\"\"\n        try:\n            screenshot = pyautogui.screenshot()\n            return screenshot\n        except Exception as e:\n            self.logger.error(f\"Failed to take screenshot: {e}\")\n            return None\n\n    def extract_text_ocr(self, image):\n        \"\"\"Extract text from image using OCR\"\"\"\n        try:\n            # Convert image to text using pytesseract\n            text = pytesseract.image_to_string(image)\n            return text.strip()\n        except Exception as e:\n            self.logger.error(f\"OCR extraction failed: {e}\")\n            return \"\"\n\n    def send_to_bridge(self, content):\n        \"\"\"Send extracted content to the Anchor Core bridge for ingestion\"\"\"\n        try:\n            # Prepare payload for memory ingestion\n            payload = {\n                \"content\": content,\n                \"source\": \"daemon_eyes\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"screen_content\",\n                \"tags\": [\"vision\", \"automatic\", \"context\"]\n            }\n            \n            # Try different endpoints for memory ingestion\n            endpoints = [\n                f\"{self.bridge_url}/v1/memory/ingest\",\n                f\"{self.bridge_url}/archivist/ingest\",\n                f\"{self.bridge_url}/v1/memory/create\",\n                f\"{self.bridge_url}/v1/memory/store\"\n            ]\n            \n            response = None\n            for endpoint in endpoints:\n                try:\n                    response = requests.post(\n                        endpoint,\n                        headers=self.headers,\n                        json=payload,\n                        timeout=10\n                    )\n                    \n                    if response.status_code in [200, 201]:\n                        self.logger.info(f\"Content successfully sent to {endpoint}\")\n                        return True\n                    else:\n                        self.logger.debug(f\"Endpoint {endpoint} returned {response.status_code}, trying next...\")\n                except requests.exceptions.RequestException as e:\n                    self.logger.debug(f\"Failed to reach {endpoint}: {e}\")\n                    continue\n            \n            # If all endpoints failed\n            if response:\n                self.logger.error(f\"All ingestion endpoints failed. Last response: {response.status_code} - {response.text}\")\n            else:\n                self.logger.error(\"All ingestion endpoints failed to connect\")\n            \n            return False\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to send content to bridge: {e}\")\n            return False\n\n    def should_process_content(self, content):\n        \"\"\"Determine if content should be processed (avoid duplicates)\"\"\"\n        if not content.strip():\n            return False\n        \n        # Create a hash of the content to detect duplicates\n        content_hash = hash(content.strip().lower())\n        \n        # If content is similar to last processed, skip\n        if content_hash == self.last_content_hash:\n            return False\n        \n        # If content is too similar to last (90% match), skip\n        if self.last_content and len(content) > 0:\n            similarity = self._calculate_similarity(content.lower(), self.last_content.lower())\n            if similarity > 0.9:\n                return False\n        \n        return True\n\n    def _calculate_similarity(self, str1, str2):\n        \"\"\"Calculate similarity between two strings\"\"\"\n        if len(str1) == 0 or len(str2) == 0:\n            return 0.0\n        \n        # Simple character-based similarity\n        common_chars = sum(min(str1.count(c), str2.count(c)) for c in set(str1 + str2))\n        total_chars = len(str1) + len(str2)\n        \n        return (2 * common_chars) / total_chars if total_chars > 0 else 0.0\n\n    def process_screenshot(self):\n        \"\"\"Process a single screenshot cycle\"\"\"\n        if not self.enabled:\n            return False\n        \n        # Take screenshot\n        screenshot = self.take_screenshot()\n        if not screenshot:\n            return False\n        \n        # Extract text via OCR\n        extracted_text = self.extract_text_ocr(screenshot)\n        \n        if not extracted_text:\n            self.logger.debug(\"No text extracted from screenshot\")\n            return False\n        \n        # Check if content should be processed (avoid duplicates)\n        if not self.should_process_content(extracted_text):\n            self.logger.debug(\"Content skipped (duplicate or empty)\")\n            return False\n        \n        # Send to bridge\n        success = self.send_to_bridge(extracted_text)\n        \n        if success:\n            # Update tracking variables\n            self.last_content = extracted_text\n            self.last_content_hash = hash(extracted_text.strip().lower())\n            self.logger.info(f\"Processed {len(extracted_text)} characters from screenshot\")\n        \n        return success\n\n    def run(self):\n        \"\"\"Main daemon loop\"\"\"\n        self.logger.info(f\"Daemon Eyes started. Capturing every {self.interval} seconds...\")\n        self.logger.info(f\"Bridge URL: {self.bridge_url}\")\n        \n        try:\n            while True:\n                if self.enabled:\n                    try:\n                        self.process_screenshot()\n                    except Exception as e:\n                        self.logger.error(f\"Error in screenshot processing: {e}\")\n                \n                # Wait for the specified interval\n                time.sleep(self.interval)\n                \n        except KeyboardInterrupt:\n            self.logger.info(\"Daemon Eyes stopped by user\")\n        except Exception as e:\n            self.logger.error(f\"Daemon Eyes crashed: {e}\")\n\n    def toggle_enabled(self):\n        \"\"\"Toggle the daemon on/off\"\"\"\n        self.enabled = not self.enabled\n        status = \"enabled\" if self.enabled else \"disabled\"\n        self.logger.info(f\"Daemon Eyes {status}\")\n        return self.enabled\n\n    def set_interval(self, new_interval):\n        \"\"\"Change the screenshot interval\"\"\"\n        if new_interval > 0:\n            self.interval = new_interval\n            self.logger.info(f\"Screenshot interval set to {new_interval} seconds\")\n            return True\n        return False\n\n\ndef main():\n    \"\"\"Command line interface for Daemon Eyes\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Daemon Eyes - Background Vision Service\")\n    parser.add_argument(\"--interval\", type=int, default=5, \n                       help=\"Screenshot interval in seconds (default: 5)\")\n    parser.add_argument(\"--bridge-url\", default=\"http://localhost:8000\",\n                       help=\"Anchor Core bridge URL (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--test\", action=\"store_true\",\n                       help=\"Run a single screenshot test instead of daemon mode\")\n    \n    args = parser.parse_args()\n    \n    daemon = DaemonEyes(\n        bridge_url=args.bridge_url,\n        interval=args.interval,\n        token=args.token\n    )\n    \n    if args.test:\n        print(\"Running single screenshot test...\")\n        success = daemon.process_screenshot()\n        print(f\"Test {'succeeded' if success else 'failed'}\")\n    else:\n        print(f\"Starting Daemon Eyes (interval: {args.interval}s)...\")\n        daemon.run()\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\daemon_eyes.py"
  },
  {
    "id": "tools\\decode_cozo_blob.py",
    "timestamp": 1765895668,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"Quick decoder for CozoDB blob files to try to recover JSON/relations.\r\n\r\nUsage: python tools\\decode_cozo_blob.py C:\\path\\to\\cozo_blob_0.bin C:\\path\\to\\cozo_blob_1.bin\r\n\r\nIt attempts: UTF-8 decode, zlib.inflate, gzip, zstd (if installed), base64 decode, and searches for JSON-like markers.\r\n\"\"\"\r\nimport sys\r\nimport json\r\nimport zlib\r\nimport gzip\r\nimport base64\r\nimport bz2\r\nimport lzma\r\nfrom pathlib import Path\r\n\r\n# Optional compressors\r\ntry:\r\n    import zstandard as zstd\r\nexcept Exception:\r\n    zstd = None\r\n\r\ntry:\r\n    import brotli\r\nexcept Exception:\r\n    brotli = None\r\n\r\ntry:\r\n    import lz4.frame as lz4frame\r\nexcept Exception:\r\n    lz4frame = None\r\n\r\n\r\ndef hexdump(b, length=64):\r\n    return ' '.join(f\"{x:02x}\" for x in b[:length])\r\n\r\n\r\ndef try_utf8(b):\r\n    try:\r\n        s = b.decode('utf-8', errors='replace')\r\n        return s\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_zlib(b):\r\n    try:\r\n        return zlib.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_gzip(b):\r\n    try:\r\n        return gzip.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_base64(b):\r\n    try:\r\n        s = b.decode('ascii', errors='ignore').strip()\r\n        s2 = ''.join(s.split())\r\n        dec = base64.b64decode(s2)\r\n        return dec\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_bz2(b):\r\n    try:\r\n        return bz2.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_lzma(b):\r\n    try:\r\n        return lzma.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_brotli(b):\r\n    if not brotli:\r\n        return None\r\n    try:\r\n        return brotli.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_lz4(b):\r\n    if not lz4frame:\r\n        return None\r\n    try:\r\n        return lz4frame.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef try_zstd(b):\r\n    if not zstd:\r\n        return None\r\n    try:\r\n        dctx = zstd.ZstdDecompressor()\r\n        return dctx.decompress(b)\r\n    except Exception:\r\n        return None\r\n\r\n\r\ndef find_json_in_bytes(b):\r\n    try:\r\n        txt = b.decode('utf-8', errors='ignore')\r\n    except Exception:\r\n        txt = None\r\n    if not txt:\r\n        return None\r\n    idx = txt.find('{')\r\n    if idx >= 0:\r\n        # Try progressive parse\r\n        for end in range(idx+100, min(len(txt), idx+20000), 100):\r\n            try:\r\n                candidate = txt[idx:end]\r\n                parsed = json.loads(candidate)\r\n                return parsed\r\n            except Exception:\r\n                continue\r\n    if 'relations' in txt or 'memory' in txt or 'NamedRows' in txt:\r\n        return txt\r\n    return None\r\n\r\n\r\ndef analyze(path: Path):\r\n    print(f\"\\n=== Analyzing: {path} ===\")\r\n    b = path.read_bytes()\r\n    print(f\"Size: {len(b)} bytes\")\r\n    print(\"Hex (first 64 bytes):\", hexdump(b, 64))\r\n\r\n    s = try_utf8(b)\r\n    if s and ('{' in s or 'relations' in s or 'memory' in s or 'NamedRows' in s):\r\n        print(\"\\n-- UTF-8 text looks promising (preview):\\n\")\r\n        print(s[:2000])\r\n        return\r\n\r\n    z = try_zlib(b)\r\n    if z:\r\n        print(\"\\n-- zlib decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = z.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(z[:200]))\r\n        return\r\n\r\n    g = try_gzip(b)\r\n    if g:\r\n        print(\"\\n-- gzip decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = g.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(g[:200]))\r\n        return\r\n\r\n    zd = try_zstd(b)\r\n    if zd:\r\n        print(\"\\n-- zstd decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = zd.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(zd[:200]))\r\n        return\r\n\r\n    bz = try_bz2(b)\r\n    if bz:\r\n        print(\"\\n-- bz2 decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = bz.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(bz[:200]))\r\n        return\r\n\r\n    lz = try_lzma(b)\r\n    if lz:\r\n        print(\"\\n-- lzma decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = lz.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(lz[:200]))\r\n        return\r\n\r\n    br = try_brotli(b)\r\n    if br:\r\n        print(\"\\n-- brotli decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = br.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(br[:200]))\r\n        return\r\n\r\n    l4 = try_lz4(b)\r\n    if l4:\r\n        print(\"\\n-- lz4 decompressed (first 2000 chars):\\n\")\r\n        try:\r\n            txt = l4.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(l4[:200]))\r\n        return\r\n\r\n    bb = try_base64(b)\r\n    if bb:\r\n        print(\"\\n-- base64 decoded (preview):\\n\")\r\n        try:\r\n            txt = bb.decode('utf-8', errors='ignore')\r\n            print(txt[:2000])\r\n        except Exception:\r\n            print(repr(bb[:200]))\r\n        return\r\n\r\n    found = find_json_in_bytes(b)\r\n    if found:\r\n        print(\"\\n-- Found JSON-like content:\\n\")\r\n        if isinstance(found, str):\r\n            print(found[:2000])\r\n        else:\r\n            print(json.dumps(found, indent=2)[:4000])\r\n        return\r\n\r\n    out = path.with_suffix(path.suffix + '.raw')\r\n    out.write_bytes(b)\r\n    print(f\"\\nNo decode succeeded. Wrote raw blob to {out}\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    if len(sys.argv) < 2:\r\n        print('Usage: python tools\\\\decode_cozo_blob.py C:\\\\path\\\\to\\\\blob1 [blob2 ...]')\r\n        sys.exit(1)\r\n    for p in sys.argv[1:]:\r\n        analyze(Path(p))",
    "source": "tools\\decode_cozo_blob.py"
  },
  {
    "id": "tools\\eyes.py",
    "timestamp": 1765659368,
    "role": "file",
    "content": "import argparse\r\nimport requests\r\nimport sys\r\nimport os\r\n\r\ndef ingest(content, source_type=\"text\", adapter=\"eyes-cli\"):\r\n    url = \"http://localhost:8000/archivist/ingest\"\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"X-API-Key\": \"ece-secret-key\" \r\n    }\r\n    payload = {\r\n        \"content\": content,\r\n        \"type\": source_type,\r\n        \"adapter\": adapter\r\n    }\r\n    \r\n    print(f\"Sending to {url}...\")\r\n    try:\r\n        response = requests.post(url, json=payload, headers=headers)\r\n        response.raise_for_status()\r\n        print(f\"‚úÖ Success: {response.json()}\")\r\n    except requests.exceptions.RequestException as e:\r\n        print(f\"‚ùå Error: {e}\")\r\n        if hasattr(e, 'response') and e.response is not None:\r\n            print(f\"Details: {e.response.text}\")\r\n        sys.exit(1)\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"Sovereign Eyes - Ingest content into ECE Memory\")\r\n    parser.add_argument(\"input\", help=\"File path or text content to ingest\")\r\n    parser.add_argument(\"--type\", default=\"text\", help=\"Source type (text, web_page, etc.)\")\r\n    parser.add_argument(\"--adapter\", default=\"eyes-cli\", help=\"Adapter name\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    content = args.input\r\n    \r\n    # Check if input is a file\r\n    if os.path.exists(args.input):\r\n        try:\r\n            with open(args.input, 'r', encoding='utf-8') as f:\r\n                content = f.read()\r\n            print(f\"üìñ Read content from file: {args.input}\")\r\n        except Exception as e:\r\n            print(f\"‚ö†Ô∏è Could not read file '{args.input}', treating as raw text.\")\r\n            \r\n    ingest(content, args.type, args.adapter)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "source": "tools\\eyes.py"
  },
  {
    "id": "tools\\ghost.html",
    "timestamp": 1767452735,
    "role": "file",
    "content": "<!doctype html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\" />\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n        <title>Ghost Engine | Active Memory</title>\n        <style>\n            :root {\n                --bg-color: #1a1a1a;\n                --card-bg: #2d2d2d;\n                --accent: #0078d4;\n                --text: #ffffff;\n                --success: #4caf50;\n                --error: #f44336;\n            }\n\n            body {\n                font-family: \"Segoe UI\", system-ui, sans-serif;\n                background-color: var(--bg-color);\n                color: var(--text);\n                margin: 0;\n                padding: 20px;\n                height: 10vh;\n                overflow: auto;\n            }\n\n            .header {\n                text-align: center;\n                margin-bottom: 30px;\n            }\n\n            .status-indicator {\n                display: inline-block;\n                padding: 8px 16px;\n                border-radius: 20px;\n                margin-left: 10px;\n                font-size: 0.9em;\n            }\n\n            .connected {\n                background-color: rgba(76, 175, 80, 0.2);\n                color: var(--success);\n            }\n\n            .disconnected {\n                background-color: rgba(244, 67, 54, 0.2);\n                color: var(--error);\n            }\n\n            .container {\n                max-width: 1200px;\n                margin: 0 auto;\n            }\n\n            .controls {\n                display: flex;\n                gap: 15px;\n                margin-bottom: 20px;\n                flex-wrap: wrap;\n            }\n\n            button {\n                background-color: var(--accent);\n                color: white;\n                border: none;\n                padding: 10px 20px;\n                border-radius: 6px;\n                cursor: pointer;\n                font-weight: 500;\n            }\n\n            button:hover {\n                opacity: 0.9;\n            }\n\n            button:disabled {\n                background-color: #555;\n                cursor: not-allowed;\n            }\n\n            .log-container {\n                background-color: var(--card-bg);\n                border-radius: 8px;\n                padding: 15px;\n                height: 400px;\n                overflow-y: auto;\n                font-family: monospace;\n                font-size: 0.9em;\n                border: 1px solid #333;\n            }\n\n            .log-entry {\n                margin-bottom: 5px;\n                padding: 5px;\n                border-radius: 4px;\n            }\n\n            .log-info {\n                color: #aaa;\n            }\n            .log-success {\n                color: var(--success);\n            }\n            .log-error {\n                color: var(--error);\n            }\n            .log-warning {\n                color: #ff9800;\n            }\n\n            .stats {\n                display: grid;\n                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n                gap: 15px;\n                margin-top: 20px;\n            }\n\n            .stat-card {\n                background-color: var(--card-bg);\n                border-radius: 8px;\n                padding: 15px;\n                text-align: center;\n                border: 1px solid #333;\n            }\n\n            .stat-value {\n                font-size: 2em;\n                font-weight: bold;\n                color: var(--accent);\n                margin: 10px 0;\n            }\n\n            .stat-label {\n                color: #aaa;\n                font-size: 0.9em;\n            }\n        </style>\n    </head>\n    <body>\n        <div class=\"container\">\n            <div class=\"header\">\n                <h1>üëª Ghost Engine | Active Memory</h1>\n                <p>\n                    Database operations run in your browser when this page is\n                    open\n                </p>\n                <div\n                    id=\"connection-status\"\n                    class=\"status-indicator disconnected\"\n                >\n                    DISCONNECTED\n                </div>\n            </div>\n\n            <div class=\"controls\">\n                <button id=\"connect-btn\">Connect to Bridge</button>\n                <button id=\"init-db-btn\" disabled>Initialize Database</button>\n                <button id=\"test-query-btn\" disabled>Test Query</button>\n                <button id=\"reload-db-btn\" disabled>Reload Database</button>\n            </div>\n\n            <div class=\"stats\">\n                <div class=\"stat-card\">\n                    <div class=\"stat-label\">Documents</div>\n                    <div id=\"doc-count\" class=\"stat-value\">-</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-label\">Memory Relations</div>\n                    <div id=\"memory-relations\" class=\"stat-value\">-</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-label\">Last Activity</div>\n                    <div id=\"last-activity\" class=\"stat-value\">-</div>\n                </div>\n                <div class=\"stat-card\">\n                    <div class=\"stat-label\">Status</div>\n                    <div id=\"engine-status\" class=\"stat-value\">Idle</div>\n                </div>\n            </div>\n\n            <h3>Operation Log</h3>\n            <div id=\"log\" class=\"log-container\"></div>\n        </div>\n\n        <!-- CozoDB WASM Module -->\n        <script src=\"cozo_lib_wasm.js\"></script>\n\n        <script type=\"module\">\n            import initCozo, { CozoDb } from \"./cozo_lib_wasm.js\";\n\n            // Configuration\n            const PORT = 8000;\n\n            // Global variables\n            let socket = null;\n            let db = null;\n            let isConnected = false;\n\n            // DOM Elements\n            const connectBtn = document.getElementById(\"connect-btn\");\n            const initDbBtn = document.getElementById(\"init-db-btn\");\n            const testQueryBtn = document.getElementById(\"test-query-btn\");\n            const reloadDbBtn = document.getElementById(\"reload-db-btn\");\n            const connectionStatus =\n                document.getElementById(\"connection-status\");\n            const logContainer = document.getElementById(\"log\");\n            const docCountEl = document.getElementById(\"doc-count\");\n            const engineStatus = document.getElementById(\"engine-status\");\n            const memoryRelationsEl =\n                document.getElementById(\"memory-relations\");\n            const lastActivityEl = document.getElementById(\"last-activity\");\n\n            // Logging function\n            function log(level, message) {\n                const timestamp = new Date().toISOString().slice(11, 19);\n                const logEntry = document.createElement(\"div\");\n                logEntry.className = `log-entry log-${level.toLowerCase()}`;\n                logEntry.textContent = `[${timestamp}] [${level.toUpperCase()}] ${message}`;\n                logContainer.appendChild(logEntry);\n                logContainer.scrollTop = logContainer.scrollHeight;\n            }\n\n            // Initialize database\n            async function initDatabase() {\n                try {\n                    log(\"INFO\", \"Initializing CozoDB...\");\n\n                    // Initialize WASM module\n                    await initCozo();\n                    log(\"SUCCESS\", \"WASM Module loaded\");\n\n                    // Create database instance with persistent storage\n                    try {\n                        db = await CozoDb.new_from_indexed_db(\n                            \"coda_memory_v2\",\n                            \"cozo_store\",\n                            () => {},\n                        );\n                        log(\"SUCCESS\", \"CozoDB (Persistent) Loaded [v2]\");\n                    } catch (e) {\n                        log(\n                            \"WARNING\",\n                            \"Persistence failed, using memory: \" + e.message,\n                        );\n                        db = CozoDb.new();\n                    }\n\n                    // Ensure schema exists\n                    const schemaOk = await ensureSchema();\n                    if (!schemaOk) {\n                        log(\"ERROR\", \"Schema creation failed, database not ready\");\n                        return false;\n                    }\n\n                    // Update stats\n                    await updateStats();\n\n                    log(\"SUCCESS\", \"Database initialized successfully\");\n                    return true;\n                } catch (e) {\n                    log(\n                        \"ERROR\",\n                        \"Database initialization failed: \" + e.message,\n                    );\n                    return false;\n                }\n            }\n\n            // Ensure database schema\n            async function ensureSchema() {\n                // Create basic table first\n                const basicSchemaQuery = `:create memory {id: String => timestamp: Int, content: String, source: String, type: String} if not exists;`;\n\n                try {\n                    const result = await db.run(basicSchemaQuery, \"{}\");\n                    const jsonResult = JSON.parse(result);\n\n                    if (jsonResult.ok) {\n                        log(\"SUCCESS\", \"Basic schema created successfully\");\n                    } else {\n                        log(\n                            \"ERROR\",\n                            \"Basic schema creation failed: \" +\n                                JSON.stringify(jsonResult.error || \"Unknown error\"),\n                        );\n                        return false;\n                    }\n                } catch (e) {\n                    log(\"ERROR\", \"Basic schema operation failed: \" + (e.message || \"Unknown error\"));\n                    return false;\n                }\n\n                // Then, try to create FTS separately\n                const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;\n\n                try {\n                    const ftsResult = await db.run(ftsQuery, \"{}\");\n                    const ftsJsonResult = JSON.parse(ftsResult);\n\n                    if (ftsJsonResult.ok) {\n                        log(\"SUCCESS\", \"FTS index created successfully\");\n                    } else {\n                        log(\n                            \"WARNING\",\n                            \"FTS creation failed (search will be limited): \" +\n                                JSON.stringify(ftsJsonResult.error || \"Unknown error\"),\n                        );\n                        // Don't return false here - basic functionality still works\n                    }\n                } catch (e) {\n                    log(\n                        \"WARNING\",\n                        \"FTS creation failed (search will be limited): \" + (e.message || \"Unknown error\")\n                    );\n                    // Don't return false - basic functionality still works\n                }\n\n                return true;\n            }\n\n            // Update statistics display\n            async function updateStats() {\n                try {\n                    const countQuery = \"?[count(id)] := *memory{id}\";\n                    const result = await db.run(countQuery, \"{}\");\n                    const jsonResult = JSON.parse(result);\n\n                    if (\n                        jsonResult.ok &&\n                        jsonResult.rows &&\n                        jsonResult.rows[0]\n                    ) {\n                        docCountEl.textContent = jsonResult.rows[0][0];\n                    } else {\n                        docCountEl.textContent = \"0\";\n                    }\n\n                    // Update other stats as needed\n                    memoryRelationsEl.textContent = \"memory\";\n                    lastActivityEl.textContent =\n                        new Date().toLocaleTimeString();\n                } catch (e) {\n                    docCountEl.textContent = \"Error\";\n                    log(\"ERROR\", \"Failed to get document count: \" + e.message);\n                }\n            }\n\n            // Connect to bridge\n            function connectToBridge() {\n                if (socket) {\n                    socket.close();\n                }\n\n                const wsUrl = `ws://localhost:${PORT}/ws/chat`;\n                log(\"INFO\", \"Connecting to Bridge at: \" + wsUrl);\n\n                try {\n                    socket = new WebSocket(wsUrl);\n\n                    socket.onopen = async () => {\n                        log(\"SUCCESS\", \"Connected to Bridge\");\n                        isConnected = true;\n                        connectionStatus.textContent = \"CONNECTED\";\n                        connectionStatus.className =\n                            \"status-indicator connected\";\n\n                        // Initialize database first before signaling readiness\n                        const dbInitialized = await initDatabase();\n                        if (dbInitialized) {\n                            // Send ready message to bridge only after database is ready\n                            socket.send(JSON.stringify({ type: \"engine_ready\" }));\n                            log(\"SUCCESS\", \"Ghost Engine Ready - Database initialized and ready for requests\");\n                        } else {\n                            log(\"ERROR\", \"Database initialization failed, Ghost Engine not ready\");\n                            socket.send(JSON.stringify({\n                                type: \"engine_error\",\n                                message: \"Database initialization failed\"\n                            }));\n                        }\n\n                        // Enable buttons\n                        initDbBtn.disabled = false;\n                        testQueryBtn.disabled = false;\n                        reloadDbBtn.disabled = false;\n                    };\n\n                    socket.onmessage = async (event) => {\n                        try {\n                            const msg = JSON.parse(event.data);\n\n                            if (msg.type === \"engine_ready\") {\n                                log(\n                                    \"INFO\",\n                                    \"Bridge acknowledged engine readiness\",\n                                );\n                            } else if (msg.type === \"engine_error\") {\n                                log(\n                                    \"ERROR\",\n                                    \"Bridge reported engine error: \" + (msg.message || \"Unknown error\"),\n                                );\n                            } else if (\n                                msg.type === \"memory_ingest\" ||\n                                msg.type === \"ingest\"\n                            ) {\n                                // Only process ingest if database is ready\n                                if (db) {\n                                    await handleIngest(msg);\n                                } else {\n                                    log(\"WARNING\", \"Received ingest request but database not ready, queuing for later\");\n                                    // For now, send an error back - in a more advanced implementation we could queue these\n                                    if (msg.id && socket && socket.readyState === WebSocket.OPEN) {\n                                        socket.send(\n                                            JSON.stringify({\n                                                id: msg.id,\n                                                type: \"ingest_complete\",\n                                                status: \"error\",\n                                                error: \"Database not ready, please wait for initialization to complete\",\n                                            }),\n                                        );\n                                    }\n                                }\n                            } else if (msg.type === \"direct_search_request\") {\n                                // Only process search if database is ready\n                                if (db) {\n                                    await handleSearch(msg);\n                                } else {\n                                    log(\"WARNING\", \"Received search request but database not ready\");\n                                    if (msg.id && socket && socket.readyState === WebSocket.OPEN) {\n                                        socket.send(\n                                            JSON.stringify({\n                                                type: \"direct_search_result\",\n                                                id: msg.id,\n                                                result: \"Database not ready, please wait for initialization to complete\",\n                                            }),\n                                        );\n                                    }\n                                }\n                            } else {\n                                log(\n                                    \"INFO\",\n                                    \"Received message type: \" + msg.type,\n                                );\n                            }\n                        } catch (e) {\n                            log(\n                                \"ERROR\",\n                                \"Error processing message: \" + e.message,\n                            );\n                        }\n                    };\n\n                    socket.onclose = () => {\n                        log(\"WARNING\", \"Bridge connection closed\");\n                        isConnected = false;\n                        connectionStatus.textContent = \"DISCONNECTED\";\n                        connectionStatus.className =\n                            \"status-indicator disconnected\";\n\n                        // Disable buttons\n                        initDbBtn.disabled = true;\n                        testQueryBtn.disabled = true;\n                        reloadDbBtn.disabled = true;\n                    };\n\n                    socket.onerror = (error) => {\n                        log(\n                            \"ERROR\",\n                            \"Bridge connection error: \" + error.message,\n                        );\n                    };\n                } catch (e) {\n                    log(\n                        \"ERROR\",\n                        \"Failed to create WebSocket connection: \" + e.message,\n                    );\n                }\n            }\n\n            // Handle memory ingestion\n            async function handleIngest(msg) {\n                log(\n                    \"INFO\",\n                    `Processing ingest request: ${msg.filename || \"unnamed\"}`,\n                );\n\n                // Check if database is initialized\n                if (!db) {\n                    const errorMsg = \"Database not initialized, cannot process ingest request\";\n                    log(\"ERROR\", errorMsg);\n\n                    // Send error acknowledgment if ID exists\n                    if (\n                        msg.id &&\n                        socket &&\n                        socket.readyState === WebSocket.OPEN\n                    ) {\n                        socket.send(\n                            JSON.stringify({\n                                id: msg.id,\n                                type: \"ingest_complete\",\n                                status: \"error\",\n                                error: errorMsg,\n                            }),\n                        );\n                    }\n                    return;\n                }\n\n                try {\n                    const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;\n                    const timestamp = Date.now();\n\n                    // Prepare the insert query\n                    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n                    const params = {\n                        data: [\n                            [\n                                id,\n                                timestamp,\n                                msg.content,\n                                msg.source || msg.filename || \"unknown\",\n                                msg.file_type || \"text\",\n                            ],\n                        ],\n                    };\n\n                    const result = await db.run(query, JSON.stringify(params));\n                    const jsonResult = JSON.parse(result);\n\n                    if (jsonResult.ok) {\n                        log(\"SUCCESS\", `Ingested document: ${id}`);\n\n                        // Update stats\n                        await updateStats();\n\n                        // Send completion acknowledgment if ID exists\n                        if (\n                            msg.id &&\n                            socket &&\n                            socket.readyState === WebSocket.OPEN\n                        ) {\n                            socket.send(\n                                JSON.stringify({\n                                    id: msg.id,\n                                    type: \"ingest_complete\",\n                                    status: \"success\",\n                                    doc_id: id,\n                                }),\n                            );\n                        }\n                    } else {\n                        log(\n                            \"ERROR\",\n                            `Ingest failed: ${JSON.stringify(jsonResult.error || \"Unknown error\")}`,\n                        );\n\n                        // Send error acknowledgment if ID exists\n                        if (\n                            msg.id &&\n                            socket &&\n                            socket.readyState === WebSocket.OPEN\n                        ) {\n                            socket.send(\n                                JSON.stringify({\n                                    id: msg.id,\n                                    type: \"ingest_complete\",\n                                    status: \"error\",\n                                    error: jsonResult.error || \"Unknown error\",\n                                }),\n                            );\n                        }\n                    }\n                } catch (e) {\n                    log(\"ERROR\", `Ingest error: ${e.message || \"Unknown error\"}`);\n\n                    // Send error acknowledgment if ID exists\n                    if (\n                        msg.id &&\n                        socket &&\n                        socket.readyState === WebSocket.OPEN\n                    ) {\n                        socket.send(\n                            JSON.stringify({\n                                id: msg.id,\n                                type: \"ingest_complete\",\n                                status: \"error\",\n                                error: e.message || \"Unknown error\",\n                            }),\n                        );\n                    }\n                }\n            }\n\n            // Handle search requests\n            async function handleSearch(msg) {\n                log(\"INFO\", `Processing search request: ${msg.query}`);\n\n                try {\n                    const limit = msg.limit || 10;\n                    let results = [];\n\n                    // 1. Try FTS (Full Text Search) first\n                    try {\n                        const ftsQuery = `?[source, content, score] := ~memory:content_fts{content | query: $q, score: s}, score = s :order -score :limit $limit`;\n                        const ftsResult = await db.run(\n                            ftsQuery,\n                            JSON.stringify({ q: msg.query, limit: limit * 2 }),\n                        ); // Get more for diversity\n                        const ftsJson = JSON.parse(ftsResult);\n\n                        if (ftsJson.ok && ftsJson.rows) {\n                            for (const row of ftsJson.rows) {\n                                // Only add if not already in results (by source)\n                                if (!results.some((r) => r[0] === row[0])) {\n                                    results.push([row[0], row[1]]); // source, content (skip score in final result)\n                                }\n                            }\n                        }\n                    } catch (e) {\n                        log(\"WARNING\", `FTS search failed: ${e.message}`);\n                    }\n\n                    // 2. Fallback to regex search if FTS didn't return enough results\n                    if (results.length < Math.min(5, limit)) {\n                        try {\n                            const regexQuery = `?[source, content] := *memory{source, content}, regex_matches($q, content) :limit $limit`;\n                            const regexResult = await db.run(\n                                regexQuery,\n                                JSON.stringify({ q: msg.query, limit: limit }),\n                            );\n                            const regexJson = JSON.parse(regexResult);\n\n                            if (regexJson.ok && regexJson.rows) {\n                                for (const row of regexJson.rows) {\n                                    // Only add if not already in results\n                                    if (!results.some((r) => r[0] === row[0])) {\n                                        results.push(row);\n                                    }\n                                }\n                            }\n                        } catch (e) {\n                            log(\"WARNING\", `Regex search failed: ${e.message}`);\n                        }\n                    }\n\n                    // 3. If still not enough results, try source name matching\n                    if (results.length < Math.min(3, limit)) {\n                        try {\n                            const sourceQuery = `?[source, content] := *memory{source, content}, regex_matches($q, source) :limit $limit`;\n                            const sourceResult = await db.run(\n                                sourceQuery,\n                                JSON.stringify({ q: msg.query, limit: limit }),\n                            );\n                            const sourceJson = JSON.parse(sourceResult);\n\n                            if (sourceJson.ok && sourceJson.rows) {\n                                for (const row of sourceJson.rows) {\n                                    // Only add if not already in results\n                                    if (!results.some((r) => r[0] === row[0])) {\n                                        results.push(row);\n                                    }\n                                }\n                            }\n                        } catch (e) {\n                            log(\n                                \"WARNING\",\n                                `Source search failed: ${e.message}`,\n                            );\n                        }\n                    }\n\n                    // Format results as markdown\n                    let output = \"\";\n                    let charCount = 0;\n                    const maxChars = msg.max_chars || 10000;\n\n                    for (const [source, content] of results) {\n                        if (charCount + content.length > maxChars) break;\n                        output += `### Source: ${source}\\n${content}\\n\\n`;\n                        charCount += content.length;\n                    }\n\n                    if (!output) output = \"No relevant context found.\";\n\n                    // Send results back to bridge\n                    if (\n                        msg.id &&\n                        socket &&\n                        socket.readyState === WebSocket.OPEN\n                    ) {\n                        socket.send(\n                            JSON.stringify({\n                                type: \"direct_search_result\",\n                                id: msg.id,\n                                result: output,\n                            }),\n                        );\n                    }\n\n                    log(\n                        \"SUCCESS\",\n                        `Search completed, found ${results.length} results`,\n                    );\n                } catch (e) {\n                    log(\"ERROR\", `Search error: ${e.message}`);\n\n                    // Send error result back to bridge\n                    if (\n                        msg.id &&\n                        socket &&\n                        socket.readyState === WebSocket.OPEN\n                    ) {\n                        socket.send(\n                            JSON.stringify({\n                                type: \"direct_search_result\",\n                                id: msg.id,\n                                result: `Search error: ${e.message}`,\n                            }),\n                        );\n                    }\n                }\n            }\n\n            // Test query function\n            async function testQuery() {\n                if (!db) {\n                    log(\"ERROR\", \"Database not initialized\");\n                    return;\n                }\n\n                try {\n                    log(\"INFO\", \"Running test query...\");\n                    const query = \"?[count(id)] := *memory{id}\";\n                    const result = await db.run(query, \"{}\");\n                    const jsonResult = JSON.parse(result);\n\n                    if (jsonResult.ok) {\n                        if (jsonResult.rows && jsonResult.rows[0]) {\n                            log(\n                                \"SUCCESS\",\n                                `Test query successful: ${jsonResult.rows[0][0]} documents in memory`,\n                            );\n                        } else {\n                            log(\n                                \"SUCCESS\",\n                                \"Test query successful: 0 documents in memory\",\n                            );\n                        }\n                    } else {\n                        log(\n                            \"ERROR\",\n                            `Test query failed: ${JSON.stringify(jsonResult.error || \"Unknown error\")}`,\n                        );\n                    }\n                } catch (e) {\n                    log(\"ERROR\", `Test query error: ${e.message || \"Unknown error\"}`);\n                }\n            }\n\n            // Event listeners\n            connectBtn.addEventListener(\"click\", connectToBridge);\n\n            initDbBtn.addEventListener(\"click\", async () => {\n                await initDatabase();\n                await updateStats();\n            });\n\n            testQueryBtn.addEventListener(\"click\", testQuery);\n\n            reloadDbBtn.addEventListener(\"click\", async () => {\n                log(\"INFO\", \"Reloading database...\");\n                if (db) {\n                    db.free(); // Free the current database instance\n                }\n                const success = await initDatabase();\n                if (success) {\n                    await updateStats();\n                }\n            });\n\n            // Initialize on load\n            window.addEventListener(\"load\", () => {\n                log(\n                    \"INFO\",\n                    'Ghost Engine UI loaded. Click \"Connect to Bridge\" to begin.',\n                );\n            });\n        </script>\n    </body>\n</html>\n",
    "source": "tools\\ghost.html"
  },
  {
    "id": "tools\\index.html",
    "timestamp": 1767451577,
    "role": "file",
    "content": "<!doctype html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\" />\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n        <title>Anchor Core | Launcher</title>\n        <style>\n            :root {\n                --bg-color: #1a1a1a;\n                --card-bg: #2d2d2d;\n                --accent: #0078d4;\n                --text: #ffffff;\n                --secondary-text: #aaaaaa;\n            }\n\n            body {\n                font-family: \"Segoe UI\", system-ui, sans-serif;\n                background-color: var(--bg-color);\n                color: var(--text);\n                display: flex;\n                flex-direction: column;\n                align-items: center;\n                justify-content: center;\n                height: 100vh;\n            }\n\n            h1 {\n                font-weight: 300;\n                margin-bottom: 40px;\n                letter-spacing: 2px;\n            }\n\n            .grid {\n                display: grid;\n                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n                gap: 20px;\n                width: 100%;\n                max-width: 900px;\n                padding: 20px;\n            }\n\n            .card {\n                background-color: var(--card-bg);\n                border-radius: 12px;\n                padding: 30px;\n                text-decoration: none;\n                color: var(--text);\n                transition:\n                    transform 0.2s,\n                    box-shadow 0.2s;\n                border: 1px solid #333;\n                display: flex;\n                flex-direction: column;\n                align-items: center;\n                text-align: center;\n            }\n\n            .card:hover {\n                transform: translateY(-5px);\n                box-shadow: 0 10px 20px rgba(0, 0, 0, 0.4);\n                border-color: var(--accent);\n            }\n\n            .icon {\n                font-size: 48px;\n                margin-bottom: 20px;\n            }\n\n            .title {\n                font-size: 1.25rem;\n                font-weight: 600;\n                margin-bottom: 10px;\n            }\n\n            .desc {\n                font-size: 0.9rem;\n                color: var(--secondary-text);\n            }\n\n            .status {\n                margin-top: 15px;\n                font-size: 0.8rem;\n                padding: 4px 8px;\n                border-radius: 4px;\n                background: #333;\n                color: #888;\n            }\n\n            .card:hover .status {\n                background: rgba(0, 120, 212, 0.2);\n                color: var(--accent);\n            }\n        </style>\n    </head>\n\n    <body>\n        <h1>\n            ANCHOR\n            <span style=\"color: var(--accent); font-weight: 600\">LITE</span>\n        </h1>\n\n        <div class=\"grid\">\n            <a href=\"context.html\" class=\"card\">\n                <div class=\"icon\">ÔøΩ</div>\n                <div class=\"title\">Context Console</div>\n                <div class=\"desc\">\n                    Primary Interface. Search and retrieve memory from the Ghost\n                    Engine.\n                </div>\n                <div class=\"status\">Active</div>\n            </a>\n\n            <a href=\"log-viewer.html\" class=\"card\">\n                <div class=\"icon\">üìä</div>\n                <div class=\"title\">Log Viewer</div>\n                <div class=\"desc\">\n                    Monitor engine performance and debug events.\n                </div>\n                <div class=\"status\">Utility</div>\n            </a>\n\n            <a href=\"anchor-mic.html\" class=\"card\">\n                <div class=\"icon\">üéôÔ∏è</div>\n                <div class=\"title\">Root Mic</div>\n                <div class=\"desc\">\n                    Dictate directly to the engine using Whisper.\n                </div>\n                <div class=\"status\">Input</div>\n            </a>\n\n            <div class=\"card\" onclick=\"spawnAnchorShell()\">\n                <div class=\"icon\">‚öì</div>\n                <div class=\"title\">Anchor Shell</div>\n                <div class=\"desc\">\n                    Launch the native PowerShell client connected to the Ghost\n                    Engine.\n                </div>\n                <div class=\"status\">Launcher</div>\n            </div>\n\n            <a href=\"ghost.html\" class=\"card\">\n                <div class=\"icon\">üëª</div>\n                <div class=\"title\">Ghost Engine</div>\n                <div class=\"desc\">\n                    Active memory engine. Opens the Ghost Engine in browser for\n                    direct operations.\n                </div>\n                <div class=\"status\">Active</div>\n            </a>\n        </div>\n\n        <div style=\"margin-top: 50px; color: #555; font-size: 0.8rem\">\n            Running on Localhost ‚Ä¢ Anchor Lite Refactor\n        </div>\n\n        <script>\n            async function spawnAnchorShell() {\n                try {\n                    const res = await fetch(\n                        \"http://localhost:8000/v1/system/spawn_shell\",\n                        {\n                            method: \"POST\",\n                            headers: {\n                                \"Content-Type\": \"application/json\",\n                                Authorization: \"Bearer sovereign-secret\",\n                            },\n                            body: JSON.stringify({}),\n                        },\n                    );\n                    if (res.ok) {\n                        console.log(\"Anchor Shell Spawned\");\n                        alert(\n                            \"Anchor terminal launched in new PowerShell window!\",\n                        );\n                    } else {\n                        alert(\"Failed to spawn. Is Bridge running?\");\n                    }\n                } catch (e) {\n                    alert(\"Error: \" + e.message);\n                }\n            }\n        </script>\n    </body>\n</html>\n",
    "source": "tools\\index.html"
  },
  {
    "id": "tools\\indexeddb.js",
    "timestamp": 1766026191,
    "role": "file",
    "content": "let db = null;\nlet cozoDbStore = null;\nlet writeCounter = 0;\nlet writeCallback = null;\n\nlet cmdFlag = false;\n\nexport function setWriteCounter(count) {\n  writeCounter = count;\n}\n\nfunction storeRequestToPromise(req) {\n  return new Promise((resolve, reject) => {\n    req.onsuccess = () => resolve(req.result);\n    req.onerror = (e) => reject(e.error);\n  });\n}\n\nasync function openDatabase(dbName, storeName) {\n  cozoDbStore = storeName;\n\n  return new Promise((resolve, reject) => {\n    const request = indexedDB.open(dbName, 1);\n    request.onupgradeneeded = function (event) {\n      const db = event.target.result;\n      if (!db.objectStoreNames.contains(storeName)) {\n        db.createObjectStore(storeName);\n      }\n    };\n\n    request.onsuccess = function (event) {\n      db = event.target.result;\n      resolve(db);\n    };\n    request.onerror = function (event) {\n      reject(event.error);\n    };\n  });\n}\n\nasync function readStore() {\n  return new Promise((resolve, reject) => {\n    const transaction = db.transaction(cozoDbStore, \"readonly\");\n    const store = transaction.objectStore(cozoDbStore);\n\n    const itemsPromise = storeRequestToPromise(store.getAll());\n    const keysPromise = storeRequestToPromise(store.getAllKeys());\n\n    Promise.all([keysPromise, itemsPromise])\n      .then((results) => {\n        const keys = results[0].map((item) => new Uint8Array(item));\n        const items = results[1];\n        resolve([keys, items]);\n      })\n      .catch(reject);\n  });\n}\n\nexport async function flushPendingWrites(timeoutDuration = 60000) {\n  let timeout = null;\n\n  // allow only one command runnig at a time\n  if (cmdFlag) {\n    await new Promise((resolve, reject) => {\n      const interval = setInterval(() => {\n        if (!cmdFlag) {\n          clearInterval(interval);\n          resolve();\n        }\n      }, 10);\n    });\n  }\n\n  cmdFlag = true;\n\n  const waitPromise = new Promise((resolve, reject) => {\n    const interval = setInterval(() => {\n      if (writeCounter <= 0) {\n        if (timeout) {\n          clearTimeout(timeout);\n        }\n        clearInterval(interval);\n        resolve();\n      }\n    }, 10);\n  });\n\n  const timeoutPromise = new Promise((_, reject) => {\n    timeout = setTimeout(() => {\n      reject(new Error(\"waitForPendingWrites timed out!\"));\n    }, timeoutDuration);\n  });\n\n  // wait until all pending writes are done\n  return Promise.race([waitPromise, timeoutPromise]).finally(() => {\n    cmdFlag = false;\n  });\n}\n\nexport async function loadAllFromIndexedDb(dbName, storeName, onWriteCallback) {\n  writeCallback = onWriteCallback;\n  await openDatabase(dbName, storeName);\n  return await readStore();\n}\n\nexport async function writeToIndexedDb(key, value) {\n  return new Promise((resolve, reject) => {\n    const transaction = db.transaction(cozoDbStore, \"readwrite\");\n    const store = transaction.objectStore(cozoDbStore);\n    const request = value ? store.put(value, key) : store.delete(key);\n    return storeRequestToPromise(request)\n      .then(resolve)\n      .catch(reject)\n      .finally(() => {\n        writeCounter--;\n        writeCallback && writeCallback(writeCounter);\n      });\n  });\n}\n\nexport function closeDatabase() {\n  if (db) {\n    db.close();\n    db = null;\n  }\n}\n\nexport async function clearIndexedDbStore(dbName, storeName) {\n  // Ensure we open a connection first if one isn't open\n  if (!db) {\n    await openDatabase(dbName, storeName);\n  }\n  return new Promise((resolve, reject) => {\n    const transaction = db.transaction(storeName, \"readwrite\");\n    const store = transaction.objectStore(storeName);\n    const request = store.clear();\n\n    request.onsuccess = () => resolve();\n    request.onerror = (e) => reject(e.target.error);\n  });\n}\n",
    "source": "tools\\indexeddb.js"
  },
  {
    "id": "tools\\log-viewer.html",
    "timestamp": 1767254282,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Anchor Core System Logs</title>\r\n    <style>\r\n        body {\r\n            background-color: #000;\r\n            color: #0f0;\r\n            font-family: 'Consolas', 'Monaco', monospace;\r\n            margin: 0;\r\n            display: flex;\r\n            flex-direction: column;\r\n            height: 100vh;\r\n            overflow: hidden;\r\n        }\r\n\r\n        #toolbar {\r\n            background-color: #111;\r\n            padding: 10px;\r\n            border-bottom: 1px solid #333;\r\n            display: flex;\r\n            gap: 10px;\r\n            align-items: center;\r\n        }\r\n\r\n        button {\r\n            background: #333;\r\n            color: #fff;\r\n            border: 1px solid #444;\r\n            cursor: pointer;\r\n            padding: 5px 10px;\r\n        }\r\n\r\n        button:hover {\r\n            background: #444;\r\n        }\r\n\r\n        #main-container {\r\n            display: flex;\r\n            flex: 1;\r\n            overflow: hidden;\r\n        }\r\n\r\n        .panel {\r\n            flex: 1;\r\n            display: flex;\r\n            flex-direction: column;\r\n            min-width: 0;\r\n        }\r\n\r\n        .panel-header {\r\n            background: #1a1a1a;\r\n            padding: 5px 10px;\r\n            font-weight: bold;\r\n            border-bottom: 1px solid #333;\r\n            display: flex;\r\n            justify-content: space-between;\r\n        }\r\n\r\n        .log-container {\r\n            flex: 1;\r\n            overflow-y: auto;\r\n            padding: 10px;\r\n            white-space: pre-wrap;\r\n            font-size: 12px;\r\n        }\r\n\r\n        .log-line {\r\n            padding: 2px 0;\r\n            border-bottom: 1px solid #111;\r\n            word-break: break-all;\r\n        }\r\n\r\n        .log-line:hover {\r\n            background-color: #111;\r\n        }\r\n\r\n        .info {\r\n            color: #88ccff;\r\n        }\r\n\r\n        .warning {\r\n            color: #ffcc00;\r\n        }\r\n\r\n        .error {\r\n            color: #ff4444;\r\n            font-weight: bold;\r\n        }\r\n\r\n        .debug {\r\n            color: #888;\r\n        }\r\n\r\n        .success {\r\n            color: #00ff00;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <div id=\"toolbar\">\r\n        <span style=\"color: #88ccff; font-weight: bold;\">Anchor Core Log Stream</span>\r\n        <span id=\"connection-status\" style=\"color: #888; margin-left: 10px;\">‚óè Listening...</span>\r\n        <div style=\"flex: 1;\"></div>\r\n        <button id=\"copy-all-btn\" title=\"Copy all logs to clipboard\">üìã Copy All</button>\r\n        <button id=\"retry-logs-btn\" title=\"Retry fetching backend logs\">Retry Backend Logs</button>\r\n        <button id=\"clear-btn\">Clear All</button>\r\n    </div>\r\n\r\n    <div id=\"main-container\">\r\n        <!-- Single Combined Log Panel -->\r\n        <div class=\"panel\">\r\n            <div class=\"panel-header\">\r\n                <span>All System Logs</span>\r\n            </div>\r\n            <div id=\"combined-logs\" class=\"log-container\"></div>\r\n        </div>\r\n    </div>\r\n\r\n    <script>\r\n        const combinedContainer = document.getElementById('combined-logs');\r\n        const clearBtn = document.getElementById('clear-btn');\r\n        const copyBtn = document.getElementById('copy-all-btn');\r\n        const statusIndicator = document.getElementById('connection-status');\r\n\r\n        copyBtn.onclick = () => {\r\n            const allText = combinedContainer.innerText;\r\n\r\n            navigator.clipboard.writeText(allText).then(() => {\r\n                const original = copyBtn.innerHTML;\r\n                copyBtn.innerHTML = \"‚úÖ Copied!\";\r\n                setTimeout(() => copyBtn.innerHTML = original, 2000);\r\n            }).catch(e => alert(\"Copy failed: \" + e));\r\n        };\r\n\r\n        const logChannel = new BroadcastChannel('sovereign-logs');\r\n        const codaChannel = new BroadcastChannel('coda_logs');\r\n        // Simple de-duplication for server polling\r\n        const serverLogSet = new Set();\r\n\r\n        function markActive() {\r\n            statusIndicator.style.color = '#0f0';\r\n            statusIndicator.innerText = '‚óè Active';\r\n            setTimeout(() => {\r\n                statusIndicator.style.color = '#888';\r\n                statusIndicator.innerText = '‚óè Listening...';\r\n            }, 2000);\r\n        }\r\n\r\n        logChannel.onmessage = (event) => {\r\n            markActive();\r\n            const data = event.data;\r\n\r\n            if (data.source === 'system') {\r\n                appendLog(combinedContainer, `[${data.time}] [${data.type}] ${data.msg}`, data.type);\r\n            } else if (data.source === 'chat') {\r\n                appendLog(combinedContainer, `[${data.time}] ${data.role}: ${data.text}`, 'info');\r\n            }\r\n        };\r\n\r\n        // Mission Control channel (coda_logs) - short JSON messages\r\n        codaChannel.onmessage = (event) => {\r\n            markActive();\r\n            const data = event.data;\r\n            try {\r\n                // Known sources: 'Sovereign-Console' | 'Sovereign-Chat' | 'Sovereign-DB' | 'Sovereign-Embed'\r\n                if (data.source === 'Sovereign-Console') {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [SOVEREIGN] ${data.message}`, data.type || 'info');\r\n                } else if (data.source === 'Sovereign-Chat') {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');\r\n                } else if (data.source === 'Sovereign-DB') {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [DB] ${data.message}`, data.type || 'info');\r\n                } else if (data.source === 'Sovereign-Embed' || data.source === 'WebGPU-Embed') {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] [EMBED] ${data.message}`, 'debug');\r\n                } else if (data.source === 'WebGPU-Chat') {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${data.role || 'assistant'}: ${data.message}`, 'info');\r\n                } else {\r\n                    appendLog(combinedContainer, `[${new Date().toLocaleTimeString()}] ${JSON.stringify(data)}`, 'debug');\r\n                }\r\n            } catch (e) {\r\n                appendLog(combinedContainer, `[coda_logs parsing error] ${e.message}`, 'error');\r\n            }\r\n        };\r\n\r\n        // Poll backend server logs every 2s with graceful 404 handling and retry\r\n        let pollIntervalId = null;\r\n        let serverLogsErrorShown = false;\r\n\r\n        async function pollServerLogs() {\r\n            try {\r\n                // Use the new logs endpoint\r\n                const resp = await fetch('http://localhost:8000/logs/recent');\r\n\r\n                if (resp.status === 404) {\r\n                    if (!serverLogsErrorShown) {\r\n                        appendLog(combinedContainer, `[ECE-Core] Logs endpoint returned 404. Is the backend running?`, 'warning');\r\n                        statusIndicator.style.color = '#ffcc00';\r\n                        statusIndicator.innerText = '‚óè Backend logs unavailable';\r\n                        serverLogsErrorShown = true;\r\n                    }\r\n                    // Stop polling to avoid tight 404 loops\r\n                    if (pollIntervalId) {\r\n                        clearInterval(pollIntervalId);\r\n                        pollIntervalId = null;\r\n                    }\r\n                    return;\r\n                }\r\n\r\n                if (!resp.ok) {\r\n                    appendLog(combinedContainer, `[ECE-Core] Poll error: HTTP ${resp.status}`, 'error');\r\n                    return;\r\n                }\r\n\r\n                const payload = await resp.json();\r\n                if (payload && Array.isArray(payload.logs)) {\r\n                    for (const logEntry of payload.logs) {\r\n                        const line = `[${new Date(logEntry.timestamp).toLocaleTimeString()}] [${logEntry.source}] ${logEntry.message}`;\r\n                        if (!serverLogSet.has(line)) {\r\n                            serverLogSet.add(line);\r\n                            appendLog(combinedContainer, `[ECE-Core] ${line}`, logEntry.type || 'info');\r\n                        }\r\n                    }\r\n                    // Prevent unbounded growth\r\n                    if (serverLogSet.size > 500) {\r\n                        serverLogSet.clear();\r\n                    }\r\n                }\r\n            } catch (e) {\r\n                if (!serverLogsErrorShown) {\r\n                    appendLog(combinedContainer, `[ECE-Core] Poll error: ${e.message}`, 'error');\r\n                    statusIndicator.style.color = '#ff4444';\r\n                    statusIndicator.innerText = '‚óè Poll error';\r\n                    serverLogsErrorShown = true;\r\n                }\r\n            }\r\n        }\r\n\r\n        function startPolling() {\r\n            if (pollIntervalId) return;\r\n            serverLogsErrorShown = false;\r\n            statusIndicator.style.color = '#0f0';\r\n            statusIndicator.innerText = '‚óè Active';\r\n            pollIntervalId = setInterval(pollServerLogs, 2000);\r\n            pollServerLogs();\r\n        }\r\n\r\n        function stopPolling() {\r\n            if (pollIntervalId) {\r\n                clearInterval(pollIntervalId);\r\n                pollIntervalId = null;\r\n            }\r\n        }\r\n\r\n        // Start polling initially\r\n        startPolling();\r\n\r\n        // Retry button\r\n        document.getElementById('retry-logs-btn').addEventListener('click', () => {\r\n            appendLog(combinedContainer, '[ECE-Core] Manual retry requested', 'info');\r\n            statusIndicator.style.color = '#88ccff';\r\n            statusIndicator.innerText = '‚óè Retrying...';\r\n            startPolling();\r\n        });\r\n\r\n        function appendLog(container, text, type) {\r\n            const div = document.createElement('div');\r\n            div.className = 'log-line';\r\n            div.innerHTML = colorize(text, type);\r\n            container.appendChild(div);\r\n            container.scrollTop = container.scrollHeight;\r\n        }\r\n\r\n        function colorize(line, type) {\r\n            if (type === 'error' || line.includes('ERROR') || line.includes('‚ùå')) return `<span class=\"error\">${line}</span>`;\r\n            if (type === 'warn' || line.includes('WARNING')) return `<span class=\"warning\">${line}</span>`;\r\n            if (type === 'success' || line.includes('SUCCESS') || line.includes('‚úÖ')) return `<span class=\"success\">${line}</span>`;\r\n            if (type === 'info' || line.includes('INFO')) return `<span class=\"info\">${line}</span>`;\r\n            return `<span class=\"debug\">${line}</span>`;\r\n        }\r\n\r\n        clearBtn.onclick = () => {\r\n            combinedContainer.innerHTML = '';\r\n        };\r\n    </script>\r\n</body>\r\n\r\n</html>",
    "source": "tools\\log-viewer.html"
  },
  {
    "id": "tools\\low_resource_config.py",
    "timestamp": 1767254282,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nConfiguration for Anchor Core optimized for low-resource devices (phones, small laptops)\r\n\"\"\"\r\n\r\nimport os\r\nimport json\r\n\r\nclass LowResourceConfig:\r\n    \"\"\"Configuration optimized for low-resource devices\"\"\"\r\n    \r\n    def __init__(self):\r\n        # Conservative GPU settings for low VRAM devices\r\n        self.gpu_config = {\r\n            \"max_buffer_size\": 64 * 1024 * 1024,  # 64MB instead of 256MB\r\n            \"max_concurrent_operations\": 1,\r\n            \"use_compressed_weights\": True,\r\n            \"power_preference\": \"low-power\",\r\n            \"force_fallback_adapter\": False,\r\n            \"max_active_webgl_contexts\": 1,\r\n            \"max_webgl_contexts_per_group\": 1\r\n        }\r\n        \r\n        # Conservative model settings\r\n        self.model_config = {\r\n            \"default_model\": \"Phi-3.5-mini-instruct-q4f16_1-MLC\",  # Smallest recommended\r\n            \"max_model_size_gb\": 0.5,  # Maximum model size for low-resource devices\r\n            \"use_cpu_fallback\": True,  # Allow CPU fallback if GPU fails\r\n            \"context_window\": 2048,  # Reduced context window\r\n            \"batch_size\": 1,  # Minimal batch size\r\n            \"quantization\": \"q4f16_1\"  # Most compressed format\r\n        }\r\n        \r\n        # Memory management settings\r\n        self.memory_config = {\r\n            \"max_cache_size_mb\": 128,  # Reduced cache size\r\n            \"enable_disk_cache\": False,  # Disable disk cache to save space\r\n            \"gc_frequency\": 10,  # More frequent garbage collection\r\n            \"preload_models\": False  # Don't preload models\r\n        }\r\n        \r\n        # Network and performance settings\r\n        self.performance_config = {\r\n            \"timeout_seconds\": 120,  # Longer timeouts for slower devices\r\n            \"max_retries\": 3,\r\n            \"concurrent_requests\": 1,  # Single-threaded for low-resource\r\n            \"enable_compression\": True\r\n        }\r\n\r\n    def get_config(self):\r\n        \"\"\"Return the complete low-resource configuration\"\"\"\r\n        return {\r\n            \"gpu\": self.gpu_config,\r\n            \"model\": self.model_config,\r\n            \"memory\": self.memory_config,\r\n            \"performance\": self.performance_config\r\n        }\r\n\r\n# Singleton instance\r\nconfig = LowResourceConfig()\r\n\r\ndef get_low_resource_config():\r\n    \"\"\"Get the low-resource configuration\"\"\"\r\n    return config.get_config()\r\n\r\nif __name__ == \"__main__\":\r\n    # Print configuration for debugging\r\n    print(json.dumps(get_low_resource_config(), indent=2))",
    "source": "tools\\low_resource_config.py"
  },
  {
    "id": "tools\\orchestrator.py",
    "timestamp": 1767190258,
    "role": "file",
    "content": "import os\nimport requests\nimport json\nimport logging\n\n# Config - Defaults match start-bridge.bat\nBRIDGE_PORT = os.getenv(\"BRIDGE_PORT\", \"8000\")\nBRIDGE_HOST = os.getenv(\"BRIDGE_HOST\", \"localhost\")\nBRIDGE_URL = f\"http://{BRIDGE_HOST}:{BRIDGE_PORT}\"\nMLC_INFERENCE_ENDPOINT = f\"{BRIDGE_URL}/v1/chat/completions\"\nBRIDGE_TOKEN = os.getenv(\"BRIDGE_TOKEN\", \"sovereign-secret\")\n\nlogger = logging.getLogger(\"Orchestrator\")\nlogging.basicConfig(level=logging.INFO)\n\nclass MLCConnectionError(Exception):\n    \"\"\"Raised when connection to the MLC Bridge fails.\"\"\"\n    pass\n\nclass Orchestrator:\n    def __init__(self, endpoint=MLC_INFERENCE_ENDPOINT, token=BRIDGE_TOKEN):\n        self.endpoint = endpoint\n        self.token = token\n        self.active_model = None\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def load_mlc_model(self, model_name):\n        \"\"\"\n        Configures the orchestrator for a specific model.\n        Verifies connectivity to the Bridge and logs available models.\n        \"\"\"\n        logger.info(f\"Preparing Orchestrator for model: {model_name}\")\n        \n        try:\n            # Health check / Model list\n            r = requests.get(f\"{BRIDGE_URL}/v1/models\", headers=self.headers, timeout=2.0)\n            r.raise_for_status()\n            models_data = r.json().get(\"data\", [])\n            available_ids = [m['id'] for m in models_data]\n            logger.info(f\"Bridge connected. Active Workers: {available_ids}\")\n            \n            if not available_ids:\n                logger.warning(\"No WebGPU workers connected to Bridge. Open model-server-chat.html\")\n            \n        except requests.exceptions.RequestException as e:\n            raise MLCConnectionError(f\"Failed to connect to Wave Bridge at {BRIDGE_URL}. Ensure start-bridge.bat is running. Error: {e}\")\n\n        self.active_model = model_name\n        return True\n\n    def invoke_mlc_inference(self, prompt, system_prompt=\"You are a helpful AI orchestrator.\"):\n        \"\"\"\n        Sends a prompt to the MLC engine via the Bridge.\n        \"\"\"\n        if not self.active_model:\n            raise ValueError(\"No model loaded. Call load_mlc_model() first.\")\n\n        payload = {\n            \"model\": self.active_model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"stream\": False\n        }\n\n        try:\n            logger.info(f\"Sending inference request to {self.endpoint}...\")\n            response = requests.post(\n                self.endpoint, \n                json=payload, \n                headers=self.headers,\n                timeout=60 # Inference can be slow\n            )\n            response.raise_for_status()\n            \n            result = self._parse_response(response.json())\n            return result\n\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Inference request failed: {e}\")\n            raise MLCConnectionError(f\"Inference failed: {e}\")\n\n    def _parse_response(self, mlc_response):\n        \"\"\"\n        Extracts the content from the OpenAI-compatible JSON response.\n        \"\"\"\n        try:\n            # Check for bridge-reported errors\n            if \"error\" in mlc_response:\n                raise Exception(mlc_response[\"error\"])\n\n            choices = mlc_response.get(\"choices\", [])\n            if not choices:\n                logger.warning(\"Empty choices in response\")\n                return \"\"\n            \n            content = choices[0].get(\"message\", {}).get(\"content\", \"\")\n            return content\n        except Exception as e:\n            logger.error(f\"Error parsing MLC response: {e}\")\n            return f\"[Error parsing output: {e}]\"\n\n# CLI usage\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Orchestrator CLI\")\n    parser.add_argument(\"--prompt\", type=str, help=\"Prompt to send\")\n    parser.add_argument(\"--model\", type=str, default=\"webgpu-chat\", help=\"Model ID\")\n    args = parser.parse_args()\n\n    if args.prompt:\n        orc = Orchestrator()\n        try:\n            orc.load_mlc_model(args.model)\n            print(f\"\\nResponse:\\n{orc.invoke_mlc_inference(args.prompt)}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    else:\n        print(\"Usage: python orchestrator.py --prompt 'Hello'\")\n",
    "source": "tools\\orchestrator.py"
  },
  {
    "id": "tools\\prepare_cozo_import.py",
    "timestamp": 1765904026,
    "role": "file",
    "content": "#!/usr/bin/env python3\r\n\"\"\"\r\nPrepare a CozoDB import payload from an existing combined_memory.json file.\r\nUsage:\r\n  python tools/prepare_cozo_import.py [input_path] [output_path]\r\nIf input_path is omitted the script will search likely locations.\r\n\"\"\"\r\nimport json\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\n# Defaults\r\nPOSSIBLE_INPUTS = [\r\n    Path(\"combined_memory.json\")\r\n]\r\nDEFAULT_OUTPUT = Path(\"cozo_import_memory.json\")\r\n\r\ndef find_input(path_arg=None):\r\n    if path_arg:\r\n        p = Path(path_arg)\r\n        if p.exists():\r\n            return p\r\n        else:\r\n            print(f\"‚ùå Specified input not found: {p}\")\r\n            return None\r\n    for p in POSSIBLE_INPUTS:\r\n        if p.exists():\r\n            return p\r\n    # fallback: search workspace for first combined_memory.json\r\n    for p in Path('.').rglob('combined_memory.json'):\r\n        return p\r\n    return None\r\n\r\n\r\ndef normalize_record(rec):\r\n    # Ensure the fields Cozo expects. Return None to skip invalid records.\r\n    uid = rec.get(\"id\") or rec.get(\"uid\") or rec.get(\"uuid\") or None\r\n    if not uid:\r\n        # try deterministic id from source+timestamp\r\n        src = rec.get(\"source\") or rec.get(\"file\") or \"\"\r\n        ts = rec.get(\"timestamp\") or rec.get(\"created_at\") or 0\r\n        uid = f\"auto:{abs(hash(src + str(ts)))}\"\r\n    try:\r\n        uid = str(uid)\r\n    except Exception:\r\n        uid = str(uid)\r\n\r\n    try:\r\n        ts = int(rec.get(\"timestamp\", rec.get(\"created_at\", 0)) or 0)\r\n    except Exception:\r\n        try:\r\n            ts = int(float(rec.get(\"timestamp\", 0)))\r\n        except Exception:\r\n            ts = 0\r\n\r\n    role = str(rec.get(\"role\", \"system\"))\r\n    content = rec.get(\"content\", \"\")\r\n    if not isinstance(content, str):\r\n        try:\r\n            content = json.dumps(content, ensure_ascii=False)\r\n        except Exception:\r\n            content = str(content)\r\n    # cap content size to be safe\r\n    MAX_CONTENT = 200_000\r\n    if len(content) > MAX_CONTENT:\r\n        content = content[:MAX_CONTENT]\r\n\r\n    source = rec.get(\"source\", rec.get(\"file\", \"historical_import\"))\r\n    try:\r\n        source = str(source)\r\n    except Exception:\r\n        source = \"historical_import\"\r\n\r\n    embedding = rec.get(\"embedding\", None)\r\n    # Keep embedding as-is if present and looks like a list of numbers\r\n    if isinstance(embedding, list):\r\n        # Optionally validate length later; leave as-is\r\n        pass\r\n    else:\r\n        embedding = None\r\n\r\n    return [uid, ts, role, content, source, embedding]\r\n\r\n\r\ndef main():\r\n    input_arg = sys.argv[1] if len(sys.argv) > 1 else None\r\n    output_arg = sys.argv[2] if len(sys.argv) > 2 else None\r\n\r\n    inp = find_input(input_arg)\r\n    if not inp:\r\n        print(\"‚ùå Could not find a combined_memory.json file. Provide the path as the first argument.\")\r\n        return 1\r\n\r\n    out = Path(output_arg) if output_arg else DEFAULT_OUTPUT\r\n\r\n    print(f\"Reading {inp}...\")\r\n    try:\r\n        raw = json.loads(inp.read_text(encoding='utf-8'))\r\n    except json.JSONDecodeError as e:\r\n        print(f\"‚ùå JSON decode error: {e}\")\r\n        return 1\r\n\r\n    if not isinstance(raw, list):\r\n        print(\"‚ùó Warning: input root is not a list. Attempting to find list under 'records' or 'data'.\")\r\n        if isinstance(raw, dict):\r\n            if 'records' in raw and isinstance(raw['records'], list):\r\n                raw = raw['records']\r\n            elif 'data' in raw and isinstance(raw['data'], list):\r\n                raw = raw['data']\r\n            else:\r\n                print(\"‚ùå Could not find the expected list of records in input JSON.\")\r\n                return 1\r\n\r\n    print(f\"Found {len(raw)} records. Normalizing and formatting...\")\r\n\r\n    rows = []\r\n    for rec in raw:\r\n        nr = normalize_record(rec)\r\n        if nr is None:\r\n            continue\r\n        rows.append(nr)\r\n\r\n    payload = {\r\n        \"relations\": [\r\n            {\r\n                \"name\": \"memory\",\r\n                \"headers\": [\"id\", \"timestamp\", \"role\", \"content\", \"source\", \"embedding\"],\r\n                \"rows\": rows,\r\n            }\r\n        ]\r\n    }\r\n\r\n    print(f\"Writing {len(rows)} rows to {out}...\")\r\n    out.write_text(json.dumps(payload, ensure_ascii=False, separators=(',', ':')), encoding='utf-8')\r\n    print(\"‚úÖ Done. You can now drag 'cozo_import_memory.json' into the Builder or use the console import helper.\")\r\n    return 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    raise SystemExit(main())\r\n",
    "source": "tools\\prepare_cozo_import.py"
  },
  {
    "id": "tools\\README.md",
    "timestamp": 1767421904,
    "role": "file",
    "content": "Core tools for the Anchor system: Bridge, UI, and CLI components.\n\n## Available Modes\n\n### Standard Mode (Default)\n- Automatically launches Ghost Engine (headless browser) for memory operations\n- Runs all services in background with auto-resurrection\n\n### Low Resource Mode\n- Set `LOW_RESOURCE_MODE=true` before running to reduce memory/GPU usage\n- Optimized for constrained devices\n\n### CPU Only Mode\n- Set `CPU_ONLY_MODE=true` before running to disable GPU acceleration\n- Uses CPU for all operations\n\n### No Resurrection Mode\n- Set `NO_RESURRECTION_MODE=true` before running to disable auto-launching of Ghost Engine\n- Allows manual control of Ghost Engine connection via browser\n- Useful when you want to use an existing browser window or conserve resources",
    "source": "tools\\README.md"
  },
  {
    "id": "tools\\sidecar_full.html",
    "timestamp": 1767254282,
    "role": "file",
    "content": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Anchor Sidecar</title>\r\n    <style>\r\n        :root { --bg: #0f172a; --panel: #1e293b; --text: #e2e8f0; --accent: #38bdf8; }\r\n        body { background: var(--bg); color: var(--text); font-family: sans-serif; margin: 0; display: flex; height: 100vh; }\r\n        \r\n        .sidebar { width: 60px; background: #000; display: flex; flex-direction: column; align-items: center; padding-top: 20px; }\r\n        .tab-btn { width: 40px; height: 40px; margin-bottom: 10px; border-radius: 8px; border: none; background: #334155; color: #fff; cursor: pointer; font-size: 20px; }\r\n        .tab-btn.active { background: var(--accent); color: #000; }\r\n\r\n        .main { flex: 1; padding: 20px; overflow-y: auto; display: none; }\r\n        .main.active { display: block; }\r\n\r\n        .card { background: var(--panel); padding: 20px; border-radius: 12px; margin-bottom: 20px; border: 1px solid #334155; }\r\n        h2 { margin-top: 0; color: var(--accent); }\r\n        \r\n        textarea { width: 100%; background: #000; color: #0f0; border: 1px solid #334155; padding: 10px; font-family: monospace; border-radius: 6px; box-sizing: border-box; }\r\n        button.action { background: var(--accent); color: #000; border: none; padding: 10px 20px; border-radius: 6px; font-weight: bold; cursor: pointer; margin-top: 10px; }\r\n        button.action:hover { opacity: 0.9; }\r\n\r\n        #drop-zone { border: 2px dashed #475569; padding: 40px; text-align: center; border-radius: 12px; cursor: pointer; transition: 0.2s; }\r\n        #drop-zone.dragover { border-color: var(--accent); background: #1e293b; }\r\n        \r\n        .log-entry { margin: 5px 0; font-family: monospace; font-size: 12px; opacity: 0.8; }\r\n        .success { color: #4ade80; }\r\n        .error { color: #f87171; }\r\n    </style>\r\n</head>\r\n<body>\r\n\r\n<div class=\"sidebar\">\r\n    <button class=\"tab-btn active\" onclick=\"switchTab('search')\">üîç</button>\r\n    <button class=\"tab-btn\" onclick=\"switchTab('vision')\">üëÅÔ∏è</button>\r\n</div>\r\n\r\n<div id=\"tab-search\" class=\"main active\">\r\n    <div class=\"card\">\r\n        <h2>Anchor Retrieval</h2>\r\n        <p>Query your graph to generate a Reality Map for external chats.</p>\r\n        <textarea id=\"queryInput\" rows=\"3\" placeholder=\"What information do you need context for?\"></textarea>\r\n        <button class=\"action\" onclick=\"runSearch()\">Fetch Context</button>\r\n    </div>\r\n\r\n    <div class=\"card\">\r\n        <h2>Result</h2>\r\n        <textarea id=\"resultOutput\" rows=\"15\" readonly placeholder=\"Context will appear here...\"></textarea>\r\n        <button class=\"action\" onclick=\"copyResult()\">Copy to Clipboard</button>\r\n    </div>\r\n</div>\r\n\r\n<div id=\"tab-vision\" class=\"main\">\r\n    <div class=\"card\">\r\n        <h2>Vision Ingestion</h2>\r\n        <p>Drop an image here to process it with the background VLM and add it to the Graph.</p>\r\n        <div id=\"drop-zone\">\r\n            <span style=\"font-size: 40px\">üì∏</span><br>\r\n            Drag & Drop Image or Click to Upload\r\n            <input type=\"file\" id=\"fileInput\" hidden accept=\"image/*\">\r\n        </div>\r\n    </div>\r\n    \r\n    <div class=\"card\">\r\n        <h2>Processing Logs</h2>\r\n        <div id=\"vision-logs\"></div>\r\n    </div>\r\n</div>\r\n\r\n<script>\r\n    function switchTab(tab) {\r\n        document.querySelectorAll('.main').forEach(el => el.classList.remove('active'));\r\n        document.querySelectorAll('.tab-btn').forEach(el => el.classList.remove('active'));\r\n        document.getElementById('tab-' + tab).classList.add('active');\r\n        event.target.classList.add('active');\r\n    }\r\n\r\n    // --- SEARCH LOGIC ---\r\n    async function runSearch() {\r\n        const query = document.getElementById('queryInput').value;\r\n        const out = document.getElementById('resultOutput');\r\n        out.value = \"Searching Graph...\";\r\n        \r\n        try {\r\n            const res = await fetch('/v1/memory/search', {\r\n                method: 'POST',\r\n                headers: {'Content-Type': 'application/json'},\r\n                body: JSON.stringify({ query })\r\n            });\r\n            const data = await res.json();\r\n            if(data.context) out.value = data.context;\r\n            else out.value = \"Error: \" + JSON.stringify(data);\r\n        } catch(e) {\r\n            out.value = \"Connection Error: \" + e;\r\n        }\r\n    }\r\n\r\n    function copyResult() {\r\n        const copyText = document.getElementById(\"resultOutput\");\r\n        copyText.select();\r\n        document.execCommand(\"copy\");\r\n        alert(\"Copied to clipboard!\");\r\n    }\r\n\r\n    // --- VISION LOGIC ---\r\n    const dropZone = document.getElementById('drop-zone');\r\n    const fileInput = document.getElementById('fileInput');\r\n\r\n    dropZone.onclick = () => fileInput.click();\r\n    fileInput.onchange = (e) => handleUpload(e.target.files[0]);\r\n    \r\n    dropZone.ondragover = (e) => { e.preventDefault(); dropZone.classList.add('dragover'); };\r\n    dropZone.ondragleave = () => dropZone.classList.remove('dragover');\r\n    dropZone.ondrop = (e) => {\r\n        e.preventDefault();\r\n        dropZone.classList.remove('dragover');\r\n        if(e.dataTransfer.files[0]) handleUpload(e.dataTransfer.files[0]);\r\n    };\r\n\r\n    async function handleUpload(file) {\r\n        if(!file) return;\r\n        log(`üì§ Uploading ${file.name}...`, 'normal');\r\n        \r\n        const formData = new FormData();\r\n        formData.append('image', file);\r\n\r\n        try {\r\n            const res = await fetch('/v1/vision/ingest', { method: 'POST', body: formData });\r\n            const data = await res.json();\r\n            if(data.status === 'success') {\r\n                log(`‚úÖ Analyzed: \"${data.description.substring(0, 50)}...\"`, 'success');\r\n                log(`üíæ Stored in Memory Graph`, 'success');\r\n            } else {\r\n                log(`‚ùå Error: ${data.message}`, 'error');\r\n            }\r\n        } catch(e) {\r\n            log(`‚ùå Network Error: ${e}`, 'error');\r\n        }\r\n    }\r\n\r\n    function log(msg, type) {\r\n        const div = document.createElement('div');\r\n        div.className = 'log-entry ' + type;\r\n        div.innerText = `[${new Date().toLocaleTimeString()}] ${msg}`;\r\n        document.getElementById('vision-logs').prepend(div);\r\n    }\r\n</script>\r\n</body>\r\n</html>",
    "source": "tools\\sidecar_full.html"
  },
  {
    "id": "tools\\start-bridge.bat",
    "timestamp": 1767190269,
    "role": "file",
    "content": "@echo off\necho Starting WebGPU Bridge...\necho This bridge allows external tools (like Wave Terminal) to talk to the browser.\necho.\nset BRIDGE_PORT=8000\nset BRIDGE_TOKEN=sovereign-secret\npython webgpu_bridge.py\npause",
    "source": "tools\\start-bridge.bat"
  },
  {
    "id": "tools\\start_anchor_detached.py",
    "timestamp": 1767446914,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nDetached Start Script for Anchor System\n\nThis script starts the anchor system in a detached mode, following the documentation policy\nthat all scripts should run detached and log to the logs directory.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nimport datetime\n\n\ndef run_detached():\n    \"\"\"Run the anchor system in detached mode with proper logging.\"\"\"\n    # Create logs directory if it doesn't exist\n    logs_dir = Path(\"../logs\")\n    logs_dir.mkdir(exist_ok=True)\n    \n    # Create a log file for this script\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = logs_dir / f\"start_anchor_{timestamp}.log\"\n    \n    # Command to run the batch file\n    cmd = [\"cmd\", \"/c\", \"start-anchor.bat\"]\n    \n    try:\n        # Run the command and redirect output to log file\n        with open(log_file, 'w') as f:\n            subprocess.Popen(\n                cmd,\n                stdout=f,\n                stderr=f,\n                cwd=os.getcwd()\n            )\n        print(f\"ANCHOR system started in detached mode. Log file: {log_file}\")\n    except Exception as e:\n        print(f\"Error starting anchor system: {e}\")\n        with open(log_file, 'a') as f:\n            f.write(f\"Error: {e}\\n\")\n\n\nif __name__ == \"__main__\":\n    run_detached()",
    "source": "tools\\start_anchor_detached.py"
  },
  {
    "id": "tools\\verify_models.py",
    "timestamp": 1767420000,
    "role": "file",
    "content": "#!/usr/bin/env python3\n\"\"\"\nCombined Model Verification Script for Anchor Core\n\nThis script combines functionality from the separate verification scripts:\n- Hugging Face availability checking\n- Local model file verification\n- Complete verification pipeline\n\nUsage:\n  python verify_models.py --online                    # Check only Hugging Face availability\n  python verify_models.py --local                     # Check only local availability\n  python verify_models.py                             # Complete verification (default)\n  python verify_models.py --models [model1 model2]    # Check specific models\n\"\"\"\n\nimport requests\nimport sys\nimport time\nimport json\nimport os\nfrom urllib.parse import urljoin\nfrom pathlib import Path\n\n\nclass ModelVerifier:\n    def __init__(self, models_dir=\"models\", base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\n        self.models_dir = Path(models_dir)\n        self.base_url = base_url\n        self.token = token\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n        self.session = requests.Session()\n        self.session.headers.update({\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        })\n        self.results = {}\n\n    def test_hf_model_availability(self, model_id, expected_files=None):\n        \"\"\"Test if a model is available on Hugging Face by checking required files\"\"\"\n        print(f\"  Testing Hugging Face availability: {model_id}\")\n\n        if expected_files is None:\n            # Default MLC-LLM required files\n            expected_files = [\n                \"ndarray-cache.json\",\n                \"tokenizer.json\", \n                \"mlc-chat-config.json\",\n                \"tokenizer_config.json\"\n            ]\n\n        model_result = {\n            \"huggingface\": {\n                \"available\": True,\n                \"files\": {},\n                \"huggingface_url\": f\"https://huggingface.co/{model_id}\"\n            }\n        }\n\n        missing_files = []\n\n        for file_name in expected_files:\n            hf_url = f\"https://huggingface.co/{model_id}/resolve/main/{file_name}\"\n\n            try:\n                response = self.session.head(hf_url, timeout=15)\n\n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": response.status_code,\n                    \"exists\": response.status_code in [200, 302, 307],  # 302/307 are redirects, meaning file exists\n                    \"checked_at\": time.time()\n                }\n\n                model_result[\"huggingface\"][\"files\"][file_name] = file_status\n\n                if response.status_code in [200, 302, 307]:\n                    print(f\"    OK {file_name}\")\n                elif response.status_code == 404:\n                    print(f\"    MISSING {file_name} - NOT FOUND on Hugging Face\")\n                    missing_files.append(file_name)\n                    model_result[\"huggingface\"][\"available\"] = False\n                else:\n                    print(f\"    WARNING {file_name} - Status {response.status_code}\")\n                    model_result[\"huggingface\"][\"available\"] = False\n\n            except requests.exceptions.RequestException as e:\n                file_status = {\n                    \"url\": hf_url,\n                    \"status_code\": \"ERROR\",\n                    \"exists\": False,\n                    \"error\": str(e),\n                    \"checked_at\": time.time()\n                }\n                model_result[\"huggingface\"][\"files\"][file_name] = file_status\n                print(f\"    ERROR {file_name} - {e}\")\n                model_result[\"huggingface\"][\"available\"] = False\n\n        if missing_files:\n            print(f\"    INFO Missing files on Hugging Face: {len(missing_files)} required files not found\")\n        else:\n            print(f\"    SUCCESS Available on Hugging Face!\")\n\n        return model_result[\"huggingface\"]\n\n    def test_local_model_availability(self, model_name):\n        \"\"\"Test if a model is available locally by checking required files\"\"\"\n        print(f\"  Testing local availability: {model_name}\")\n\n        # Define the required model files for MLC-LLM\n        required_files = [\n            \"ndarray-cache.json\",\n            \"tokenizer.json\",\n            \"mlc-chat-config.json\", \n            \"tokenizer_config.json\"\n        ]\n\n        model_path = self.models_dir / model_name\n        if not model_path.exists():\n            print(f\"    MISSING Model directory does not exist: {model_path}\")\n            return {\n                \"available\": False,\n                \"directory_exists\": False,\n                \"files\": {},\n                \"reason\": \"Model directory does not exist\"\n            }\n\n        model_result = {\n            \"available\": True,\n            \"directory_exists\": True,\n            \"files\": {},\n            \"reason\": \"All required files present\"\n        }\n\n        missing_files = []\n\n        # Check standard files\n        for file_name in required_files:\n            file_path = model_path / file_name\n            file_exists = file_path.exists()\n\n            file_status = {\n                \"path\": str(file_path),\n                \"exists\": file_exists,\n                \"checked_at\": time.time()\n            }\n\n            model_result[\"files\"][file_name] = file_status\n\n            if file_exists:\n                print(f\"    OK {file_name}\")\n            else:\n                print(f\"    MISSING {file_name}\")\n                missing_files.append(file_name)\n                model_result[\"available\"] = False\n                model_result[\"reason\"] = f\"Missing required file: {file_name}\"\n\n        if missing_files:\n            print(f\"    INFO Missing files locally: {len(missing_files)} required files not found\")\n        else:\n            print(f\"    SUCCESS Available locally!\")\n\n        return model_result\n\n    def run_hf_verification_only(self, model_list):\n        \"\"\"Run only Hugging Face availability tests\"\"\"\n        print(\"Running Hugging Face Model Availability Tests\")\n        print(\"=\" * 60)\n\n        available_models = []\n        unavailable_models = []\n\n        for model_id in model_list:\n            print(f\"\\nTesting: {model_id}\")\n            result = self.test_hf_model_availability(model_id)\n\n            if result[\"available\"]:\n                available_models.append(model_id)\n                print(f\"  RESULT: AVAILABLE on Hugging Face\")\n            else:\n                unavailable_models.append(model_id)\n                print(f\"  RESULT: NOT AVAILABLE on Hugging Face\")\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"HUGGING FACE VERIFICATION SUMMARY:\")\n        print(f\"  Available Models: {len(available_models)}\")\n        for model in available_models:\n            print(f\"    OK {model}\")\n\n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\n        for model in unavailable_models:\n            print(f\"    MISSING {model}\")\n\n        return available_models, unavailable_models\n\n    def run_local_verification_only(self, model_list):\n        \"\"\"Run only local availability tests\"\"\"\n        print(\"Running Local Model Availability Tests\")\n        print(\"=\" * 60)\n\n        available_models = []\n        unavailable_models = []\n\n        for model_name in model_list:\n            print(f\"\\nTesting: {model_name}\")\n            result = self.test_local_model_availability(model_name)\n\n            if result[\"available\"]:\n                available_models.append(model_name)\n                print(f\"  RESULT: AVAILABLE locally\")\n            else:\n                unavailable_models.append(model_name)\n                print(f\"  RESULT: NOT AVAILABLE locally - {result.get('reason', 'Unknown reason')}\")\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"LOCAL VERIFICATION SUMMARY:\")\n        print(f\"  Available Models: {len(available_models)}\")\n        for model in available_models:\n            print(f\"    OK {model}\")\n\n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\n        for model in unavailable_models:\n            print(f\"    MISSING {model}\")\n\n        return available_models, unavailable_models\n\n    def run_complete_verification(self, model_list):\n        \"\"\"Run complete verification tests (Hugging Face + Local)\"\"\"\n        print(\"Running Complete Model Verification Tests\")\n        print(\"Phase 1: Hugging Face availability check\")\n        print(\"Phase 2: Local file availability check\")\n        print(\"=\" * 80)\n\n        results = []\n        available_locally = []\n        download_required = []\n        completely_unavailable = []\n\n        for model_id in model_list:\n            print(f\"\\nVerifying model: {model_id}\")\n            print(\"-\" * 50)\n\n            # Extract model name from model_id (remove mlc-ai/ prefix if present)\n            model_name = model_id.split('/')[-1] if '/' in model_id else model_id\n\n            verification_result = {\n                \"model_id\": model_id,\n                \"model_name\": model_name,\n                \"huggingface\": None,\n                \"local\": None,\n                \"overall_status\": \"UNKNOWN\",\n                \"recommendation\": \"UNKNOWN\"\n            }\n\n            # Step 1: Check Hugging Face availability\n            hf_result = self.test_hf_model_availability(model_id)\n            verification_result[\"huggingface\"] = hf_result\n\n            if not hf_result[\"available\"]:\n                verification_result[\"overall_status\"] = \"UNAVAILABLE\"\n                verification_result[\"recommendation\"] = \"Model not available on Hugging Face - cannot proceed\"\n                print(f\"  ERROR: Model not available on Hugging Face\")\n                completely_unavailable.append(model_id)\n                results.append(verification_result)\n                continue\n\n            # Step 2: Check local availability\n            local_result = self.test_local_model_availability(model_name)\n            verification_result[\"local\"] = local_result\n\n            # Determine overall status\n            if local_result[\"available\"]:\n                verification_result[\"overall_status\"] = \"AVAILABLE_LOCALLY\"\n                verification_result[\"recommendation\"] = \"Ready to use - available locally\"\n                available_locally.append(model_id)\n            else:\n                verification_result[\"overall_status\"] = \"DOWNLOAD_REQUIRED\"\n                verification_result[\"recommendation\"] = \"Not available locally, needs download via /v1/models/pull\"\n                download_required.append(model_id)\n\n            print(f\"  Overall Status: {verification_result['overall_status']}\")\n            print(f\"  Recommendation: {verification_result['recommendation']}\")\n\n            results.append(verification_result)\n\n        # Final summary\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPLETE VERIFICATION SUMMARY:\")\n\n        if available_locally:\n            print(f\"\\n‚úÖ AVAILABLE LOCALLY - Ready to use immediately:\")\n            for model in available_locally:\n                print(f\"    {model}\")\n\n        if download_required:\n            print(f\"\\nDOWNLOAD REQUIRED - Use /v1/models/pull to download:\")\n            for model in download_required:\n                print(f\"    {model}\")\n\n        if completely_unavailable:\n            print(f\"\\nCOMPLETELY UNAVAILABLE - Not on Hugging Face:\")\n            for model in completely_unavailable:\n                print(f\"    {model}\")\n\n        print(f\"\\nTOTALS:\")\n        print(f\"  Models tested: {len(model_list)}\")\n        print(f\"  Available locally: {len(available_locally)}\")\n        print(f\"  Need download: {len(download_required)}\")\n        print(f\"  Completely unavailable: {len(completely_unavailable)}\")\n\n        return results, {\n            \"available_locally\": available_locally,\n            \"download_required\": download_required,\n            \"completely_unavailable\": completely_unavailable\n        }\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Model Verification for Anchor Core\")\n    parser.add_argument(\"--online\", action=\"store_true\",\n                       help=\"Check only Hugging Face availability\")\n    parser.add_argument(\"--local\", action=\"store_true\", \n                       help=\"Check only local availability\")\n    parser.add_argument(\"--models\", nargs=\"+\",\n                       help=\"Specific models to test (default: predefined list)\")\n    parser.add_argument(\"--models-dir\", default=\"models\",\n                       help=\"Models directory path (default: models)\")\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\n                       help=\"Authentication token (default: sovereign-secret)\")\n    parser.add_argument(\"--output\",\n                       help=\"Output file for verification report (JSON format)\")\n\n    args = parser.parse_args()\n\n    verifier = ModelVerifier(\n        models_dir=args.models_dir,\n        base_url=args.url,\n        token=args.token\n    )\n\n    # Default model list if none provided\n    default_models = [\n        \"mlc-ai/Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\", \n        \"mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\n        \"mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC\",\n        \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\",\n        \"mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC\",\n        \"mlc-ai/gemma-2-9b-it-q4f16_1-MLC\",\n        \"mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC\"\n    ]\n\n    models_to_test = args.models if args.models else default_models\n\n    print(f\"Starting model verification for {len(models_to_test)} models...\")\n    print(f\"Mode: {'Online only' if args.online else 'Local only' if args.local else 'Complete'}\")\n    print(f\"Models directory: {args.models_dir}\")\n    print(f\"Server URL: {args.url}\")\n\n    if args.online:\n        available, unavailable = verifier.run_hf_verification_only(models_to_test)\n    elif args.local:\n        available, unavailable = verifier.run_local_verification_only(models_to_test)\n    else:  # Complete verification (default)\n        results, summary = verifier.run_complete_verification(models_to_test)\n\n        if args.output:\n            report = {\n                \"models_dir\": args.models_dir,\n                \"server_url\": args.url,\n                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"tested_models\": models_to_test,\n                \"results\": results,\n                \"summary\": summary\n            }\n            with open(args.output, 'w') as f:\n                json.dump(report, f, indent=2)\n            print(f\"\\nComplete verification report saved to: {args.output}\")\n\n        # Exit with error code if no models are available at all\n        total_available = len(summary[\"available_locally\"])\n        if total_available == 0 and args.models:  # Only error if specific models were requested\n            print(\"\\nERROR: No requested models are available!\")\n            sys.exit(1)\n        else:\n            print(f\"\\nSUCCESS: {total_available} model(s) are available!\")\n            sys.exit(0)\n        return\n\n    if args.output:\n        report = {\n            \"models_dir\": args.models_dir,\n            \"mode\": \"online\" if args.online else \"local\",\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"tested_models\": models_to_test,\n            \"available_models\": available,\n            \"unavailable_models\": unavailable\n        }\n        with open(args.output, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"\\nVerification report saved to: {args.output}\")\n\n    # Exit with error code if no models are available\n    if not available and args.models:  # Only error if specific models were requested\n        print(\"\\nERROR: No requested models are available!\")\n        sys.exit(1)\n    else:\n        print(f\"\\nSUCCESS: {len(available)} model(s) are available!\")\n        sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()",
    "source": "tools\\verify_models.py"
  },
  {
    "id": "tools\\modules\\agents.js",
    "timestamp": 1766934872,
    "role": "file",
    "content": "import { AnchorLogger } from './anchor.js';\n\n/**\n * THE AUDITOR (Quality Assurance)\n * Enforces the \"Clean Table\" schema defined by NotebookLM.\n * Rejects memories that don't fit the \"Law\" of the Graph.\n */\nexport class MemoryAuditor {\n    constructor() {\n        this.logger = new AnchorLogger('Auditor');\n        \n        // The \"Law\" - Derived from your NotebookLM Table & Specs\n        this.schema = {\n            required: ['id', 'timestamp', 'role', 'content'],\n            roles: ['user', 'assistant', 'system', 'file', 'manual', 'thought'],\n            constraints: {\n                id_format: /^[0-9]+-[a-z0-9]+$/, // e.g. \"1735165200000-x9s8d7f\"\n                content_min_len: 1,\n                content_max_len: 20000 // Limit for WASM stability\n            }\n        };\n    }\n\n    /**\n     * Audit a proposed memory before insertion.\n     * @param {Object} memory - The candidate memory object\n     * @returns {Object} { passed: boolean, reason: string }\n     */\n    audit(memory) {\n        // 1. Schema Check (Missing Fields)\n        for (const field of this.schema.required) {\n            if (memory[field] === undefined || memory[field] === null) {\n                return this._reject(`Missing required field: ${field}`);\n            }\n        }\n\n        // 2. Type Safety\n        if (typeof memory.id !== 'string') return this._reject(\"ID must be a string\");\n        if (typeof memory.timestamp !== 'number') return this._reject(\"Timestamp must be a number\");\n        if (typeof memory.content !== 'string') return this._reject(\"Content must be a string\");\n\n        // 3. Role Enforcement\n        if (!this.schema.roles.includes(memory.role)) {\n            return this._reject(`Invalid Role: '${memory.role}'. Allowed: ${this.schema.roles.join(', ')}`);\n        }\n\n        // 4. Content Hygiene\n        if (memory.content.length < this.schema.constraints.content_min_len) {\n            return this._reject(\"Content is empty\");\n        }\n        \n        // Auto-Truncate warning (Auditor doesn't modify, just warns/rejects)\n        if (memory.content.length > this.schema.constraints.content_max_len) {\n            this.logger.warn(`Content exceeds safety limit (${memory.content.length} chars). Truncation recommended.`);\n            // We allow it but warn, assuming Builder/Console handles truncation\n        }\n\n        // 5. Logic Check (Timestamp Sanity)\n        // If timestamp is from 1970 (0) or future (> 1 day ahead), flag it\n        const now = Date.now();\n        const oneDay = 86400000;\n        if (memory.timestamp < 1000000000000) return this._reject(\"Timestamp appears to be in seconds, not ms (or invalid)\");\n        if (memory.timestamp > now + oneDay) return this._reject(\"Timestamp is in the future\");\n\n        return { passed: true, reason: \"Valid\" };\n    }\n\n    _reject(reason) {\n        this.logger.warn(`Audit Failed: ${reason}`);\n        return { passed: false, reason };\n    }\n}",
    "source": "tools\\modules\\agents.js"
  },
  {
    "id": "tools\\modules\\anchor.js",
    "timestamp": 1767189803,
    "role": "file",
    "content": "/* tools/modules/anchor.js */\n\n// Import CozoDB bindings from the parent directory\nimport initWasm, { CozoDb } from '../cozo_lib_wasm.js';\n\n/**\n * Anchor Coda Kernel (v2.0)\n * Standard Library for Logging, State, Hardware, and Memory.\n */\n\n// --- 1. THE NERVOUS SYSTEM (Unified Logging) ---\nexport class AnchorLogger {\n    constructor(sourceId) {\n        this.source = sourceId;\n        this.logChannel = new BroadcastChannel('anchor-logs');\n        this.codaChannel = new BroadcastChannel('coda_logs');\n    }\n\n    info(msg) { this._emit(msg, 'info'); }\n    warn(msg) { this._emit(msg, 'warn'); }\n    error(msg) { this._emit(msg, 'error'); }\n    success(msg) { this._emit(msg, 'success'); }\n\n    _emit(msg, type) {\n        // 1. Console Fallback\n        const style = type === 'error' ? 'color:red' : (type === 'success' ? 'color:green' : 'color:blue');\n        console.log(`%c[${this.source}] ${msg}`, style);\n\n        // 2. Broadcast to Mission Control\n        const timestamp = new Date().toISOString();\n        const timeShort = new Date().toLocaleTimeString();\n\n        try {\n            // New JSON Channel (for Mission Control)\n            this.codaChannel.postMessage({\n                source: this.source,\n                type,\n                message: msg,\n                timestamp\n            });\n            // Legacy Channel (for Log Viewer compatibility)\n            this.logChannel.postMessage({\n                source: 'system',\n                msg: `[${this.source}] ${msg}`,\n                type,\n                time: timeShort\n            });\n        } catch (e) {\n            console.warn('Logger broadcast failed', e);\n        }\n    }\n}\n\n// --- 2. THE STATE MANAGER (Nano Store) ---\n// Zero-dependency reactive state.\nexport function createStore(initialState) {\n    const listeners = new Set();\n    \n    const proxy = new Proxy(initialState, {\n        set(target, property, value) {\n            target[property] = value;\n            listeners.forEach(fn => fn(property, value));\n            return true;\n        }\n    });\n\n    return {\n        state: proxy,\n        subscribe: (fn) => listeners.add(fn),\n        unsubscribe: (fn) => listeners.delete(fn)\n    };\n}\n\n// --- 3. HARDWARE DETECTOR (The XPS Fix) ---\n// Centralized WebGPU configuration to prevent crashes on 256MB cards.\nexport async function getWebGPUConfig(profile = 'mid') {\n    if (!navigator.gpu) {\n        throw new Error(\"WebGPU is not supported by this browser. Ensure you are using a modern browser (Chrome 113+, Edge 113+) and it is not disabled in flags.\");\n    }\n    \n    // 1. Request Adapter (Progressive Fallback)\n    let adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' });\n    \n    if (!adapter) {\n        console.warn(\"[Kernel] High-performance adapter failed. Trying default...\");\n        adapter = await navigator.gpu.requestAdapter();\n    }\n    \n    if (!adapter) {\n        console.warn(\"[Kernel] Default adapter failed. Trying low-power fallback...\");\n        adapter = await navigator.gpu.requestAdapter({ powerPreference: 'low-power' });\n    }\n    \n    if (!adapter) {\n        throw new Error(\"No WebGPU Adapter found. This often happens after a GPU crash. Please RESTART YOUR BROWSER or check for driver updates.\");\n    }\n\n    // 2. Detect Hardware Limits\n    const hardwareLimit = adapter.limits.maxStorageBufferBindingSize;\n    let requested = 1024 * 1024 * 1024; // Default 1GB\n\n    // 3. Apply Profile Strategy\n    if (profile === 'lite') requested = 256 * 1024 * 1024; // 256MB\n    else if (profile === 'mid') requested = 1024 * 1024 * 1024; // 1GB\n    else if (profile === 'high') requested = 2048 * 1024 * 1024; // 2GB\n\n    // 4. The Safety Clamp\n    const finalLimit = Math.min(requested, hardwareLimit);\n    const isConstrained = finalLimit < requested;\n\n    return {\n        adapter,\n        deviceConfig: {\n            requiredLimits: { maxStorageBufferBindingSize: finalLimit },\n            requiredFeatures: adapter.features.has(\"shader-f16\") ? [\"shader-f16\"] : []\n        },\n        maxBufferSize: finalLimit,\n        isConstrained\n    };\n}\n\n// --- 4. MEMORY CORE (CozoDB Helper) ---\nexport async function initCozo(wasmUrl = '../cozo_lib_wasm_bg.wasm') {\n    await initWasm(wasmUrl);\n    return CozoDb;\n}\n\n// --- 5. THE BLOCKER (GPU Mutex) ---\nclass GPUController {\n    static get BRIDGE_URL() { return 'http://localhost:8000'; }\n\n    // System Consciousness States\n    static States = {\n        IDLE: 'IDLE',\n        DREAMING: 'DREAMING',\n        COGNITION: 'COGNITION',\n        LISTENING: 'LISTENING'\n    };\n\n    static currentState = 'IDLE';\n    static stateChannel = new BroadcastChannel('anchor-state');\n\n    // Separate locks for different operations\n    static modelLoadPromise = null;  // Promise to serialize model loading\n    static activeModelLoaders = new Set();  // Track active model loaders\n\n    static broadcastState(newState) {\n        if (this.currentState === newState) return;\n        this.currentState = newState;\n        this.stateChannel.postMessage({ type: 'STATE_CHANGE', state: newState });\n        console.log(`[GPUController] State changed to: ${newState}`);\n    }\n\n    // Enhanced GPU lock with retry logic and better error handling\n    static async acquireLock(agentId, timeout = 120000) { // Increased default timeout to 120 seconds\n        const startTime = Date.now();\n\n        // 1. Determine Intended State\n        let intendedState = this.States.IDLE;\n        if (agentId.includes('Dreamer')) intendedState = this.States.DREAMING;\n        else if (agentId.includes('Console') || agentId.includes('Chat')) intendedState = this.States.COGNITION;\n        else if (agentId.includes('Mic')) intendedState = this.States.LISTENING;\n\n        // 2. SEMAPHORE CHECK (Consciousness Priority)\n        const highPriority = [this.States.COGNITION, this.States.LISTENING];\n        \n        // If Dreamer tries to wake while Brain/Ears are active -> REJECT IMMEDIATELY\n        if (intendedState === this.States.DREAMING && highPriority.includes(this.currentState)) {\n             return { success: false, error: `Consciousness Semaphore: System is busy (${this.currentState}). Dreamer suppressed.` };\n        }\n\n        // If High Priority Agent (Brain/Ears) starts -> Assert Dominance\n        if (highPriority.includes(intendedState)) {\n            this.broadcastState(intendedState);\n        } else if (intendedState === this.States.DREAMING) {\n            this.broadcastState(this.States.DREAMING);\n        }\n\n        while (Date.now() - startTime < timeout) {\n            try {\n                // This request will HANG until the lock is available (Queue)\n                const controller = new AbortController();\n                const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n                const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/lock`, {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json',\n                        'Authorization': 'Bearer sovereign-secret'\n                    },\n                    body: JSON.stringify({ id: agentId }),\n                    signal: controller.signal\n                });\n\n                clearTimeout(timeoutId);\n\n                if (res.ok) {\n                    const data = await res.json();\n                    return { success: true, token: data.token };\n                }\n\n                // Handle specific error codes\n                if (res.status === 503) {\n                    const errorData = await res.json();\n                    return { success: false, error: errorData.msg || `Queue Timeout (${res.status})` };\n                }\n\n                return { success: false, error: `GPU Lock Failed (${res.status})` };\n            } catch (e) {\n                if (e.name === 'AbortError') {\n                    return { success: false, error: 'Lock acquisition timeout' };\n                }\n\n                console.warn(\"Bridge unreachable (No Lock)\", e);\n\n                // If bridge is down, try direct WebGPU access as fallback\n                if (e.message.includes('fetch') || e.message.includes('network')) {\n                    console.warn(\"Bridge offline, attempting direct WebGPU access...\");\n                    return { success: true, token: \"direct-webgpu-fallback\" };\n                }\n\n                return { success: false, error: e.message };\n            }\n\n            // Small delay before retry to avoid excessive polling\n            await new Promise(resolve => setTimeout(resolve, 1000));\n        }\n\n        return { success: false, error: `Lock acquisition timeout after ${timeout}ms` };\n    }\n\n    static async releaseLock(agentId) {\n        try {\n            await fetch(`${this.BRIDGE_URL}/v1/gpu/unlock`, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'Authorization': 'Bearer sovereign-secret'\n                },\n                body: JSON.stringify({ id: agentId }),\n                keepalive: true // Critical: Ensure request survives tab close\n            });\n        } catch (e) {\n            console.warn(\"Failed to release lock\", e);\n            // Don't throw error on release failure to avoid blocking cleanup\n        } finally {\n             // Reset state to IDLE if a high-priority agent is finishing\n             if (agentId.includes('Console') || agentId.includes('Chat') || agentId.includes('Mic')) {\n                 this.broadcastState(this.States.IDLE);\n             }\n        }\n    }\n\n    // Enhanced withLock with better error handling and retry logic\n    static async withLock(agentId, taskFn, timeout = 120000) {\n        const lock = await this.acquireLock(agentId, timeout);\n        if (!lock.success) {\n            console.error(`GPU lock acquisition failed for ${agentId}: ${lock.error}`);\n            throw new Error(`Could not acquire GPU lock: ${lock.error}`);\n        }\n\n        let taskResult;\n        let taskError;\n\n        try {\n            taskResult = await taskFn();\n        } catch (e) {\n            taskError = e;\n        } finally {\n            // Always try to release the lock, even if the task fails\n            await this.releaseLock(agentId);\n        }\n\n        if (taskError) {\n            throw taskError;\n        }\n\n        return taskResult;\n    }\n\n    // NEW: Serialize model loading to prevent multiple models loading simultaneously\n    static async withModelLoadLock(agentId, taskFn, timeout = 300000) { // 5-minute timeout for model loading\n        // Create a promise chain to serialize model loading at the application level\n        // This ensures only one model loading operation happens at a time across all components\n        const previousLoad = this.modelLoadPromise;\n\n        // Create a new promise that waits for the previous one to complete\n        this.modelLoadPromise = (async () => {\n            if (previousLoad) {\n                try {\n                    await previousLoad;\n                } catch (e) {\n                    console.warn(\"Previous model load failed, continuing:\", e);\n                }\n            }\n\n            // Mark this loader as active\n            this.activeModelLoaders.add(agentId);\n\n            try {\n                console.log(`[${agentId}] Starting sequential model loading (Queue: ${this.activeModelLoaders.size - 1} waiting)`);\n                // The task function itself will handle GPU lock acquisition as needed\n                const result = await taskFn();\n                console.log(`[${agentId}] Sequential model loading completed`);\n                return result;\n            } finally {\n                // Remove from active loaders\n                this.activeModelLoaders.delete(agentId);\n            }\n        })();\n\n        try {\n            return await this.modelLoadPromise;\n        } catch (error) {\n            // Clean up on error\n            this.activeModelLoaders.delete(agentId);\n            throw error;\n        }\n    }\n\n    // Get status of model loading queue\n    static getModelLoadStatus() {\n        return {\n            queueSize: this.activeModelLoaders.size,\n            activeLoaders: Array.from(this.activeModelLoaders),\n            hasPendingLoad: this.modelLoadPromise !== null,\n            modelLoadQueueInfo: `Model Load Queue: ${this.activeModelLoaders.size} active, ${this.modelLoadPromise ? 'loading' : 'idle'}`\n        };\n    }\n\n    // Get mind blanking status\n    static getMindBlankingStatus() {\n        const isBlanking = Date.now() < this.blankingUntil;\n        const remainingTime = Math.max(0, this.blankingUntil - Date.now());\n        return {\n            isBlanking,\n            blankingUntil: new Date(this.blankingUntil).toISOString(),\n            remainingMs: remainingTime,\n            remainingSeconds: Math.ceil(remainingTime / 1000),\n            lastIntensiveTask: this.lastIntensiveTask,\n            blankingDuration: this.BLANKING_DURATION\n        };\n    }\n\n    // New: Check GPU status to help with debugging\n    static async checkStatus() {\n        try {\n            const res = await fetch(`${this.BRIDGE_URL}/v1/gpu/status`, {\n                method: 'GET',\n                headers: {\n                    'Authorization': 'Bearer sovereign-secret'\n                }\n            });\n\n            if (res.ok) {\n                const bridgeStatus = await res.json();\n                // Add mind blanking status to the bridge status\n                const blankingStatus = this.getMindBlankingStatus();\n                return {\n                    ...bridgeStatus,\n                    mindBlanking: blankingStatus\n                };\n            }\n            return { error: `Status check failed (${res.status})` };\n        } catch (e) {\n            return { error: e.message };\n        }\n    }\n}\n\n// Ensure explicit export if previous style failed in some browsers\nexport { GPUController };\n",
    "source": "tools\\modules\\anchor.js"
  },
  {
    "id": "tools\\modules\\gpu-hot-reloader.js",
    "timestamp": 1767142089,
    "role": "file",
    "content": "/*\n * GPU Management Hot Reloader\n * Provides hot reload capability for GPU management in the browser\n */\n\nclass GPUHotReloader {\n    constructor() {\n        this.gpuController = null;\n        this.checkInterval = 5000; // Check every 5 seconds\n        this.lastModified = {};\n        this.isReloading = false;\n        this.reloaderEnabled = true;\n    }\n\n    // Enable/disable hot reload\n    setEnabled(enabled) {\n        this.reloaderEnabled = enabled;\n        if (enabled) {\n            this.startMonitoring();\n        } else {\n            this.stopMonitoring();\n        }\n    }\n\n    // Start monitoring for changes\n    startMonitoring() {\n        if (this.monitorInterval) return;\n\n        console.log(\"üîÑ GPU Hot Reloader: Starting monitoring...\");\n        this.monitorInterval = setInterval(() => {\n            this.checkForChanges();\n        }, this.checkInterval);\n    }\n\n    // Stop monitoring\n    stopMonitoring() {\n        if (this.monitorInterval) {\n            clearInterval(this.monitorInterval);\n            this.monitorInterval = null;\n            console.log(\"üõë GPU Hot Reloader: Stopped monitoring\");\n        }\n    }\n\n    // Check for changes in GPU-related files\n    async checkForChanges() {\n        if (!this.reloaderEnabled || this.isReloading) return;\n\n        try {\n            // Check if any GPU-related files have been modified\n            const gpuFiles = [\n                'tools/modules/anchor.js',\n                'tools/model-server-chat.html',\n                'tools/anchor-mic.html',\n                'tools/memory-builder.html'\n            ];\n\n            for (const file of gpuFiles) {\n                const modified = await this.checkFileModified(file);\n                if (modified) {\n                    console.log(`üîÑ GPU Hot Reloader: Detected change in ${file}`);\n                    await this.reloadGPUManagement();\n                    break; // Only reload once per check cycle\n                }\n            }\n        } catch (error) {\n            console.warn('GPU Hot Reloader: Error checking for changes:', error);\n        }\n    }\n\n    // Check if a file has been modified since last check\n    async checkFileModified(filepath) {\n        try {\n            // Try to check file modification time via server endpoint\n            const response = await fetch(`http://localhost:8080/file-mod-time?path=${encodeURIComponent(filepath)}`);\n            if (response.ok) {\n                const data = await response.json();\n                const currentModTime = data.modTime;\n\n                if (filepath in this.lastModified) {\n                    if (this.lastModified[filepath] !== currentModTime) {\n                        this.lastModified[filepath] = currentModTime;\n                        return true;\n                    }\n                } else {\n                    this.lastModified[filepath] = currentModTime;\n                }\n            }\n        } catch (error) {\n            // Fallback: try to re-fetch and compare content\n            try {\n                const response = await fetch(filepath + '?t=' + Date.now(), { method: 'HEAD' });\n                const lastModified = response.headers.get('Last-Modified');\n\n                if (filepath in this.lastModified) {\n                    if (this.lastModified[filepath] !== lastModified) {\n                        this.lastModified[filepath] = lastModified;\n                        return true;\n                    }\n                } else {\n                    this.lastModified[filepath] = lastModified;\n                }\n            } catch (e) {\n                // If we can't check, assume no change to avoid constant reloads\n            }\n        }\n        return false;\n    }\n\n    // Reload GPU management logic\n    async reloadGPUManagement() {\n        if (this.isReloading) return;\n\n        this.isReloading = true;\n        console.log(\"üîÑ GPU Hot Reloader: Reloading GPU management logic...\");\n\n        try {\n            // Force release any current GPU locks to prevent stale state\n            await this.forceReleaseGPU();\n\n            // Clear any cached modules if possible\n            this.clearModuleCache();\n\n            // Reload the GPU controller with fresh logic\n            await this.reloadGPUController();\n\n            console.log(\"‚úÖ GPU Hot Reloader: GPU management reloaded successfully\");\n        } catch (error) {\n            console.error(\"‚ùå GPU Hot Reloader: Error during reload:\", error);\n        } finally {\n            this.isReloading = false;\n        }\n    }\n\n    // Force release current GPU locks\n    async forceReleaseGPU() {\n        try {\n            // Call the emergency release endpoint if available\n            await fetch(\"http://localhost:8080/v1/gpu/force-release-all\", {\n                method: \"POST\",\n                headers: { \"Authorization\": \"Bearer sovereign-secret\" }\n            });\n            console.log(\"‚úÖ GPU Hot Reloader: Force released all GPU locks\");\n        } catch (e) {\n            console.warn(\"‚ö†Ô∏è GPU Hot Reloader: Could not force release GPU locks:\", e);\n        }\n    }\n\n    // Clear any cached modules (attempt to force reload)\n    clearModuleCache() {\n        // In a real implementation, this would clear module caches\n        // For now, we'll just log\n        console.log(\"üßπ GPU Hot Reloader: Clearing module cache (not implemented in browser)\");\n    }\n\n    // Reload GPU controller with fresh logic\n    async reloadGPUController() {\n        // Since we can't truly reload modules in the browser,\n        // we'll trigger a soft reload of GPU state\n        if (window.GPUController) {\n            // If there's a way to refresh the GPU controller state, do it here\n            console.log(\"üîÑ GPU Hot Reloader: Refreshing GPU controller state\");\n        }\n    }\n\n    // Manual trigger for hot reload\n    async triggerReload() {\n        console.log(\"üîÑ GPU Hot Reloader: Manual reload triggered\");\n        await this.reloadGPUManagement();\n    }\n}\n\n// Global instance\nwindow.GPU_HOT_RELOADER = new GPUHotReloader();\n\n// Auto-start if in development mode\nif (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {\n    window.GPU_HOT_RELOADER.startMonitoring();\n    console.log(\"üîÑ GPU Hot Reloader: Auto-started in development mode\");\n}\n\n// Expose for manual control\nwindow.triggerGPUHotReload = () => window.GPU_HOT_RELOADER.triggerReload();\nwindow.setGPUHotReloadEnabled = (enabled) => window.GPU_HOT_RELOADER.setEnabled(enabled);\n\nconsole.log(\"üîÑ GPU Hot Reloader: Initialized and ready\");",
    "source": "tools\\modules\\gpu-hot-reloader.js"
  },
  {
    "id": "tools\\modules\\llm-engine-manager.js",
    "timestamp": 1766645044,
    "role": "file",
    "content": "/*\r\n * LLM Engine Manager\r\n * Provides lifecycle management for LLM engine instances with hibernation capabilities\r\n */\r\n\r\nclass LLMEngineManager {\r\n    constructor() {\r\n        this.engines = new Map(); // Map of componentId -> engine instance\r\n        this.engineWorkers = new Map(); // Map of componentId -> worker instance\r\n        this.engineConfigs = new Map(); // Map of componentId -> config\r\n        this.engineStates = new Map(); // Map of componentId -> state (loaded, hibernated, etc.)\r\n    }\r\n\r\n    // Create a new engine instance for a specific component\r\n    async createEngine(componentId, modelConfig, webllmModule, options = {}) {\r\n        try {\r\n            // Store the config for potential hibernation/resume\r\n            this.engineConfigs.set(componentId, modelConfig);\r\n\r\n            // Create a dedicated worker for this component\r\n            const worker = new Worker('./modules/llm-worker.js', { type: 'module' });\r\n            this.engineWorkers.set(componentId, worker);\r\n\r\n            // Create the engine using the provided webllm module\r\n            const engine = await this.createWebWorkerMLCEngine(worker, modelConfig.model_id, webllmModule, {\r\n                appConfig: { model_list: [modelConfig] },\r\n                ...options\r\n            });\r\n\r\n            // Store the engine\r\n            this.engines.set(componentId, engine);\r\n            this.engineStates.set(componentId, 'loaded');\r\n\r\n            return engine;\r\n        } catch (error) {\r\n            console.error(`Failed to create engine for ${componentId}:`, error);\r\n            throw error;\r\n        }\r\n    }\r\n\r\n    // Get an existing engine instance\r\n    getEngine(componentId) {\r\n        return this.engines.get(componentId);\r\n    }\r\n\r\n    // Check if an engine is loaded\r\n    isLoaded(componentId) {\r\n        return this.engineStates.get(componentId) === 'loaded';\r\n    }\r\n\r\n    // Hibernate an engine (save its state and dispose resources)\r\n    async hibernate(componentId) {\r\n        const engine = this.engines.get(componentId);\r\n        if (!engine) return false;\r\n\r\n        try {\r\n            // In a real implementation, we would save the engine state here\r\n            // For now, we'll just dispose the engine and mark as hibernated\r\n            if (typeof engine.dispose === 'function') {\r\n                await engine.dispose();\r\n            }\r\n\r\n            this.engineStates.set(componentId, 'hibernated');\r\n            return true;\r\n        } catch (error) {\r\n            console.error(`Failed to hibernate engine for ${componentId}:`, error);\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // Resume an engine from hibernation\r\n    async resume(componentId) {\r\n        if (this.engineStates.get(componentId) !== 'hibernated') {\r\n            // If not hibernated, try to get existing engine\r\n            return this.getEngine(componentId);\r\n        }\r\n\r\n        const config = this.engineConfigs.get(componentId);\r\n        if (!config) {\r\n            throw new Error(`No config found for hibernated engine: ${componentId}`);\r\n        }\r\n\r\n        // Recreate the engine\r\n        return await this.createEngine(componentId, config);\r\n    }\r\n\r\n    // Dispose an engine completely\r\n    async dispose(componentId) {\r\n        const engine = this.engines.get(componentId);\r\n        const worker = this.engineWorkers.get(componentId);\r\n\r\n        try {\r\n            if (engine && typeof engine.dispose === 'function') {\r\n                await engine.dispose();\r\n            }\r\n        } catch (error) {\r\n            console.warn(`Error disposing engine for ${componentId}:`, error);\r\n        }\r\n\r\n        try {\r\n            if (worker && typeof worker.terminate === 'function') {\r\n                worker.terminate();\r\n            }\r\n        } catch (error) {\r\n            console.warn(`Error terminating worker for ${componentId}:`, error);\r\n        }\r\n\r\n        // Clean up maps\r\n        this.engines.delete(componentId);\r\n        this.engineWorkers.delete(componentId);\r\n        this.engineConfigs.delete(componentId);\r\n        this.engineStates.delete(componentId);\r\n\r\n        return true;\r\n    }\r\n\r\n    // Get all component IDs\r\n    getComponentIds() {\r\n        return Array.from(this.engines.keys());\r\n    }\r\n\r\n    // Create WebWorkerMLCEngine with error handling\r\n    async createWebWorkerMLCEngine(worker, modelId, webllmModule, options) {\r\n        // Use the provided webllm module to create the engine\r\n        if (webllmModule && typeof webllmModule.CreateWebWorkerMLCEngine !== 'undefined') {\r\n            return await webllmModule.CreateWebWorkerMLCEngine(worker, modelId, options);\r\n        } else if (webllmModule && typeof webllmModule.CreateMLCEngine !== 'undefined') {\r\n            // Fallback to CreateMLCEngine if CreateWebWorkerMLCEngine is not available\r\n            return await webllmModule.CreateMLCEngine(modelId, options);\r\n        } else {\r\n            throw new Error('WebLLM module not provided or does not contain required engine creation functions');\r\n        }\r\n    }\r\n\r\n    // Save engine state to storage (simplified approach)\r\n    saveStateToStorage() {\r\n        const state = {\r\n            components: Array.from(this.engineStates.entries()),\r\n            configs: Array.from(this.engineConfigs.entries())\r\n        };\r\n\r\n        try {\r\n            localStorage.setItem('llm_engine_manager_state', JSON.stringify(state));\r\n            return true;\r\n        } catch (error) {\r\n            console.error('Failed to save engine state:', error);\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // Load engine state from storage\r\n    loadStateFromStorage() {\r\n        try {\r\n            const stateStr = localStorage.getItem('llm_engine_manager_state');\r\n            if (!stateStr) return false;\r\n\r\n            const state = JSON.parse(stateStr);\r\n\r\n            // Restore states and configs\r\n            state.components.forEach(([id, status]) => {\r\n                this.engineStates.set(id, status);\r\n            });\r\n\r\n            state.configs.forEach(([id, config]) => {\r\n                this.engineConfigs.set(id, config);\r\n            });\r\n\r\n            return true;\r\n        } catch (error) {\r\n            console.error('Failed to load engine state:', error);\r\n            return false;\r\n        }\r\n    }\r\n}\r\n\r\n// Export the class for module usage\r\nexport { LLMEngineManager };\r\n\r\n// Also make it available globally for backward compatibility\r\nif (typeof window !== 'undefined') {\r\n    window.LLMEngineManager = LLMEngineManager;\r\n    console.log('üîÑ LLM Engine Manager: Module loaded');\r\n}",
    "source": "tools\\modules\\llm-engine-manager.js"
  },
  {
    "id": "tools\\modules\\llm-worker.js",
    "timestamp": 1766864507,
    "role": "file",
    "content": "// --- CACHE OVERRIDE: Prevent Cache API usage in worker thread ---\n// This is critical for WebLLM to work in environments with strict cache policies\nif ('caches' in self) {\n    try {\n        // Override the Cache API to prevent WebLLM from using it in the worker\n        const originalAdd = Cache.prototype.add;\n        const originalAddAll = Cache.prototype.addAll;\n        const originalPut = Cache.prototype.put;\n\n        Cache.prototype.add = function(request) {\n            console.warn(\"Worker: Cache.add blocked by Root Coda security override\");\n            return Promise.resolve();\n        };\n\n        Cache.prototype.addAll = function(requests) {\n            console.warn(\"Worker: Cache.addAll blocked by Root Coda security override\");\n            return Promise.resolve();\n        };\n\n        Cache.prototype.put = function(request, response) {\n            console.warn(\"Worker: Cache.put blocked by Root Coda security override\");\n            return Promise.resolve();\n        };\n\n        console.log(\"üõ°Ô∏è Worker: Cache API overridden to prevent tracking prevention errors\");\n    } catch (e) {\n        console.warn(\"Worker: Could not override Cache API:\", e);\n    }\n}\n\nimport { WebWorkerMLCEngineHandler, MLCEngine } from \"https://esm.run/@mlc-ai/web-llm\";\n\n// The handler bridges messages between the Main Thread and the Engine\nconst engine = new MLCEngine();\nconst handler = new WebWorkerMLCEngineHandler(engine);\n\nself.onmessage = (msg) => {\n    handler.onmessage(msg);\n};",
    "source": "tools\\modules\\llm-worker.js"
  },
  {
    "id": "tools\\modules\\vision.js",
    "timestamp": 1766690399,
    "role": "file",
    "content": "/**\n * VisionController - Handles drag-and-drop, clipboard paste, and image preview logic\n */\nexport class VisionController {\n    constructor() {\n        this.imageBase64 = null;\n        this.dropZone = null;\n        this.previewContainer = null;\n        this.inputId = null;\n    }\n\n    setup(dropZoneId, previewContainerId, inputId) {\n        this.dropZone = document.getElementById(dropZoneId);\n        this.previewContainer = document.getElementById(previewContainerId);\n        this.inputId = inputId;\n\n        if (!this.dropZone || !this.previewContainer) {\n            console.error('VisionController: Required elements not found');\n            return;\n        }\n\n        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n            this.dropZone.addEventListener(eventName, (e) => { e.preventDefault(); e.stopPropagation(); }, false);\n            document.body.addEventListener(eventName, (e) => { e.preventDefault(); e.stopPropagation(); }, false);\n        });\n\n        // Visual feedback\n        ['dragenter', 'dragover'].forEach(eventName => {\n            this.dropZone.addEventListener(eventName, () => this.dropZone.classList.add('drag-active'), false);\n        });\n\n        ['dragleave', 'drop'].forEach(eventName => {\n            this.dropZone.addEventListener(eventName, () => this.dropZone.classList.remove('drag-active'), false);\n        });\n\n        this.dropZone.addEventListener('drop', (e) => this.handleDrop(e), false);\n        document.addEventListener('paste', (e) => this.handlePaste(e));\n\n        // File Input Fallback\n        const fileInput = document.createElement('input');\n        fileInput.type = 'file';\n        fileInput.accept = 'image/*';\n        fileInput.style.display = 'none';\n        fileInput.id = 'vision-file-input';\n        document.body.appendChild(fileInput);\n\n        const uploadButton = document.getElementById('image-upload-btn');\n        if (uploadButton) {\n            uploadButton.addEventListener('click', (e) => {\n                e.preventDefault();\n                fileInput.click();\n            });\n        }\n\n        fileInput.addEventListener('change', (e) => {\n            if (e.target.files && e.target.files[0]) this.handleFile(e.target.files[0]);\n        });\n    }\n\n    handleDrop(e) {\n        const dt = e.dataTransfer;\n        const files = dt.files;\n        if (files.length > 0) this.handleFile(files[0]);\n    }\n\n    handlePaste(e) {\n        const items = e.clipboardData.items;\n        for (let i = 0; i < items.length; i++) {\n            if (items[i].type.indexOf('image') !== -1) {\n                const blob = items[i].getAsFile();\n                if (blob) {\n                    this.handleFile(blob);\n                    break;\n                }\n            }\n        }\n    }\n\n    handleFile(file) {\n        if (!file.type.match('image.*')) {\n            console.error('VisionController: Only image files are supported');\n            return;\n        }\n        const reader = new FileReader();\n        reader.onload = (e) => {\n            this.imageBase64 = e.target.result;\n            this.showPreview(this.imageBase64, file.name);\n        };\n        reader.readAsDataURL(file);\n    }\n\n    showPreview(base64Url, fileName) {\n        if (!this.previewContainer) return;\n        this.previewContainer.innerHTML = '';\n        this.previewContainer.style.display = 'block'; // Show container\n\n        const previewDiv = document.createElement('div');\n        previewDiv.className = 'image-preview';\n        \n        const img = document.createElement('img');\n        img.src = base64Url;\n        img.style.maxWidth = '100px';\n        img.style.maxHeight = '100px';\n        img.style.borderRadius = '4px';\n        \n        const removeBtn = document.createElement('button');\n        removeBtn.textContent = '‚ùå';\n        removeBtn.style.marginLeft = '10px';\n        removeBtn.onclick = () => this.clear();\n        \n        previewDiv.appendChild(img);\n        previewDiv.appendChild(removeBtn);\n        this.previewContainer.appendChild(previewDiv);\n    }\n\n    clear() {\n        this.imageBase64 = null;\n        if (this.previewContainer) {\n            this.previewContainer.innerHTML = '';\n            this.previewContainer.style.display = 'none';\n        }\n        const fileInput = document.getElementById('vision-file-input');\n        if (fileInput) fileInput.value = '';\n    }\n\n    getImage() {\n        return this.imageBase64;\n    }\n}",
    "source": "tools\\modules\\vision.js"
  },
  {
    "id": "tools\\modules\\whisper-worker.js",
    "timestamp": 1766591401,
    "role": "file",
    "content": "import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\r\n\r\n// Configure for WASM\r\nenv.allowLocalModels = false;\r\nenv.useBrowserCache = true;\r\n\r\nclass WhisperWorker {\r\n    constructor() {\r\n        this.pipe = null;\r\n    }\r\n\r\n    async init() {\r\n        if (!this.pipe) {\r\n            console.log(\"[WhisperWorker] Loading model...\");\r\n            this.pipe = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', { \r\n                device: 'wasm'\r\n            });\r\n            console.log(\"[WhisperWorker] Model loaded.\");\r\n        }\r\n    }\r\n\r\n    async transcribe(audioData) {\r\n        if (!this.pipe) await this.init();\r\n        \r\n        console.log(\"[WhisperWorker] Processing audio...\");\r\n        const result = await this.pipe(audioData, { \r\n            language: 'english',\r\n            chunk_length_s: 30,\r\n            stride_length_s: 5\r\n        });\r\n        \r\n        return result.text.trim();\r\n    }\r\n}\r\n\r\nconst worker = new WhisperWorker();\r\n\r\nself.onmessage = async (e) => {\r\n    const { type, data, id } = e.data;\r\n\r\n    try {\r\n        if (type === 'init') {\r\n            await worker.init();\r\n            self.postMessage({ type: 'init_done', id });\r\n        } \r\n        else if (type === 'transcribe') {\r\n            const text = await worker.transcribe(data);\r\n            self.postMessage({ type: 'transcribe_result', text, id });\r\n        }\r\n    } catch (err) {\r\n        self.postMessage({ type: 'error', error: err.message, id });\r\n    }\r\n};",
    "source": "tools\\modules\\whisper-worker.js"
  }
]