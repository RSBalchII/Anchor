project_structure: C:\Users\rsbiiw\Projects\ECE_Core
files:
  - path: .gitignore
    content: "# Python-generated files\r\n__pycache__/\r\n*.py[oc]\r\nbuild/\r\ndist/\r\nwheels/\r\n*.egg-info\r\n.qwen\r\n*.json\r\n*.yaml\r\n*.txt\r\nnode_modules\r\n\r\n# Virtual environments\r\n.venv\r\n\r\n# Large model files\r\nmodels/\r\n\r\nbackend/archive/db/\r\n\r\n*context/\r\n/archive/\r\n/browser_data/\r\n"
    size: 258
    tokens: 93
  - path: context.db\data\CURRENT
    content: |
      MANIFEST-000005
    size: 16
    tokens: 6
  - path: context.db\data\IDENTITY
    content: f0113401-e93f-11f0-96b3-401a58c3d854
    size: 36
    tokens: 14
  - path: context.db\data\LOCK
    content: ''
    size: 0
    tokens: 0
  - path: context.db\data\LOG
    content: "2026/01/04-00:36:36.322738 11c6c RocksDB version: 8.8.1\n2026/01/04-00:36:36.323292 11c6c Compile date 2022-10-11 08:49:59\n2026/01/04-00:36:36.323314 11c6c DB SUMMARY\n2026/01/04-00:36:36.323328 11c6c Host name (Env):  WIN11OMEN\n2026/01/04-00:36:36.323337 11c6c DB Session ID:  2XPUYBTHS49JT8XM1E04\n2026/01/04-00:36:36.323802 11c6c CURRENT file:  CURRENT\n2026/01/04-00:36:36.323816 11c6c IDENTITY file:  IDENTITY\n2026/01/04-00:36:36.323881 11c6c MANIFEST file:  MANIFEST-000005 size: 83 Bytes\n2026/01/04-00:36:36.323892 11c6c SST files in ../context.db\\data dir, Total Num: 0, files: \n2026/01/04-00:36:36.323902 11c6c Write Ahead Log file in ../context.db\\data: 000004.log size: 2993 ; \n2026/01/04-00:36:36.323912 11c6c                         Options.error_if_exists: 0\n2026/01/04-00:36:36.324022 11c6c                       Options.create_if_missing: 0\n2026/01/04-00:36:36.324029 11c6c                         Options.paranoid_checks: 1\n2026/01/04-00:36:36.324032 11c6c             Options.flush_verify_memtable_count: 1\n2026/01/04-00:36:36.324036 11c6c          Options.compaction_verify_record_count: 1\n2026/01/04-00:36:36.324039 11c6c                               Options.track_and_verify_wals_in_manifest: 0\n2026/01/04-00:36:36.324042 11c6c        Options.verify_sst_unique_id_in_manifest: 1\n2026/01/04-00:36:36.324046 11c6c                                     Options.env: 000001AD0C6BB9E0\n2026/01/04-00:36:36.324050 11c6c                                      Options.fs: WinFS\n2026/01/04-00:36:36.324053 11c6c                                Options.info_log: 000001AD46CF5D30\n2026/01/04-00:36:36.324057 11c6c                Options.max_file_opening_threads: 16\n2026/01/04-00:36:36.324060 11c6c                              Options.statistics: 0000000000000000\n2026/01/04-00:36:36.324064 11c6c                               Options.use_fsync: 0\n2026/01/04-00:36:36.324067 11c6c                       Options.max_log_file_size: 0\n2026/01/04-00:36:36.324071 11c6c                  Options.max_manifest_file_size: 1073741824\n2026/01/04-00:36:36.324074 11c6c                   Options.log_file_time_to_roll: 0\n2026/01/04-00:36:36.324078 11c6c                       Options.keep_log_file_num: 1000\n2026/01/04-00:36:36.324081 11c6c                    Options.recycle_log_file_num: 0\n2026/01/04-00:36:36.324085 11c6c                         Options.allow_fallocate: 1\n2026/01/04-00:36:36.324088 11c6c                        Options.allow_mmap_reads: 0\n2026/01/04-00:36:36.324092 11c6c                       Options.allow_mmap_writes: 0\n2026/01/04-00:36:36.324095 11c6c                        Options.use_direct_reads: 0\n2026/01/04-00:36:36.324098 11c6c                        Options.use_direct_io_for_flush_and_compaction: 0\n2026/01/04-00:36:36.324102 11c6c          Options.create_missing_column_families: 1\n2026/01/04-00:36:36.324105 11c6c                              Options.db_log_dir: \n2026/01/04-00:36:36.324108 11c6c                                 Options.wal_dir: \n2026/01/04-00:36:36.324112 11c6c                Options.table_cache_numshardbits: 6\n2026/01/04-00:36:36.324115 11c6c                         Options.WAL_ttl_seconds: 0\n2026/01/04-00:36:36.324119 11c6c                       Options.WAL_size_limit_MB: 0\n2026/01/04-00:36:36.324122 11c6c                        Options.max_write_batch_group_size_bytes: 1048576\n2026/01/04-00:36:36.324125 11c6c             Options.manifest_preallocation_size: 4194304\n2026/01/04-00:36:36.324129 11c6c                     Options.is_fd_close_on_exec: 1\n2026/01/04-00:36:36.324132 11c6c                   Options.advise_random_on_open: 1\n2026/01/04-00:36:36.324135 11c6c                    Options.db_write_buffer_size: 0\n2026/01/04-00:36:36.324139 11c6c                    Options.write_buffer_manager: 000001AD46CED9D0\n2026/01/04-00:36:36.324142 11c6c         Options.access_hint_on_compaction_start: 1\n2026/01/04-00:36:36.324146 11c6c           Options.random_access_max_buffer_size: 1048576\n2026/01/04-00:36:36.324149 11c6c                      Options.use_adaptive_mutex: 0\n2026/01/04-00:36:36.324152 11c6c                            Options.rate_limiter: 0000000000000000\n2026/01/04-00:36:36.324182 11c6c     Options.sst_file_manager.rate_bytes_per_sec: 0\n2026/01/04-00:36:36.324187 11c6c                       Options.wal_recovery_mode: 2\n2026/01/04-00:36:36.324191 11c6c                  Options.enable_thread_tracking: 0\n2026/01/04-00:36:36.324194 11c6c                  Options.enable_pipelined_write: 0\n2026/01/04-00:36:36.324198 11c6c                  Options.unordered_write: 0\n2026/01/04-00:36:36.324201 11c6c         Options.allow_concurrent_memtable_write: 1\n2026/01/04-00:36:36.324204 11c6c      Options.enable_write_thread_adaptive_yield: 1\n2026/01/04-00:36:36.324208 11c6c             Options.write_thread_max_yield_usec: 100\n2026/01/04-00:36:36.324211 11c6c            Options.write_thread_slow_yield_usec: 3\n2026/01/04-00:36:36.324215 11c6c                               Options.row_cache: None\n2026/01/04-00:36:36.324218 11c6c                              Options.wal_filter: None\n2026/01/04-00:36:36.324221 11c6c             Options.avoid_flush_during_recovery: 0\n2026/01/04-00:36:36.324225 11c6c             Options.allow_ingest_behind: 0\n2026/01/04-00:36:36.324228 11c6c             Options.two_write_queues: 0\n2026/01/04-00:36:36.324231 11c6c             Options.manual_wal_flush: 0\n2026/01/04-00:36:36.324235 11c6c             Options.wal_compression: 0\n2026/01/04-00:36:36.324238 11c6c             Options.atomic_flush: 0\n2026/01/04-00:36:36.324241 11c6c             Options.avoid_unnecessary_blocking_io: 0\n2026/01/04-00:36:36.324245 11c6c                 Options.persist_stats_to_disk: 0\n2026/01/04-00:36:36.324248 11c6c                 Options.write_dbid_to_manifest: 0\n2026/01/04-00:36:36.324251 11c6c                 Options.log_readahead_size: 0\n2026/01/04-00:36:36.324254 11c6c                 Options.file_checksum_gen_factory: Unknown\n2026/01/04-00:36:36.324258 11c6c                 Options.best_efforts_recovery: 0\n2026/01/04-00:36:36.324261 11c6c                Options.max_bgerror_resume_count: 2147483647\n2026/01/04-00:36:36.324265 11c6c            Options.bgerror_resume_retry_interval: 1000000\n2026/01/04-00:36:36.324268 11c6c             Options.allow_data_in_errors: 0\n2026/01/04-00:36:36.324271 11c6c             Options.db_host_id: __hostname__\n2026/01/04-00:36:36.324275 11c6c             Options.enforce_single_del_contracts: true\n2026/01/04-00:36:36.324279 11c6c             Options.max_background_jobs: 6\n2026/01/04-00:36:36.324282 11c6c             Options.max_background_compactions: -1\n2026/01/04-00:36:36.324285 11c6c             Options.max_subcompactions: 1\n2026/01/04-00:36:36.324289 11c6c             Options.avoid_flush_during_shutdown: 0\n2026/01/04-00:36:36.324292 11c6c           Options.writable_file_max_buffer_size: 1048576\n2026/01/04-00:36:36.324295 11c6c             Options.delayed_write_rate : 16777216\n2026/01/04-00:36:36.324299 11c6c             Options.max_total_wal_size: 0\n2026/01/04-00:36:36.324302 11c6c             Options.delete_obsolete_files_period_micros: 21600000000\n2026/01/04-00:36:36.324306 11c6c                   Options.stats_dump_period_sec: 600\n2026/01/04-00:36:36.324309 11c6c                 Options.stats_persist_period_sec: 600\n2026/01/04-00:36:36.324313 11c6c                 Options.stats_history_buffer_size: 1048576\n2026/01/04-00:36:36.324316 11c6c                          Options.max_open_files: -1\n2026/01/04-00:36:36.324320 11c6c                          Options.bytes_per_sync: 1048576\n2026/01/04-00:36:36.324323 11c6c                      Options.wal_bytes_per_sync: 0\n2026/01/04-00:36:36.324326 11c6c                   Options.strict_bytes_per_sync: 0\n2026/01/04-00:36:36.324330 11c6c       Options.compaction_readahead_size: 2097152\n2026/01/04-00:36:36.324333 11c6c                  Options.max_background_flushes: -1\n2026/01/04-00:36:36.324336 11c6c Options.daily_offpeak_time_utc: \n2026/01/04-00:36:36.324340 11c6c Compression algorithms supported:\n2026/01/04-00:36:36.324345 11c6c \tkZSTD supported: 1\n2026/01/04-00:36:36.324349 11c6c \tkSnappyCompression supported: 0\n2026/01/04-00:36:36.324352 11c6c \tkBZip2Compression supported: 0\n2026/01/04-00:36:36.324372 11c6c \tkZlibCompression supported: 0\n2026/01/04-00:36:36.324376 11c6c \tkLZ4Compression supported: 1\n2026/01/04-00:36:36.324380 11c6c \tkXpressCompression supported: 0\n2026/01/04-00:36:36.324383 11c6c \tkLZ4HCCompression supported: 1\n2026/01/04-00:36:36.324386 11c6c \tkZSTDNotFinalCompression supported: 1\n2026/01/04-00:36:36.324391 11c6c Fast CRC32 supported: Not supported on x86\n2026/01/04-00:36:36.324394 11c6c DMutex implementation: std::mutex\n2026/01/04-00:36:36.324781 11c6c [WARN] [db/db_impl/db_impl_open.cc:2248] DB::Open() failed: IO error: Failed to create lock file: ../context.db\\data/LOCK: The process cannot access the file because it is being used by another process.\r\n2026/01/04-00:36:36.324800 11c6c [db/db_impl/db_impl.cc:487] Shutdown: canceling all background work\n2026/01/04-00:36:36.324833 11c6c [db/db_impl/db_impl.cc:668] Shutdown complete\n"
    size: 9085
    tokens: 3544
  - path: context.db\data\LOG.old.1767512196321134
    content: "2026/01/04-00:35:27.838110 12464 RocksDB version: 8.8.1\n2026/01/04-00:35:27.838577 12464 Compile date 2022-10-11 08:49:59\n2026/01/04-00:35:27.838601 12464 DB SUMMARY\n2026/01/04-00:35:27.838618 12464 Host name (Env):  WIN11OMEN\n2026/01/04-00:35:27.838629 12464 DB Session ID:  VB8Y5S8AI0YMDA5Y5JG1\n2026/01/04-00:35:27.838989 12464 SST files in ../context.db\\data dir, Total Num: 0, files: \n2026/01/04-00:35:27.839026 12464 Write Ahead Log file in ../context.db\\data: \n2026/01/04-00:35:27.839042 12464                         Options.error_if_exists: 0\n2026/01/04-00:35:27.839053 12464                       Options.create_if_missing: 1\n2026/01/04-00:35:27.839063 12464                         Options.paranoid_checks: 1\n2026/01/04-00:35:27.839074 12464             Options.flush_verify_memtable_count: 1\n2026/01/04-00:35:27.839236 12464          Options.compaction_verify_record_count: 1\n2026/01/04-00:35:27.839243 12464                               Options.track_and_verify_wals_in_manifest: 0\n2026/01/04-00:35:27.839248 12464        Options.verify_sst_unique_id_in_manifest: 1\n2026/01/04-00:35:27.839252 12464                                     Options.env: 000001A8C964FA40\n2026/01/04-00:35:27.839256 12464                                      Options.fs: WinFS\n2026/01/04-00:35:27.839261 12464                                Options.info_log: 000001A8C973C010\n2026/01/04-00:35:27.839265 12464                Options.max_file_opening_threads: 16\n2026/01/04-00:35:27.839269 12464                              Options.statistics: 0000000000000000\n2026/01/04-00:35:27.839273 12464                               Options.use_fsync: 0\n2026/01/04-00:35:27.839277 12464                       Options.max_log_file_size: 0\n2026/01/04-00:35:27.839282 12464                  Options.max_manifest_file_size: 1073741824\n2026/01/04-00:35:27.839286 12464                   Options.log_file_time_to_roll: 0\n2026/01/04-00:35:27.839290 12464                       Options.keep_log_file_num: 1000\n2026/01/04-00:35:27.839294 12464                    Options.recycle_log_file_num: 0\n2026/01/04-00:35:27.839298 12464                         Options.allow_fallocate: 1\n2026/01/04-00:35:27.839302 12464                        Options.allow_mmap_reads: 0\n2026/01/04-00:35:27.839306 12464                       Options.allow_mmap_writes: 0\n2026/01/04-00:35:27.839310 12464                        Options.use_direct_reads: 0\n2026/01/04-00:35:27.839314 12464                        Options.use_direct_io_for_flush_and_compaction: 0\n2026/01/04-00:35:27.839318 12464          Options.create_missing_column_families: 1\n2026/01/04-00:35:27.839322 12464                              Options.db_log_dir: \n2026/01/04-00:35:27.839326 12464                                 Options.wal_dir: \n2026/01/04-00:35:27.839330 12464                Options.table_cache_numshardbits: 6\n2026/01/04-00:35:27.839334 12464                         Options.WAL_ttl_seconds: 0\n2026/01/04-00:35:27.839338 12464                       Options.WAL_size_limit_MB: 0\n2026/01/04-00:35:27.839342 12464                        Options.max_write_batch_group_size_bytes: 1048576\n2026/01/04-00:35:27.839346 12464             Options.manifest_preallocation_size: 4194304\n2026/01/04-00:35:27.839350 12464                     Options.is_fd_close_on_exec: 1\n2026/01/04-00:35:27.839354 12464                   Options.advise_random_on_open: 1\n2026/01/04-00:35:27.839357 12464                    Options.db_write_buffer_size: 0\n2026/01/04-00:35:27.839362 12464                    Options.write_buffer_manager: 000001A8C964FB30\n2026/01/04-00:35:27.839366 12464         Options.access_hint_on_compaction_start: 1\n2026/01/04-00:35:27.839370 12464           Options.random_access_max_buffer_size: 1048576\n2026/01/04-00:35:27.839374 12464                      Options.use_adaptive_mutex: 0\n2026/01/04-00:35:27.839377 12464                            Options.rate_limiter: 0000000000000000\n2026/01/04-00:35:27.839385 12464     Options.sst_file_manager.rate_bytes_per_sec: 0\n2026/01/04-00:35:27.839389 12464                       Options.wal_recovery_mode: 2\n2026/01/04-00:35:27.839393 12464                  Options.enable_thread_tracking: 0\n2026/01/04-00:35:27.839425 12464                  Options.enable_pipelined_write: 0\n2026/01/04-00:35:27.839432 12464                  Options.unordered_write: 0\n2026/01/04-00:35:27.839436 12464         Options.allow_concurrent_memtable_write: 1\n2026/01/04-00:35:27.839440 12464      Options.enable_write_thread_adaptive_yield: 1\n2026/01/04-00:35:27.839444 12464             Options.write_thread_max_yield_usec: 100\n2026/01/04-00:35:27.839448 12464            Options.write_thread_slow_yield_usec: 3\n2026/01/04-00:35:27.839452 12464                               Options.row_cache: None\n2026/01/04-00:35:27.839456 12464                              Options.wal_filter: None\n2026/01/04-00:35:27.839460 12464             Options.avoid_flush_during_recovery: 0\n2026/01/04-00:35:27.839464 12464             Options.allow_ingest_behind: 0\n2026/01/04-00:35:27.839468 12464             Options.two_write_queues: 0\n2026/01/04-00:35:27.839472 12464             Options.manual_wal_flush: 0\n2026/01/04-00:35:27.839476 12464             Options.wal_compression: 0\n2026/01/04-00:35:27.839479 12464             Options.atomic_flush: 0\n2026/01/04-00:35:27.839483 12464             Options.avoid_unnecessary_blocking_io: 0\n2026/01/04-00:35:27.839487 12464                 Options.persist_stats_to_disk: 0\n2026/01/04-00:35:27.839491 12464                 Options.write_dbid_to_manifest: 0\n2026/01/04-00:35:27.839495 12464                 Options.log_readahead_size: 0\n2026/01/04-00:35:27.839499 12464                 Options.file_checksum_gen_factory: Unknown\n2026/01/04-00:35:27.839503 12464                 Options.best_efforts_recovery: 0\n2026/01/04-00:35:27.839507 12464                Options.max_bgerror_resume_count: 2147483647\n2026/01/04-00:35:27.839511 12464            Options.bgerror_resume_retry_interval: 1000000\n2026/01/04-00:35:27.839515 12464             Options.allow_data_in_errors: 0\n2026/01/04-00:35:27.839519 12464             Options.db_host_id: __hostname__\n2026/01/04-00:35:27.839523 12464             Options.enforce_single_del_contracts: true\n2026/01/04-00:35:27.839527 12464             Options.max_background_jobs: 6\n2026/01/04-00:35:27.839531 12464             Options.max_background_compactions: -1\n2026/01/04-00:35:27.839535 12464             Options.max_subcompactions: 1\n2026/01/04-00:35:27.839539 12464             Options.avoid_flush_during_shutdown: 0\n2026/01/04-00:35:27.839543 12464           Options.writable_file_max_buffer_size: 1048576\n2026/01/04-00:35:27.839547 12464             Options.delayed_write_rate : 16777216\n2026/01/04-00:35:27.839551 12464             Options.max_total_wal_size: 0\n2026/01/04-00:35:27.839555 12464             Options.delete_obsolete_files_period_micros: 21600000000\n2026/01/04-00:35:27.839559 12464                   Options.stats_dump_period_sec: 600\n2026/01/04-00:35:27.839563 12464                 Options.stats_persist_period_sec: 600\n2026/01/04-00:35:27.839567 12464                 Options.stats_history_buffer_size: 1048576\n2026/01/04-00:35:27.839571 12464                          Options.max_open_files: -1\n2026/01/04-00:35:27.839575 12464                          Options.bytes_per_sync: 1048576\n2026/01/04-00:35:27.839579 12464                      Options.wal_bytes_per_sync: 0\n2026/01/04-00:35:27.839582 12464                   Options.strict_bytes_per_sync: 0\n2026/01/04-00:35:27.839586 12464       Options.compaction_readahead_size: 2097152\n2026/01/04-00:35:27.839590 12464                  Options.max_background_flushes: -1\n2026/01/04-00:35:27.839594 12464 Options.daily_offpeak_time_utc: \n2026/01/04-00:35:27.839598 12464 Compression algorithms supported:\n2026/01/04-00:35:27.839606 12464 \tkZSTD supported: 1\n2026/01/04-00:35:27.839610 12464 \tkSnappyCompression supported: 0\n2026/01/04-00:35:27.839614 12464 \tkBZip2Compression supported: 0\n2026/01/04-00:35:27.839618 12464 \tkZlibCompression supported: 0\n2026/01/04-00:35:27.839622 12464 \tkLZ4Compression supported: 1\n2026/01/04-00:35:27.839626 12464 \tkXpressCompression supported: 0\n2026/01/04-00:35:27.839630 12464 \tkLZ4HCCompression supported: 1\n2026/01/04-00:35:27.839652 12464 \tkZSTDNotFinalCompression supported: 1\n2026/01/04-00:35:27.839658 12464 Fast CRC32 supported: Not supported on x86\n2026/01/04-00:35:27.839662 12464 DMutex implementation: std::mutex\n2026/01/04-00:35:27.843806 12464 [db/db_impl/db_impl_open.cc:325] Creating manifest 1 \n2026/01/04-00:35:27.849816 12464 [db/version_set.cc:5906] Recovering from manifest file: ../context.db\\data/MANIFEST-000001\n2026/01/04-00:35:27.850026 12464 [db/column_family.cc:618] --------------- Options for column family [default]:\n2026/01/04-00:35:27.850037 12464               Options.comparator: leveldb.BytewiseComparator\n2026/01/04-00:35:27.850042 12464           Options.merge_operator: None\n2026/01/04-00:35:27.850046 12464        Options.compaction_filter: None\n2026/01/04-00:35:27.850049 12464        Options.compaction_filter_factory: None\n2026/01/04-00:35:27.850053 12464  Options.sst_partitioner_factory: None\n2026/01/04-00:35:27.850057 12464         Options.memtable_factory: SkipListFactory\n2026/01/04-00:35:27.850061 12464            Options.table_factory: BlockBasedTable\n2026/01/04-00:35:27.850100 12464            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (000001A8C97A6480)\n  cache_index_and_filter_blocks: 0\n  cache_index_and_filter_blocks_with_high_priority: 1\n  pin_l0_filter_and_index_blocks_in_cache: 0\n  pin_top_level_index_and_filter: 1\n  index_type: 0\n  data_block_index_type: 0\n  index_shortening: 1\n  data_block_hash_table_util_ratio: 0.750000\n  checksum: 4\n  no_block_cache: 0\n  block_cache: 000001A8C964FC30\n  block_cache_name: LRUCache\n  block_cache_options:\n    capacity : 33554432\n    num_shard_bits : 6\n    strict_capacity_limit : 0\n    memory_allocator : None\n    high_pri_pool_ratio: 0.500\n    low_pri_pool_ratio: 0.000\n  persistent_cache: 0000000000000000\n  block_size: 4096\n  block_size_deviation: 10\n  block_restart_interval: 16\n  index_block_restart_interval: 1\n  metadata_block_size: 4096\n  partition_filters: 0\n  use_delta_encoding: 1\n  filter_policy: bloomfilter\n  whole_key_filtering: 1\n  verify_compression: 0\n  read_amp_bytes_per_bit: 0\n  format_version: 5\n  enable_index_compression: 1\n  block_align: 0\n  max_auto_readahead_size: 262144\n  prepopulate_block_cache: 0\n  initial_auto_readahead_size: 8192\n  num_file_reads_for_auto_readahead: 2\n2026/01/04-00:35:27.850105 12464        Options.write_buffer_size: 67108864\n2026/01/04-00:35:27.850109 12464  Options.max_write_buffer_number: 2\n2026/01/04-00:35:27.850113 12464          Options.compression: LZ4\n2026/01/04-00:35:27.850117 12464                  Options.bottommost_compression: ZSTD\n2026/01/04-00:35:27.850121 12464       Options.prefix_extractor: rocksdb.CappedPrefix\n2026/01/04-00:35:27.850125 12464   Options.memtable_insert_with_hint_prefix_extractor: nullptr\n2026/01/04-00:35:27.850129 12464             Options.num_levels: 7\n2026/01/04-00:35:27.850133 12464        Options.min_write_buffer_number_to_merge: 1\n2026/01/04-00:35:27.850137 12464     Options.max_write_buffer_number_to_maintain: 0\n2026/01/04-00:35:27.850141 12464     Options.max_write_buffer_size_to_maintain: 134217728\n2026/01/04-00:35:27.850145 12464            Options.bottommost_compression_opts.window_bits: -14\n2026/01/04-00:35:27.850149 12464                  Options.bottommost_compression_opts.level: 32767\n2026/01/04-00:35:27.850152 12464               Options.bottommost_compression_opts.strategy: 0\n2026/01/04-00:35:27.850156 12464         Options.bottommost_compression_opts.max_dict_bytes: 0\n2026/01/04-00:35:27.850160 12464         Options.bottommost_compression_opts.zstd_max_train_bytes: 0\n2026/01/04-00:35:27.850164 12464         Options.bottommost_compression_opts.parallel_threads: 1\n2026/01/04-00:35:27.850168 12464                  Options.bottommost_compression_opts.enabled: false\n2026/01/04-00:35:27.850172 12464         Options.bottommost_compression_opts.max_dict_buffer_bytes: 0\n2026/01/04-00:35:27.850176 12464         Options.bottommost_compression_opts.use_zstd_dict_trainer: true\n2026/01/04-00:35:27.850180 12464            Options.compression_opts.window_bits: -14\n2026/01/04-00:35:27.850187 12464                  Options.compression_opts.level: 32767\n2026/01/04-00:35:27.850192 12464               Options.compression_opts.strategy: 0\n2026/01/04-00:35:27.850196 12464         Options.compression_opts.max_dict_bytes: 0\n2026/01/04-00:35:27.850200 12464         Options.compression_opts.zstd_max_train_bytes: 0\n2026/01/04-00:35:27.850204 12464         Options.compression_opts.use_zstd_dict_trainer: true\n2026/01/04-00:35:27.850208 12464         Options.compression_opts.parallel_threads: 1\n2026/01/04-00:35:27.850212 12464                  Options.compression_opts.enabled: false\n2026/01/04-00:35:27.850216 12464         Options.compression_opts.max_dict_buffer_bytes: 0\n2026/01/04-00:35:27.850220 12464      Options.level0_file_num_compaction_trigger: 4\n2026/01/04-00:35:27.850223 12464          Options.level0_slowdown_writes_trigger: 20\n2026/01/04-00:35:27.850227 12464              Options.level0_stop_writes_trigger: 36\n2026/01/04-00:35:27.850231 12464                   Options.target_file_size_base: 67108864\n2026/01/04-00:35:27.850235 12464             Options.target_file_size_multiplier: 1\n2026/01/04-00:35:27.850239 12464                Options.max_bytes_for_level_base: 268435456\n2026/01/04-00:35:27.850243 12464 Options.level_compaction_dynamic_level_bytes: 1\n2026/01/04-00:35:27.850246 12464          Options.max_bytes_for_level_multiplier: 10.000000\n2026/01/04-00:35:27.850251 12464 Options.max_bytes_for_level_multiplier_addtl[0]: 1\n2026/01/04-00:35:27.850255 12464 Options.max_bytes_for_level_multiplier_addtl[1]: 1\n2026/01/04-00:35:27.850258 12464 Options.max_bytes_for_level_multiplier_addtl[2]: 1\n2026/01/04-00:35:27.850262 12464 Options.max_bytes_for_level_multiplier_addtl[3]: 1\n2026/01/04-00:35:27.850266 12464 Options.max_bytes_for_level_multiplier_addtl[4]: 1\n2026/01/04-00:35:27.850270 12464 Options.max_bytes_for_level_multiplier_addtl[5]: 1\n2026/01/04-00:35:27.850274 12464 Options.max_bytes_for_level_multiplier_addtl[6]: 1\n2026/01/04-00:35:27.850278 12464       Options.max_sequential_skip_in_iterations: 8\n2026/01/04-00:35:27.850282 12464                    Options.max_compaction_bytes: 1677721600\n2026/01/04-00:35:27.850297 12464   Options.ignore_max_compaction_bytes_for_input: true\n2026/01/04-00:35:27.850301 12464                        Options.arena_block_size: 1048576\n2026/01/04-00:35:27.850305 12464   Options.soft_pending_compaction_bytes_limit: 68719476736\n2026/01/04-00:35:27.850309 12464   Options.hard_pending_compaction_bytes_limit: 274877906944\n2026/01/04-00:35:27.850313 12464                Options.disable_auto_compactions: 1\n2026/01/04-00:35:27.850318 12464                        Options.compaction_style: kCompactionStyleLevel\n2026/01/04-00:35:27.850322 12464                          Options.compaction_pri: kMinOverlappingRatio\n2026/01/04-00:35:27.850326 12464 Options.compaction_options_universal.size_ratio: 1\n2026/01/04-00:35:27.850330 12464 Options.compaction_options_universal.min_merge_width: 2\n2026/01/04-00:35:27.850336 12464 Options.compaction_options_universal.max_merge_width: 4294967295\n2026/01/04-00:35:27.850340 12464 Options.compaction_options_universal.max_size_amplification_percent: 200\n2026/01/04-00:35:27.850344 12464 Options.compaction_options_universal.compression_size_percent: -1\n2026/01/04-00:35:27.850348 12464 Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize\n2026/01/04-00:35:27.850352 12464 Options.compaction_options_fifo.max_table_files_size: 1073741824\n2026/01/04-00:35:27.850356 12464 Options.compaction_options_fifo.allow_compaction: 0\n2026/01/04-00:35:27.850362 12464                   Options.table_properties_collectors: \n2026/01/04-00:35:27.850366 12464                   Options.inplace_update_support: 0\n2026/01/04-00:35:27.850369 12464                 Options.inplace_update_num_locks: 10000\n2026/01/04-00:35:27.850373 12464               Options.memtable_prefix_bloom_size_ratio: 0.000000\n2026/01/04-00:35:27.850377 12464               Options.memtable_whole_key_filtering: 0\n2026/01/04-00:35:27.850416 12464   Options.memtable_huge_page_size: 0\n2026/01/04-00:35:27.850421 12464                           Options.bloom_locality: 0\n2026/01/04-00:35:27.850425 12464                    Options.max_successive_merges: 0\n2026/01/04-00:35:27.850429 12464                Options.optimize_filters_for_hits: 0\n2026/01/04-00:35:27.850433 12464                Options.paranoid_file_checks: 0\n2026/01/04-00:35:27.850437 12464                Options.force_consistency_checks: 1\n2026/01/04-00:35:27.850440 12464                Options.report_bg_io_stats: 0\n2026/01/04-00:35:27.850444 12464                               Options.ttl: 2592000\n2026/01/04-00:35:27.850448 12464          Options.periodic_compaction_seconds: 0\n2026/01/04-00:35:27.850452 12464                        Options.default_temperature: kUnknown\n2026/01/04-00:35:27.850456 12464  Options.preclude_last_level_data_seconds: 0\n2026/01/04-00:35:27.850460 12464    Options.preserve_internal_time_seconds: 0\n2026/01/04-00:35:27.850464 12464                       Options.enable_blob_files: false\n2026/01/04-00:35:27.850467 12464                           Options.min_blob_size: 0\n2026/01/04-00:35:27.850471 12464                          Options.blob_file_size: 268435456\n2026/01/04-00:35:27.850475 12464                   Options.blob_compression_type: NoCompression\n2026/01/04-00:35:27.850479 12464          Options.enable_blob_garbage_collection: false\n2026/01/04-00:35:27.850483 12464      Options.blob_garbage_collection_age_cutoff: 0.250000\n2026/01/04-00:35:27.850487 12464 Options.blob_garbage_collection_force_threshold: 1.000000\n2026/01/04-00:35:27.850491 12464          Options.blob_compaction_readahead_size: 0\n2026/01/04-00:35:27.850495 12464                Options.blob_file_starting_level: 0\n2026/01/04-00:35:27.850499 12464         Options.experimental_mempurge_threshold: 0.000000\n2026/01/04-00:35:27.850503 12464            Options.memtable_max_range_deletions: 0\n2026/01/04-00:35:27.852116 12464 [db/version_set.cc:5957] Recovered from manifest file:../context.db\\data/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0\n2026/01/04-00:35:27.852136 12464 [db/version_set.cc:5966] Column family [default] (ID 0), log number is 0\n2026/01/04-00:35:27.852430 12464 [db/db_impl/db_impl_open.cc:646] DB ID: f0113401-e93f-11f0-96b3-401a58c3d854\n2026/01/04-00:35:27.853536 12464 [db/version_set.cc:5417] Creating manifest 5\n2026/01/04-00:35:27.863942 12464 [db/db_impl/db_impl_open.cc:2150] SstFileManager instance 000001A8C97F07C0\n2026/01/04-00:35:27.864619 12464 DB pointer 000001A8C97BF180\n2026/01/04-00:35:27.864709 12464 [WARN] [utilities/transactions/pessimistic_transaction_db.cc:262] Transaction write_policy is 0\n2026/01/04-00:35:27.865134 130cc [db/db_impl/db_impl.cc:1141] ------- DUMPING STATS -------\n2026/01/04-00:35:27.865153 130cc [db/db_impl/db_impl.cc:1142] \n** DB Stats **\nUptime(secs): 0.0 total, 0.0 interval\nCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\nWrite Stall (count): write-buffer-manager-limit-stops: 0\n\n** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\n** Compaction Stats [default] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nBlob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0\n\nUptime(secs): 0.0 total, 0.0 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nWrite Stall (count): cf-l0-file-count-limit-delays-with-ongoing-compaction: 0, cf-l0-file-count-limit-stops-with-ongoing-compaction: 0, l0-file-count-limit-delays: 0, l0-file-count-limit-stops: 0, memtable-limit-delays: 0, memtable-limit-stops: 0, pending-compaction-bytes-delays: 0, pending-compaction-bytes-stops: 0, total-delays: 0, total-stops: 0\nBlock cache LRUCache@000001A8C964FC30#79376 capacity: 32.00 MB seed: 142953577 usage: 0.08 KB table_size: 1024 occupancy: 1 collections: 1 last_copies: 0 last_secs: 7.6e-05 secs_since: 0\nBlock cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)\n\n** File Read Latency Histogram By Level [default] **\n2026/01/04-00:35:27.870676 12464 [db/db_impl/db_impl.cc:1245] SetOptions() on column family [default], inputs:\n2026/01/04-00:35:27.870693 12464 [db/db_impl/db_impl.cc:1248] disable_auto_compactions: false\n2026/01/04-00:35:27.870698 12464 [db/db_impl/db_impl.cc:1252] [default] SetOptions() succeeded\n2026/01/04-00:35:27.870702 12464 [options/cf_options.cc:1054]                         write_buffer_size: 67108864\n2026/01/04-00:35:27.870707 12464 [options/cf_options.cc:1056]                   max_write_buffer_number: 2\n2026/01/04-00:35:27.870711 12464 [options/cf_options.cc:1059]                          arena_block_size: 1048576\n2026/01/04-00:35:27.870715 12464 [options/cf_options.cc:1061]               memtable_prefix_bloom_ratio: 0.000000\n2026/01/04-00:35:27.870720 12464 [options/cf_options.cc:1063]               memtable_whole_key_filtering: 0\n2026/01/04-00:35:27.870724 12464 [options/cf_options.cc:1066]                   memtable_huge_page_size: 0\n2026/01/04-00:35:27.870728 12464 [options/cf_options.cc:1069]                     max_successive_merges: 0\n2026/01/04-00:35:27.870732 12464 [options/cf_options.cc:1072]                  inplace_update_num_locks: 10000\n2026/01/04-00:35:27.870736 12464 [options/cf_options.cc:1076]                          prefix_extractor: rocksdb.CappedPrefix.9\n2026/01/04-00:35:27.870740 12464 [options/cf_options.cc:1078]                  disable_auto_compactions: 0\n2026/01/04-00:35:27.870744 12464 [options/cf_options.cc:1080]       soft_pending_compaction_bytes_limit: 68719476736\n2026/01/04-00:35:27.870748 12464 [options/cf_options.cc:1082]       hard_pending_compaction_bytes_limit: 274877906944\n2026/01/04-00:35:27.870752 12464 [options/cf_options.cc:1084]        level0_file_num_compaction_trigger: 4\n2026/01/04-00:35:27.870756 12464 [options/cf_options.cc:1086]            level0_slowdown_writes_trigger: 20\n2026/01/04-00:35:27.870760 12464 [options/cf_options.cc:1088]                level0_stop_writes_trigger: 36\n2026/01/04-00:35:27.870764 12464 [options/cf_options.cc:1090]                      max_compaction_bytes: 1677721600\n2026/01/04-00:35:27.870768 12464 [options/cf_options.cc:1092]     ignore_max_compaction_bytes_for_input: true\n2026/01/04-00:35:27.870776 12464 [options/cf_options.cc:1094]                     target_file_size_base: 67108864\n2026/01/04-00:35:27.870780 12464 [options/cf_options.cc:1096]               target_file_size_multiplier: 1\n2026/01/04-00:35:27.870784 12464 [options/cf_options.cc:1098]                  max_bytes_for_level_base: 268435456\n2026/01/04-00:35:27.870789 12464 [options/cf_options.cc:1100]            max_bytes_for_level_multiplier: 10.000000\n2026/01/04-00:35:27.870793 12464 [options/cf_options.cc:1102]                                       ttl: 2592000\n2026/01/04-00:35:27.870797 12464 [options/cf_options.cc:1104]               periodic_compaction_seconds: 0\n2026/01/04-00:35:27.870802 12464 [options/cf_options.cc:1118] max_bytes_for_level_multiplier_additional: 1, 1, 1, 1, 1, 1, 1\n2026/01/04-00:35:27.870806 12464 [options/cf_options.cc:1120]         max_sequential_skip_in_iterations: 8\n2026/01/04-00:35:27.870810 12464 [options/cf_options.cc:1122]          check_flush_compaction_key_order: 1\n2026/01/04-00:35:27.870814 12464 [options/cf_options.cc:1124]                      paranoid_file_checks: 0\n2026/01/04-00:35:27.870818 12464 [options/cf_options.cc:1126]                        report_bg_io_stats: 0\n2026/01/04-00:35:27.870822 12464 [options/cf_options.cc:1128]                               compression: 4\n2026/01/04-00:35:27.870826 12464 [options/cf_options.cc:1131]                        experimental_mempurge_threshold: 0.000000\n2026/01/04-00:35:27.870830 12464 [options/cf_options.cc:1133]          bottommost_file_compaction_delay: 0\n2026/01/04-00:35:27.870834 12464 [options/cf_options.cc:1137] compaction_options_universal.size_ratio : 1\n2026/01/04-00:35:27.870838 12464 [options/cf_options.cc:1139] compaction_options_universal.min_merge_width : 2\n2026/01/04-00:35:27.870842 12464 [options/cf_options.cc:1141] compaction_options_universal.max_merge_width : -1\n2026/01/04-00:35:27.870846 12464 [options/cf_options.cc:1144] compaction_options_universal.max_size_amplification_percent : 200\n2026/01/04-00:35:27.870850 12464 [options/cf_options.cc:1147] compaction_options_universal.compression_size_percent : -1\n2026/01/04-00:35:27.870854 12464 [options/cf_options.cc:1149] compaction_options_universal.stop_style : 1\n2026/01/04-00:35:27.870858 12464 [options/cf_options.cc:1152] compaction_options_universal.allow_trivial_move : 0\n2026/01/04-00:35:27.870861 12464 [options/cf_options.cc:1154] compaction_options_universal.incremental        : 0\n2026/01/04-00:35:27.870865 12464 [options/cf_options.cc:1158] compaction_options_fifo.max_table_files_size : 1073741824\n2026/01/04-00:35:27.870869 12464 [options/cf_options.cc:1160] compaction_options_fifo.allow_compaction : 0\n2026/01/04-00:35:27.870873 12464 [options/cf_options.cc:1164]                         enable_blob_files: false\n2026/01/04-00:35:27.870877 12464 [options/cf_options.cc:1166]                             min_blob_size: 0\n2026/01/04-00:35:27.870881 12464 [options/cf_options.cc:1168]                            blob_file_size: 268435456\n2026/01/04-00:35:27.870885 12464 [options/cf_options.cc:1170]                     blob_compression_type: NoCompression\n2026/01/04-00:35:27.870889 12464 [options/cf_options.cc:1172]            enable_blob_garbage_collection: false\n2026/01/04-00:35:27.870892 12464 [options/cf_options.cc:1174]        blob_garbage_collection_age_cutoff: 0.250000\n2026/01/04-00:35:27.870897 12464 [options/cf_options.cc:1176]   blob_garbage_collection_force_threshold: 1.000000\n2026/01/04-00:35:27.870901 12464 [options/cf_options.cc:1178]            blob_compaction_readahead_size: 0\n2026/01/04-00:35:27.870905 12464 [options/cf_options.cc:1180]                  blob_file_starting_level: 0\n2026/01/04-00:35:27.870909 12464 [options/cf_options.cc:1184]                    prepopulate_blob_cache: disable\n2026/01/04-00:35:27.870913 12464 [options/cf_options.cc:1186]                    last_level_temperature: 0\n2026/01/04-00:45:27.869230 130cc [db/db_impl/db_impl.cc:1141] ------- DUMPING STATS -------\n2026/01/04-00:45:27.869264 130cc [db/db_impl/db_impl.cc:1142] \n** DB Stats **\nUptime(secs): 600.0 total, 600.0 interval\nCumulative writes: 6 writes, 18 keys, 6 commit groups, 1.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 6 writes, 0 syncs, 6.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 6 writes, 18 keys, 6 commit groups, 1.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 6 writes, 0 syncs, 6.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\nWrite Stall (count): write-buffer-manager-limit-stops: 0\n\n** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n\n** Compaction Stats [default] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nBlob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB, space amp: 0.0\n\nUptime(secs): 600.0 total, 600.0 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nWrite Stall (count): cf-l0-file-count-limit-delays-with-ongoing-compaction: 0, cf-l0-file-count-limit-stops-with-ongoing-compaction: 0, l0-file-count-limit-delays: 0, l0-file-count-limit-stops: 0, memtable-limit-delays: 0, memtable-limit-stops: 0, pending-compaction-bytes-delays: 0, pending-compaction-bytes-stops: 0, total-delays: 0, total-stops: 0\nBlock cache LRUCache@000001A8C964FC30#79376 capacity: 32.00 MB seed: 142953577 usage: 0.08 KB table_size: 1024 occupancy: 1 collections: 2 last_copies: 0 last_secs: 8e-05 secs_since: 0\nBlock cache entry stats(count,size,portion): Misc(1,0.00 KB,0%)\n\n** File Read Latency Histogram By Level [default] **\n"
    size: 31688
    tokens: 12314
  - path: context.db\data\MANIFEST-000005
    content: "\x05+(C\0\0\x015@?� \0\x01\x01\x1Aleveldb.BytewiseComparator�@\x01\x01�\x03\fZ\x04\0\x01\x02\0\x04\0�,&�\x06\0\x01\t\0\x03\x06\x04\0�|�i\x06\0\x01\t\0\x03\b\x04\0"
    size: 83
    tokens: 26
  - path: context.db\data\OPTIONS-000007
    content: |
      # This is a RocksDB option file.
      #
      # For detailed file format spec, please refer to the example file
      # in examples/rocksdb_option_file_example.ini
      #

      [Version]
        rocksdb_version=8.8.1
        options_file_version=1.1

      [DBOptions]
        delayed_write_rate=16777216
        delete_obsolete_files_period_micros=21600000000
        writable_file_max_buffer_size=1048576
        max_background_compactions=-1
        max_background_jobs=6
        max_subcompactions=1
        avoid_flush_during_shutdown=false
        max_total_wal_size=0
        stats_dump_period_sec=600
        max_background_flushes=-1
        stats_persist_period_sec=600
        stats_history_buffer_size=1048576
        max_open_files=-1
        bytes_per_sync=1048576
        wal_bytes_per_sync=0
        strict_bytes_per_sync=false
        compaction_readahead_size=2097152
        allow_fallocate=true
        advise_random_on_open=true
        dump_malloc_stats=false
        track_and_verify_wals_in_manifest=false
        WAL_ttl_seconds=0
        use_direct_reads=false
        allow_2pc=true
        allow_mmap_reads=false
        random_access_max_buffer_size=1048576
        allow_mmap_writes=false
        wal_compression=kNoCompression
        two_write_queues=false
        use_direct_io_for_flush_and_compaction=false
        skip_stats_update_on_db_open=false
        fail_if_options_file_error=true
        max_manifest_file_size=1073741824
        wal_filter=nullptr
        create_if_missing=true
        error_if_exists=false
        create_missing_column_families=true
        compaction_verify_record_count=true
        enable_thread_tracking=false
        use_fsync=false
        log_file_time_to_roll=0
        keep_log_file_num=1000
        is_fd_close_on_exec=true
        paranoid_checks=true
        flush_verify_memtable_count=true
        info_log_level=INFO_LEVEL
        verify_sst_unique_id_in_manifest=true
        skip_checking_sst_file_sizes_on_db_open=false
        enable_pipelined_write=false
        use_adaptive_mutex=false
        max_log_file_size=0
        max_file_opening_threads=16
        table_cache_numshardbits=6
        max_write_batch_group_size_bytes=1048576
        db_write_buffer_size=0
        recycle_log_file_num=0
        manifest_preallocation_size=4194304
        write_thread_slow_yield_usec=3
        unordered_write=false
        WAL_size_limit_MB=0
        persist_stats_to_disk=false
        allow_concurrent_memtable_write=true
        wal_recovery_mode=kPointInTimeRecovery
        enable_write_thread_adaptive_yield=true
        write_thread_max_yield_usec=100
        access_hint_on_compaction_start=NORMAL
        avoid_flush_during_recovery=false
        allow_ingest_behind=false
        manual_wal_flush=false
        atomic_flush=false
        enforce_single_del_contracts=true
        avoid_unnecessary_blocking_io=false
        write_dbid_to_manifest=false
        log_readahead_size=0
        best_efforts_recovery=false
        max_bgerror_resume_count=2147483647
        bgerror_resume_retry_interval=1000000
        db_host_id=__hostname__
        allow_data_in_errors=false
        file_checksum_gen_factory=nullptr
        lowest_used_cache_tier=kNonVolatileBlockTier
        

      [CFOptions "default"]
        blob_compression_type=kNoCompression
        blob_compaction_readahead_size=0
        hard_pending_compaction_bytes_limit=274877906944
        level0_file_num_compaction_trigger=4
        experimental_mempurge_threshold=0.000000
        max_bytes_for_level_base=268435456
        report_bg_io_stats=false
        max_bytes_for_level_multiplier=10.000000
        disable_auto_compactions=true
        check_flush_compaction_key_order=true
        enable_blob_files=false
        paranoid_file_checks=false
        blob_file_starting_level=0
        blob_file_size=268435456
        soft_pending_compaction_bytes_limit=68719476736
        bottommost_compression_opts={enabled=false;max_dict_bytes=0;window_bits=-14;level=32767;parallel_threads=1;strategy=0;max_compressed_bytes_per_kb=896;zstd_max_train_bytes=0;max_dict_buffer_bytes=0;use_zstd_dict_trainer=true;checksum=false;}
        max_compaction_bytes=1677721600
        ignore_max_compaction_bytes_for_input=true
        max_sequential_skip_in_iterations=8
        level0_slowdown_writes_trigger=20
        level0_stop_writes_trigger=36
        max_write_buffer_number=2
        target_file_size_multiplier=1
        prefix_extractor=rocksdb.CappedPrefix.9
        arena_block_size=1048576
        prepopulate_blob_cache=kDisable
        inplace_update_num_locks=10000
        max_successive_merges=0
        memtable_huge_page_size=0
        write_buffer_size=67108864
        enable_blob_garbage_collection=false
        memtable_prefix_bloom_size_ratio=0.000000
        memtable_whole_key_filtering=false
        max_bytes_for_level_multiplier_additional=1:1:1:1:1:1:1
        target_file_size_base=67108864
        min_blob_size=0
        compression=kLZ4Compression
        compaction_options_fifo={allow_compaction=false;max_table_files_size=1073741824;age_for_warm=0;file_temperature_age_thresholds=;}
        compaction_options_universal={allow_trivial_move=false;max_size_amplification_percent=200;size_ratio=1;incremental=false;stop_style=kCompactionStopStyleTotalSize;min_merge_width=2;compression_size_percent=-1;max_merge_width=4294967295;}
        ttl=2592000
        periodic_compaction_seconds=0
        last_level_temperature=kUnknown
        blob_garbage_collection_age_cutoff=0.250000
        blob_garbage_collection_force_threshold=1.000000
        sample_for_compression=0
        bottommost_compression=kZSTD
        memtable_protection_bytes_per_key=0
        compression_opts={enabled=false;max_dict_bytes=0;window_bits=-14;level=32767;parallel_threads=1;strategy=0;max_compressed_bytes_per_kb=896;zstd_max_train_bytes=0;max_dict_buffer_bytes=0;use_zstd_dict_trainer=true;checksum=false;}
        bottommost_file_compaction_delay=0
        block_protection_bytes_per_key=0
        memtable_max_range_deletions=0
        bloom_locality=0
        level_compaction_dynamic_file_size=true
        merge_operator=nullptr
        preclude_last_level_data_seconds=0
        level_compaction_dynamic_level_bytes=true
        num_levels=7
        inplace_update_support=false
        min_write_buffer_number_to_merge=1
        optimize_filters_for_hits=false
        force_consistency_checks=true
        default_temperature=kUnknown
        compaction_filter=nullptr
        preserve_internal_time_seconds=0
        max_write_buffer_number_to_maintain=0
        max_write_buffer_size_to_maintain=134217728
        comparator=leveldb.BytewiseComparator
        memtable_insert_with_hint_prefix_extractor=nullptr
        memtable_factory=SkipListFactory
        table_factory=BlockBasedTable
        compaction_filter_factory=nullptr
        compaction_style=kCompactionStyleLevel
        compaction_pri=kMinOverlappingRatio
        sst_partitioner_factory=nullptr
        persist_user_defined_timestamps=true
        
      [TableOptions/BlockBasedTable "default"]
        pin_top_level_index_and_filter=true
        flush_block_policy_factory=FlushBlockBySizePolicyFactory
        cache_index_and_filter_blocks=false
        cache_index_and_filter_blocks_with_high_priority=true
        index_shortening=kShortenSeparators
        pin_l0_filter_and_index_blocks_in_cache=false
        index_type=kBinarySearch
        data_block_index_type=kDataBlockBinarySearch
        data_block_hash_table_util_ratio=0.750000
        checksum=kXXH3
        no_block_cache=false
        block_size=4096
        block_size_deviation=10
        block_restart_interval=16
        index_block_restart_interval=1
        metadata_block_size=4096
        partition_filters=false
        optimize_filters_for_memory=false
        filter_policy=bloomfilter:9.9:false
        whole_key_filtering=true
        verify_compression=false
        detect_filter_construct_corruption=false
        num_file_reads_for_auto_readahead=2
        format_version=5
        read_amp_bytes_per_bit=0
        block_align=false
        enable_index_compression=true
        metadata_cache_options={top_level_index_pinning=kFallback;unpartitioned_pinning=kFallback;partition_pinning=kFallback;}
        max_auto_readahead_size=262144
        prepopulate_block_cache=kDisable
        initial_auto_readahead_size=8192
        
    size: 7267
    tokens: 2311
  - path: context.db\data\OPTIONS-000009
    content: |
      # This is a RocksDB option file.
      #
      # For detailed file format spec, please refer to the example file
      # in examples/rocksdb_option_file_example.ini
      #

      [Version]
        rocksdb_version=8.8.1
        options_file_version=1.1

      [DBOptions]
        delayed_write_rate=16777216
        delete_obsolete_files_period_micros=21600000000
        writable_file_max_buffer_size=1048576
        max_background_compactions=-1
        max_background_jobs=6
        max_subcompactions=1
        avoid_flush_during_shutdown=false
        max_total_wal_size=0
        stats_dump_period_sec=600
        max_background_flushes=-1
        stats_persist_period_sec=600
        stats_history_buffer_size=1048576
        max_open_files=-1
        bytes_per_sync=1048576
        wal_bytes_per_sync=0
        strict_bytes_per_sync=false
        compaction_readahead_size=2097152
        allow_fallocate=true
        advise_random_on_open=true
        dump_malloc_stats=false
        track_and_verify_wals_in_manifest=false
        WAL_ttl_seconds=0
        use_direct_reads=false
        allow_2pc=true
        allow_mmap_reads=false
        random_access_max_buffer_size=1048576
        allow_mmap_writes=false
        wal_compression=kNoCompression
        two_write_queues=false
        use_direct_io_for_flush_and_compaction=false
        skip_stats_update_on_db_open=false
        fail_if_options_file_error=true
        max_manifest_file_size=1073741824
        wal_filter=nullptr
        create_if_missing=true
        error_if_exists=false
        create_missing_column_families=true
        compaction_verify_record_count=true
        enable_thread_tracking=false
        use_fsync=false
        log_file_time_to_roll=0
        keep_log_file_num=1000
        is_fd_close_on_exec=true
        paranoid_checks=true
        flush_verify_memtable_count=true
        info_log_level=INFO_LEVEL
        verify_sst_unique_id_in_manifest=true
        skip_checking_sst_file_sizes_on_db_open=false
        enable_pipelined_write=false
        use_adaptive_mutex=false
        max_log_file_size=0
        max_file_opening_threads=16
        table_cache_numshardbits=6
        max_write_batch_group_size_bytes=1048576
        db_write_buffer_size=0
        recycle_log_file_num=0
        manifest_preallocation_size=4194304
        write_thread_slow_yield_usec=3
        unordered_write=false
        WAL_size_limit_MB=0
        persist_stats_to_disk=false
        allow_concurrent_memtable_write=true
        wal_recovery_mode=kPointInTimeRecovery
        enable_write_thread_adaptive_yield=true
        write_thread_max_yield_usec=100
        access_hint_on_compaction_start=NORMAL
        avoid_flush_during_recovery=false
        allow_ingest_behind=false
        manual_wal_flush=false
        atomic_flush=false
        enforce_single_del_contracts=true
        avoid_unnecessary_blocking_io=false
        write_dbid_to_manifest=false
        log_readahead_size=0
        best_efforts_recovery=false
        max_bgerror_resume_count=2147483647
        bgerror_resume_retry_interval=1000000
        db_host_id=__hostname__
        allow_data_in_errors=false
        file_checksum_gen_factory=nullptr
        lowest_used_cache_tier=kNonVolatileBlockTier
        

      [CFOptions "default"]
        blob_compression_type=kNoCompression
        blob_compaction_readahead_size=0
        hard_pending_compaction_bytes_limit=274877906944
        level0_file_num_compaction_trigger=4
        experimental_mempurge_threshold=0.000000
        max_bytes_for_level_base=268435456
        report_bg_io_stats=false
        max_bytes_for_level_multiplier=10.000000
        disable_auto_compactions=false
        check_flush_compaction_key_order=true
        enable_blob_files=false
        paranoid_file_checks=false
        blob_file_starting_level=0
        blob_file_size=268435456
        soft_pending_compaction_bytes_limit=68719476736
        bottommost_compression_opts={enabled=false;max_dict_bytes=0;window_bits=-14;level=32767;parallel_threads=1;strategy=0;max_compressed_bytes_per_kb=896;zstd_max_train_bytes=0;max_dict_buffer_bytes=0;use_zstd_dict_trainer=true;checksum=false;}
        max_compaction_bytes=1677721600
        ignore_max_compaction_bytes_for_input=true
        max_sequential_skip_in_iterations=8
        level0_slowdown_writes_trigger=20
        level0_stop_writes_trigger=36
        max_write_buffer_number=2
        target_file_size_multiplier=1
        prefix_extractor=rocksdb.CappedPrefix.9
        arena_block_size=1048576
        prepopulate_blob_cache=kDisable
        inplace_update_num_locks=10000
        max_successive_merges=0
        memtable_huge_page_size=0
        write_buffer_size=67108864
        enable_blob_garbage_collection=false
        memtable_prefix_bloom_size_ratio=0.000000
        memtable_whole_key_filtering=false
        max_bytes_for_level_multiplier_additional=1:1:1:1:1:1:1
        target_file_size_base=67108864
        min_blob_size=0
        compression=kLZ4Compression
        compaction_options_fifo={allow_compaction=false;max_table_files_size=1073741824;age_for_warm=0;file_temperature_age_thresholds=;}
        compaction_options_universal={allow_trivial_move=false;max_size_amplification_percent=200;size_ratio=1;incremental=false;stop_style=kCompactionStopStyleTotalSize;min_merge_width=2;compression_size_percent=-1;max_merge_width=4294967295;}
        ttl=2592000
        periodic_compaction_seconds=0
        last_level_temperature=kUnknown
        blob_garbage_collection_age_cutoff=0.250000
        blob_garbage_collection_force_threshold=1.000000
        sample_for_compression=0
        bottommost_compression=kZSTD
        memtable_protection_bytes_per_key=0
        compression_opts={enabled=false;max_dict_bytes=0;window_bits=-14;level=32767;parallel_threads=1;strategy=0;max_compressed_bytes_per_kb=896;zstd_max_train_bytes=0;max_dict_buffer_bytes=0;use_zstd_dict_trainer=true;checksum=false;}
        bottommost_file_compaction_delay=0
        block_protection_bytes_per_key=0
        memtable_max_range_deletions=0
        bloom_locality=0
        level_compaction_dynamic_file_size=true
        merge_operator=nullptr
        preclude_last_level_data_seconds=0
        level_compaction_dynamic_level_bytes=true
        num_levels=7
        inplace_update_support=false
        min_write_buffer_number_to_merge=1
        optimize_filters_for_hits=false
        force_consistency_checks=true
        default_temperature=kUnknown
        compaction_filter=nullptr
        preserve_internal_time_seconds=0
        max_write_buffer_number_to_maintain=0
        max_write_buffer_size_to_maintain=134217728
        comparator=leveldb.BytewiseComparator
        memtable_insert_with_hint_prefix_extractor=nullptr
        memtable_factory=SkipListFactory
        table_factory=BlockBasedTable
        compaction_filter_factory=nullptr
        compaction_style=kCompactionStyleLevel
        compaction_pri=kMinOverlappingRatio
        sst_partitioner_factory=nullptr
        persist_user_defined_timestamps=true
        
      [TableOptions/BlockBasedTable "default"]
        pin_top_level_index_and_filter=true
        flush_block_policy_factory=FlushBlockBySizePolicyFactory
        cache_index_and_filter_blocks=false
        cache_index_and_filter_blocks_with_high_priority=true
        index_shortening=kShortenSeparators
        pin_l0_filter_and_index_blocks_in_cache=false
        index_type=kBinarySearch
        data_block_index_type=kDataBlockBinarySearch
        data_block_hash_table_util_ratio=0.750000
        checksum=kXXH3
        no_block_cache=false
        block_size=4096
        block_size_deviation=10
        block_restart_interval=16
        index_block_restart_interval=1
        metadata_block_size=4096
        partition_filters=false
        optimize_filters_for_memory=false
        filter_policy=bloomfilter:9.9:false
        whole_key_filtering=true
        verify_compression=false
        detect_filter_construct_corruption=false
        num_file_reads_for_auto_readahead=2
        format_version=5
        read_amp_bytes_per_bit=0
        block_align=false
        enable_index_compression=true
        metadata_cache_options={top_level_index_pinning=kFallback;unpartitioned_pinning=kFallback;partition_pinning=kFallback;}
        max_auto_readahead_size=262144
        prepopulate_block_cache=kDisable
        initial_auto_readahead_size=8192
        
    size: 7268
    tokens: 2312
  - path: context.db\manifest
    content: "��storage_version\x03"
    size: 18
    tokens: 5
  - path: engine\context.db\manifest
    content: "��storage_version\x03"
    size: 18
    tokens: 5
  - path: engine\package-lock.json
    content: |
      {
        "name": "sovereign-context-engine",
        "version": "3.0.0",
        "lockfileVersion": 3,
        "requires": true,
        "packages": {
          "": {
            "name": "sovereign-context-engine",
            "version": "3.0.0",
            "dependencies": {
              "body-parser": "^1.20.2",
              "chokidar": "^3.6.0",
              "cors": "^2.8.5",
              "cozo-node": "^0.7.5",
              "express": "^4.18.2",
              "js-yaml": "^4.1.1"
            }
          },
          "node_modules/@mapbox/node-pre-gyp": {
            "version": "1.0.11",
            "resolved": "https://registry.npmjs.org/@mapbox/node-pre-gyp/-/node-pre-gyp-1.0.11.tgz",
            "integrity": "sha512-Yhlar6v9WQgUp/He7BdgzOz8lqMQ8sU+jkCq7Wx8Myc5YFJLbEe7lgui/V7G1qB1DJykHSGwreceSaD60Y0PUQ==",
            "license": "BSD-3-Clause",
            "dependencies": {
              "detect-libc": "^2.0.0",
              "https-proxy-agent": "^5.0.0",
              "make-dir": "^3.1.0",
              "node-fetch": "^2.6.7",
              "nopt": "^5.0.0",
              "npmlog": "^5.0.1",
              "rimraf": "^3.0.2",
              "semver": "^7.3.5",
              "tar": "^6.1.11"
            },
            "bin": {
              "node-pre-gyp": "bin/node-pre-gyp"
            }
          },
          "node_modules/abbrev": {
            "version": "1.1.1",
            "resolved": "https://registry.npmjs.org/abbrev/-/abbrev-1.1.1.tgz",
            "integrity": "sha512-nne9/IiQ/hzIhY6pdDnbBtz7DjPTKrY00P/zvPSm5pOFkl6xuGrGnXn/VtTNNfNtAfZ9/1RtehkszU9qcTii0Q==",
            "license": "ISC"
          },
          "node_modules/accepts": {
            "version": "1.3.8",
            "resolved": "https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz",
            "integrity": "sha512-PYAthTa2m2VKxuvSD3DPC/Gy+U+sOA1LAuT8mkmRuvw+NACSaeXEQ+NHcVF7rONl6qcaxV3Uuemwawk+7+SJLw==",
            "license": "MIT",
            "dependencies": {
              "mime-types": "~2.1.34",
              "negotiator": "0.6.3"
            },
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/agent-base": {
            "version": "6.0.2",
            "resolved": "https://registry.npmjs.org/agent-base/-/agent-base-6.0.2.tgz",
            "integrity": "sha512-RZNwNclF7+MS/8bDg70amg32dyeZGZxiDuQmZxKLAlQjr3jGyLx+4Kkk58UO7D2QdgFIQCovuSuZESne6RG6XQ==",
            "license": "MIT",
            "dependencies": {
              "debug": "4"
            },
            "engines": {
              "node": ">= 6.0.0"
            }
          },
          "node_modules/agent-base/node_modules/debug": {
            "version": "4.4.3",
            "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
            "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
            "license": "MIT",
            "dependencies": {
              "ms": "^2.1.3"
            },
            "engines": {
              "node": ">=6.0"
            },
            "peerDependenciesMeta": {
              "supports-color": {
                "optional": true
              }
            }
          },
          "node_modules/agent-base/node_modules/ms": {
            "version": "2.1.3",
            "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
            "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
            "license": "MIT"
          },
          "node_modules/ansi-regex": {
            "version": "5.0.1",
            "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
            "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
            "license": "MIT",
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/anymatch": {
            "version": "3.1.3",
            "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz",
            "integrity": "sha512-KMReFUr0B4t+D+OBkjR3KYqvocp2XaSzO55UcB6mgQMd3KbcE+mWTyvVV7D/zsdEbNnV6acZUutkiHQXvTr1Rw==",
            "license": "ISC",
            "dependencies": {
              "normalize-path": "^3.0.0",
              "picomatch": "^2.0.4"
            },
            "engines": {
              "node": ">= 8"
            }
          },
          "node_modules/aproba": {
            "version": "2.1.0",
            "resolved": "https://registry.npmjs.org/aproba/-/aproba-2.1.0.tgz",
            "integrity": "sha512-tLIEcj5GuR2RSTnxNKdkK0dJ/GrC7P38sUkiDmDuHfsHmbagTFAxDVIBltoklXEVIQ/f14IL8IMJ5pn9Hez1Ew==",
            "license": "ISC"
          },
          "node_modules/are-we-there-yet": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/are-we-there-yet/-/are-we-there-yet-2.0.0.tgz",
            "integrity": "sha512-Ci/qENmwHnsYo9xKIcUJN5LeDKdJ6R1Z1j9V/J5wyq8nh/mYPEpIKJbBZXtZjG04HiK7zV/p6Vs9952MrMeUIw==",
            "deprecated": "This package is no longer supported.",
            "license": "ISC",
            "dependencies": {
              "delegates": "^1.0.0",
              "readable-stream": "^3.6.0"
            },
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/argparse": {
            "version": "2.0.1",
            "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz",
            "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==",
            "license": "Python-2.0"
          },
          "node_modules/array-flatten": {
            "version": "1.1.1",
            "resolved": "https://registry.npmjs.org/array-flatten/-/array-flatten-1.1.1.tgz",
            "integrity": "sha512-PCVAQswWemu6UdxsDFFX/+gVeYqKAod3D3UVm91jHwynguOwAvYPhx8nNlM++NqRcK6CxxpUafjmhIdKiHibqg==",
            "license": "MIT"
          },
          "node_modules/balanced-match": {
            "version": "1.0.2",
            "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
            "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
            "license": "MIT"
          },
          "node_modules/binary-extensions": {
            "version": "2.3.0",
            "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.3.0.tgz",
            "integrity": "sha512-Ceh+7ox5qe7LJuLHoY0feh3pHuUDHAcRUeyL2VYghZwfpkNIy/+8Ocg0a3UuSoYzavmylwuLWQOf3hl0jjMMIw==",
            "license": "MIT",
            "engines": {
              "node": ">=8"
            },
            "funding": {
              "url": "https://github.com/sponsors/sindresorhus"
            }
          },
          "node_modules/body-parser": {
            "version": "1.20.4",
            "resolved": "https://registry.npmjs.org/body-parser/-/body-parser-1.20.4.tgz",
            "integrity": "sha512-ZTgYYLMOXY9qKU/57FAo8F+HA2dGX7bqGc71txDRC1rS4frdFI5R7NhluHxH6M0YItAP0sHB4uqAOcYKxO6uGA==",
            "license": "MIT",
            "dependencies": {
              "bytes": "~3.1.2",
              "content-type": "~1.0.5",
              "debug": "2.6.9",
              "depd": "2.0.0",
              "destroy": "~1.2.0",
              "http-errors": "~2.0.1",
              "iconv-lite": "~0.4.24",
              "on-finished": "~2.4.1",
              "qs": "~6.14.0",
              "raw-body": "~2.5.3",
              "type-is": "~1.6.18",
              "unpipe": "~1.0.0"
            },
            "engines": {
              "node": ">= 0.8",
              "npm": "1.2.8000 || >= 1.4.16"
            }
          },
          "node_modules/brace-expansion": {
            "version": "1.1.12",
            "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.12.tgz",
            "integrity": "sha512-9T9UjW3r0UW5c1Q7GTwllptXwhvYmEzFhzMfZ9H7FQWt+uZePjZPjBP/W1ZEyZ1twGWom5/56TF4lPcqjnDHcg==",
            "license": "MIT",
            "dependencies": {
              "balanced-match": "^1.0.0",
              "concat-map": "0.0.1"
            }
          },
          "node_modules/braces": {
            "version": "3.0.3",
            "resolved": "https://registry.npmjs.org/braces/-/braces-3.0.3.tgz",
            "integrity": "sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==",
            "license": "MIT",
            "dependencies": {
              "fill-range": "^7.1.1"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/bytes": {
            "version": "3.1.2",
            "resolved": "https://registry.npmjs.org/bytes/-/bytes-3.1.2.tgz",
            "integrity": "sha512-/Nf7TyzTx6S3yRJObOAV7956r8cr2+Oj8AC5dt8wSP3BQAoeX58NoHyCU8P8zGkNXStjTSi6fzO6F0pBdcYbEg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/call-bind-apply-helpers": {
            "version": "1.0.2",
            "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz",
            "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==",
            "license": "MIT",
            "dependencies": {
              "es-errors": "^1.3.0",
              "function-bind": "^1.1.2"
            },
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/call-bound": {
            "version": "1.0.4",
            "resolved": "https://registry.npmjs.org/call-bound/-/call-bound-1.0.4.tgz",
            "integrity": "sha512-+ys997U96po4Kx/ABpBCqhA9EuxJaQWDQg7295H4hBphv3IZg0boBKuwYpt4YXp6MZ5AmZQnU/tyMTlRpaSejg==",
            "license": "MIT",
            "dependencies": {
              "call-bind-apply-helpers": "^1.0.2",
              "get-intrinsic": "^1.3.0"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/chokidar": {
            "version": "3.6.0",
            "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-3.6.0.tgz",
            "integrity": "sha512-7VT13fmjotKpGipCW9JEQAusEPE+Ei8nl6/g4FBAmIm0GOOLMua9NDDo/DWp0ZAxCr3cPq5ZpBqmPAQgDda2Pw==",
            "license": "MIT",
            "dependencies": {
              "anymatch": "~3.1.2",
              "braces": "~3.0.2",
              "glob-parent": "~5.1.2",
              "is-binary-path": "~2.1.0",
              "is-glob": "~4.0.1",
              "normalize-path": "~3.0.0",
              "readdirp": "~3.6.0"
            },
            "engines": {
              "node": ">= 8.10.0"
            },
            "funding": {
              "url": "https://paulmillr.com/funding/"
            },
            "optionalDependencies": {
              "fsevents": "~2.3.2"
            }
          },
          "node_modules/chownr": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/chownr/-/chownr-2.0.0.tgz",
            "integrity": "sha512-bIomtDF5KGpdogkLd9VspvFzk9KfpyyGlS8YFVZl7TGPBHL5snIOnxeshwVgPteQ9b4Eydl+pVbIyE1DcvCWgQ==",
            "license": "ISC",
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/color-support": {
            "version": "1.1.3",
            "resolved": "https://registry.npmjs.org/color-support/-/color-support-1.1.3.tgz",
            "integrity": "sha512-qiBjkpbMLO/HL68y+lh4q0/O1MZFj2RX6X/KmMa3+gJD3z+WwI1ZzDHysvqHGS3mP6mznPckpXmw1nI9cJjyRg==",
            "license": "ISC",
            "bin": {
              "color-support": "bin.js"
            }
          },
          "node_modules/concat-map": {
            "version": "0.0.1",
            "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
            "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==",
            "license": "MIT"
          },
          "node_modules/console-control-strings": {
            "version": "1.1.0",
            "resolved": "https://registry.npmjs.org/console-control-strings/-/console-control-strings-1.1.0.tgz",
            "integrity": "sha512-ty/fTekppD2fIwRvnZAVdeOiGd1c7YXEixbgJTNzqcxJWKQnjJ/V1bNEEE6hygpM3WjwHFUVK6HTjWSzV4a8sQ==",
            "license": "ISC"
          },
          "node_modules/content-disposition": {
            "version": "0.5.4",
            "resolved": "https://registry.npmjs.org/content-disposition/-/content-disposition-0.5.4.tgz",
            "integrity": "sha512-FveZTNuGw04cxlAiWbzi6zTAL/lhehaWbTtgluJh4/E95DqMwTmha3KZN1aAWA8cFIhHzMZUvLevkw5Rqk+tSQ==",
            "license": "MIT",
            "dependencies": {
              "safe-buffer": "5.2.1"
            },
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/content-type": {
            "version": "1.0.5",
            "resolved": "https://registry.npmjs.org/content-type/-/content-type-1.0.5.tgz",
            "integrity": "sha512-nTjqfcBFEipKdXCv4YDQWCfmcLZKm81ldF0pAopTvyrFGVbcR6P/VAAd5G7N+0tTr8QqiU0tFadD6FK4NtJwOA==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/cookie": {
            "version": "0.7.2",
            "resolved": "https://registry.npmjs.org/cookie/-/cookie-0.7.2.tgz",
            "integrity": "sha512-yki5XnKuf750l50uGTllt6kKILY4nQ1eNIQatoXEByZ5dWgnKqbnqmTrBE5B4N7lrMJKQ2ytWMiTO2o0v6Ew/w==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/cookie-signature": {
            "version": "1.0.7",
            "resolved": "https://registry.npmjs.org/cookie-signature/-/cookie-signature-1.0.7.tgz",
            "integrity": "sha512-NXdYc3dLr47pBkpUCHtKSwIOQXLVn8dZEuywboCOJY/osA0wFSLlSawr3KN8qXJEyX66FcONTH8EIlVuK0yyFA==",
            "license": "MIT"
          },
          "node_modules/cors": {
            "version": "2.8.5",
            "resolved": "https://registry.npmjs.org/cors/-/cors-2.8.5.tgz",
            "integrity": "sha512-KIHbLJqu73RGr/hnbrO9uBeixNGuvSQjul/jdFvS/KFSIH1hWVd1ng7zOHx+YrEfInLG7q4n6GHQ9cDtxv/P6g==",
            "license": "MIT",
            "dependencies": {
              "object-assign": "^4",
              "vary": "^1"
            },
            "engines": {
              "node": ">= 0.10"
            }
          },
          "node_modules/cozo-node": {
            "version": "0.7.6",
            "resolved": "https://registry.npmjs.org/cozo-node/-/cozo-node-0.7.6.tgz",
            "integrity": "sha512-St2I4A9mD1I9LmSQo0r/EuOZ0Y0dknSCidLx8+BU5HzjrhqSbgoScDZ0nL/2sXOcUfJnSOYKNOKFUrv10j3MHA==",
            "hasInstallScript": true,
            "license": "MIT",
            "dependencies": {
              "@mapbox/node-pre-gyp": "^1.0.10"
            }
          },
          "node_modules/debug": {
            "version": "2.6.9",
            "resolved": "https://registry.npmjs.org/debug/-/debug-2.6.9.tgz",
            "integrity": "sha512-bC7ElrdJaJnPbAP+1EotYvqZsb3ecl5wi6Bfi6BJTUcNowp6cvspg0jXznRTKDjm/E7AdgFBVeAPVMNcKGsHMA==",
            "license": "MIT",
            "dependencies": {
              "ms": "2.0.0"
            }
          },
          "node_modules/delegates": {
            "version": "1.0.0",
            "resolved": "https://registry.npmjs.org/delegates/-/delegates-1.0.0.tgz",
            "integrity": "sha512-bd2L678uiWATM6m5Z1VzNCErI3jiGzt6HGY8OVICs40JQq/HALfbyNJmp0UDakEY4pMMaN0Ly5om/B1VI/+xfQ==",
            "license": "MIT"
          },
          "node_modules/depd": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/depd/-/depd-2.0.0.tgz",
            "integrity": "sha512-g7nH6P6dyDioJogAAGprGpCtVImJhpPk/roCzdb3fIh61/s/nPsfR6onyMwkCAR/OlC3yBC0lESvUoQEAssIrw==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/destroy": {
            "version": "1.2.0",
            "resolved": "https://registry.npmjs.org/destroy/-/destroy-1.2.0.tgz",
            "integrity": "sha512-2sJGJTaXIIaR1w4iJSNoN0hnMY7Gpc/n8D4qSCJw8QqFWXf7cuAgnEHxBpweaVcPevC2l3KpjYCx3NypQQgaJg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8",
              "npm": "1.2.8000 || >= 1.4.16"
            }
          },
          "node_modules/detect-libc": {
            "version": "2.1.2",
            "resolved": "https://registry.npmjs.org/detect-libc/-/detect-libc-2.1.2.tgz",
            "integrity": "sha512-Btj2BOOO83o3WyH59e8MgXsxEQVcarkUOpEYrubB0urwnN10yQ364rsiByU11nZlqWYZm05i/of7io4mzihBtQ==",
            "license": "Apache-2.0",
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/dunder-proto": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
            "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==",
            "license": "MIT",
            "dependencies": {
              "call-bind-apply-helpers": "^1.0.1",
              "es-errors": "^1.3.0",
              "gopd": "^1.2.0"
            },
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/ee-first": {
            "version": "1.1.1",
            "resolved": "https://registry.npmjs.org/ee-first/-/ee-first-1.1.1.tgz",
            "integrity": "sha512-WMwm9LhRUo+WUaRN+vRuETqG89IgZphVSNkdFgeb6sS/E4OrDIN7t48CAewSHXc6C8lefD8KKfr5vY61brQlow==",
            "license": "MIT"
          },
          "node_modules/emoji-regex": {
            "version": "8.0.0",
            "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz",
            "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==",
            "license": "MIT"
          },
          "node_modules/encodeurl": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/encodeurl/-/encodeurl-2.0.0.tgz",
            "integrity": "sha512-Q0n9HRi4m6JuGIV1eFlmvJB7ZEVxu93IrMyiMsGC0lrMJMWzRgx6WGquyfQgZVb31vhGgXnfmPNNXmxnOkRBrg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/es-define-property": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
            "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/es-errors": {
            "version": "1.3.0",
            "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz",
            "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/es-object-atoms": {
            "version": "1.1.1",
            "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz",
            "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==",
            "license": "MIT",
            "dependencies": {
              "es-errors": "^1.3.0"
            },
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/escape-html": {
            "version": "1.0.3",
            "resolved": "https://registry.npmjs.org/escape-html/-/escape-html-1.0.3.tgz",
            "integrity": "sha512-NiSupZ4OeuGwr68lGIeym/ksIZMJodUGOSCZ/FSnTxcrekbvqrgdUxlJOMpijaKZVjAJrWrGs/6Jy8OMuyj9ow==",
            "license": "MIT"
          },
          "node_modules/etag": {
            "version": "1.8.1",
            "resolved": "https://registry.npmjs.org/etag/-/etag-1.8.1.tgz",
            "integrity": "sha512-aIL5Fx7mawVa300al2BnEE4iNvo1qETxLrPI/o05L7z6go7fCw1J6EQmbK4FmJ2AS7kgVF/KEZWufBfdClMcPg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/express": {
            "version": "4.22.1",
            "resolved": "https://registry.npmjs.org/express/-/express-4.22.1.tgz",
            "integrity": "sha512-F2X8g9P1X7uCPZMA3MVf9wcTqlyNp7IhH5qPCI0izhaOIYXaW9L535tGA3qmjRzpH+bZczqq7hVKxTR4NWnu+g==",
            "license": "MIT",
            "dependencies": {
              "accepts": "~1.3.8",
              "array-flatten": "1.1.1",
              "body-parser": "~1.20.3",
              "content-disposition": "~0.5.4",
              "content-type": "~1.0.4",
              "cookie": "~0.7.1",
              "cookie-signature": "~1.0.6",
              "debug": "2.6.9",
              "depd": "2.0.0",
              "encodeurl": "~2.0.0",
              "escape-html": "~1.0.3",
              "etag": "~1.8.1",
              "finalhandler": "~1.3.1",
              "fresh": "~0.5.2",
              "http-errors": "~2.0.0",
              "merge-descriptors": "1.0.3",
              "methods": "~1.1.2",
              "on-finished": "~2.4.1",
              "parseurl": "~1.3.3",
              "path-to-regexp": "~0.1.12",
              "proxy-addr": "~2.0.7",
              "qs": "~6.14.0",
              "range-parser": "~1.2.1",
              "safe-buffer": "5.2.1",
              "send": "~0.19.0",
              "serve-static": "~1.16.2",
              "setprototypeof": "1.2.0",
              "statuses": "~2.0.1",
              "type-is": "~1.6.18",
              "utils-merge": "1.0.1",
              "vary": "~1.1.2"
            },
            "engines": {
              "node": ">= 0.10.0"
            },
            "funding": {
              "type": "opencollective",
              "url": "https://opencollective.com/express"
            }
          },
          "node_modules/fill-range": {
            "version": "7.1.1",
            "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz",
            "integrity": "sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==",
            "license": "MIT",
            "dependencies": {
              "to-regex-range": "^5.0.1"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/finalhandler": {
            "version": "1.3.2",
            "resolved": "https://registry.npmjs.org/finalhandler/-/finalhandler-1.3.2.tgz",
            "integrity": "sha512-aA4RyPcd3badbdABGDuTXCMTtOneUCAYH/gxoYRTZlIJdF0YPWuGqiAsIrhNnnqdXGswYk6dGujem4w80UJFhg==",
            "license": "MIT",
            "dependencies": {
              "debug": "2.6.9",
              "encodeurl": "~2.0.0",
              "escape-html": "~1.0.3",
              "on-finished": "~2.4.1",
              "parseurl": "~1.3.3",
              "statuses": "~2.0.2",
              "unpipe": "~1.0.0"
            },
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/forwarded": {
            "version": "0.2.0",
            "resolved": "https://registry.npmjs.org/forwarded/-/forwarded-0.2.0.tgz",
            "integrity": "sha512-buRG0fpBtRHSTCOASe6hD258tEubFoRLb4ZNA6NxMVHNw2gOcwHo9wyablzMzOA5z9xA9L1KNjk/Nt6MT9aYow==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/fresh": {
            "version": "0.5.2",
            "resolved": "https://registry.npmjs.org/fresh/-/fresh-0.5.2.tgz",
            "integrity": "sha512-zJ2mQYM18rEFOudeV4GShTGIQ7RbzA7ozbU9I/XBpm7kqgMywgmylMwXHxZJmkVoYkna9d2pVXVXPdYTP9ej8Q==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/fs-minipass": {
            "version": "2.1.0",
            "resolved": "https://registry.npmjs.org/fs-minipass/-/fs-minipass-2.1.0.tgz",
            "integrity": "sha512-V/JgOLFCS+R6Vcq0slCuaeWEdNC3ouDlJMNIsacH2VtALiu9mV4LPrHc5cDl8k5aw6J8jwgWWpiTo5RYhmIzvg==",
            "license": "ISC",
            "dependencies": {
              "minipass": "^3.0.0"
            },
            "engines": {
              "node": ">= 8"
            }
          },
          "node_modules/fs-minipass/node_modules/minipass": {
            "version": "3.3.6",
            "resolved": "https://registry.npmjs.org/minipass/-/minipass-3.3.6.tgz",
            "integrity": "sha512-DxiNidxSEK+tHG6zOIklvNOwm3hvCrbUrdtzY74U6HKTJxvIDfOUL5W5P2Ghd3DTkhhKPYGqeNUIh5qcM4YBfw==",
            "license": "ISC",
            "dependencies": {
              "yallist": "^4.0.0"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/fs.realpath": {
            "version": "1.0.0",
            "resolved": "https://registry.npmjs.org/fs.realpath/-/fs.realpath-1.0.0.tgz",
            "integrity": "sha512-OO0pH2lK6a0hZnAdau5ItzHPI6pUlvI7jMVnxUQRtw4owF2wk8lOSabtGDCTP4Ggrg2MbGnWO9X8K1t4+fGMDw==",
            "license": "ISC"
          },
          "node_modules/fsevents": {
            "version": "2.3.3",
            "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
            "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
            "hasInstallScript": true,
            "license": "MIT",
            "optional": true,
            "os": [
              "darwin"
            ],
            "engines": {
              "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
            }
          },
          "node_modules/function-bind": {
            "version": "1.1.2",
            "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
            "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
            "license": "MIT",
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/gauge": {
            "version": "3.0.2",
            "resolved": "https://registry.npmjs.org/gauge/-/gauge-3.0.2.tgz",
            "integrity": "sha512-+5J6MS/5XksCuXq++uFRsnUd7Ovu1XenbeuIuNRJxYWjgQbPuFhT14lAvsWfqfAmnwluf1OwMjz39HjfLPci0Q==",
            "deprecated": "This package is no longer supported.",
            "license": "ISC",
            "dependencies": {
              "aproba": "^1.0.3 || ^2.0.0",
              "color-support": "^1.1.2",
              "console-control-strings": "^1.0.0",
              "has-unicode": "^2.0.1",
              "object-assign": "^4.1.1",
              "signal-exit": "^3.0.0",
              "string-width": "^4.2.3",
              "strip-ansi": "^6.0.1",
              "wide-align": "^1.1.2"
            },
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/get-intrinsic": {
            "version": "1.3.0",
            "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz",
            "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==",
            "license": "MIT",
            "dependencies": {
              "call-bind-apply-helpers": "^1.0.2",
              "es-define-property": "^1.0.1",
              "es-errors": "^1.3.0",
              "es-object-atoms": "^1.1.1",
              "function-bind": "^1.1.2",
              "get-proto": "^1.0.1",
              "gopd": "^1.2.0",
              "has-symbols": "^1.1.0",
              "hasown": "^2.0.2",
              "math-intrinsics": "^1.1.0"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/get-proto": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz",
            "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==",
            "license": "MIT",
            "dependencies": {
              "dunder-proto": "^1.0.1",
              "es-object-atoms": "^1.0.0"
            },
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/glob": {
            "version": "7.2.3",
            "resolved": "https://registry.npmjs.org/glob/-/glob-7.2.3.tgz",
            "integrity": "sha512-nFR0zLpU2YCaRxwoCJvL6UvCH2JFyFVIvwTLsIf21AuHlMskA1hhTdk+LlYJtOlYt9v6dvszD2BGRqBL+iQK9Q==",
            "deprecated": "Glob versions prior to v9 are no longer supported",
            "license": "ISC",
            "dependencies": {
              "fs.realpath": "^1.0.0",
              "inflight": "^1.0.4",
              "inherits": "2",
              "minimatch": "^3.1.1",
              "once": "^1.3.0",
              "path-is-absolute": "^1.0.0"
            },
            "engines": {
              "node": "*"
            },
            "funding": {
              "url": "https://github.com/sponsors/isaacs"
            }
          },
          "node_modules/glob-parent": {
            "version": "5.1.2",
            "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz",
            "integrity": "sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==",
            "license": "ISC",
            "dependencies": {
              "is-glob": "^4.0.1"
            },
            "engines": {
              "node": ">= 6"
            }
          },
          "node_modules/gopd": {
            "version": "1.2.0",
            "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz",
            "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/has-symbols": {
            "version": "1.1.0",
            "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz",
            "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/has-unicode": {
            "version": "2.0.1",
            "resolved": "https://registry.npmjs.org/has-unicode/-/has-unicode-2.0.1.tgz",
            "integrity": "sha512-8Rf9Y83NBReMnx0gFzA8JImQACstCYWUplepDa9xprwwtmgEZUF0h/i5xSA625zB/I37EtrswSST6OXxwaaIJQ==",
            "license": "ISC"
          },
          "node_modules/hasown": {
            "version": "2.0.2",
            "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
            "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
            "license": "MIT",
            "dependencies": {
              "function-bind": "^1.1.2"
            },
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/http-errors": {
            "version": "2.0.1",
            "resolved": "https://registry.npmjs.org/http-errors/-/http-errors-2.0.1.tgz",
            "integrity": "sha512-4FbRdAX+bSdmo4AUFuS0WNiPz8NgFt+r8ThgNWmlrjQjt1Q7ZR9+zTlce2859x4KSXrwIsaeTqDoKQmtP8pLmQ==",
            "license": "MIT",
            "dependencies": {
              "depd": "~2.0.0",
              "inherits": "~2.0.4",
              "setprototypeof": "~1.2.0",
              "statuses": "~2.0.2",
              "toidentifier": "~1.0.1"
            },
            "engines": {
              "node": ">= 0.8"
            },
            "funding": {
              "type": "opencollective",
              "url": "https://opencollective.com/express"
            }
          },
          "node_modules/https-proxy-agent": {
            "version": "5.0.1",
            "resolved": "https://registry.npmjs.org/https-proxy-agent/-/https-proxy-agent-5.0.1.tgz",
            "integrity": "sha512-dFcAjpTQFgoLMzC2VwU+C/CbS7uRL0lWmxDITmqm7C+7F0Odmj6s9l6alZc6AELXhrnggM2CeWSXHGOdX2YtwA==",
            "license": "MIT",
            "dependencies": {
              "agent-base": "6",
              "debug": "4"
            },
            "engines": {
              "node": ">= 6"
            }
          },
          "node_modules/https-proxy-agent/node_modules/debug": {
            "version": "4.4.3",
            "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
            "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
            "license": "MIT",
            "dependencies": {
              "ms": "^2.1.3"
            },
            "engines": {
              "node": ">=6.0"
            },
            "peerDependenciesMeta": {
              "supports-color": {
                "optional": true
              }
            }
          },
          "node_modules/https-proxy-agent/node_modules/ms": {
            "version": "2.1.3",
            "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
            "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
            "license": "MIT"
          },
          "node_modules/iconv-lite": {
            "version": "0.4.24",
            "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
            "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
            "license": "MIT",
            "dependencies": {
              "safer-buffer": ">= 2.1.2 < 3"
            },
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/inflight": {
            "version": "1.0.6",
            "resolved": "https://registry.npmjs.org/inflight/-/inflight-1.0.6.tgz",
            "integrity": "sha512-k92I/b08q4wvFscXCLvqfsHCrjrF7yiXsQuIVvVE7N82W3+aqpzuUdBbfhWcy/FZR3/4IgflMgKLOsvPDrGCJA==",
            "deprecated": "This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.",
            "license": "ISC",
            "dependencies": {
              "once": "^1.3.0",
              "wrappy": "1"
            }
          },
          "node_modules/inherits": {
            "version": "2.0.4",
            "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz",
            "integrity": "sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==",
            "license": "ISC"
          },
          "node_modules/ipaddr.js": {
            "version": "1.9.1",
            "resolved": "https://registry.npmjs.org/ipaddr.js/-/ipaddr.js-1.9.1.tgz",
            "integrity": "sha512-0KI/607xoxSToH7GjN1FfSbLoU0+btTicjsQSWQlh/hZykN8KpmMf7uYwPW3R+akZ6R/w18ZlXSHBYXiYUPO3g==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.10"
            }
          },
          "node_modules/is-binary-path": {
            "version": "2.1.0",
            "resolved": "https://registry.npmjs.org/is-binary-path/-/is-binary-path-2.1.0.tgz",
            "integrity": "sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==",
            "license": "MIT",
            "dependencies": {
              "binary-extensions": "^2.0.0"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/is-extglob": {
            "version": "2.1.1",
            "resolved": "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz",
            "integrity": "sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==",
            "license": "MIT",
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/is-fullwidth-code-point": {
            "version": "3.0.0",
            "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz",
            "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==",
            "license": "MIT",
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/is-glob": {
            "version": "4.0.3",
            "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz",
            "integrity": "sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==",
            "license": "MIT",
            "dependencies": {
              "is-extglob": "^2.1.1"
            },
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/is-number": {
            "version": "7.0.0",
            "resolved": "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz",
            "integrity": "sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==",
            "license": "MIT",
            "engines": {
              "node": ">=0.12.0"
            }
          },
          "node_modules/js-yaml": {
            "version": "4.1.1",
            "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.1.tgz",
            "integrity": "sha512-qQKT4zQxXl8lLwBtHMWwaTcGfFOZviOJet3Oy/xmGk2gZH677CJM9EvtfdSkgWcATZhj/55JZ0rmy3myCT5lsA==",
            "license": "MIT",
            "dependencies": {
              "argparse": "^2.0.1"
            },
            "bin": {
              "js-yaml": "bin/js-yaml.js"
            }
          },
          "node_modules/make-dir": {
            "version": "3.1.0",
            "resolved": "https://registry.npmjs.org/make-dir/-/make-dir-3.1.0.tgz",
            "integrity": "sha512-g3FeP20LNwhALb/6Cz6Dd4F2ngze0jz7tbzrD2wAV+o9FeNHe4rL+yK2md0J/fiSf1sa1ADhXqi5+oVwOM/eGw==",
            "license": "MIT",
            "dependencies": {
              "semver": "^6.0.0"
            },
            "engines": {
              "node": ">=8"
            },
            "funding": {
              "url": "https://github.com/sponsors/sindresorhus"
            }
          },
          "node_modules/make-dir/node_modules/semver": {
            "version": "6.3.1",
            "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
            "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
            "license": "ISC",
            "bin": {
              "semver": "bin/semver.js"
            }
          },
          "node_modules/math-intrinsics": {
            "version": "1.1.0",
            "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz",
            "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            }
          },
          "node_modules/media-typer": {
            "version": "0.3.0",
            "resolved": "https://registry.npmjs.org/media-typer/-/media-typer-0.3.0.tgz",
            "integrity": "sha512-dq+qelQ9akHpcOl/gUVRTxVIOkAJ1wR3QAvb4RsVjS8oVoFjDGTc679wJYmUmknUF5HwMLOgb5O+a3KxfWapPQ==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/merge-descriptors": {
            "version": "1.0.3",
            "resolved": "https://registry.npmjs.org/merge-descriptors/-/merge-descriptors-1.0.3.tgz",
            "integrity": "sha512-gaNvAS7TZ897/rVaZ0nMtAyxNyi/pdbjbAwUpFQpN70GqnVfOiXpeUUMKRBmzXaSQ8DdTX4/0ms62r2K+hE6mQ==",
            "license": "MIT",
            "funding": {
              "url": "https://github.com/sponsors/sindresorhus"
            }
          },
          "node_modules/methods": {
            "version": "1.1.2",
            "resolved": "https://registry.npmjs.org/methods/-/methods-1.1.2.tgz",
            "integrity": "sha512-iclAHeNqNm68zFtnZ0e+1L2yUIdvzNoauKU4WBA3VvH/vPFieF7qfRlwUZU+DA9P9bPXIS90ulxoUoCH23sV2w==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/mime": {
            "version": "1.6.0",
            "resolved": "https://registry.npmjs.org/mime/-/mime-1.6.0.tgz",
            "integrity": "sha512-x0Vn8spI+wuJ1O6S7gnbaQg8Pxh4NNHb7KSINmEWKiPE4RKOplvijn+NkmYmmRgP68mc70j2EbeTFRsrswaQeg==",
            "license": "MIT",
            "bin": {
              "mime": "cli.js"
            },
            "engines": {
              "node": ">=4"
            }
          },
          "node_modules/mime-db": {
            "version": "1.52.0",
            "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz",
            "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/mime-types": {
            "version": "2.1.35",
            "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz",
            "integrity": "sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==",
            "license": "MIT",
            "dependencies": {
              "mime-db": "1.52.0"
            },
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/minimatch": {
            "version": "3.1.2",
            "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz",
            "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==",
            "license": "ISC",
            "dependencies": {
              "brace-expansion": "^1.1.7"
            },
            "engines": {
              "node": "*"
            }
          },
          "node_modules/minipass": {
            "version": "5.0.0",
            "resolved": "https://registry.npmjs.org/minipass/-/minipass-5.0.0.tgz",
            "integrity": "sha512-3FnjYuehv9k6ovOEbyOswadCDPX1piCfhV8ncmYtHOjuPwylVWsghTLo7rabjC3Rx5xD4HDx8Wm1xnMF7S5qFQ==",
            "license": "ISC",
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/minizlib": {
            "version": "2.1.2",
            "resolved": "https://registry.npmjs.org/minizlib/-/minizlib-2.1.2.tgz",
            "integrity": "sha512-bAxsR8BVfj60DWXHE3u30oHzfl4G7khkSuPW+qvpd7jFRHm7dLxOjUk1EHACJ/hxLY8phGJ0YhYHZo7jil7Qdg==",
            "license": "MIT",
            "dependencies": {
              "minipass": "^3.0.0",
              "yallist": "^4.0.0"
            },
            "engines": {
              "node": ">= 8"
            }
          },
          "node_modules/minizlib/node_modules/minipass": {
            "version": "3.3.6",
            "resolved": "https://registry.npmjs.org/minipass/-/minipass-3.3.6.tgz",
            "integrity": "sha512-DxiNidxSEK+tHG6zOIklvNOwm3hvCrbUrdtzY74U6HKTJxvIDfOUL5W5P2Ghd3DTkhhKPYGqeNUIh5qcM4YBfw==",
            "license": "ISC",
            "dependencies": {
              "yallist": "^4.0.0"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/mkdirp": {
            "version": "1.0.4",
            "resolved": "https://registry.npmjs.org/mkdirp/-/mkdirp-1.0.4.tgz",
            "integrity": "sha512-vVqVZQyf3WLx2Shd0qJ9xuvqgAyKPLAiqITEtqW0oIUjzo3PePDd6fW9iFz30ef7Ysp/oiWqbhszeGWW2T6Gzw==",
            "license": "MIT",
            "bin": {
              "mkdirp": "bin/cmd.js"
            },
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/ms": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/ms/-/ms-2.0.0.tgz",
            "integrity": "sha512-Tpp60P6IUJDTuOq/5Z8cdskzJujfwqfOTkrwIwj7IRISpnkJnT6SyJ4PCPnGMoFjC9ddhal5KVIYtAt97ix05A==",
            "license": "MIT"
          },
          "node_modules/negotiator": {
            "version": "0.6.3",
            "resolved": "https://registry.npmjs.org/negotiator/-/negotiator-0.6.3.tgz",
            "integrity": "sha512-+EUsqGPLsM+j/zdChZjsnX51g4XrHFOIXwfnCVPGlQk/k5giakcKsuxCObBRu6DSm9opw/O6slWbJdghQM4bBg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/node-fetch": {
            "version": "2.7.0",
            "resolved": "https://registry.npmjs.org/node-fetch/-/node-fetch-2.7.0.tgz",
            "integrity": "sha512-c4FRfUm/dbcWZ7U+1Wq0AwCyFL+3nt2bEw05wfxSz+DWpWsitgmSgYmy2dQdWyKC1694ELPqMs/YzUSNozLt8A==",
            "license": "MIT",
            "dependencies": {
              "whatwg-url": "^5.0.0"
            },
            "engines": {
              "node": "4.x || >=6.0.0"
            },
            "peerDependencies": {
              "encoding": "^0.1.0"
            },
            "peerDependenciesMeta": {
              "encoding": {
                "optional": true
              }
            }
          },
          "node_modules/nopt": {
            "version": "5.0.0",
            "resolved": "https://registry.npmjs.org/nopt/-/nopt-5.0.0.tgz",
            "integrity": "sha512-Tbj67rffqceeLpcRXrT7vKAN8CwfPeIBgM7E6iBkmKLV7bEMwpGgYLGv0jACUsECaa/vuxP0IjEont6umdMgtQ==",
            "license": "ISC",
            "dependencies": {
              "abbrev": "1"
            },
            "bin": {
              "nopt": "bin/nopt.js"
            },
            "engines": {
              "node": ">=6"
            }
          },
          "node_modules/normalize-path": {
            "version": "3.0.0",
            "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz",
            "integrity": "sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==",
            "license": "MIT",
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/npmlog": {
            "version": "5.0.1",
            "resolved": "https://registry.npmjs.org/npmlog/-/npmlog-5.0.1.tgz",
            "integrity": "sha512-AqZtDUWOMKs1G/8lwylVjrdYgqA4d9nu8hc+0gzRxlDb1I10+FHBGMXs6aiQHFdCUUlqH99MUMuLfzWDNDtfxw==",
            "deprecated": "This package is no longer supported.",
            "license": "ISC",
            "dependencies": {
              "are-we-there-yet": "^2.0.0",
              "console-control-strings": "^1.1.0",
              "gauge": "^3.0.0",
              "set-blocking": "^2.0.0"
            }
          },
          "node_modules/object-assign": {
            "version": "4.1.1",
            "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
            "integrity": "sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==",
            "license": "MIT",
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/object-inspect": {
            "version": "1.13.4",
            "resolved": "https://registry.npmjs.org/object-inspect/-/object-inspect-1.13.4.tgz",
            "integrity": "sha512-W67iLl4J2EXEGTbfeHCffrjDfitvLANg0UlX3wFUUSTx92KXRFegMHUVgSqE+wvhAbi4WqjGg9czysTV2Epbew==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/on-finished": {
            "version": "2.4.1",
            "resolved": "https://registry.npmjs.org/on-finished/-/on-finished-2.4.1.tgz",
            "integrity": "sha512-oVlzkg3ENAhCk2zdv7IJwd/QUD4z2RxRwpkcGY8psCVcCYZNq4wYnVWALHM+brtuJjePWiYF/ClmuDr8Ch5+kg==",
            "license": "MIT",
            "dependencies": {
              "ee-first": "1.1.1"
            },
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/once": {
            "version": "1.4.0",
            "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz",
            "integrity": "sha512-lNaJgI+2Q5URQBkccEKHTQOPaXdUxnZZElQTZY0MFUAuaEqe1E+Nyvgdz/aIyNi6Z9MzO5dv1H8n58/GELp3+w==",
            "license": "ISC",
            "dependencies": {
              "wrappy": "1"
            }
          },
          "node_modules/parseurl": {
            "version": "1.3.3",
            "resolved": "https://registry.npmjs.org/parseurl/-/parseurl-1.3.3.tgz",
            "integrity": "sha512-CiyeOxFT/JZyN5m0z9PfXw4SCBJ6Sygz1Dpl0wqjlhDEGGBP1GnsUVEL0p63hoG1fcj3fHynXi9NYO4nWOL+qQ==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/path-is-absolute": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/path-is-absolute/-/path-is-absolute-1.0.1.tgz",
            "integrity": "sha512-AVbw3UJ2e9bq64vSaS9Am0fje1Pa8pbGqTTsmXfaIiMpnr5DlDhfJOuLj9Sf95ZPVDAUerDfEk88MPmPe7UCQg==",
            "license": "MIT",
            "engines": {
              "node": ">=0.10.0"
            }
          },
          "node_modules/path-to-regexp": {
            "version": "0.1.12",
            "resolved": "https://registry.npmjs.org/path-to-regexp/-/path-to-regexp-0.1.12.tgz",
            "integrity": "sha512-RA1GjUVMnvYFxuqovrEqZoxxW5NUZqbwKtYz/Tt7nXerk0LbLblQmrsgdeOxV5SFHf0UDggjS/bSeOZwt1pmEQ==",
            "license": "MIT"
          },
          "node_modules/picomatch": {
            "version": "2.3.1",
            "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz",
            "integrity": "sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==",
            "license": "MIT",
            "engines": {
              "node": ">=8.6"
            },
            "funding": {
              "url": "https://github.com/sponsors/jonschlinkert"
            }
          },
          "node_modules/proxy-addr": {
            "version": "2.0.7",
            "resolved": "https://registry.npmjs.org/proxy-addr/-/proxy-addr-2.0.7.tgz",
            "integrity": "sha512-llQsMLSUDUPT44jdrU/O37qlnifitDP+ZwrmmZcoSKyLKvtZxpyV0n2/bD/N4tBAAZ/gJEdZU7KMraoK1+XYAg==",
            "license": "MIT",
            "dependencies": {
              "forwarded": "0.2.0",
              "ipaddr.js": "1.9.1"
            },
            "engines": {
              "node": ">= 0.10"
            }
          },
          "node_modules/qs": {
            "version": "6.14.1",
            "resolved": "https://registry.npmjs.org/qs/-/qs-6.14.1.tgz",
            "integrity": "sha512-4EK3+xJl8Ts67nLYNwqw/dsFVnCf+qR7RgXSK9jEEm9unao3njwMDdmsdvoKBKHzxd7tCYz5e5M+SnMjdtXGQQ==",
            "license": "BSD-3-Clause",
            "dependencies": {
              "side-channel": "^1.1.0"
            },
            "engines": {
              "node": ">=0.6"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/range-parser": {
            "version": "1.2.1",
            "resolved": "https://registry.npmjs.org/range-parser/-/range-parser-1.2.1.tgz",
            "integrity": "sha512-Hrgsx+orqoygnmhFbKaHE6c296J+HTAQXoxEF6gNupROmmGJRoyzfG3ccAveqCBrwr/2yxQ5BVd/GTl5agOwSg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/raw-body": {
            "version": "2.5.3",
            "resolved": "https://registry.npmjs.org/raw-body/-/raw-body-2.5.3.tgz",
            "integrity": "sha512-s4VSOf6yN0rvbRZGxs8Om5CWj6seneMwK3oDb4lWDH0UPhWcxwOWw5+qk24bxq87szX1ydrwylIOp2uG1ojUpA==",
            "license": "MIT",
            "dependencies": {
              "bytes": "~3.1.2",
              "http-errors": "~2.0.1",
              "iconv-lite": "~0.4.24",
              "unpipe": "~1.0.0"
            },
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/readable-stream": {
            "version": "3.6.2",
            "resolved": "https://registry.npmjs.org/readable-stream/-/readable-stream-3.6.2.tgz",
            "integrity": "sha512-9u/sniCrY3D5WdsERHzHE4G2YCXqoG5FTHUiCC4SIbr6XcLZBY05ya9EKjYek9O5xOAwjGq+1JdGBAS7Q9ScoA==",
            "license": "MIT",
            "dependencies": {
              "inherits": "^2.0.3",
              "string_decoder": "^1.1.1",
              "util-deprecate": "^1.0.1"
            },
            "engines": {
              "node": ">= 6"
            }
          },
          "node_modules/readdirp": {
            "version": "3.6.0",
            "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz",
            "integrity": "sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==",
            "license": "MIT",
            "dependencies": {
              "picomatch": "^2.2.1"
            },
            "engines": {
              "node": ">=8.10.0"
            }
          },
          "node_modules/rimraf": {
            "version": "3.0.2",
            "resolved": "https://registry.npmjs.org/rimraf/-/rimraf-3.0.2.tgz",
            "integrity": "sha512-JZkJMZkAGFFPP2YqXZXPbMlMBgsxzE8ILs4lMIX/2o0L9UBw9O/Y3o6wFw/i9YLapcUJWwqbi3kdxIPdC62TIA==",
            "deprecated": "Rimraf versions prior to v4 are no longer supported",
            "license": "ISC",
            "dependencies": {
              "glob": "^7.1.3"
            },
            "bin": {
              "rimraf": "bin.js"
            },
            "funding": {
              "url": "https://github.com/sponsors/isaacs"
            }
          },
          "node_modules/safe-buffer": {
            "version": "5.2.1",
            "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.2.1.tgz",
            "integrity": "sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==",
            "funding": [
              {
                "type": "github",
                "url": "https://github.com/sponsors/feross"
              },
              {
                "type": "patreon",
                "url": "https://www.patreon.com/feross"
              },
              {
                "type": "consulting",
                "url": "https://feross.org/support"
              }
            ],
            "license": "MIT"
          },
          "node_modules/safer-buffer": {
            "version": "2.1.2",
            "resolved": "https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz",
            "integrity": "sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==",
            "license": "MIT"
          },
          "node_modules/semver": {
            "version": "7.7.3",
            "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.3.tgz",
            "integrity": "sha512-SdsKMrI9TdgjdweUSR9MweHA4EJ8YxHn8DFaDisvhVlUOe4BF1tLD7GAj0lIqWVl+dPb/rExr0Btby5loQm20Q==",
            "license": "ISC",
            "bin": {
              "semver": "bin/semver.js"
            },
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/send": {
            "version": "0.19.2",
            "resolved": "https://registry.npmjs.org/send/-/send-0.19.2.tgz",
            "integrity": "sha512-VMbMxbDeehAxpOtWJXlcUS5E8iXh6QmN+BkRX1GARS3wRaXEEgzCcB10gTQazO42tpNIya8xIyNx8fll1OFPrg==",
            "license": "MIT",
            "dependencies": {
              "debug": "2.6.9",
              "depd": "2.0.0",
              "destroy": "1.2.0",
              "encodeurl": "~2.0.0",
              "escape-html": "~1.0.3",
              "etag": "~1.8.1",
              "fresh": "~0.5.2",
              "http-errors": "~2.0.1",
              "mime": "1.6.0",
              "ms": "2.1.3",
              "on-finished": "~2.4.1",
              "range-parser": "~1.2.1",
              "statuses": "~2.0.2"
            },
            "engines": {
              "node": ">= 0.8.0"
            }
          },
          "node_modules/send/node_modules/ms": {
            "version": "2.1.3",
            "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
            "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
            "license": "MIT"
          },
          "node_modules/serve-static": {
            "version": "1.16.3",
            "resolved": "https://registry.npmjs.org/serve-static/-/serve-static-1.16.3.tgz",
            "integrity": "sha512-x0RTqQel6g5SY7Lg6ZreMmsOzncHFU7nhnRWkKgWuMTu5NN0DR5oruckMqRvacAN9d5w6ARnRBXl9xhDCgfMeA==",
            "license": "MIT",
            "dependencies": {
              "encodeurl": "~2.0.0",
              "escape-html": "~1.0.3",
              "parseurl": "~1.3.3",
              "send": "~0.19.1"
            },
            "engines": {
              "node": ">= 0.8.0"
            }
          },
          "node_modules/set-blocking": {
            "version": "2.0.0",
            "resolved": "https://registry.npmjs.org/set-blocking/-/set-blocking-2.0.0.tgz",
            "integrity": "sha512-KiKBS8AnWGEyLzofFfmvKwpdPzqiy16LvQfK3yv/fVH7Bj13/wl3JSR1J+rfgRE9q7xUJK4qvgS8raSOeLUehw==",
            "license": "ISC"
          },
          "node_modules/setprototypeof": {
            "version": "1.2.0",
            "resolved": "https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz",
            "integrity": "sha512-E5LDX7Wrp85Kil5bhZv46j8jOeboKq5JMmYM3gVGdGH8xFpPWXUMsNrlODCrkoxMEeNi/XZIwuRvY4XNwYMJpw==",
            "license": "ISC"
          },
          "node_modules/side-channel": {
            "version": "1.1.0",
            "resolved": "https://registry.npmjs.org/side-channel/-/side-channel-1.1.0.tgz",
            "integrity": "sha512-ZX99e6tRweoUXqR+VBrslhda51Nh5MTQwou5tnUDgbtyM0dBgmhEDtWGP/xbKn6hqfPRHujUNwz5fy/wbbhnpw==",
            "license": "MIT",
            "dependencies": {
              "es-errors": "^1.3.0",
              "object-inspect": "^1.13.3",
              "side-channel-list": "^1.0.0",
              "side-channel-map": "^1.0.1",
              "side-channel-weakmap": "^1.0.2"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/side-channel-list": {
            "version": "1.0.0",
            "resolved": "https://registry.npmjs.org/side-channel-list/-/side-channel-list-1.0.0.tgz",
            "integrity": "sha512-FCLHtRD/gnpCiCHEiJLOwdmFP+wzCmDEkc9y7NsYxeF4u7Btsn1ZuwgwJGxImImHicJArLP4R0yX4c2KCrMrTA==",
            "license": "MIT",
            "dependencies": {
              "es-errors": "^1.3.0",
              "object-inspect": "^1.13.3"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/side-channel-map": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/side-channel-map/-/side-channel-map-1.0.1.tgz",
            "integrity": "sha512-VCjCNfgMsby3tTdo02nbjtM/ewra6jPHmpThenkTYh8pG9ucZ/1P8So4u4FGBek/BjpOVsDCMoLA/iuBKIFXRA==",
            "license": "MIT",
            "dependencies": {
              "call-bound": "^1.0.2",
              "es-errors": "^1.3.0",
              "get-intrinsic": "^1.2.5",
              "object-inspect": "^1.13.3"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/side-channel-weakmap": {
            "version": "1.0.2",
            "resolved": "https://registry.npmjs.org/side-channel-weakmap/-/side-channel-weakmap-1.0.2.tgz",
            "integrity": "sha512-WPS/HvHQTYnHisLo9McqBHOJk2FkHO/tlpvldyrnem4aeQp4hai3gythswg6p01oSoTl58rcpiFAjF2br2Ak2A==",
            "license": "MIT",
            "dependencies": {
              "call-bound": "^1.0.2",
              "es-errors": "^1.3.0",
              "get-intrinsic": "^1.2.5",
              "object-inspect": "^1.13.3",
              "side-channel-map": "^1.0.1"
            },
            "engines": {
              "node": ">= 0.4"
            },
            "funding": {
              "url": "https://github.com/sponsors/ljharb"
            }
          },
          "node_modules/signal-exit": {
            "version": "3.0.7",
            "resolved": "https://registry.npmjs.org/signal-exit/-/signal-exit-3.0.7.tgz",
            "integrity": "sha512-wnD2ZE+l+SPC/uoS0vXeE9L1+0wuaMqKlfz9AMUo38JsyLSBWSFcHR1Rri62LZc12vLr1gb3jl7iwQhgwpAbGQ==",
            "license": "ISC"
          },
          "node_modules/statuses": {
            "version": "2.0.2",
            "resolved": "https://registry.npmjs.org/statuses/-/statuses-2.0.2.tgz",
            "integrity": "sha512-DvEy55V3DB7uknRo+4iOGT5fP1slR8wQohVdknigZPMpMstaKJQWhwiYBACJE3Ul2pTnATihhBYnRhZQHGBiRw==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/string_decoder": {
            "version": "1.3.0",
            "resolved": "https://registry.npmjs.org/string_decoder/-/string_decoder-1.3.0.tgz",
            "integrity": "sha512-hkRX8U1WjJFd8LsDJ2yQ/wWWxaopEsABU1XfkM8A+j0+85JAGppt16cr1Whg6KIbb4okU6Mql6BOj+uup/wKeA==",
            "license": "MIT",
            "dependencies": {
              "safe-buffer": "~5.2.0"
            }
          },
          "node_modules/string-width": {
            "version": "4.2.3",
            "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz",
            "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==",
            "license": "MIT",
            "dependencies": {
              "emoji-regex": "^8.0.0",
              "is-fullwidth-code-point": "^3.0.0",
              "strip-ansi": "^6.0.1"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/strip-ansi": {
            "version": "6.0.1",
            "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
            "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
            "license": "MIT",
            "dependencies": {
              "ansi-regex": "^5.0.1"
            },
            "engines": {
              "node": ">=8"
            }
          },
          "node_modules/tar": {
            "version": "6.2.1",
            "resolved": "https://registry.npmjs.org/tar/-/tar-6.2.1.tgz",
            "integrity": "sha512-DZ4yORTwrbTj/7MZYq2w+/ZFdI6OZ/f9SFHR+71gIVUZhOQPHzVCLpvRnPgyaMpfWxxk/4ONva3GQSyNIKRv6A==",
            "license": "ISC",
            "dependencies": {
              "chownr": "^2.0.0",
              "fs-minipass": "^2.0.0",
              "minipass": "^5.0.0",
              "minizlib": "^2.1.1",
              "mkdirp": "^1.0.3",
              "yallist": "^4.0.0"
            },
            "engines": {
              "node": ">=10"
            }
          },
          "node_modules/to-regex-range": {
            "version": "5.0.1",
            "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz",
            "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==",
            "license": "MIT",
            "dependencies": {
              "is-number": "^7.0.0"
            },
            "engines": {
              "node": ">=8.0"
            }
          },
          "node_modules/toidentifier": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz",
            "integrity": "sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==",
            "license": "MIT",
            "engines": {
              "node": ">=0.6"
            }
          },
          "node_modules/tr46": {
            "version": "0.0.3",
            "resolved": "https://registry.npmjs.org/tr46/-/tr46-0.0.3.tgz",
            "integrity": "sha512-N3WMsuqV66lT30CrXNbEjx4GEwlow3v6rr4mCcv6prnfwhS01rkgyFdjPNBYd9br7LpXV1+Emh01fHnq2Gdgrw==",
            "license": "MIT"
          },
          "node_modules/type-is": {
            "version": "1.6.18",
            "resolved": "https://registry.npmjs.org/type-is/-/type-is-1.6.18.tgz",
            "integrity": "sha512-TkRKr9sUTxEH8MdfuCSP7VizJyzRNMjj2J2do2Jr3Kym598JVdEksuzPQCnlFPW4ky9Q+iA+ma9BGm06XQBy8g==",
            "license": "MIT",
            "dependencies": {
              "media-typer": "0.3.0",
              "mime-types": "~2.1.24"
            },
            "engines": {
              "node": ">= 0.6"
            }
          },
          "node_modules/unpipe": {
            "version": "1.0.0",
            "resolved": "https://registry.npmjs.org/unpipe/-/unpipe-1.0.0.tgz",
            "integrity": "sha512-pjy2bYhSsufwWlKwPc+l3cN7+wuJlK6uz0YdJEOlQDbl6jo/YlPi4mb8agUkVC8BF7V8NuzeyPNqRksA3hztKQ==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/util-deprecate": {
            "version": "1.0.2",
            "resolved": "https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz",
            "integrity": "sha512-EPD5q1uXyFxJpCrLnCc1nHnq3gOa6DZBocAIiI2TaSCA7VCJ1UJDMagCzIkXNsUYfD1daK//LTEQ8xiIbrHtcw==",
            "license": "MIT"
          },
          "node_modules/utils-merge": {
            "version": "1.0.1",
            "resolved": "https://registry.npmjs.org/utils-merge/-/utils-merge-1.0.1.tgz",
            "integrity": "sha512-pMZTvIkT1d+TFGvDOqodOclx0QWkkgi6Tdoa8gC8ffGAAqz9pzPTZWAybbsHHoED/ztMtkv/VoYTYyShUn81hA==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.4.0"
            }
          },
          "node_modules/vary": {
            "version": "1.1.2",
            "resolved": "https://registry.npmjs.org/vary/-/vary-1.1.2.tgz",
            "integrity": "sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==",
            "license": "MIT",
            "engines": {
              "node": ">= 0.8"
            }
          },
          "node_modules/webidl-conversions": {
            "version": "3.0.1",
            "resolved": "https://registry.npmjs.org/webidl-conversions/-/webidl-conversions-3.0.1.tgz",
            "integrity": "sha512-2JAn3z8AR6rjK8Sm8orRC0h/bcl/DqL7tRPdGZ4I1CjdF+EaMLmYxBHyXuKL849eucPFhvBoxMsflfOb8kxaeQ==",
            "license": "BSD-2-Clause"
          },
          "node_modules/whatwg-url": {
            "version": "5.0.0",
            "resolved": "https://registry.npmjs.org/whatwg-url/-/whatwg-url-5.0.0.tgz",
            "integrity": "sha512-saE57nupxk6v3HY35+jzBwYa0rKSy0XR8JSxZPwgLr7ys0IBzhGviA1/TUGJLmSVqs8pb9AnvICXEuOHLprYTw==",
            "license": "MIT",
            "dependencies": {
              "tr46": "~0.0.3",
              "webidl-conversions": "^3.0.0"
            }
          },
          "node_modules/wide-align": {
            "version": "1.1.5",
            "resolved": "https://registry.npmjs.org/wide-align/-/wide-align-1.1.5.tgz",
            "integrity": "sha512-eDMORYaPNZ4sQIuuYPDHdQvf4gyCF9rEEV/yPxGfwPkRodwEgiMUUXTx/dex+Me0wxx53S+NgUHaP7y3MGlDmg==",
            "license": "ISC",
            "dependencies": {
              "string-width": "^1.0.2 || 2 || 3 || 4"
            }
          },
          "node_modules/wrappy": {
            "version": "1.0.2",
            "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
            "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==",
            "license": "ISC"
          },
          "node_modules/yallist": {
            "version": "4.0.0",
            "resolved": "https://registry.npmjs.org/yallist/-/yallist-4.0.0.tgz",
            "integrity": "sha512-3wdGidZyq5PB084XLES5TpOSRA3wjXAlIWMhum2kRcv/41Sn2emQ0dycQW4uZXLejwKvg6EsvbdlVL+FYEct7A==",
            "license": "ISC"
          }
        }
      }
    size: 60018
    tokens: 20679
  - path: engine\package.json
    content: |
      {
        "name": "sovereign-context-engine",
        "version": "3.0.0",
        "description": "Headless Context Engine & Knowledge Graph",
        "main": "src/index.js",
        "scripts": {
          "start": "node src/index.js",
          "migrate": "node src/migrate_history.js",
          "read-all": "node src/read_all.js",
          "hydrate": "node src/hydrate.js"
        },
        "dependencies": {
          "body-parser": "^1.20.2",
          "chokidar": "^3.6.0",
          "cors": "^2.8.5",
          "cozo-node": "^0.7.5",
          "express": "^4.18.2",
          "js-yaml": "^4.1.1"
        }
      }
    size: 508
    tokens: 195
  - path: engine\run_context_read.js
    content: |-
      const { createFullCorpusRecursive } = require('./src/read_all.js');

      // Run the function to aggregate content from the context directory
      // This is a wrapper to run the read_all functionality from the server directory
      // where all dependencies are properly installed

      console.log('Starting context aggregation from server directory...');
      createFullCorpusRecursive();
    size: 366
    tokens: 135
  - path: engine\src\combined_memory.json
    content: |-
      [
        {
          "role": "system",
          "type": "document",
          "source": "index.js",
          "timestamp": 1767473486191,
          "content": "const express = require('express');\nconst cors = require('cors');\nconst bodyParser = require('body-parser');\nconst { CozoDb } = require('cozo-node');\nconst chokidar = require('chokidar');\nconst fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\nconst { createReadStream } = require('fs');\nconst { join } = require('path');\n\n// Initialize CozoDB with RocksDB backend\nconst db = new CozoDb('rocksdb', './context.db');\n\n// Set up Express app\nconst app = express();\nconst PORT = 3000;\n\n// Serve static files from tools directory\napp.use(express.static(join(__dirname, '..', '..', 'tools')));\n\n// Middleware\napp.use(cors());\napp.use(bodyParser.json({ limit: '50mb' }));\napp.use(bodyParser.urlencoded({ extended: true }));\n\n// Initialize database schema\nasync function initializeDb() {\n  try {\n    // Check if the memory relation already exists\n    const checkQuery = '::relations';\n    const relations = await db.run(checkQuery);\n\n    // Only create the memory table if it doesn't already exist\n    if (!relations.rows.some(row => row[0] === 'memory')) {\n        const schemaQuery = ':create memory {id: String, timestamp: Int, content: String, source: String, type: String}';\n        await db.run(schemaQuery);\n        console.log('Database schema initialized');\n    } else {\n        console.log('Database schema already exists');\n    }\n\n    // Try to create FTS index (optional, may not be supported in all builds)\n    try {\n      const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;\n      await db.run(ftsQuery);\n      console.log('FTS index created');\n    } catch (e) {\n      console.log('FTS creation failed (optional feature):', e.message);\n    }\n  } catch (error) {\n    console.error('Error initializing database:', error);\n    throw error;\n  }\n}\n\n// POST /v1/ingest endpoint\napp.post('/v1/ingest', async (req, res) => {\n  try {\n    const { content, filename, source, type = 'text' } = req.body;\n    \n    if (!content) {\n      return res.status(400).json({ error: 'Content is required' });\n    }\n    \n    const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;\n    const timestamp = Date.now();\n    \n    // Insert into CozoDB\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const params = {\n      data: [[\n        id,\n        timestamp,\n        content,\n        source || filename || 'unknown',\n        type\n      ]]\n    };\n    \n    const result = await db.run(query, params);\n    \n    res.json({ \n      status: 'success', \n      id: id,\n      message: 'Content ingested successfully'\n    });\n  } catch (error) {\n    console.error('Ingest error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/query endpoint\napp.post('/v1/query', async (req, res) => {\n  try {\n    const { query, params = {} } = req.body;\n\n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n\n    const result = await db.run(query, params);\n\n    res.json(result);\n  } catch (error) {\n    console.error('Query error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/memory/search endpoint (for context.html)\napp.post('/v1/memory/search', async (req, res) => {\n  try {\n    const { query, max_chars = 5000 } = req.body;\n\n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n\n    // Simple search implementation - retrieve all memory entries\n    // We'll filter on the server side since CozoDB's text search functions may vary\n    const searchQuery = `?[*] := *memory{id, timestamp, content, source, type}`;\n    const params = {};\n\n    const result = await db.run(searchQuery, params);\n\n    if (result.ok) {\n      let context = '';\n      let charCount = 0;\n\n      if (result.rows) {\n        // Filter rows that contain the query term (case insensitive)\n        const filteredRows = result.rows.filter(row => {\n          const [id, timestamp, content, source, type] = row;\n          return content.toLowerCase().includes(query.toLowerCase()) ||\n                 source.toLowerCase().includes(query.toLowerCase());\n        });\n\n        // Sort by relevance (rows with query in content first, then in source)\n        filteredRows.sort((a, b) => {\n          const [a_id, a_timestamp, a_content, a_source, a_type] = a;\n          const [b_id, b_timestamp, b_content, b_source, b_type] = b;\n\n          const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());\n          const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());\n\n          // Prioritize content matches over source matches\n          if (aContentMatch && !bContentMatch) return -1;\n          if (!aContentMatch && bContentMatch) return 1;\n          return 0;\n        });\n\n        for (const row of filteredRows) {\n          const [id, timestamp, content, source, type] = row;\n          const entryText = `### Source: ${source}\\n${content}\\n\\n`;\n          if (charCount + entryText.length > max_chars) {\n            // Add partial content if we're near the limit\n            const remainingChars = max_chars - charCount;\n            context += entryText.substring(0, remainingChars);\n            break;\n          }\n          context += entryText;\n          charCount += entryText.length;\n        }\n      }\n\n      res.json({ context: context || 'No results found.' });\n    } else {\n      res.status(500).json({ error: 'Search failed' });\n    }\n  } catch (error) {\n    console.error('Memory search error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/system/spawn_shell endpoint (for index.html)\napp.post('/v1/system/spawn_shell', async (req, res) => {\n  try {\n    // For now, just return success - spawning a shell is complex and platform-dependent\n    // In a real implementation, this would spawn a PowerShell terminal\n    res.json({ success: true, message: \"Shell spawned successfully\" });\n  } catch (error) {\n    console.error('Spawn shell error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// GET /health endpoint\napp.get('/health', (req, res) => {\n  res.json({ \n    status: 'Sovereign',\n    timestamp: new Date().toISOString(),\n    uptime: process.uptime()\n  });\n});\n\n// Set up file watcher for context directory\nfunction setupFileWatcher() {\n  const contextDir = path.join(__dirname, '..', 'context');\n  \n  // Ensure context directory exists\n  if (!fs.existsSync(contextDir)) {\n    fs.mkdirSync(contextDir, { recursive: true });\n  }\n  \n  const watcher = chokidar.watch(contextDir, {\n    ignored: /(^|[\\/\\\\])\\../, // ignore dotfiles\n    persistent: true,\n    ignoreInitial: true, // Don't trigger events for existing files\n    awaitWriteFinish: {\n      stabilityThreshold: 2000,\n      pollInterval: 100\n    }\n  });\n\n  watcher\n    .on('add', filePath => handleFileChange(filePath))\n    .on('change', filePath => handleFileChange(filePath))\n    .on('error', error => console.error('Watcher error:', error));\n    \n  console.log('File watcher initialized for context directory');\n}\n\nasync function handleFileChange(filePath) {\n  console.log(`File changed: ${filePath}`);\n  \n  try {\n    const content = fs.readFileSync(filePath, 'utf8');\n    const relPath = path.relative(\n      path.join(__dirname, '..', 'context'), \n      filePath\n    );\n    \n    // Ingest the file content\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const id = `file_${Date.now()}_${path.basename(filePath)}`;\n    const params = {\n      data: [[\n        id,\n        Date.now(),\n        content,\n        relPath,\n        path.extname(filePath) || 'unknown'\n      ]]\n    };\n    \n    await db.run(query, params);\n    console.log(`File ingested: ${relPath}`);\n  } catch (error) {\n    console.error(`Error processing file ${filePath}:`, error);\n  }\n}\n\n// Initialize and start server\nasync function startServer() {\n  try {\n    await initializeDb();\n    setupFileWatcher();\n    \n    app.listen(PORT, () => {\n      console.log(`Sovereign Context Engine listening on port ${PORT}`);\n      console.log(`Health check: http://localhost:${PORT}/health`);\n    });\n  } catch (error) {\n    console.error('Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Handle graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('Shutting down gracefully...');\n  try {\n    await db.close();\n  } catch (e) {\n    console.error('Error closing database:', e);\n  }\n  process.exit(0);\n});\n\n// Start the server\nstartServer();\n\nmodule.exports = { db, app };"
        },
        {
          "role": "system",
          "type": "document",
          "source": "migrate_history.js",
          "timestamp": 1767457063827,
          "content": "const fs = require('fs');\nconst path = require('path');\nconst glob = require('glob');\nconst yaml = require('js-yaml');\n\n// Migration script to consolidate legacy session files\nasync function migrateHistory() {\n  console.log('Starting legacy session migration...');\n\n  // Find all session files\n  const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');\n  const pattern = path.join(sessionsDir, 'sessions_part_*.json');\n\n  // Use glob to find all matching files\n  const sessionFiles = glob.sync(pattern);\n\n  if (sessionFiles.length === 0) {\n    console.log('No session files found in the expected location.');\n    // Try alternative path\n    const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');\n    const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');\n    const altSessionFiles = glob.sync(altPattern);\n\n    if (altSessionFiles.length === 0) {\n      console.log('No session files found in alternative location either.');\n      return;\n    }\n\n    console.log(`Found ${altSessionFiles.length} session files in alternative location.`);\n    processSessionFiles(altSessionFiles);\n    return;\n  }\n\n  console.log(`Found ${sessionFiles.length} session files`);\n  processSessionFiles(sessionFiles);\n}\n\nfunction processSessionFiles(sessionFiles) {\n  // Sort files numerically (part_1, part_2, ..., part_10, etc.)\n  sessionFiles.sort((a, b) => {\n    const matchA = a.match(/part_(\\d+)/);\n    const matchB = b.match(/part_(\\d+)/);\n\n    if (matchA && matchB) {\n      return parseInt(matchA[1]) - parseInt(matchB[1]);\n    }\n    return a.localeCompare(b);\n  });\n\n  let allSessions = [];\n\n  for (const file of sessionFiles) {\n    console.log(`Processing: ${path.basename(file)}`);\n    try {\n      const content = fs.readFileSync(file, 'utf8');\n\n      // Try to extract valid JSON from potentially corrupted files\n      let data = extractValidJson(content);\n\n      if (!data) {\n        console.error(`Could not extract valid JSON from ${file}`);\n        continue;\n      }\n\n      // Handle both list and object formats\n      if (Array.isArray(data)) {\n        allSessions = allSessions.concat(data);\n      } else if (typeof data === 'object') {\n        allSessions.push(data);\n      } else {\n        console.log(`Unexpected data format in ${file}, skipping...`);\n      }\n    } catch (error) {\n      console.error(`Error reading ${file}:`, error.message);\n    }\n  }\n\n  console.log(`Merged ${allSessions.length} total sessions`);\n\n  // Save to YAML file\n  const outputDir = path.join(__dirname, '..', '..', 'context');\n  const outputFile = path.join(outputDir, 'full_history.yaml');\n\n  // Custom YAML representer for multiline strings\n  yaml.representer = {\n    ...yaml.representer,\n    string: (data) => {\n      if (data.includes('\\n')) {\n        return new yaml.types.Str(data, { style: '|' });\n      }\n      return data;\n    }\n  };\n\n  try {\n    const yamlContent = yaml.dump(allSessions, {\n      lineWidth: -1,\n      noRefs: true,\n      skipInvalid: true\n    });\n\n    fs.writeFileSync(outputFile, yamlContent, 'utf8');\n    console.log(`YAML file created: ${outputFile}`);\n\n    // Also save as JSON for compatibility\n    const jsonOutputFile = path.join(outputDir, 'full_history.json');\n    fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');\n    console.log(`JSON file created: ${jsonOutputFile}`);\n\n  } catch (error) {\n    console.error('Error saving YAML file:', error.message);\n    return;\n  }\n\n  console.log('Migration completed successfully!');\n}\n\n// Function to extract valid JSON from potentially corrupted files\nfunction extractValidJson(content) {\n  try {\n    // First, try to parse as regular JSON\n    return JSON.parse(content);\n  } catch (e) {\n    // If that fails, clean the content and try again\n    try {\n      // Remove null bytes and other control characters that often corrupt JSON\n      let cleanContent = content.replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]/g, '');\n\n      // Try to parse the cleaned content\n      return JSON.parse(cleanContent);\n    } catch (e2) {\n      // If still failing, try to extract JSON array from the content\n      try {\n        // Find the main JSON array by looking for opening [ and closing ]\n        const startIdx = cleanContent.indexOf('[');\n        const endIdx = cleanContent.lastIndexOf(']');\n\n        if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {\n          const arrayContent = cleanContent.substring(startIdx, endIdx + 1);\n\n          // Try to parse the extracted array\n          return JSON.parse(arrayContent);\n        }\n      } catch (e3) {\n        // If all attempts fail, return null\n        return null;\n      }\n    }\n  }\n\n  return null;\n}\n\n// Run migration if this file is executed directly\nif (require.main === module) {\n  migrateHistory().catch(console.error);\n}\n\nmodule.exports = { migrateHistory };"
        },
        {
          "role": "system",
          "type": "document",
          "source": "read_all.js",
          "timestamp": 1767457667701,
          "content": "const fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\n\n/**\n * Aggregates all readable text content from a directory and its subdirectories\n * into:\n * 1. A single text corpus (combined_text.txt) for human reading.\n * 2. A structured JSON memory file (combined_memory.json) for Sovereign DB ingestion.\n * 3. A structured YAML memory file (combined_memory.yaml) for easier processing and migration.\n */\nfunction createFullCorpusRecursive() {\n  // Set the root directory to scan as the directory containing this script\n  const rootDirToScan = path.dirname(__filename);\n\n  const outputTextFile = path.join(rootDirToScan, 'combined_text.txt');\n  const outputJsonFile = path.join(rootDirToScan, 'combined_memory.json');\n  const outputYamlFile = path.join(rootDirToScan, 'combined_memory.yaml');\n\n  console.log(`Scanning Target Directory: ${rootDirToScan}`);\n\n  const textExtensions = new Set([\n    '.json', '.md', '.poml', '.yaml', '.yml', '.txt', \n    '.py', '.js', '.ts', '.css', '.sh', '.ps1', '.html', '.bat'\n  ]);\n\n  const excludeDirs = new Set([\n    '.venv', '.git', '.vscode', '__pycache__', \n    'node_modules', '.obsidian', 'random', 'archive', \n    'build', 'dist', 'logs'\n  ]);\n\n  // Files to exclude from the corpus itself to avoid recursion\n  const excludeFiles = new Set([\n    path.basename(outputTextFile),\n    path.basename(outputJsonFile),\n    path.basename(outputYamlFile),\n    'package-lock.json',\n    'yarn.lock'\n  ]);\n\n  const filesToProcess = [];\n\n  function walkDirectory(currentPath) {\n    const items = fs.readdirSync(currentPath);\n\n    for (const item of items) {\n      const itemPath = path.join(currentPath, item);\n      const stat = fs.statSync(itemPath);\n\n      if (stat.isDirectory()) {\n        if (!excludeDirs.has(item)) {\n          walkDirectory(itemPath);\n        }\n      } else if (stat.isFile()) {\n        const ext = path.extname(item).toLowerCase();\n        if (textExtensions.has(ext) && !excludeFiles.has(item)) {\n          filesToProcess.push(itemPath);\n        }\n      }\n    }\n  }\n\n  walkDirectory(rootDirToScan);\n  filesToProcess.sort();\n\n  if (filesToProcess.length === 0) {\n    console.log(`No processable files found in '${rootDirToScan}'.`);\n    return;\n  }\n\n  console.log(`Found ${filesToProcess.length} files to process.`);\n\n  const memoryRecords = [];\n\n  // 1. Generate Text Corpus\n  const textStream = fs.createWriteStream(outputTextFile, { encoding: 'utf-8' });\n\n  for (const filePath of filesToProcess) {\n    console.log(`Processing '${filePath}'...`);\n    try {\n      // Get file metadata\n      const fileStats = fs.statSync(filePath);\n      const modTime = fileStats.mtimeMs; // milliseconds timestamp\n      const relPath = path.relative(rootDirToScan, filePath);\n\n      // Read file content\n      const rawContent = fs.readFileSync(filePath);\n      // For simplicity in JS, we'll assume UTF-8, but could implement encoding detection\n      const decodedContent = rawContent.toString('utf-8');\n\n      // Write to Text File\n      textStream.write(`--- START OF FILE: ${relPath} ---\\n`);\n      textStream.write(decodedContent + \"\\n\");\n      textStream.write(`--- END OF FILE: ${relPath} ---\\n\\n`);\n\n      // Add to Memory Records\n      memoryRecords.push({\n        role: 'system',\n        type: 'document',\n        source: relPath,\n        timestamp: Math.floor(modTime), // Convert to integer\n        content: decodedContent\n      });\n\n    } catch (e) {\n      console.log(`An unexpected error occurred with file '${filePath}': ${e.message}`);\n    }\n  }\n\n  textStream.end();\n\n  // 2. Generate JSON Memory File\n  console.log(`Generating Structured Memory: ${outputJsonFile}`);\n  fs.writeFileSync(outputJsonFile, JSON.stringify(memoryRecords, null, 2), 'utf-8');\n\n  // 3. Generate YAML Memory File\n  console.log(`Generating YAML Memory: ${outputYamlFile}`);\n\n  // Custom YAML representer for multiline strings\n  const schema = yaml.DEFAULT_SCHEMA.extend([\n    new yaml.Type('!long-string', {\n      kind: 'scalar',\n      predicate: (data) => typeof data === 'string' && data.includes('\\n'),\n      represent: (data) => ({ value: data, style: '|' })\n    })\n  ]);\n\n  // Use default representer with multiline string style\n  const yamlContent = yaml.dump(memoryRecords, {\n    lineWidth: -1, // Don't wrap lines\n    noRefs: true,\n    quotingType: '\"', // Use double quotes when needed\n    forceQuotes: false\n  });\n\n  fs.writeFileSync(outputYamlFile, yamlContent, 'utf-8');\n\n  console.log('\\nCorpus aggregation complete.');\n  console.log(`1. Text Corpus: '${outputTextFile}'`);\n  console.log(`2. JSON Memory: '${outputJsonFile}' (Drop this into Coda Console)`);\n  console.log(`3. YAML Memory: '${outputYamlFile}' (Alternative format for easier processing)`);\n}\n\n// Run the function if this script is executed directly\nif (require.main === module) {\n  createFullCorpusRecursive();\n}\n\nmodule.exports = { createFullCorpusRecursive };"
        }
      ]
    size: 19519
    tokens: 7488
  - path: engine\src\hydrate.js
    content: "const fs = require('fs');\r\nconst path = require('path');\r\nconst yaml = require('js-yaml');\r\nconst crypto = require('crypto');\r\nconst { CozoDb } = require('cozo-node');\r\n\r\n// Helper to calculate hash\r\nfunction getHash(content) {\r\n    return crypto.createHash('md5').update(content || '').digest('hex');\r\n}\r\n\r\nasync function hydrate(db, filePath) {\r\n    console.log(`💧 Hydrating Schema 2.0 from: ${filePath}`);\r\n    \r\n    try {\r\n        // 1. Force Re-Create Schema with new columns\r\n        // We drop the old table to ensure clean migration if needed, but :create if not exists is safer\r\n        // To force an upgrade, we rely on the user deleting context.db manually or we just run the create command\r\n        // Since we are changing columns, we must ensure the schema matches.\r\n        \r\n        const schema = ':create memory {id: String => timestamp: Int, content: String, source: String, type: String, hash: String, bucket: String}';\r\n        try {\r\n            await db.run(schema);\r\n        } catch (e) {\r\n            if (!e.message.includes('already exists') && !e.message.includes('conflicts with an existing one')) throw e;\r\n        }\r\n        \r\n        // FTS Update\r\n        try {\r\n            await db.run(`::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]}`);\r\n        } catch (e) {\r\n            if (!e.message.includes('already exists')) console.error('FTS Error:', e.message);\r\n        }\r\n\r\n        // 2. Load Data\r\n        const fileContent = fs.readFileSync(filePath, 'utf8');\r\n        const records = yaml.load(fileContent);\r\n        \r\n        if (!Array.isArray(records)) throw new Error(\"Invalid snapshot format\");\r\n\r\n        console.log(`Found ${records.length} memories. Upgrading...`);\r\n\r\n        const BATCH_SIZE = 100;\r\n        let processed = 0;\r\n\r\n        while (processed < records.length) {\r\n            const batch = records.slice(processed, processed + BATCH_SIZE);\r\n            \r\n            // Map legacy records to new format\r\n            const values = batch.map(r => [\r\n                r.id, \r\n                parseInt(r.timestamp), \r\n                r.content, \r\n                r.source, \r\n                r.type,\r\n                r.hash || getHash(r.content),      // Backfill hash\r\n                r.bucket || 'core'                 // Backfill bucket\r\n            ]);\r\n            \r\n            const q = `\r\n                ?[id, timestamp, content, source, type, hash, bucket] <- $values\r\n                :put memory {id, timestamp, content, source, type, hash, bucket}\r\n            `;\r\n            \r\n            await db.run(q, { values });\r\n            processed += batch.length;\r\n            process.stdout.write(`\\rProgress: ${processed}/${records.length}`);\r\n        }\r\n        console.log(\"\\n✅ Hydration & Upgrade Complete.\");\r\n\r\n    } catch (e) {\r\n        console.error(\"\\n❌ Hydration Failed:\", e.message);\r\n    }\r\n}\r\n\r\nmodule.exports = { hydrate };\r\n\r\nif (require.main === module) {\r\n    const targetFile = process.argv[2];\r\n    const dbPath = path.join(__dirname, '..', 'context.db');\r\n    const db = new CozoDb('rocksdb', dbPath);\r\n    if (!targetFile) { console.log(\"Usage: node src/hydrate.js <snapshot.yaml>\"); process.exit(1); }\r\n    hydrate(db, targetFile);\r\n}\r\n"
    size: 3266
    tokens: 1168
  - path: engine\src\index.js
    content: |-
      const express = require('express');
      const cors = require('cors');
      const bodyParser = require('body-parser');
      const { CozoDb } = require('cozo-node');
      const chokidar = require('chokidar');
      const fs = require('fs');
      const path = require('path');
      const yaml = require('js-yaml');
      const crypto = require('crypto');
      const { createReadStream } = require('fs');
      const { join } = require('path');
      const { hydrate } = require('./hydrate');

      // Initialize CozoDB with RocksDB backend
      const db = new CozoDb('rocksdb', path.join(__dirname, '..', 'context.db'));

      // Set up Express app
      const app = express();
      const PORT = 3000;

      // Serve static files from interface directory
      app.use(express.static(join(__dirname, '..', '..', 'interface')));

      // Middleware
      app.use(cors());
      app.use(bodyParser.json({ limit: '50mb' }));
      app.use(bodyParser.urlencoded({ extended: true }));

      // Initialize database schema
      async function initializeDb() {
        try {
          // Check if the memory relation already exists
          const checkQuery = '::relations';
          const relations = await db.run(checkQuery);

          // Only create the memory table if it doesn't already exist
          if (!relations.rows.some(row => row[0] === 'memory')) {
              const schemaQuery = ':create memory {id: String => timestamp: Int, content: String, source: String, type: String, hash: String, bucket: String}';
              await db.run(schemaQuery);
              console.log('Database schema initialized');
          } else {
              console.log('Database schema already exists');
          }

          // Try to create FTS index (optional, may not be supported in all builds)
          try {
            // Create FTS index for content field - using correct CozoDB syntax
            // We use a simpler syntax and handle existing index gracefully
            const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]}`;
            await db.run(ftsQuery);
            console.log('FTS index created');
          } catch (e) {
            // FTS might not be supported in this build or index might already exist
            if (e.message && e.message.includes('already exists')) {
              console.log('FTS index already exists');
            } else {
              console.log('FTS creation failed (optional feature):', e.message);
            }
          }
          console.log('Database initialization complete');
        } catch (error) {
          console.error('Error initializing database:', error);
          throw error;
        }
      }

      // Automatically hydrate from the latest snapshot in the backups folder
      async function autoHydrate() {
        const backupsDir = path.join(__dirname, '..', '..', 'backups');
        if (!fs.existsSync(backupsDir)) {
          console.log('No backups directory found at ' + backupsDir + ', skipping auto-hydration.');
          return;
        }

        try {
          // Check if the database already has data
          const countQuery = '?[count(id)] := *memory{id}';
          const countResult = await db.run(countQuery);
          const count = countResult.rows[0][0];

          if (count > 0) {
            console.log(`📡 Database already contains ${count} memories. Skipping auto-hydration.`);
            console.log(`💡 To force re-hydration, delete the 'engine/context.db' folder or clear the 'memory' relation.`);
            return;
          }

          const files = fs.readdirSync(backupsDir)
            .filter(f => f.endsWith('.yaml') || f.endsWith('.yml'))
            .map(f => ({
              name: f,
              path: path.join(backupsDir, f),
              mtime: fs.statSync(path.join(backupsDir, f)).mtime
            }))
            .sort((a, b) => b.mtime - a.mtime);

          if (files.length > 0) {
            const latest = files[0];
            console.log(`🔄 Auto-hydration: Found ${files.length} snapshots. Picking latest: ${latest.name}`);
            await hydrate(db, latest.path);
            console.log(`✅ Auto-hydration complete.`);
          } else {
            console.log('No snapshots found in backups directory.');
          }
        } catch (error) {
          console.error('Auto-hydration failed:', error);
        }
      }

      // POST /v1/ingest endpoint
      app.post('/v1/ingest', async (req, res) => {
        try {
          const { content, filename, source, type = 'text', bucket = 'core' } = req.body;
          
          if (!content) return res.status(400).json({ error: 'Content required' });
          
          // 1. Calculate Hash
          const hash = crypto.createHash('md5').update(content).digest('hex');

          // 2. Check for Duplicates (Deduplication)
          const checkQuery = `?[id] := *memory{id, hash: $hash, bucket: $bucket}`;
          const checkResult = await db.run(checkQuery, { hash, bucket });

          if (checkResult.ok && checkResult.rows.length > 0) {
              return res.json({ 
                  status: 'skipped', 
                  id: checkResult.rows[0][0], 
                  message: 'Duplicate content detected. Skipped.' 
              });
          }
          
          // 3. Insert New Memory
          const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
          const timestamp = Date.now();
          
          const query = `?[id, timestamp, content, source, type, hash, bucket] <- $data :insert memory {id, timestamp, content, source, type, hash, bucket}`;
          const params = {
            data: [[ id, timestamp, content, source || filename || 'unknown', type, hash, bucket ]]
          };
          
          await db.run(query, params);
          
          res.json({ status: 'success', id, message: 'Ingested.' });
        } catch (error) {
          console.error('Ingest error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // POST /v1/query endpoint
      app.post('/v1/query', async (req, res) => {
        try {
          const { query, params = {} } = req.body;

          if (!query) {
            return res.status(400).json({ error: 'Query is required' });
          }

          const result = await db.run(query, params);

          res.json(result);
        } catch (error) {
          console.error('Query error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // Basic search function as fallback when FTS fails
      async function basicSearch(query, max_chars = 5000, buckets) {
        try {
          // Simple search implementation - retrieve all memory entries
          const useBuckets = Array.isArray(buckets) && buckets.length > 0;
          const searchQuery = useBuckets 
            ? `?[id, timestamp, content, source, type, bucket] := *memory{id, timestamp, content, source, type, bucket}, is_in(bucket, $b)`
            : `?[id, timestamp, content, source, type, bucket] := *memory{id, timestamp, content, source, type, bucket}`;
          
          const result = await db.run(searchQuery, useBuckets ? { b: buckets } : {});

          let context = '';
          let charCount = 0;

          if (result.rows) {
            // Filter rows that contain the query term (case insensitive)
            const filteredRows = result.rows.filter(row => {
              const [id, timestamp, content, source, type, b] = row;
              return content.toLowerCase().includes(query.toLowerCase()) ||
                     source.toLowerCase().includes(query.toLowerCase());
            });

            // Sort by relevance (rows with query in content first, then in source)
            filteredRows.sort((a, b) => {
              const [a_id, a_timestamp, a_content, a_source, a_type, a_b] = a;
              const [b_id, b_timestamp, b_content, b_source, b_type, b_b] = b;

              const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());
              const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());

              // Prioritize content matches over source matches
              if (aContentMatch && !bContentMatch) return -1;
              if (!aContentMatch && bContentMatch) return 1;
              return 0;
            });

            for (const row of filteredRows) {
              const [id, timestamp, content, source, type, b] = row;
              const entryText = `### Source: ${source}\n${content}\n\n`;
              if (charCount + entryText.length > max_chars) {
                // Add partial content if we're near the limit
                const remainingChars = max_chars - charCount;
                context += entryText.substring(0, remainingChars);
                break;
              }
              context += entryText;
              charCount += entryText.length;
            }
          }

          return { context: context || 'No results found.' };
        } catch (error) {
          console.error('Basic search error:', error);
          return { context: 'Search failed' };
        }
      }

      // POST /v1/memory/search endpoint (for context.html)
      app.post('/v1/memory/search', async (req, res) => {
        try {
          const { query, max_chars = 5000, bucket, buckets } = req.body;
          if (!query) return res.status(400).json({ error: 'Query required' });

          // Support both single 'bucket' and array 'buckets'
          const targetBuckets = buckets || (bucket ? [bucket] : null);

          // 1. FTS Search (Get Candidates)
          // If buckets are provided, filter by them. Otherwise search all.
          const useBuckets = Array.isArray(targetBuckets) && targetBuckets.length > 0;
          const ftsQuery = useBuckets 
            ? `?[id, score] := ~memory:content_fts{id | query: $q, k: 30, bind_score: s}, *memory{id, bucket: b}, is_in(b, $b), score = s`
            : `?[id, score] := ~memory:content_fts{id | query: $q, k: 30, bind_score: s}, score = s`;

          const ftsParams = useBuckets ? { q: query, b: targetBuckets } : { q: query };
          let ftsResult;
          
          try {
              ftsResult = await db.run(ftsQuery, ftsParams);
          } catch (e) {
              // Fallback if FTS fails
              console.error('FTS Error, falling back to basic:', e.message);
              return res.json(await basicSearch(query, max_chars, targetBuckets));
          }

          if (ftsResult.rows.length === 0) {
              return res.json(await basicSearch(query, max_chars, targetBuckets));
          }

          // 2. Fetch Content for Candidates
          const ids = ftsResult.rows.map(row => row[0]);
          const scores = Object.fromEntries(ftsResult.rows);
          
          const contentQuery = `
            ?[id, content, source] := 
              *memory{id, content, source},
              is_in(id, $ids)
          `;
          
          const contentResult = await db.run(contentQuery, { ids });
          
          // 3. Elastic Window Processing
          let allHits = [];
          const searchRegex = new RegExp(query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'gi');

          for (const row of contentResult.rows) {
              const [id, content, source] = row;
              let match;
              searchRegex.lastIndex = 0; 
              while ((match = searchRegex.exec(content)) !== null) {
                  allHits.push({
                      id, source, content, 
                      start: match.index,
                      end: match.index + match[0].length,
                      score: scores[id]
                  });
              }
          }

          if (allHits.length === 0) {
              return res.json(await basicSearch(query, max_chars));
          }

          // 4. Budgeting (Optimized)
          // Increase Min Window to 300 chars for better context
          const rawWindowSize = Math.floor(max_chars / allHits.length);
          const windowSize = Math.min(Math.max(rawWindowSize, 300), 1500); 
          const padding = Math.floor(windowSize / 2);

          // 5. Grouping & Merging
          const docsMap = {};

          for (const hit of allHits) {
              if (!docsMap[hit.id]) {
                  docsMap[hit.id] = { 
                      source: hit.source, 
                      score: hit.score, 
                      ranges: [], 
                      content: hit.content 
                  };
              }
              
              const start = Math.max(0, hit.start - padding);
              const end = Math.min(hit.content.length, hit.end + padding);
              docsMap[hit.id].ranges.push({ start, end });
          }

          // 6. Build Final Context (Grouped)
          let finalContext = "";
          let totalCharsUsed = 0;
          
          // Sort files by relevance score
          const sortedDocs = Object.values(docsMap).sort((a, b) => b.score - a.score);

          for (const doc of sortedDocs) {
              if (totalCharsUsed >= max_chars) break;

              // Sort ranges and merge overlaps within this file
              doc.ranges.sort((a, b) => a.start - b.start);
              const merged = [];
              if (doc.ranges.length > 0) {
                  let current = doc.ranges[0];
                  for (let i = 1; i < doc.ranges.length; i++) {
                      if (doc.ranges[i].start <= current.end + 50) { // Merge if close (50 chars)
                          current.end = Math.max(current.end, doc.ranges[i].end);
                      } else {
                          merged.push(current);
                          current = doc.ranges[i];
                      }
                  }
                  merged.push(current);
              }

              // Header (Printed Once)
              const header = `### Source: ${doc.source} (Score: ${Math.round(doc.score)})\n`;
              if (totalCharsUsed + header.length > max_chars) break;
              finalContext += header;
              totalCharsUsed += header.length;

              // Snippets
              for (const range of merged) {
                  const snippet = doc.content.substring(range.start, range.end).replace(/\n/g, ' ');
                  const entry = `...${snippet}...\n\n`;
                  
                  if (totalCharsUsed + entry.length > max_chars) break;
                  finalContext += entry;
                  totalCharsUsed += entry.length;
              }
              
              finalContext += "---\n"; // Separator between files
          }

          res.json({ context: finalContext });

        } catch (error) {
          console.error('Search error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // POST /v1/system/spawn_shell endpoint (for index.html)
      app.post('/v1/system/spawn_shell', async (req, res) => {
        try {
          // For now, just return success - spawning a shell is complex and platform-dependent
          // In a real implementation, this would spawn a PowerShell terminal
          res.json({ success: true, message: "Shell spawned successfully" });
        } catch (error) {
          console.error('Spawn shell error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // GET /v1/backup endpoint
      app.get('/v1/backup', async (req, res) => {
        try {
          console.log("[Backup] Starting full database export...");

          // 1. Query EVERYTHING from the memory relation
          // We explicitly select columns to ensure order
          const query = `?[id, timestamp, content, source, type, hash, bucket] := *memory{id, timestamp, content, source, type, hash, bucket}`;
          const result = await db.run(query);

          // 2. Format as a clean List of Objects
          const records = result.rows.map(row => ({
            id: row[0],
            timestamp: row[1],
            content: row[2],
            source: row[3],
            type: row[4],
            hash: row[5],
            bucket: row[6]
          }));

          // 3. Convert to YAML (Block style for readability)
          const yamlStr = yaml.dump(records, {
            lineWidth: -1,        // Don't wrap long lines
            noRefs: true,         // No aliases
            quotingType: '"',     // Force quotes for safety
            forceQuotes: false
          });

          // 4. Save to local backups directory for safety
          const filename = `cozo_memory_snapshot_${new Date().toISOString().replace(/[:.]/g, '-')}.yaml`;
          const backupPath = path.join(__dirname, '../../backups', filename);
          
          try {
            fs.writeFileSync(backupPath, yamlStr);
            console.log(`[Backup] Local copy saved to ${backupPath}`);
          } catch (fsErr) {
            console.error('[Backup] Failed to save local copy:', fsErr.message);
          }

          // 5. Send as Downloadable File
          res.setHeader('Content-Type', 'text/yaml');
          res.setHeader('Content-Disposition', `attachment; filename="${filename}"`);
          res.send(yamlStr);

          console.log(`[Backup] Exported ${records.length} memories to ${filename}`);

        } catch (error) {
          console.error('[Backup] Error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // GET /v1/buckets endpoint
      app.get('/v1/buckets', async (req, res) => {
        try {
          const query = '?[bucket] := *memory{bucket}';
          const result = await db.run(query);
          let buckets = [...new Set(result.rows.map(row => row[0]))].sort();
          if (buckets.length === 0) buckets = ['core'];
          res.json(buckets);
        } catch (error) {
          console.error('Buckets error:', error);
          res.status(500).json({ error: error.message });
        }
      });

      // GET /health endpoint
      app.get('/health', (req, res) => {
        res.json({
          status: 'Sovereign',
          timestamp: new Date().toISOString(),
          uptime: process.uptime()
        });
      });

      // Set up file watcher for context directory
      function setupFileWatcher() {
        const contextDir = path.join(__dirname, '..', '..', 'context');

        // Ensure context directory exists
        if (!fs.existsSync(contextDir)) {
          fs.mkdirSync(contextDir, { recursive: true });
        }
        
        const watcher = chokidar.watch(contextDir, {
          ignored: [
            /(^|[\/\\])\../, // ignore dotfiles
            /cozo_memory_snapshot_.*\.yaml$/
          ],
          persistent: true,
          ignoreInitial: false, // Ingest existing files on startup
          awaitWriteFinish: {
            stabilityThreshold: 2000,
            pollInterval: 100
          }
        });

        watcher
          .on('add', filePath => handleFileChange(filePath))
          .on('change', filePath => handleFileChange(filePath))
          .on('error', error => console.error('Watcher error:', error));
          
        console.log('File watcher initialized for context directory');
      }

      async function handleFileChange(filePath) {
        // Skip backup files
        if (filePath.includes('cozo_memory_snapshot_')) return;

        try {
          const stats = fs.statSync(filePath);
          if (stats.isDirectory()) return;
          
          if (stats.size > 10 * 1024 * 1024) { // Skip files > 10MB
            console.log(`Skipping large file: ${filePath} (${stats.size} bytes)`);
            return;
          }

          const ext = path.extname(filePath).toLowerCase();
          const textExtensions = ['.txt', '.md', '.json', '.yaml', '.yml', '.js', '.ts', '.py', '.html', '.css', '.bat', '.ps1', '.sh'];
          if (!textExtensions.includes(ext) && ext !== '') {
            return;
          }

          console.log(`Processing: ${filePath}`);
          const content = fs.readFileSync(filePath, 'utf8');
          const hash = crypto.createHash('md5').update(content).digest('hex');
          
          const relPath = path.relative(
            path.join(__dirname, '..', '..', 'context'),
            filePath
          );
          
          // Auto-Bucket Logic: Top-level folder name = Bucket
          const pathParts = relPath.split(path.sep);
          const bucket = pathParts.length > 1 ? pathParts[0] : 'core';

          // Deduplication Check
          const checkQuery = `?[id] := *memory{id, hash: $hash, bucket: $bucket}`;
          const check = await db.run(checkQuery, { hash, bucket });
          
          if (check.ok && check.rows.length > 0) {
              // Content exists. Optionally update the path/source if it moved, but for now skip.
              // console.log(`Skipping duplicate: ${filePath}`);
              return; 
          }

          // Use a stable ID based on the relative path to allow updates
          const id = `file_${Buffer.from(relPath).toString('base64').replace(/=/g, '')}`;
          
          // Using :put to update if ID matches (file edit) but hash changed
          const query = `?[id, timestamp, content, source, type, hash, bucket] <- $data :put memory {id, timestamp, content, source, type, hash, bucket}`;
          const params = {
            data: [[ id, Date.now(), content, relPath, ext || 'text', hash, bucket ]]
          };
          
          await db.run(query, params);
          console.log(`Ingested: ${relPath}`);
        } catch (error) {
          console.error(`Error processing file ${filePath}:`, error);
        }
      }

      // Initialize and start server
      async function startServer() {
        try {
          await initializeDb();
          
          // Small delay to ensure DB is ready
          await new Promise(resolve => setTimeout(resolve, 1000));
          
          await autoHydrate();
          setupFileWatcher();
          
          app.listen(PORT, () => {
            console.log(`Sovereign Context Engine listening on port ${PORT}`);
            console.log(`Health check: http://localhost:${PORT}/health`);
          });
        } catch (error) {
          console.error('Failed to start server:', error);
          process.exit(1);
        }
      }

      // Handle graceful shutdown
      process.on('SIGINT', async () => {
        console.log('Shutting down gracefully...');
        try {
          await db.close();
        } catch (e) {
          console.error('Error closing database:', e);
        }
        process.exit(0);
      });

      // Start the server
      startServer();

      module.exports = { db, app };
    size: 19771
    tokens: 7186
  - path: engine\src\migrate_history.js
    content: |-
      const fs = require('fs');
      const path = require('path');
      const glob = require('glob');
      const yaml = require('js-yaml');

      // Migration script to consolidate legacy session files
      async function migrateHistory() {
        console.log('Starting legacy session migration...');

        // Find all session files
        const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');
        const pattern = path.join(sessionsDir, 'sessions_part_*.json');

        // Use glob to find all matching files
        const sessionFiles = glob.sync(pattern);

        if (sessionFiles.length === 0) {
          console.log('No session files found in the expected location.');
          // Try alternative path
          const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');
          const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');
          const altSessionFiles = glob.sync(altPattern);

          if (altSessionFiles.length === 0) {
            console.log('No session files found in alternative location either.');
            return;
          }

          console.log(`Found ${altSessionFiles.length} session files in alternative location.`);
          processSessionFiles(altSessionFiles);
          return;
        }

        console.log(`Found ${sessionFiles.length} session files`);
        processSessionFiles(sessionFiles);
      }

      function processSessionFiles(sessionFiles) {
        // Sort files numerically (part_1, part_2, ..., part_10, etc.)
        sessionFiles.sort((a, b) => {
          const matchA = a.match(/part_(\d+)/);
          const matchB = b.match(/part_(\d+)/);

          if (matchA && matchB) {
            return parseInt(matchA[1]) - parseInt(matchB[1]);
          }
          return a.localeCompare(b);
        });

        let allSessions = [];

        for (const file of sessionFiles) {
          console.log(`Processing: ${path.basename(file)}`);
          try {
            const content = fs.readFileSync(file, 'utf8');

            // Try to extract valid JSON from potentially corrupted files
            let data = extractValidJson(content);

            if (!data) {
              console.error(`Could not extract valid JSON from ${file}`);
              continue;
            }

            // Handle both list and object formats
            if (Array.isArray(data)) {
              allSessions = allSessions.concat(data);
            } else if (typeof data === 'object') {
              allSessions.push(data);
            } else {
              console.log(`Unexpected data format in ${file}, skipping...`);
            }
          } catch (error) {
            console.error(`Error reading ${file}:`, error.message);
          }
        }

        console.log(`Merged ${allSessions.length} total sessions`);

        // Save to YAML file
        const outputDir = path.join(__dirname, '..', '..', 'context');
        const outputFile = path.join(outputDir, 'full_history.yaml');

        // Custom YAML representer for multiline strings
        yaml.representer = {
          ...yaml.representer,
          string: (data) => {
            if (data.includes('\n')) {
              return new yaml.types.Str(data, { style: '|' });
            }
            return data;
          }
        };

        try {
          const yamlContent = yaml.dump(allSessions, {
            lineWidth: -1,
            noRefs: true,
            skipInvalid: true
          });

          fs.writeFileSync(outputFile, yamlContent, 'utf8');
          console.log(`YAML file created: ${outputFile}`);

          // Also save as JSON for compatibility
          const jsonOutputFile = path.join(outputDir, 'full_history.json');
          fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');
          console.log(`JSON file created: ${jsonOutputFile}`);

        } catch (error) {
          console.error('Error saving YAML file:', error.message);
          return;
        }

        console.log('Migration completed successfully!');
      }

      // Function to extract valid JSON from potentially corrupted files
      function extractValidJson(content) {
        try {
          // First, try to parse as regular JSON
          return JSON.parse(content);
        } catch (e) {
          // If that fails, clean the content and try again
          try {
            // Remove null bytes and other control characters that often corrupt JSON
            let cleanContent = content.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]/g, '');

            // Try to parse the cleaned content
            return JSON.parse(cleanContent);
          } catch (e2) {
            // If still failing, try to extract JSON array from the content
            try {
              // Find the main JSON array by looking for opening [ and closing ]
              const startIdx = cleanContent.indexOf('[');
              const endIdx = cleanContent.lastIndexOf(']');

              if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {
                const arrayContent = cleanContent.substring(startIdx, endIdx + 1);

                // Try to parse the extracted array
                return JSON.parse(arrayContent);
              }
            } catch (e3) {
              // If all attempts fail, return null
              return null;
            }
          }
        }

        return null;
      }

      // Run migration if this file is executed directly
      if (require.main === module) {
        migrateHistory().catch(console.error);
      }

      module.exports = { migrateHistory };
    size: 4987
    tokens: 1794
  - path: engine\src\read_all.js
    content: |-
      /**
       * Context Aggregation Tool for ECE_Core Engine
       *
       * This script recursively scans all text files in the context directory,
       * aggregates their content into a single YAML file with a 200k token limit.
       */

      const fs = require('fs');
      const path = require('path');
      const yaml = require('js-yaml');

      // Simple token counting function
      function countTokens(text) {
          // A rough approximation: 1 token ≈ 4 characters or 1 word
          const words = text.match(/\b\w+\b/g) || [];
          return words.length + Math.floor(text.length / 4);
      }

      // Function to check if a path should be ignored
      function shouldIgnore(filePath) {
          const relativePath = path.relative(process.cwd(), filePath);
          const fileName = path.basename(filePath);
          const ext = path.extname(filePath).toLowerCase();

          // Always ignore certain files
          const ignoreFiles = ['.DS_Store', 'Thumbs.db'];
          if (ignoreFiles.includes(fileName) || fileName.endsWith('.log') || fileName.endsWith('.tmp') || fileName.endsWith('.temp')) {
              return true;
          }

          // Skip binary files based on extension
          const binaryExts = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.exe', '.bin', '.zip', '.tar', '.gz', '.rar', '.7z', '.pdf'];
          if (binaryExts.includes(ext)) {
              return true;
          }

          // Skip large files to avoid memory issues
          try {
              const stats = fs.statSync(filePath);
              if (stats.size > 10 * 1024 * 1024) { // 10MB limit
                  return true;
              }
          } catch (e) {
              // If we can't stat the file, skip it
              return true;
          }

          return false;
      }

      // Function to aggregate all file contents from context directory
      function createFullCorpusRecursive() {
          const contextDir = path.join(__dirname, '..', '..', 'context');
          console.log(`Scanning context directory: ${contextDir}`);

          if (!fs.existsSync(contextDir)) {
              console.log('Context directory does not exist, creating it...');
              fs.mkdirSync(contextDir, { recursive: true });
              return;
          }

          const aggregatedData = {
              project_structure: contextDir,
              files: []
          };

          let totalTokens = 0;
          const tokenLimit = 200000; // 200k tokens

          // Walk through all files in the context directory
          function walkDirectory(currentPath) {
              let items;
              try {
                  items = fs.readdirSync(currentPath);
              } catch (e) {
                  // If we can't read the directory, skip it
                  console.error(`Cannot read directory ${currentPath}: ${e.message}`);
                  return;
              }

              for (const item of items) {
                  const itemPath = path.join(currentPath, item);

                  let stat;
                  try {
                      stat = fs.statSync(itemPath);
                  } catch (e) {
                      // If we can't access the file/directory (e.g., broken symlink), skip it
                      continue;
                  }

                  if (stat.isDirectory()) {
                      // Skip specific directories
                      if (item === '.git' || item === '__pycache__' || item === '.pytest_cache' ||
                          item === '.vscode' || item === 'node_modules' ||
                          item === '.venv' || item === 'venv' || item === 'archive') {
                          continue;
                      }

                      walkDirectory(itemPath);
                  } else {
                      // Check if file should be ignored
                      if (shouldIgnore(itemPath)) {
                          continue;
                      }

                      try {
                          // Read file as text
                          const content = fs.readFileSync(itemPath, 'utf-8');

                          // Count tokens in this file
                          const fileTokens = countTokens(content);

                          // Check if adding this file would exceed the token limit
                          if (totalTokens + fileTokens > tokenLimit) {
                              console.log(`Token limit reached. Skipping: ${itemPath}`);
                              break;
                          }

                          // Add file data to aggregated content
                          const relativePath = path.relative(contextDir, itemPath);
                          const fileData = {
                              path: relativePath,
                              content: content,
                              size: content.length,
                              tokens: fileTokens
                          };

                          aggregatedData.files.push(fileData);
                          totalTokens += fileTokens;

                          console.log(`Processed: ${relativePath} (${fileTokens} tokens)`);

                      } catch (e) {
                          // If it's not a text file or there's an error, skip it
                          console.log(`Error reading file ${itemPath}: ${e.message}`);
                      }
                  }
              }
          }

          walkDirectory(contextDir);

          aggregatedData.metadata = {
              total_files: aggregatedData.files.length,
              total_tokens: totalTokens,
              token_limit: tokenLimit,
              token_limit_reached: totalTokens >= tokenLimit,
              timestamp: new Date().toISOString()
          };

          // Write to YAML file in context directory
          const outputFile = path.join(contextDir, "combined_context.yaml");
          const yamlContent = yaml.dump(aggregatedData, { lineWidth: -1 });
          fs.writeFileSync(outputFile, yamlContent);

          console.log("Aggregation complete!");
          console.log(`Output file: ${outputFile}`);
          console.log(`Total files processed: ${aggregatedData.metadata.total_files}`);
          console.log(`Total tokens: ${aggregatedData.metadata.total_tokens}`);

          return aggregatedData;
      }

      module.exports = { createFullCorpusRecursive };

      // Run if this file is executed directly
      if (require.main === module) {
          console.log('Starting context aggregation from context directory...');
          createFullCorpusRecursive();
      }
    size: 5822
    tokens: 2011
  - path: engine\test_cozo_fts.js
    content: "const { CozoDb } = require('cozo-node');\r\n\r\nasync function test() {\r\n    const db = new CozoDb('rocksdb', 'test_fts.db'); // Use rocksdb\r\n    try {\r\n        await db.run(':create test_mem {id: String => content: String}');\r\n        await db.run('::fts create test_mem:idx {extractor: content, tokenizer: Simple}');\r\n        await db.run('?[id, content] <- [[\"1\", \"hello world\"], [\"2\", \"foo bar\"]] :put test_mem');\r\n        \r\n        console.log(\"Testing Option 1 (index.js style): ?[id, score] := ~test_mem:idx{content | query: 'hello', k: 1, bind_score: s}, score = s\");\r\n        try {\r\n            const res1 = await db.run(\"?[id, score] := ~test_mem:idx{content | query: 'hello', k: 1, bind_score: s}, score = s\");\r\n            console.log(\"Option 1 result:\", JSON.stringify(res1));\r\n        } catch (e) {\r\n            console.log(\"Option 1 failed:\", e.message);\r\n        }\r\n\r\n        console.log(\"Testing Option 11: ?[id] := *test_mem{id, content}, ~test_mem:idx{content | query: 'hello', k: 1}\");\r\n        try {\r\n            const res11 = await db.run(\"?[id] := *test_mem{id, content}, ~test_mem:idx{content | query: 'hello', k: 1}\");\r\n            console.log(\"Option 11 result:\", JSON.stringify(res11));\r\n        } catch (e) {\r\n            console.log(\"Option 11 failed:\", e.message);\r\n        }\r\n\r\n        console.log(\"\\nTesting Option 3: ?[id] := ~test_mem:idx{content | query: 'hello', k: 1}, *test_mem{id}\");\r\n        try {\r\n            const res3 = await db.run(\"?[id] := ~test_mem:idx{content | query: 'hello', k: 1}, *test_mem{id}\");\r\n            console.log(\"Option 3 result:\", JSON.stringify(res3));\r\n        } catch (e) {\r\n            console.log(\"Option 3 failed:\", e.message);\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(\"Setup failed:\", e);\r\n    }\r\n}\r\n\r\ntest();\r\n"
    size: 1793
    tokens: 662
  - path: interface\index.html
    content: |-
      <!DOCTYPE html>
      <html lang="en">

      <head>
          <meta charset="UTF-8">
          <title>Anchor Context Console</title>
          <style>
              :root {
                  --bg: #0f172a;
                  --panel: #1e293b;
                  --text: #e2e8f0;
                  --accent: #38bdf8;
              }

              body {
                  background: var(--bg);
                  color: var(--text);
                  font-family: system-ui;
                  margin: 0;
                  padding: 20px;
                  height: 100vh;
                  display: flex;
                  gap: 20px;
                  box-sizing: border-box;
              }

              .sidebar {
                  width: 300px;
                  display: flex;
                  flex-direction: column;
                  gap: 20px;
              }

              .main {
                  flex: 1;
                  display: flex;
                  flex-direction: column;
                  background: var(--panel);
                  border-radius: 12px;
                  border: 1px solid #334155;
                  padding: 20px;
              }

              input,
              button {
                  width: 100%;
                  padding: 12px;
                  border-radius: 8px;
                  border: 1px solid #334155;
                  background: #000;
                  color: #fff;
                  box-sizing: border-box;
              }

              button {
                  background: var(--accent);
                  color: #000;
                  font-weight: bold;
                  cursor: pointer;
                  border: none;
              }

              button:hover {
                  opacity: 0.9;
              }

              textarea {
                  flex: 1;
                  background: #000;
                  color: #a5f3fc;
                  border: none;
                  padding: 15px;
                  font-family: monospace;
                  resize: none;
                  outline: none;
                  border-radius: 8px;
              }

              .slider-group {
                  background: var(--panel);
                  padding: 15px;
                  border-radius: 8px;
                  border: 1px solid #334155;
              }

              label {
                  display: block;
                  margin-bottom: 10px;
                  font-size: 0.9rem;
                  color: #94a3b8;
              }

              .bucket-container {
                  display: flex;
                  flex-wrap: wrap;
                  gap: 8px;
                  margin-bottom: 15px;
              }

              .bucket-chip {
                  padding: 6px 12px;
                  border-radius: 16px;
                  background: #334155;
                  color: #94a3b8;
                  font-size: 0.8rem;
                  cursor: pointer;
                  border: 1px solid transparent;
                  transition: all 0.2s;
              }

              .bucket-chip.active {
                  background: var(--accent);
                  color: #000;
                  font-weight: bold;
              }

              .bucket-chip:hover {
                  border-color: var(--accent);
              }

              .bucket-add {
                  background: #10b981;
                  color: white;
                  padding: 6px 10px;
                  border-radius: 16px;
                  font-size: 0.8rem;
                  cursor: pointer;
                  border: none;
              }
          </style>
      </head>

      <body>
          <div class="sidebar">
              <div>
                  <label>📦 Context Buckets</label>
                  <div id="bucket-list" class="bucket-container">
                      <!-- Buckets will be loaded here -->
                  </div>
              </div>
              <div>
                  <label>🔎 Search Memory</label>
                  <input type="text" id="query" placeholder="Type keyword..." onkeyup="if(event.key==='Enter') search()">
              </div>
              <div class="slider-group">
                  <label>Volume: <span id="vol-val">5000</span> chars</label>
                  <input type="range" id="vol" min="1000" max="50000" step="1000" value="5000"
                      oninput="document.getElementById('vol-val').innerText=this.value">
              </div>
              <button onclick="search()">Fetch Context</button>
              <button onclick="copy()" style="background: #334155; color: #fff; border: 1px solid #475569">📋 Copy to
                  Clipboard</button>
              <button onclick="downloadBackup()" style="background: #10b981; color: white; margin-top: 10px; border: 1px solid #059669">
                  💾 Eject Memory (Backup)
              </button>

              <div style="margin-top: 20px; border-top: 1px solid #334155; padding-top: 20px;">
                  <label>📥 Ingest New Memory</label>
                  <textarea id="ingest-text" style="height: 100px; width: 100%; margin-bottom: 10px;" placeholder="Paste chat session here..."></textarea>
                  <input type="text" id="ingest-source" placeholder="Source (e.g. Gemini Chat)" style="margin-bottom: 10px;">
                  <button onclick="ingest()" style="background: #6366f1; color: white;">Ingest Memory</button>
              </div>
          </div>
          <div class="main">
              <textarea id="output" readonly placeholder="Context results will appear here..."></textarea>
          </div>
          <script>
              let activeBuckets = ['core'];

              async function loadBuckets() {
                  try {
                      const res = await fetch('http://localhost:3000/v1/buckets');
                      const buckets = await res.json();
                      const list = document.getElementById('bucket-list');
                      list.innerHTML = '';

                      // Add "All" option
                      const allChip = document.createElement('div');
                      allChip.className = `bucket-chip ${activeBuckets.includes('all') ? 'active' : ''}`;
                      allChip.innerText = 'all';
                      allChip.onclick = () => selectBucket('all');
                      list.appendChild(allChip);

                      buckets.forEach(b => {
                          const chip = document.createElement('div');
                          chip.className = `bucket-chip ${activeBuckets.includes(b) ? 'active' : ''}`;
                          chip.innerText = b;
                          chip.onclick = () => selectBucket(b);
                          list.appendChild(chip);
                      });

                      const addBtn = document.createElement('button');
                      addBtn.className = 'bucket-add';
                      addBtn.innerText = '+';
                      addBtn.onclick = () => {
                          const name = prompt("New Bucket Name:");
                          if (name) selectBucket(name.toLowerCase().trim());
                      };
                      list.appendChild(addBtn);
                  } catch (e) {
                      console.error("Failed to load buckets", e);
                  }
              }

              function selectBucket(name) {
                  if (name === 'all') {
                      activeBuckets = ['all'];
                  } else {
                      // Remove 'all' if it was selected
                      activeBuckets = activeBuckets.filter(b => b !== 'all');
                      
                      if (activeBuckets.includes(name)) {
                          // Toggle off
                          activeBuckets = activeBuckets.filter(b => b !== name);
                          if (activeBuckets.length === 0) activeBuckets = ['core'];
                      } else {
                          // Toggle on
                          activeBuckets.push(name);
                      }
                  }
                  loadBuckets();
              }

              async function search() {
                  const query = document.getElementById('query').value;
                  const limit = document.getElementById('vol').value;
                  const out = document.getElementById('output');
                  if (!query) return;
                  out.value = "Searching...";
                  try {
                      const body = { query, max_chars: parseInt(limit) };
                      if (!activeBuckets.includes('all')) {
                          body.buckets = activeBuckets;
                      }

                      const res = await fetch('http://localhost:3000/v1/memory/search', {
                          method: 'POST',
                          headers: {
                              'Content-Type': 'application/json'
                          },
                          body: JSON.stringify(body)
                      });
                      const data = await res.json();
                      out.value = data.context || "No results.";

                      // Send API response to log viewer for debugging
                      const logChannel = new BroadcastChannel('sovereign-logs');
                      logChannel.postMessage({
                          source: 'Context Console',
                          time: new Date().toISOString(),
                          type: 'info',
                          msg: `Search query: ${query} [Buckets: ${activeBuckets.join(', ')}], Results: ${data.context ? data.context.length : 0} chars`
                      });
                  } catch (e) {
                      out.value = "Error: " + e;

                      // Send error to log viewer for debugging
                      const logChannel = new BroadcastChannel('sovereign-logs');
                      logChannel.postMessage({
                          source: 'Context Console',
                          time: new Date().toISOString(),
                          type: 'error',
                          msg: `Search error: ${e.message}`
                      });
                  }
              }
              function copy() {
                  const el = document.getElementById('output');
                  el.select();
                  document.execCommand('copy');
              }

              async function downloadBackup() {
                  const btn = event.target;
                  const originalText = btn.innerText;
                  btn.innerText = "⏳ Exporting...";

                  try {
                      const res = await fetch('http://localhost:3000/v1/backup');
                      if (!res.ok) throw new Error("Backup failed");

                      // Convert response to blob and trigger download
                      const blob = await res.blob();
                      const url = window.URL.createObjectURL(blob);
                      const a = document.createElement('a');
                      a.href = url;
                      // The server provides the filename in headers, but we can default
                      a.download = `cozo_memory_snapshot_${new Date().toISOString().slice(0,10)}.yaml`;
                      document.body.appendChild(a);
                      a.click();
                      window.URL.revokeObjectURL(url);
                      document.body.removeChild(a);

                      btn.innerText = "✅ Exported!";
                      setTimeout(() => btn.innerText = originalText, 2000);

                  } catch (e) {
                      alert("Export Error: " + e.message);
                      btn.innerText = "❌ Failed";
                      setTimeout(() => btn.innerText = originalText, 2000);
                  }
              }

              async function ingest() {
                  const content = document.getElementById('ingest-text').value;
                  const source = document.getElementById('ingest-source').value || 'Manual Ingest';
                  if (!content) return alert("Please paste some content first.");

                  const btn = event.target;
                  const originalText = btn.innerText;
                  btn.innerText = "⏳ Ingesting...";

                  try {
                      // For ingestion, we use the first active bucket (excluding 'all')
                      const targetBucket = activeBuckets.find(b => b !== 'all') || 'core';

                      const res = await fetch('http://localhost:3000/v1/ingest', {
                          method: 'POST',
                          headers: { 'Content-Type': 'application/json' },
                          body: JSON.stringify({ content, source, type: 'chat', bucket: targetBucket })
                      });
                      
                      if (!res.ok) throw new Error("Ingest failed");
                      
                      alert(`✅ Memory ingested successfully into [${targetBucket}]!`);
                      document.getElementById('ingest-text').value = "";
                      btn.innerText = originalText;
                      loadBuckets(); // Refresh bucket list
                  } catch (e) {
                      alert("Ingest Error: " + e.message);
                      btn.innerText = originalText;
                  }
              }

              // Initial load
              loadBuckets();
          </script>
      </body>

      </html>
    size: 11772
    tokens: 3952
  - path: kill-edge.bat
    content: "@echo off\r\necho 🔪 Killing all Microsoft Edge processes...\r\ntaskkill /F /IM msedge.exe /T\r\necho.\r\necho ✅ Edge terminated. You can now run 'launch-edge-unsafe.bat' cleanly.\r\npause\r\n"
    size: 180
    tokens: 73
  - path: launch-chromium-d3d12.bat
    content: "@echo off\r\nsetlocal EnableDelayedExpansion\r\n\r\n:: Define the User Data Directory (Project Relative)\r\nset \"USER_DATA=%~dp0browser_data\"\r\nif not exist \"%USER_DATA%\" mkdir \"%USER_DATA%\"\r\n\r\n:: Define Flags for D3D12 (Default for Windows)\r\nset \"FLAGS=--user-data-dir=\"%USER_DATA%\" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --disable-gpu-watchdog --disable-web-security --disable-site-isolation-trials --disable-features=IsolateOrigins,site-per-process\"\r\nset \"URL=http://localhost:8000/chat.html\"\r\n\r\necho ---------------------------------------------------\r\necho 🔍 Detecting Browsers...\r\necho ---------------------------------------------------\r\n\r\nset \"count=0\"\r\n\r\n:: 1. Check Microsoft Edge\r\nif exist \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Microsoft Edge\"\r\n    set \"path[!count!]=C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\r\n)\r\n\r\n:: 2. Check Google Chrome\r\nif exist \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome\"\r\n    set \"path[!count!]=C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\r\n) else if exist \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome (x86)\"\r\n    set \"path[!count!]=C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\r\n) else if exist \"%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome (User)\"\r\n    set \"path[!count!]=%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\"\r\n)\r\n\r\n:: 3. Check Brave\r\nif exist \"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Brave Browser\"\r\n    set \"path[!count!]=C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\r\n)\r\n\r\n:: Check if any found\r\nif %count%==0 (\r\n    echo ❌ No compatible Chromium browser found.\r\n    pause\r\n    exit /b\r\n)\r\n\r\n:: Display Menu\r\necho Select a browser to launch:\r\nfor /L %%i in (1,1,%count%) do (\r\n    echo [%%i] !name[%%i]!\r\n)\r\necho.\r\n\r\n:prompt\r\nset /p \"choice=Enter number (1-%count%): \"\r\n\r\n:: Validate Input\r\nif \"%choice%\"==\"\" goto prompt\r\nif %choice% LSS 1 goto prompt\r\nif %choice% GTR %count% goto prompt\r\n\r\nset \"BROWSER=!path[%choice%]!\"\r\nset \"BROWSER_NAME=!name[%choice%]!\"\r\n\r\necho.\r\necho 🚀 Launching %BROWSER_NAME% with D3D12 (Default) backend...\r\necho Path: \"%BROWSER%\"\r\necho Data: \"%USER_DATA%\"\r\necho URL: \"%URL%\"\r\necho.\r\necho Executing: \"%BROWSER%\" %FLAGS% %URL%\r\necho.\r\n\r\n\"%BROWSER%\" %FLAGS% %URL%\r\npause\r\n"
    size: 2664
    tokens: 1010
  - path: launch-chromium-vulkan.bat
    content: "@echo off\r\nsetlocal EnableDelayedExpansion\r\n\r\n:: Define the User Data Directory (Project Relative)\r\nset \"USER_DATA=%~dp0browser_data\"\r\nif not exist \"%USER_DATA%\" mkdir \"%USER_DATA%\"\r\n\r\n:: Define Flags (Critical for Snapdragon)\r\nset \"FLAGS=--user-data-dir=\"%USER_DATA%\" --ignore-gpu-blocklist --enable-webgpu-developer-features --enable-unsafe-webgpu --enable-dawn-features=allow_unsafe_apis --enable-features=Vulkan --use-angle=vulkan --disable-gpu-watchdog\"\r\nset \"URL=http://localhost:8000/chat.html\"\r\n\r\necho ---------------------------------------------------\r\necho 🔍 Detecting Browsers (Vulkan Mode)...\r\necho ---------------------------------------------------\r\n\r\nset \"count=0\"\r\n\r\n:: 1. Check Microsoft Edge\r\nif exist \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Microsoft Edge\"\r\n    set \"path[!count!]=C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\r\n)\r\n\r\n:: 2. Check Google Chrome\r\nif exist \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome\"\r\n    set \"path[!count!]=C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\r\n) else if exist \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome (x86)\"\r\n    set \"path[!count!]=C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\r\n) else if exist \"%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Google Chrome (User)\"\r\n    set \"path[!count!]=%LOCALAPPDATA%\\Google\\Chrome\\Application\\chrome.exe\"\r\n)\r\n\r\n:: 3. Check Brave\r\nif exist \"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\" (\r\n    set /a count+=1\r\n    set \"name[!count!]=Brave Browser\"\r\n    set \"path[!count!]=C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\r\n)\r\n\r\n:: Check if any found\r\nif %count%==0 (\r\n    echo ❌ No compatible Chromium browser found.\r\n    pause\r\n    exit /b\r\n)\r\n\r\n:: Display Menu\r\necho Select a browser to launch:\r\nfor /L %%i in (1,1,%count%) do (\r\n    echo [%%i] !name[%%i]!\r\n)\r\necho.\r\n\r\n:prompt\r\nset /p \"choice=Enter number (1-%count%): \"\r\n\r\n:: Validate Input\r\nif \"%choice%\"==\"\" goto prompt\r\nif %choice% LSS 1 goto prompt\r\nif %choice% GTR %count% goto prompt\r\n\r\nset \"BROWSER=!path[%choice%]!\"\r\nset \"BROWSER_NAME=!name[%choice%]!\"\r\n\r\necho.\r\necho 🚀 Launching %BROWSER_NAME% with VULKAN backend...\r\necho Path: \"%BROWSER%\"\r\necho Data: \"%USER_DATA%\"\r\necho.\r\n\r\n\"%BROWSER%\" %FLAGS% %URL%\r\npause\r\n"
    size: 2532
    tokens: 960
  - path: logs\README.md
    content: "# Logs Directory\r\n\r\nThis directory contains individual log files for each system component to facilitate debugging and monitoring.\r\n\r\n## Log File Naming Convention\r\n\r\nEach component writes to its own log file named after the source:\r\n\r\n- `system.log` - System startup and general operations\r\n- `chat_api.log` - Chat API requests and responses\r\n- `memory_api.log` - Memory search API operations\r\n- `websocket_bridge.log` - WebSocket connection events\r\n- `python_stdout.log` - Python standard output\r\n- `python_stderr.log` - Python standard error\r\n\r\n## Log Rotation\r\n\r\nEach log file is automatically truncated to keep only the last 1000 lines to prevent excessive disk usage.\r\n\r\n## Log Format\r\n\r\nEach log entry follows this format:\r\n```\r\n[YYYY-MM-DD HH:MM:SS] [LEVEL] Message content\r\n```\r\n\r\nWhere LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG\r\n\r\n## Accessing Logs\r\n\r\n- **Real-time viewing**: Use the log viewer at `http://localhost:8000/log-viewer.html`\r\n- **File access**: Individual log files are available in this directory\r\n- **API access**: Recent logs available via `/logs/recent` endpoint"
    size: 1102
    tokens: 425
  - path: README.md
    content: "# Context Engine (Sovereign Edition)\r\n\r\n> **Philosophy:** Your mind, augmented. Your data, sovereign. Your tools, open.\r\n\r\nA **Headless Node.js** cognitive extraction system. No browser dependencies. No cloud. No installation.\r\nJust you, Node.js, and your infinite context.\r\n\r\n---\r\n\r\n## ⚡ Quick Start\r\n\r\n1.  **Download** this repository.\r\n2.  **Install** Node.js dependencies: `cd engine && npm install`\r\n3.  **Inject Context**: Drop any `.md`, `.txt`, or `.yaml` files into the `context/` directory.\r\n4.  **Launch** the unified system: `start_engine.bat`\r\n5.  **Search**: Access the dashboard at `http://localhost:3000` to query your memories.\r\n\r\n*That's it. You are running a headless context engine with persistent Graph Memory.*\r\n\r\n## 📋 Script Running Protocol\r\n\r\n**IMPORTANT**: To prevent getting stuck in long-running loops, follow the script running protocol:\r\n\r\n- All services run in detached mode with logging to the `logs/` directory\r\n- Never run long-running processes in attached mode\r\n- Check log files in `logs/` directory to monitor system status\r\n- Use `start_engine.bat` or `start_engine.ps1` to start the system properly\r\n\r\n---\r\n\r\n## 🏗️ Architecture\r\n\r\nThe system now runs in `engine/` using Node.js with direct CozoDB integration.\r\n\r\n### 1. The Sovereign Loop\r\n```mermaid\r\ngraph TD\r\n    User -->|Input| API[\"HTTP API\"]\r\n\r\n    subgraph \"SOVEREIGN ENGINE (Port 3000)\"\r\n        Engine[Express Server]\r\n        API[\"API Endpoints\"]\r\n        Cozo[\"CozoDB Node\"]\r\n    end\r\n\r\n    API -->|REST| Engine\r\n    Engine -->|Direct| Cozo[\"CozoDB (RocksDB)\"]\r\n\r\n    subgraph File_Watcher [\"Context Watcher\"]\r\n        Watcher[chokidar] -->|Monitor| Context[\"context/ directory\"]\r\n        Watcher -->|Ingest| Cozo\r\n    end\r\n\r\n    subgraph Cognitive_Engine\r\n        Engine -->|Query| Cozo\r\n        Cozo -->|Results| Engine\r\n    end\r\n```\r\n\r\n### 2. Core Components\r\n*   **Engine**: `src/index.js` - Node.js server with Express, CORS, and body-parser.\r\n*   **Memory**: `CozoDB (Node)` - Stores relations (`*memory`) with RocksDB persistence.\r\n*   **Ingestion**: `chokidar` - Watches `context/` directory for file changes and auto-ingests.\r\n*   **Core**: `engine/` - Node.js monolith handling API, ingestion, and database operations.\r\n*   **Data**: `context/` - Directory for storing context files that are automatically monitored.\r\n\r\n---\r\n\r\n## 🏛️ Node.js Monolith Architecture\r\n\r\nThe system now features the unified Node.js monolith architecture for simplified deployment:\r\n\r\n*   **Sovereign Engine**: Single-process Node.js server handling API, ingestion, and database on port 3000\r\n*   **CozoDB Integration**: Direct integration with `cozo-node` using RocksDB backend\r\n*   **File Watcher**: `chokidar` monitors `context/` directory for automatic ingestion\r\n*   **API Endpoints**: Standardized endpoints for ingestion, querying, and health checks\r\n\r\n### Getting Started with Node.js Monolith\r\n1. Install dependencies: `cd engine && npm install`\r\n2. Start the unified system: `start_engine.bat` (or `start_engine.ps1`)\r\n3. Access the health check: `http://localhost:3000/health`\r\n4. Use the API endpoints for ingestion and querying\r\n\r\n### API Endpoints\r\n*   `POST /v1/memory/search` - **Cognitive Search**: Returns \"Elastic Window\" snippets grouped by file.\r\n*   `POST /v1/ingest` - Content ingestion endpoint (manual override).\r\n*   `POST /v1/query` - Raw CozoDB query execution endpoint.\r\n*   `GET /health` - Service health verification endpoint.\r\n\r\n---\r\n\r\n## 🧠 Cognitive Retrieval (Elastic Window)\r\n\r\nThe engine uses a sophisticated **\"Elastic Window\"** strategy to solve the \"Keyword Saturation\" problem (where large files flood the LLM context).\r\n\r\n### 1. BM25 Full-Text Search\r\nWhen you query the engine, it first performs a **BM25 Full-Text Search** across the entire database to find the most relevant documents.\r\n\r\n### 2. Elastic Snippeting\r\nInstead of returning the whole file, the engine:\r\n- **Scans** for every occurrence of your keywords.\r\n- **Allocates** a character budget (default 5000) across all hits.\r\n- **Expands** a \"window\" of context (min 300 chars) around each hit to capture full thoughts.\r\n- **Merges** overlapping or nearby snippets to ensure coherence.\r\n\r\n### 3. Grouped Output\r\nResults are returned as a **Context Digest**:\r\n- **Source Header**: Printed once per file with a relevance score.\r\n- **Snippets**: All relevant sections from that file listed below the header.\r\n- **Separators**: Clear `---` markers between different sources.\r\n\r\nThis ensures the LLM receives **high-density, relevant paragraphs** instead of \"sentence soup\" or massive, irrelevant file chunks.\r\n\r\n---\r\n\r\n## 🔄 Context Collection & Migration\r\n\r\nThe system now includes comprehensive context collection and legacy migration:\r\n\r\n*   **Legacy Migration**: `engine/src/migrate_history.js` consolidates legacy session files into YAML/JSON\r\n*   **Context Collection**: `read_all.js` and `engine/src/read_all.js` aggregate content from project directories\r\n*   **File Monitoring**: Automatic ingestion of new files in `context/` directory\r\n*   **Multi-Format Output**: Generates text, JSON, and YAML formats for maximum compatibility\r\n*   **Archive Strategy**: Legacy V2 artifacts archived to `archive/v2_python_bridge/`\r\n\r\n### Data Migration Process\r\n1. Legacy session files consolidated from `context/Coding-Notes/Notebook/history/important-context/sessions/`\r\n2. Converted to YAML format in `context/full_history.yaml` and `context/full_history.json`\r\n3. Auto-ingested into CozoDB for persistent storage\r\n4. Legacy Python infrastructure archived for historical reference\r\n\r\n### Context Collection Strategy\r\n*   **File Discovery**: Recursive scanning of `context/` directory\r\n*   **Format Support**: Handles .json, .md, .yaml, .txt, .py, .js, .html, .css, .sh, .ps1, .bat\r\n*   **Exclusion Rules**: Skips common build directories, logs, and combined outputs\r\n*   **Encoding Detection**: Robust encoding handling for various file types\r\n*   **Structured Output**: Generates both JSON and YAML memory files\r\n\r\n---\r\n\r\n## 📚 Documentation\r\n\r\n*   **Architecture**: [specs/spec.md](specs/spec.md)\r\n*   **Roadmap**: [specs/plan.md](specs/plan.md)\r\n*   **Migration Guide**: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)\r\n*   **Autonomous Execution**: [specs/protocols/001-autonomous-execution.md](specs/protocols/001-autonomous-execution.md)\r\n\r\n---\r\n\r\n## 🧹 Legacy Support\r\nThe old Python/Browser Bridge (V2) has been **archived**.\r\n*   Legacy artifacts: `archive/v2_python_bridge/`\r\n*   Legacy code: `webgpu_bridge.py`, `anchor_watchdog.py`, `start-anchor.bat`, etc.\r\n*   Migration guide: [specs/standards/034-nodejs-monolith-migration.md](specs/standards/034-nodejs-monolith-migration.md)\r\n"
    size: 6739
    tokens: 2523
  - path: read_all.js
    content: |
      #!/usr/bin/env node

      /**
       * Context Aggregation Tool for ECE_Core
       *
       * This script recursively scans all directories and files in the project,
       * aggregates their content into a single YAML file with a 200k token limit,
       * and omits files/directories specified in .gitignore.
       */

      const fs = require('fs');
      const path = require('path');

      // Try to load js-yaml, with fallback error handling
      let yaml;
      try {
          yaml = require('js-yaml');
      } catch (e) {
          console.error('js-yaml not found. Please install it with: npm install js-yaml');
          process.exit(1);
      }

      // Simple token counting function
      function countTokens(text) {
          // A rough approximation: 1 token ≈ 4 characters or 1 word
          const words = text.match(/\b\w+\b/g) || [];
          return words.length + Math.floor(text.length / 4);
      }

      // Function to check if a path should be ignored
      function shouldIgnore(filePath) {
          const relativePath = path.relative(process.cwd(), filePath);
          const fileName = path.basename(filePath);
          const ext = path.extname(filePath).toLowerCase();

          // Always ignore certain directories
          const pathParts = relativePath.split(path.sep);
          const ignoreDirs = new Set(['.git', '__pycache__', '.pytest_cache', '.vscode', 'node_modules', '.venv', 'venv', 'archive', 'context']);
          if (pathParts.some(part => ignoreDirs.has(part))) {
              return true;
          }

          // Always ignore certain files
          const ignoreFiles = ['.DS_Store', 'Thumbs.db'];
          if (ignoreFiles.includes(fileName) || fileName.endsWith('.log') || fileName.endsWith('.tmp') || fileName.endsWith('.txt') || fileName.endsWith('.yaml')   || fileName.endsWith('.temp')) {
              return true;w
          }

          // Skip binary files based on extension
          const binaryExts = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.exe', '.bin', '.zip', '.tar', '.gz', '.rar', '.7z'];
          if (binaryExts.includes(ext)) {
              return true;
          }

          return false;
      }

      // Function to aggregate all file contents
      function aggregateContent(rootPath) {
          const aggregatedData = {
              project_structure: rootPath,
              files: []
          };

          let totalTokens = 0;
          const tokenLimit = 200000; // 200k tokens

          // Walk through all files in the directory
          function walkDirectory(currentPath) {
              let items;
              try {
                  items = fs.readdirSync(currentPath);
              } catch (e) {
                  // If we can't read the directory, skip it
                  return;
              }

              for (const item of items) {
                  const itemPath = path.join(currentPath, item);

                  let stat;
                  try {
                      stat = fs.statSync(itemPath);
                  } catch (e) {
                      // If we can't access the file/directory (e.g., broken symlink), skip it
                      continue;
                  }

                  if (stat.isDirectory()) {
                      // Skip specific directories
                      if (item === '.git' || item === '__pycache__' || item === '.pytest_cache' ||
                          item === '.vscode' || item === 'node_modules' ||
                          item === '.venv' || item === 'venv' || item === 'archive') {
                          continue;
                      }

                      walkDirectory(itemPath);
                  } else {
                      // Check if file should be ignored
                      if (shouldIgnore(itemPath)) {
                          continue;
                      }

                      try {
                          // Read file as text
                          const content = fs.readFileSync(itemPath, 'utf-8');

                          // Count tokens in this file
                          const fileTokens = countTokens(content);

                          // Check if adding this file would exceed the token limit
                          if (totalTokens + fileTokens > tokenLimit) {
                              console.log(`Token limit reached. Skipping: ${itemPath}`);
                              break;
                          }

                          // Add file data to aggregated content
                          const relativePath = path.relative(rootPath, itemPath);
                          const fileData = {
                              path: relativePath,
                              content: content,
                              size: content.length,
                              tokens: fileTokens
                          };

                          aggregatedData.files.push(fileData);
                          totalTokens += fileTokens;

                      } catch (e) {
                          // If it's not a text file or there's an error, skip it
                          // Only log if it's not a binary file error
                          if (e.code !== 'EISDIR' && !e.message.includes('Invalid character') && !e.message.includes('Unexpected')) {
                              // console.log(`Error reading file ${itemPath}: ${e.message}`);
                          }
                      }
                  }
              }
          }

          walkDirectory(rootPath);

          aggregatedData.metadata = {
              total_files: aggregatedData.files.length,
              total_tokens: totalTokens,
              token_limit: tokenLimit,
              token_limit_reached: totalTokens >= tokenLimit
          };

          return aggregatedData;
      }

      // Main function
      function main() {
          const rootPath = process.cwd();

          console.log("Starting context aggregation...");
          console.log(`Root path: ${rootPath}`);

          const aggregatedData = aggregateContent(rootPath);

          // Write to YAML file
          const outputFile = path.join(rootPath, "combined_context.yaml");
          const yamlContent = yaml.dump(aggregatedData, { lineWidth: -1 });
          fs.writeFileSync(outputFile, yamlContent);

          console.log("Aggregation complete!");
          console.log(`Output file: ${outputFile}`);
          console.log(`Total files processed: ${aggregatedData.metadata.total_files}`);
          console.log(`Total tokens: ${aggregatedData.metadata.total_tokens}`);
          console.log(`Token limit: ${aggregatedData.metadata.token_limit}`);
          console.log(`Token limit reached: ${aggregatedData.metadata.token_limit_reached}`);
      }

      // Run the main function
      main();
    size: 5982
    tokens: 2080
  - path: run_context_aggregation.bat
    content: |-
      @echo off
      REM ECE_Core Context Aggregation Script with Logging Protocol
      REM Follows SCRIPT_PROTOCOL.md standards for detached execution

      echo Starting context aggregation with logging protocol...

      REM Create logs directory if it doesn't exist
      if not exist "logs" mkdir logs

      REM Run the context aggregation in background with logging
      start /b cmd /c "node read_all.js > logs/read_all.log 2>&1"

      REM Wait a moment
      timeout /t 2 /nobreak >nul

      REM Check the log
      echo Last 5 lines of read_all log:
      powershell -Command "Get-Content logs/read_all.log -Tail 5"

      echo.
      echo Context aggregation started in background. Check logs/read_all.log for output.
    size: 644
    tokens: 264
  - path: run_context_read.bat
    content: |-
      @echo off
      REM ECE_Core Context Read Script with Logging Protocol
      REM Follows SCRIPT_PROTOCOL.md standards for detached execution

      echo Starting context read with logging protocol...

      REM Create logs directory if it doesn't exist
      if not exist "..\logs" mkdir "..\logs"

      REM Run the context read in background with logging
      cd /d "%~dp0\engine"
      start /b cmd /c "node run_context_read.js > ../logs/context_read.log 2>&1"

      REM Wait a moment
      timeout /t 2 /nobreak >nul

      REM Check the log
      echo Last 5 lines of context_read log:
      powershell -Command "Get-Content ../logs/context_read.log -Tail 5"

      echo.
      echo Context read started in background. Check logs/context_read.log for output.
    size: 675
    tokens: 275
  - path: run_context_read.ps1
    content: |-
      # ECE_Core Context Read Script with Logging Protocol
      # Follows SCRIPT_PROTOCOL.md standards for detached execution

      Write-Host "Starting context read with logging protocol..." -ForegroundColor Green

      # Create logs directory if it doesn't exist
      if (!(Test-Path "../logs")) {
          New-Item -ItemType Directory -Path "../logs" | Out-Null
          Write-Host "Created logs directory" -ForegroundColor Cyan
      }

      # Change to engine directory
      Set-Location -Path "$PSScriptRoot\engine"

      # Start the context read in background with logging
      $process = Start-Process -FilePath "node" -ArgumentList "run_context_read.js" -RedirectStandardOutput "../logs/context_read.log" -RedirectStandardError "../logs/context_read.log" -PassThru -WindowStyle Hidden

      Write-Host "Context read process started with PID: $($process.Id)" -ForegroundColor Green

      # Wait a moment
      Start-Sleep -Seconds 2

      # Check the log
      Write-Host "Last 5 lines of context_read log:" -ForegroundColor Cyan
      Get-Content -Path "../logs/context_read.log" -Tail 5

      Write-Host ""
      Write-Host "Context read started in background. Check logs/context_read.log for output." -ForegroundColor Green
    size: 1128
    tokens: 426
  - path: SCRIPT_PROTOCOL.md
    content: |-
      # Script Running Protocol for ECE_Core

      ## Purpose
      This document outlines the protocol to prevent getting stuck in long-running loops and ensure proper script execution with logging capabilities.

      ## Core Principles

      ### 1. Never Run Long-Running Processes in Attached Mode
      - Always run services in background mode
      - Use `is_background: true` for any process that might run indefinitely
      - Never run servers, watchers, or long-running tasks in attached mode

      ### 2. Log Everything
      - All script outputs must be directed to log files
      - Log files should be named with the same name as the script + `.log` extension
      - Store logs in a dedicated `logs/` directory
      - Example: `server.js` → `logs/server.log`

      ### 3. Time Limits
      - Set explicit timeouts for all operations
      - Use non-blocking alternatives when available
      - Implement graceful termination for long-running processes

      ## Implementation Guidelines

      ### For Node.js Applications
      ```bash
      # Instead of: node server.js
      # Use: node server.js > logs/server.log 2>&1 &

      # For npm scripts:
      # Instead of: npm start
      # Use: npm start > logs/npm_start.log 2>&1 &
      ```

      ### For Python Applications
      ```bash
      # Instead of: python script.py
      # Use: python script.py > logs/script.log 2>&1 &
      ```

      ### For Shell Commands
      ```bash
      # Instead of: long_running_command
      # Use: long_running_command > logs/command.log 2>&1 &
      ```

      ## Directory Structure
      ```
      ECE_Core/
      ├── logs/
      │   ├── server.log
      │   ├── npm_start.log
      │   ├── watchdog.log
      │   └── ...
      ```

      ## Monitoring Protocol
      1. Always check logs instead of waiting for command completion
      2. Use `tail -f logs/filename.log` to monitor in real-time
      3. Implement health checks via HTTP endpoints when possible
      4. Set up process monitoring to detect stuck processes

      ## Recovery Protocol
      1. If a process appears stuck, check the logs first
      2. Kill the process if necessary: `pkill -f process_name`
      3. Restart with proper logging: `command > logs/filename.log 2>&1 &`
      4. Document the issue and update protocols if needed

      ## Best Practices
      - Always verify process is running in background with `jobs` or `ps aux`
      - Use process managers for production services
      - Implement circuit breakers for potentially infinite loops
      - Use timeouts in shell commands: `timeout 300 command`
      - Monitor resource usage to detect runaway processes

      ## Emergency Procedures
      1. If stuck in a loop: Cancel the request immediately
      2. Check running processes: `ps aux | grep -i process_name`
      3. Kill problematic processes: `pkill -f process_name`
      4. Check log files for error patterns
      5. Restart with proper logging protocols

      ## Implementation Checklist
      - [ ] Create logs directory
      - [ ] Update all existing scripts to use logging
      - [ ] Verify background execution for all services
      - [ ] Set up monitoring for common stuck scenarios
      - [ ] Document recovery procedures for each service
    size: 2834
    tokens: 1109
  - path: specs\doc_policy.md
    content: "# Documentation Policy (Root Coda)\r\n\r\n**Status:** Active | **Authority:** Human-Locked\r\n\r\n## Core Philosophy\r\n1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.\r\n2. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.\r\n3. **Brevity:** Text sections must be <500 characters.\r\n4. **Pain into Patterns:** Every major bug must become a Standard.\r\n5. **LLM-First Documentation:** Documentation must be structured for LLM consumption and automated processing.\r\n\r\n## Structure\r\n\r\n### 1. The Blueprint (`specs/spec.md`)\r\n*   **Role:** The single architectural source of truth.\r\n*   **Format:** \"Visual Monolith\".\r\n*   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.\r\n\r\n### 2. The Tracker (`specs/tasks.md`)\r\n*   **Role:** Current work queue.\r\n*   **Format:** Checklist.\r\n*   **Maintenance:** Updated by Agents after every major task.\r\n\r\n### 3. The Roadmap (`specs/plan.md`)\r\n*   **Role:** Strategic vision.\r\n*   **Format:** Phased goals.\r\n\r\n### 4. Standards (`specs/standards/*.md`)\r\n*   **Role:** Institutional Memory (The \"Laws\" of the codebase).\r\n*   **Trigger:** Created after any bug that took >1 hour to fix OR any systemic improvement that affects multiple components.\r\n*   **Format:** \"The Triangle of Pain\"\r\n    1.  **What Happened:** The specific failure mode (e.g., \"Bridge crashed on start\").\r\n    2.  **The Cost:** The impact (e.g., \"3 hours debugging Unicode errors\").\r\n    3.  **The Rule:** The permanent constraint (e.g., \"Force UTF-8 encoding on Windows stdout\").\r\n\r\n### 5. Root-Level Documents\r\n*   **Role:** System-wide protocols and policies.\r\n*   **Examples:** `SCRIPT_PROTOCOL.md`, `README.md`\r\n*   **Purpose:** Critical system-wide protocols that apply to the entire project.\r\n\r\n### 6. Local Context (`*/README.md`)\r\n*   **Role:** Directory-specific context.\r\n*   **Limit:** 1 sentence explaining the folder's purpose.\r\n\r\n### 7. System-Wide Standards\r\n*   **Universal Logging:** All system components must route logs to the central log collection system (Standard 013)\r\n*   **Single Source of Truth:** The log viewer at `/log-viewer.html` is the single point for all system diagnostics\r\n*   **Async Best Practices:** All async/await operations must follow proper patterns for FastAPI integration (Standard 014)\r\n*   **Browser Control Center:** All primary operations must be accessible through unified browser interface (Standard 015)\r\n*   **Detached Script Execution:** All data processing scripts must run in detached mode with logging to `logs/` directory (Standard 025)\r\n*   **Never Attached Mode:** Long-running services and scripts must NEVER be run in attached mode to prevent command-line blocking (Standard 035 in 30-OPS)\r\n*   **Script Running Protocol:** All long-running processes must execute in detached mode with output redirected to timestamped log files (Standard 035 in 30-OPS)\r\n*   **Ghost Engine Connection Management:** All memory operations must handle Ghost Engine disconnections gracefully with proper error reporting and auto-reconnection (Standard 026)\r\n*   **No Resurrection Mode:** System must support manual Ghost Engine control via NO_RESURRECTION_MODE flag (Standard 027)\r\n*   **Default No Resurrection:** Ghost Engine resurrection is disabled by default, requiring manual activation (Standard 028)\r\n*   **Consolidated Data Aggregation:** Single authoritative script for data aggregation with multi-format output (Standard 029)\r\n*   **Multi-Format Output:** Project aggregation tools must generate JSON, YAML, and text outputs for maximum compatibility (Standard 030)\r\n*   **Ghost Engine Stability:** CozoDB schema creation must handle FTS failures gracefully to prevent browser crashes (Standard 031)\r\n*   **Ghost Engine Initialization Flow:** Database initialization must complete before processing ingestion requests to prevent race conditions (Standard 032)\r\n*   **CozoDB Syntax Compliance:** All CozoDB queries must use proper syntax to ensure successful execution (Standard 033)\r\n*   **Node.js Monolith Migration:** System must migrate from Python/Browser Bridge to Node.js Monolith architecture (Standard 034)\r\n\r\n## LLM Protocol\r\n1. **Read-First:** Always read `specs/spec.md`, `SCRIPT_PROTOCOL.md`, AND `specs/standards/` before coding.\r\n2. **Drafting:** When asked to document, produce **Mermaid diagrams** and short summaries.\r\n3. **Editing:** Do not modify `specs/doc_policy.md` or `specs/spec.md` structure unless explicitly instructed.\r\n4. **Archival:** Move stale docs to `archive/` immediately.\r\n5. **Enforcement:** If a solution violates a Standard, reject it immediately.\r\n6. **Standards Evolution:** New standards should follow the \"Triangle of Pain\" format and be numbered sequentially (001, 002, etc.).\r\n7. **Cross-Reference:** When creating new standards, reference related existing standards to maintain consistency.\r\n8. **Detached Mode:** All LLM development scripts must run in detached mode (non-interactive) and log to files in the `logs/` directory with timestamped names (Standard 025).\r\n\r\n---\r\n*Verified by Architecture Council. Edited by Humans Only.*\r\n"
    size: 5134
    tokens: 1959
  - path: specs\plan.md
    content: "# Anchor Core Roadmap (V2.3)\r\n\r\n**Status:** Text-Only + Watchdog Deployed\r\n**Focus:** Stability & Passive Text Ingestion.\r\n\r\n## Phase 1: Foundation (Completed)\r\n- [x] Pivot to WebLLM/WebGPU stack.\r\n- [x] Implement CozoDB (WASM) for memory.\r\n- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).\r\n\r\n## Phase 2: Stabilization (Completed)\r\n- [x] Fix Model Loading (Quota/VRAM config).\r\n- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).\r\n- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).\r\n\r\n## Phase 2.5: Root Refactor (Completed)\r\n- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).\r\n- [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.\r\n- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.\r\n- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).\r\n\r\n## Phase 3: Expansion & Hardening (Completed)\r\n- [x] **Resource Hardening**: Implemented \"Consciousness Semaphore\" in `sovereign.js`.\r\n- [x] **Documentation Refactor**: Executed \"Visual Monolith\" strategy.\r\n- [x] **Memory Hygiene**: Implement \"Forgetting Curve\" in `root-dreamer.html`.\r\n- [x] **Active Memory Persistence**: Enable chat to write back to the Graph.\r\n- [x] **Temporal Awareness**: Ground the model in real-time.\r\n- [x] **Mobile Optimization**: Polish mobile UX for `model-server-chat.html`.\r\n\r\n## Phase 4: Text-Only Architecture (Completed)\r\n- [x] **Vision Removal**: Remove brittle Vision/Ollama dependencies.\r\n- [x] **Watchdog Implementation**: Create passive text ingestion service.\r\n- [x] **Debounce & Hash Check**: Prevent duplicate file ingestion.\r\n- [x] **Auto-Resurrection**: Enhance browser process management.\r\n- [x] **Streaming CLI**: Improve terminal UX with streaming responses.\r\n\r\n## Phase 5: Context Expansion & Persistence (Completed)\r\n- [x] **Code File Support**: Expand to monitor programming language extensions.\r\n- [x] **Browser Profile Management**: Implement temporary profile cleanup.\r\n- [x] **Chat Session Persistence**: Auto-save conversations to context directory.\r\n- [x] **Ingestion Loop Closure**: Ensure chat sessions become ingested context.\r\n\r\n## Phase 6: Session Recorder & Text-File Source of Truth (Completed)\r\n- [x] **Daily Session Files**: Create `chat_YYYY-MM-DD.md` files for each day's conversations.\r\n- [x] **Text-File Source of Truth**: Implement \"Database is Cache\" philosophy.\r\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access.\r\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat.\r\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking.\r\n\r\n## Phase 7: Model Loading Reliability (Completed)\r\n- [x] **URL Construction Fix**: Implemented `/models/{model}/resolve/main/{file}` redirect for MLC-LLM compatibility.\r\n- [x] **File Renaming**: Standardized component names (`anchor-mic.html`, `memory-builder.html`, `db_builder.html`).\r\n- [x] **Server Stability**: Fixed hanging issues with problematic path parameter syntax.\r\n- [x] **Endpoint Completeness**: Verified all documented endpoints are accessible.\r\n\r\n## Phase 5.5: Search Enhancement (Completed)\r\n- [x] **BM25 Implementation**: Replaced regex-based search with CozoDB FTS using BM25 algorithm.\r\n- [x] **Hybrid Search**: Combined vector search (semantic) with BM25 (lexical) for better results.\r\n- [x] **Index Creation**: Added FTS index creation in memory initialization routines.\r\n- [x] **Stemming Support**: Enabled English stemming for improved word variation matching.\r\n\r\n## Phase 6: GPU Resource Management (Completed)\r\n- [x] **GPU Queuing System**: Implemented automatic queuing for GPU resource requests to prevent conflicts\r\n- [x] **Resource Status Management**: Added GPU lock status tracking with owner identification\r\n- [x] **503 Error Resolution**: Fixed \"Service Unavailable\" errors by implementing proper resource queuing\r\n- [x] **Endpoint Integration**: Added `/v1/gpu/lock`, `/v1/gpu/unlock`, `/v1/gpu/status` endpoints\r\n- [x] **Log Integration**: Added GPU resource management to centralized logging system\r\n\r\n## Phase 7: Async/Await Best Practices (Completed)\r\n- [x] **Coroutine Fixes**: Resolved \"coroutine was never awaited\" warnings in webgpu_bridge.py\r\n- [x] **Event Loop Integration**: Properly integrated async functions with FastAPI's event loop\r\n- [x] **Startup Sequence**: Ensured logging system initializes properly with application lifecycle\r\n- [x] **Resource Management**: Fixed resource cleanup in WebSocket handlers to prevent leaks\r\n- [x] **Error Handling**: Enhanced async error handling with proper cleanup procedures\r\n\r\n## Phase 8: Browser-Based Control Center (Completed)\r\n- [x] **UI Integration**: Implemented browser-based sidecar with retrieval and vision tabs\r\n- [x] **Vision Engine**: Created Python-powered VLM integration for image analysis\r\n- [x] **Endpoint Expansion**: Added vision ingestion and enhanced logging endpoints\r\n- [x] **File Logging**: Implemented persistent file-based logging with truncation\r\n- [x] **UI Serving**: Extended bridge to serve HTML interfaces for unified workflow\r\n\r\n## Phase 9: Context Ingestion Pipeline Fixes (Completed)\r\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\r\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\r\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\r\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\r\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\r\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\r\n\r\n## Phase 10: Federation\r\n- [ ] **Device Sync**: Sync IndexedDB across devices (Peer-to-Peer).\r\n- [ ] **Local-First Cloud**: Optional encrypted backup.\r\n"
    size: 6033
    tokens: 2261
  - path: specs\spec.md
    content: "# Anchor Core: The Visual Monolith (v4.0)\r\n\r\n**Status:** Node.js Monolith + CozoDB (RocksDB) + Snapshot Portability | **Philosophy:** Database as Source of Truth, Resource-Queued.\r\n\r\n## 1. The Anchor Architecture\r\nThe **Anchor Core** is now a Node.js Monolith (`engine/src/index.js`). It integrates the API, File Watcher, and CozoDB (RocksDB) into a single process.\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph Node_Monolith [Localhost:3000]\r\n        Express[Express API]\r\n        Watcher[Chokidar Watcher]\r\n        Cozo[CozoDB + RocksDB]\r\n        FTS[BM25 Search Engine]\r\n    end\r\n\r\n    subgraph Storage\r\n        DB_File[\"engine/context.db\"]\r\n        Backups[\"backups/*.yaml\"]\r\n    end\r\n\r\n    subgraph Ingestion_Sources\r\n        Context_Folder[\"context/ folder\"]\r\n        Sessions[\"context/sessions/\"]\r\n        Coding_Notes[\"context/Coding-Notes/\"]\r\n    end\r\n\r\n    Ingestion_Sources -->|File Events| Watcher\r\n    Watcher -->|:replace| Cozo\r\n    Express -->|Query| Cozo\r\n    Cozo -->|Persist| DB_File\r\n    \r\n    User -->|Search/Ingest| Express\r\n    \r\n    subgraph Portability_Loop\r\n        Cozo -->|Eject| Backups\r\n        Backups -->|Hydrate| Cozo\r\n    end\r\n```\r\n\r\n## 2. Memory Lifecycle (Standard 037)\r\nThe system follows a \"Docker-style\" build-and-ship workflow:\r\n1. **Ingest**: Watcher indexes local files into CozoDB.\r\n2. **Eject**: Export DB state to a portable YAML Snapshot.\r\n3. **Ship**: Move the YAML Snapshot to any machine.\r\n4. **Hydrate**: Restore the DB state instantly from the Snapshot.\r\n\r\n## 3. Port Map\r\n\r\n* **3000**: **The One Port.** Serves UI, API, and Database operations.\r\n\r\n## 4. Search Architecture\r\n\r\n* **BM25 FTS**: Native CozoDB Full Text Search with relevance scoring.\r\n* **Truncation**: Large search results are automatically truncated to prevent API overflows.\r\n\r\n* **Auto-Resurrection**: Automatic recovery when Ghost Engine becomes available\r\n* **Queue Processing**: Pending operations are processed when connection resumes\r\n\r\n## 6. No Resurrection Mode (Standard 027)\r\n\r\n* **Manual Control**: Option to disable automatic Ghost Engine launching via NO_RESURRECTION_MODE flag\r\n* **Resource Efficiency**: Reduces resource usage by avoiding automatic browser launches\r\n* **User Flexibility**: Allows users to connect Ghost Engine manually when needed\r\n* **Existing Browser**: Enables use of existing browser windows instead of launching headless instances\r\n* **Environment Variable**: Controlled via `set NO_RESURRECTION_MODE=true` before startup\r\n\r\n## 7. Default No Resurrection Behavior (Standard 028)\r\n\r\n* **Default Setting**: Ghost Engine resurrection is now disabled by default for resource efficiency\r\n* **Manual Activation Required**: Users must explicitly open ghost.html to connect the Ghost Engine\r\n* **Environment Override**: Set `NO_RESURRECTION_MODE=false` to enable auto-launching\r\n* **Queued Processing**: Files and requests are queued until Ghost Engine connects\r\n* **User Control**: Provides maximum control over when computational resources are used\r\n\r\n## 8. Consolidated Data Aggregation (Standard 029)\r\n\r\n* **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation\r\n* **Multi-Format Output**: Generates three formats - text corpus, JSON memory, and YAML memory\r\n* **YAML Formatting**: Uses proper multiline string formatting for readability\r\n* **Encoding Handling**: Robust encoding detection using chardet for reliable processing\r\n* **Recursive Processing**: Processes all subdirectories while respecting exclusions\r\n* **Metadata Preservation**: Maintains file metadata in structured outputs\r\n\r\n## 9. Multi-Format Output for Project Aggregation (Standard 030)\r\n\r\n* **Multi-Format Support**: The `read_all.py` script in the root directory generates both JSON and YAML versions of memory records\r\n* **YAML Formatting**: Uses proper multiline string formatting (literal style with `|`) for readability\r\n* **Consistent Naming**: Output files follow consistent naming patterns (`combined_text.txt`, `combined_memory.json`, `combined_text.yaml`)\r\n* **Custom Representers**: Implements custom YAML representers to handle multiline content appropriately\r\n* **Maximum Compatibility**: Provides format flexibility for different downstream processing tools\r\n\r\n## 10. Ghost Engine Stability Fix (Standard 031)\r\n\r\n* **Separate Schema Creation**: Basic schema and FTS index creation must be handled separately to prevent failures\r\n* **Graceful FTS Handling**: FTS creation failures should not prevent basic database functionality\r\n* **Error Prevention**: Proper error handling prevents \"undefined\" error messages\r\n* **Browser Stability**: Prevents browser crashes during database initialization\r\n* **Fallback Operations**: System continues to function even if advanced features fail\r\n\r\n## 11. Ghost Engine Initialization Flow (Standard 032)\r\n\r\n* **Sequential Initialization**: Database must be fully initialized before signaling readiness to Bridge\r\n* **Database Readiness Checks**: All operations verify database is properly initialized before execution\r\n* **Proper Error Handling**: Return appropriate errors when database is not ready instead of failing silently\r\n* **Synchronous Connection Flow**: Connect → Initialize Database → Signal Ready → Process Requests\r\n* **Graceful Degradation**: Report initialization failures and avoid processing requests when database fails\r\n* **Message Type Support**: Properly handle all message types including error responses\r\n\r\n## 12. CozoDB Syntax Compliance (Standard 033)\r\n\r\n* **Schema Creation Syntax**: Use proper CozoDB syntax without line breaks in schema definitions\r\n* **FTS Creation Syntax**: Use correct FTS creation syntax for full-text search indexes\r\n* **Insert Query Syntax**: Use proper `:insert` or `:replace` syntax with correct parameter binding\r\n* **Parameter Formatting**: Format parameters correctly as nested arrays for bulk operations\r\n* **Schema Validation**: Properly propagate schema creation success/failure status\r\n* **Error Propagation**: Ensure all database operations properly handle and report errors\r\n\r\n## 13. Node.js Monolith Migration (Standard 034)\r\n\r\n* **Node.js Runtime**: Use Node.js as the primary runtime environment for the Context Engine\r\n* **CozoDB Integration**: Integrate CozoDB directly using `cozo-node` for persistent storage\r\n* **Autonomous Execution**: Implement Protocol 001 for detached service execution with proper logging\r\n* **File Watchdog**: Use `chokidar` for efficient file system monitoring and automatic ingestion\r\n* **Standardized Endpoints**: Implement standardized API endpoints for ingestion, querying, and health checks\r\n* **Legacy Archival**: Archive all V2 Python infrastructure to preserve historical code\r\n* **JavaScript Conversion**: Convert Python utility scripts to JavaScript equivalents for consistency\r\n* **Platform Compatibility**: Ensure architecture works on Termux/Linux environments\r\n\r\n## 14. Never Attached Mode (Standard 035)\r\n\r\n* **Detached Execution Only**: All long-running services must be started in detached mode using appropriate backgrounding techniques\r\n* **No Attached Mode**: Never run services like `npm start`, `python server.py` or similar long-running processes directly in an attached terminal session\r\n* **Proper Logging**: All detached processes must log to the designated `logs/` directory for monitoring and debugging\r\n* **Platform-Specific Detaching**: Use appropriate backgrounding techniques for each platform (nohup, start /min, etc.)\r\n* **Verification Method**: Verify detached services are running by checking logs or connecting to interfaces, not by waiting for terminal output\r\n* **Documentation Requirement**: All startup procedures must specify detached execution methods\r\n* **Domain Organization**: Standards are now organized by domain (CORE, ARCH, DATA, OPS, BRIDGE) for easier navigation\r\n"
    size: 7846
    tokens: 2904
  - path: specs\standards\00-CORE\002-cache-api-security-policy.md
    content: "# Standard 002: Cache API Security Policy\r\n\r\n## What Happened?\r\nBrowser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as \"data in RAM\" rather than \"persistent storage\", causing initialization failures.\r\n\r\n## The Cost\r\n- Multiple failed model loading attempts\r\n- Browser security errors preventing WebGPU initialization\r\n- \"Stealth Mode\" required to bypass strict policies\r\n- Significant debugging time to understand browser security model\r\n\r\n## The Rule\r\n1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:\r\n   ```python\r\n   class NoCacheStaticFiles(StaticFiles):\r\n       async def __call__(self, scope, receive, send):\r\n           async def send_wrapper(message):\r\n               if message['type'] == 'http.response.start':\r\n                   headers = message.get('headers', [])\r\n                   headers.extend([\r\n                       (b\"Cache-Control\", b\"no-store, no-cache, must-revalidate, proxy-revalidate\"),\r\n                       (b\"Pragma\", b\"no-cache\"),\r\n                       (b\"Expires\", b\"0\"),\r\n                   ])\r\n                   message['headers'] = headers\r\n               await send(message)\r\n           await super().__call__(scope, receive, send_wrapper)\r\n   ```\r\n\r\n2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.\r\n\r\n3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components."
    size: 1610
    tokens: 585
  - path: specs\standards\00-CORE\012-context-utility-manifest.md
    content: "<<<<<<< HEAD\r\n# Standard 012: Context Utility Manifest\r\n\r\n**Authority:** Active | **Philosophy:** Invisible Infrastructure\r\n\r\n## The Principle\r\nThe Anchor Core is not a \"Chat App\". It is a **Context Utility** (like electricity or WiFi).\r\n1.  **Headless First**: The system must provide value without a visible UI window.\r\n2.  **Passive Observation**: Data ingestion should happen automatically (Daemon Eyes) rather than requiring manual user input.\r\n3.  **Universal Availability**: Context must be accessible via standard HTTP endpoints (`/v1/memory/search`) to any client (Terminal, VS Code, Browser).\r\n\r\n## The Rules\r\n1.  **No UI Blocking**: Long-running tasks (like VLM analysis) MUST run in background threads/processes.\r\n2.  **Zero-Touch Ingestion**: Screen/Audio capture must require zero clicks after initial activation.\r\n3.  **Ground Truth**: All ingested context is immutable \"Ground Truth\" until proven otherwise.\r\n=======\r\n# Standard 012: Context Utility Manifest - The Invisible Infrastructure\r\n\r\n## What Happened?\r\nThe Anchor Core system was originally conceived as a chat application, but has evolved into a unified cognitive infrastructure. The system now needs to transition from \"active user input\" to \"passive observation\" to function as truly invisible infrastructure like electricity - always present but never demanding attention.\r\n\r\n## The Cost\r\n- UI bloat with multiple chat interfaces competing for user attention\r\n- Manual data entry required to populate context\r\n- Users having to copy/paste information instead of automatic capture\r\n- Architecture treating UI as primary rather than as debugging tool\r\n- Missing opportunity to create true \"ambient intelligence\"\r\n\r\n## The Rule\r\n1. **Headless by Default**: All core functionality must operate without user interface interaction\r\n   ```python\r\n   # Core services run as background daemons\r\n   daemon_services = [\r\n       \"memory_graph\",      # CozoDB persistence\r\n       \"gpu_engine\",        # WebLLM inference\r\n       \"context_capture\",   # Screen/Audio observation\r\n       \"data_ingestion\"     # Memory writing\r\n   ]\r\n   ```\r\n\r\n2. **Passive Observation**: System captures context automatically rather than waiting for user input\r\n   - **Eyes**: Automated screen sampling and OCR\r\n   - **Ears**: Continuous audio transcription (when enabled)\r\n   - **Memory**: Automatic ingestion without user intervention\r\n\r\n3. **Architecture Priority**: `webgpu_bridge.py` is the nervous system; UIs are merely debugging/interaction tools\r\n   - UIs are temporary visualization layers\r\n   - Core logic exists independently of any UI\r\n   - Background services operate without UI presence\r\n\r\n4. **Invisible Utility**: The system should function like electricity - always available, rarely noticed, essential infrastructure\r\n   - Zero user interaction required for core functions\r\n   - Automatic context capture and storage\r\n   - Seamless integration with user's workflow\r\n\r\n5. **Context First**: Prioritize capturing and understanding user context over responding to queries\r\n   - Short-term context populated automatically\r\n   - Long-term memory built passively\r\n   - Responses based on observed reality rather than explicit input\r\n\r\n## Implementation Requirements\r\n\r\n### Core Daemon Services\r\n- **Memory Daemon**: Continuous CozoDB operations in background\r\n- **Vision Daemon**: Automated screen capture and OCR (daemon_eyes.py)\r\n- **Audio Daemon**: Optional background audio processing\r\n- **Ingestion Daemon**: Automatic data flow to memory graph\r\n\r\n### API-First Design\r\n- All functionality accessible via API endpoints\r\n- UIs as thin clients consuming API services\r\n- Background services operating independently\r\n\r\n### Error Handling\r\n- Daemons must handle errors gracefully without user intervention\r\n- Automatic recovery from common failures\r\n- Silent operation with optional logging for debugging\r\n\r\n## Transition Protocol\r\n\r\nWhen implementing new features:\r\n1. Design for headless operation first\r\n2. Add UI as optional visualization layer\r\n3. Ensure all functionality available via API\r\n4. Test daemon operation independently of UI\r\n\r\nThis standard ensures that Anchor Core evolves into true invisible infrastructure rather than remaining a traditional application.\r\n>>>>>>> 3cd511631b7eaf7d033a1bacccff36325545fc78\r\n"
    size: 4285
    tokens: 1591
  - path: specs\standards\00-CORE\027-no-resurrection-mode.md
    content: |-
      # Standard 027: No Resurrection Mode for Manual Ghost Engine Control

      ## What Happened?
      The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues for users who wanted to use an existing browser window or manually control when the Ghost Engine connects. Users needed an option to disable the automatic resurrection protocol and connect the Ghost Engine manually when needed.

      ## The Cost
      - Unnecessary browser processes launched automatically
      - Resource usage when Ghost Engine not needed
      - Inability to use existing browser windows for Ghost Engine operations
      - Confusion when multiple browser instances were running
      - Users wanting more control over when the Ghost Engine connects

      ## The Rule
      1. **Environment Variable Control**: The system must support a `NO_RESURRECTION_MODE=true` environment variable to disable automatic Ghost Engine launching.

      2. **Conditional Launch**: When `NO_RESURECTION_MODE=true`, the system shall NOT automatically launch the Ghost Engine during startup.

      3. **Manual Connection**: In no resurrection mode, users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

      4. **Clear Messaging**: The system shall provide clear instructions to users when no resurrection mode is enabled, indicating they need to open ghost.html manually.

      5. **Resource Management**: When no resurrection mode is enabled, the system shall not attempt to kill browser processes during shutdown.

      6. **API Behavior**: API endpoints that require the Ghost Engine shall return appropriate 503 errors with clear messaging when the Ghost Engine is disconnected, regardless of resurrection mode setting.

      ## Implementation
      - Set environment variable: `set NO_RESURRECTION_MODE=true` before running `start-anchor.bat`
      - The Bridge will log a message indicating manual connection is required
      - Users open `http://localhost:8000/ghost.html` in their browser to connect the Ghost Engine
      - All functionality remains the same, just with manual control over Ghost Engine connection
    size: 2089
    tokens: 817
  - path: specs\standards\00-CORE\028-default-no-resurrection-mode.md
    content: |-
      # Standard 028: Configuration-Driven System with Default No Resurrection Mode

      ## What Happened?
      The system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues with resource usage and prevented users from controlling when the Ghost Engine connects. The system now defaults to "No Resurrection Mode" where the Ghost Engine must be manually started by opening ghost.html in the browser. Additionally, ALL system variables are now abstracted to a central configuration file (config.json) to support future settings menu implementation.

      ## The Cost
      - Excessive resource usage from automatically launching headless browser
      - Browser processes that couldn't be controlled by the user
      - Confusion when multiple browser instances were running
      - Unnecessary complexity in the startup process
      - Users wanting more control over when the Ghost Engine connects
      - Hard-coded values throughout the codebase that made customization difficult

      ## The Rule
      1. **Default Behavior**: The system shall default to `NO_RESURRECTION_MODE=true`, meaning the Ghost Engine is not automatically launched.

      2. **Manual Connection**: Users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.

      3. **Environment Override**: Users can set `NO_RESURRECTION_MODE=false` to return to auto-launching behavior.

      4. **Queued Operations**: When Ghost Engine is disconnected, operations shall be queued and processed when connection is established.

      5. **Clear Messaging**: The system shall provide clear instructions when Ghost Engine is not connected, indicating how to establish the connection.

      6. **Configurable Values**: All system parameters shall be configurable via the config.json file, including:
         - Server settings (port, host, CORS origins)
         - Ghost Engine settings (auto resurrection, browser paths, flags)
         - Logging configuration (max lines, directory, format)
         - Memory settings (max ingest size, default limits, char limits)
         - GPU management (enabled, concurrent ops, timeout)
         - Model loading (timeout, default model, base URL)
         - Watchdog settings (enabled, watch directory, allowed extensions, debounce time)

      7. **Detached Operation**: All scripts shall run in detached mode with logging to the logs/ directory as per Standard 025.

      ## Implementation
      - Default configuration sets `"ghost_engine.auto_resurrection_enabled": false`
      - The start-anchor.bat script defaults to NO_RESURRECTION_MODE=true
      - All system variables abstracted to config.json with config_manager.py
      - Watchdog logs appropriate messages when Ghost Engine is disconnected
      - API endpoints return 503 with clear messaging when Ghost Engine is disconnected
      - Files are queued for ingestion when Ghost Engine is not available
      - Created start_anchor_detached.py for proper detached operation with logging
    size: 2873
    tokens: 1104
  - path: specs\standards\10-ARCH\003-webgpu-initialization-stability.md
    content: "# Standard 003: WebGPU Initialization Stability\r\n\r\n## What Happened?\r\nWebGPU failed to initialize properly in headless browsers, causing GPU access failures and preventing AI model execution. This occurred because browsers require visible windows for GPU access in some configurations.\r\n\r\n## The Cost\r\n- Failed AI model execution in headless environments\r\n- Hours of debugging GPU initialization issues\r\n- Unreliable AI processing in automated systems\r\n- Need for complex workarounds to achieve stable GPU access\r\n\r\n## The Rule\r\n1. **Minimized Window Approach:** Always use `--start-minimized` flag when launching headless browsers that require GPU access:\r\n   ```bash\r\n   start \"Ghost Engine\" /min msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222\r\n   ```\r\n\r\n2. **GPU Buffer Configuration:** Implement 256MB override for Adreno GPUs and other constrained hardware:\r\n   ```javascript\r\n   // In WebGPU configuration\r\n   const adapter = await navigator.gpu.requestAdapter({\r\n       powerPreference: 'high-performance',\r\n       forceFallbackAdapter: false\r\n   });\r\n   ```\r\n\r\n3. **Hardware Abstraction Layer:** Use clamp buffer techniques for Snapdragon/Mobile stability to prevent VRAM crashes.\r\n\r\n4. **Consciousness Semaphore:** Ensure resource arbitration between different components to prevent GPU memory conflicts."
    size: 1372
    tokens: 508
  - path: specs\standards\10-ARCH\004-wasm-memory-management.md
    content: "# Standard 004: WASM Memory Management\r\n\r\n## What Happened?\r\nWASM applications experienced \"memory access out of bounds\" errors and crashes when handling large JSON payloads or complex database operations. This was particularly problematic in `sovereign-db-builder.html` and `unified-coda.html` where JSON parameters were passed to `db.run()`.\r\n\r\n## The Cost\r\n- Crashes during database operations in browser-based CozoDB\r\n- \"Maximum call stack size exceeded\" errors with large JSON payloads\r\n- Unreliable memory operations in browser-based systems\r\n- Hours of debugging memory access violations in WASM\r\n\r\n## The Rule\r\n1. **JSON Stringification:** Always properly stringify JSON parameters before passing to WASM functions:\r\n   ```javascript\r\n   // Before calling db.run() or similar WASM functions\r\n   const jsonString = JSON.stringify(data);\r\n   db.run(query, jsonString);\r\n   ```\r\n\r\n2. **Payload Size Limits:** Implement size checks before processing large JSON payloads in browser workers:\r\n   ```javascript\r\n   if (JSON.stringify(payload).length > MAX_SAFE_SIZE) {\r\n       // Handle large payloads differently or chunk them\r\n   }\r\n   ```\r\n\r\n3. **Error Handling:** Add timeout protection and fallback mechanisms for hanging WASM calls:\r\n   ```javascript\r\n   try {\r\n       const result = await Promise.race([\r\n           db.run(query),\r\n           new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))\r\n       ]);\r\n   } catch (error) {\r\n       // Handle timeout or memory errors gracefully\r\n   }\r\n   ```\r\n\r\n4. **IndexedDB Fallback:** Use `CozoDb.new_from_indexed_db` instead of `new_from_path` for persistent browser storage to avoid filesystem access issues."
    size: 1690
    tokens: 626
  - path: specs\standards\10-ARCH\005-model-loading-configuration.md
    content: |-
      # Standard 005: Model Loading Configuration & Endpoint Verification

      ## What Happened?
      Model loading failed due to various configuration issues including "Cannot find model record" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.

      ## The Cost
      - Failed model initialization preventing AI functionality
      - Multiple 404 errors for specific model types
      - 503 and 405 errors during embedding and model download requests
      - Hours spent debugging model configuration issues
      - Unreliable model loading across different model types
      - Significant time wasted discovering that documented endpoints didn't exist in the backend
      - Frontend-backend integration failures due to missing API endpoints

      ## The Rule
      1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:
         ```python
         # Example mapping for problematic models
         MODEL_MAPPINGS = {
             'OpenHermes': 'Mistral-v0.3',
             'NeuralHermes': 'Mistral-v0.3',
             # Add other mappings as needed
         }
         ```

      2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:
         ```python
         # In webgpu_bridge.py - ensure flexible model name handling
         # Don't validate model names strictly on the bridge side
         ```

      3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:
         ```javascript
         // In frontend code
         const internalModelId = getModelInternalId(userModelName);
         const modelUrl = getModelUrl(internalModelId);
         ```

      4. **Verification Registry:** Maintain `specs/mlc-urls.md` as a registry for verified WASM binaries to ensure compatibility.

      5. **Bridge-Based URLs:** Use bridge-based model URLs (`http://localhost:8000/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.

      6. **Endpoint Verification Protocol:** Always verify that documented endpoints exist in the backend implementation before deploying frontend code that depends on them:
         ```python
         # Example: Required endpoints for model management
         REQUIRED_ENDPOINTS = [
             "/v1/models/pull",
             "/v1/models/pull/status",
             "/v1/gpu/lock",
             "/v1/gpu/unlock",
             "/v1/gpu/status",
             "/v1/gpu/reset",
             "/v1/gpu/force-release-all",
             "/v1/system/spawn_shell",
             "/v1/shell/exec"
         ]
         ```

      7. **Documentation-Implementation Synchronization:** When documenting an endpoint, immediately implement it in the backend to prevent documentation-code drift.

      8. **Server Startup Verification:** After adding new endpoints, always verify that the server starts properly and doesn't hang due to problematic async operations or path parameter syntax:
         - Test import functionality: `python -c "import webgpu_bridge; print('Import successful')"`
         - Verify server startup and response to requests
         - Avoid problematic syntax like `:path` in route definitions that can cause server hangs
         - Use simple synchronous operations when possible to avoid blocking the event loop
    size: 3395
    tokens: 1280
  - path: specs\standards\10-ARCH\006-model-url-construction-fix.md
    content: |-
      # Standard 006: Model URL Construction for MLC-LLM Compatibility

      ## What Happened?
      The Anchor Console (`chat.html`) failed to load models with the error "TypeError: Failed to construct 'URL': Invalid URL", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.

      ## The Cost
      - 4+ hours debugging model loading failures
      - Confusion between working and failing components
      - Inconsistent model loading across different UI components
      - User frustration with non-functional chat interface
      - Multiple failed attempts with different URL construction approaches

      ## The Rule
      1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path}` endpoint to redirect MLC-LLM requests to local model files:
         ```python
         @app.get("/models/{model_name}/resolve/main/{file_path:path}")
         async def model_resolve_redirect(model_name: str, file_path: str):
             import os
             from fastapi.responses import FileResponse, JSONResponse

             models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models")
             actual_path = os.path.join(models_base, model_name, file_path)

             if os.path.exists(actual_path) and os.path.isfile(actual_path):
                 return FileResponse(actual_path)
             else:
                 return JSONResponse(status_code=404, content={
                     "error": f"File {file_path} not found for model {model_name}"
                 })
         ```

      2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.

      3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.

      4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.

      5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly.
    size: 2217
    tokens: 838
  - path: specs\standards\10-ARCH\007-model-loading-transition-standard.md
    content: |-
      # Standard 007: Model Loading Transition - Online-Only Implementation

      ## What Happened?
      The Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.

      The old implementation in `chat.html` was trying to:
      1. Check for local model files using the `/models/{model}/resolve/main/` pattern
      2. Download models through the bridge if not found locally
      3. Use a complex configuration with multiple model entries and local file resolution

      This approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.

      ## The Cost
      - Hours spent debugging model loading failures in `chat.html`
      - Confusion between working and failing components (anchor-mic.html vs chat.html)
      - Inconsistent model loading across different UI components
      - User frustration with non-functional chat interface
      - Time wasted on attempting to fix complex local model resolution logic
      - Delayed development due to complex debugging of the local file + bridge download approach

      ## The Rule
      1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:
         ```javascript
         // Use direct HuggingFace URLs like anchor-mic.html
         const appConfig = {
             model_list: [{
                 model: "https://huggingface.co/" + selectedModelId + "/resolve/main/",
                 model_id: selectedModelId,
                 model_lib: modelLib,  // WASM library URL
                 // ... other config
             }],
             useIndexedDBCache: false, // Disable caching to prevent issues
         };
         ```

      2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.

      3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:
         ```javascript
         // Archive the old function with a descriptive name
         async function loadModel_archived() {
             // Original complex implementation
         }
         
         // Implement the working online-only approach
         async function loadModel() {
             // Simplified online-only implementation
         }
         ```

      4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.

      5. **Progressive Enhancement**: Start with a working online-only solution, then add local model loading capabilities in a separate iteration after the basic functionality is stable.

      6. **Model Loading Verification**: Always test model loading with the same models across different UI components to ensure consistency.

      ## Implementation Pattern

      ### Working Online-Only Format (Recommended):
      ```javascript
      // Based on the working anchor-mic.html implementation
      async function loadModel() {
          // ... setup code ...
          
          const appConfig = {
              model_list: [{
                  model: "https://huggingface.co/" + selectedModelId + "/resolve/main/",
                  model_id: selectedModelId,
                  model_lib: modelLib,  // WASM library URL from mapper
                  vram_required_MB: 2000,
                  low_resource_required: true,
                  buffer_size_required_bytes: gpuConfig.maxBufferSize,
                  overrides: {
                      context_window_size: gpuConfig.isConstrained ? 2048 : 4096
                  }
              }],
              useIndexedDBCache: false, // Disable caching to prevent issues
          };

          engine = await CreateWebWorkerMLCEngine(
              new Worker('./modules/llm-worker.js', { type: 'module' }),
              selectedModelId,
              {
                  initProgressCallback: (report) => {
                      // Progress reporting
                  },
                  appConfig: appConfig,
                  logLevel: "INFO",
                  useIndexedDBCache: false, // Force disable cache
              }
          );
      }
      ```

      ### Complex Local Resolution (Problematic - Avoid):
      ```javascript
      // DO NOT USE - This causes hangs after GPU configuration
      // Complex local file checking and bridge download logic
      const localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;
      const check = await fetch(localModelUrl, { method: 'HEAD' });
      // ... complex download and resolution logic that causes hangs
      ```

      ## Transition Protocol

      When transitioning model loading implementations:

      1. **Identify Working Component**: Find a UI component that successfully loads models (e.g., `anchor-mic.html`)
      2. **Analyze Working Pattern**: Study the model loading approach in the working component
      3. **Archive Complex Logic**: Preserve the old implementation for future reference
      4. **Implement Simple Approach**: Adopt the working pattern from the successful component
      5. **Test Thoroughly**: Verify the new implementation works with multiple models
      6. **Document Changes**: Record the transition in standards documentation

      ## Future Considerations

      The archived local model loading approach should be revisited after further prototyping and debugging. The online-only approach provides immediate functionality while the more complex local approach can be refined separately without blocking development progress.
    size: 5486
    tokens: 2014
  - path: specs\standards\10-ARCH\008-model-loading-online-only-approach.md
    content: "# Standard 008: Model Loading - Online-Only Approach for Browser Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.\r\n\r\nThe issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.\r\n\r\n## The Cost\r\n- All models showing as unavailable in API tests\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with limited model availability\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: window.location.origin + \"/models/\" + selectedModelId, // This will redirect to online source\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Bridge Redirect Endpoint**: Ensure the `/models/{model}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist."
    size: 2401
    tokens: 916
  - path: specs\standards\10-ARCH\009-model-loading-configuration-bridge-vs-direct.md
    content: "# Standard 009: Model Loading Configuration - Bridge vs Direct Online\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:\r\n\r\n1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources\r\n2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser\r\n\r\nThe inconsistency occurred because:\r\n- Some components (like `anchor-mic.html`) work with direct online URLs\r\n- Other components (like `chat.html`) were configured for local file resolution\r\n- The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist\r\n\r\n## The Cost\r\n- Confusion about which model loading approach to use\r\n- Inconsistent behavior across different UI components\r\n- Models working in some components but not others\r\n- Debugging time spent on understanding different loading mechanisms\r\n- Users experiencing different model availability depending on which UI they use\r\n\r\n## The Rule\r\n1. **Consistent Model Configuration**: All UI components should use the same model loading approach:\r\n   ```javascript\r\n   // Recommended configuration pattern\r\n   const modelConfig = {\r\n       model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect\r\n       model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID\r\n       model_lib: modelLib,                                  // WASM library URL\r\n   };\r\n   ```\r\n\r\n2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:\r\n   - First check for local files in the models directory\r\n   - If local file doesn't exist, redirect to the corresponding HuggingFace URL:\r\n     `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`\r\n\r\n3. **Fallback Handling**: Implement proper fallback when local files are not available:\r\n   ```javascript\r\n   // In the UI, handle both local and online availability\r\n   async function loadModel(modelId) {\r\n       try {\r\n           // Try bridge-based loading first\r\n           await initializeEngine(bridgeConfig(modelId));\r\n       } catch (error) {\r\n           // Fallback to direct online loading if bridge fails\r\n           await initializeEngine(onlineConfig(modelId));\r\n       }\r\n   }\r\n   ```\r\n\r\n4. **Testing Protocol**: Test model loading through both pathways:\r\n   - Verify local file resolution works when files exist\r\n   - Verify online fallback works when local files don't exist\r\n   - Test both the redirect endpoint and direct access patterns\r\n\r\n5. **Documentation Consistency**: All UI components should follow the same documented approach to avoid confusion."
    size: 2777
    tokens: 1035
  - path: specs\standards\10-ARCH\014-async-best-practices.md
    content: "# Standard 014: Async/Await Best Practices for FastAPI\r\n\r\n## What Happened?\r\nThe system had multiple \"coroutine was never awaited\" warnings due to improper async/await usage in the webgpu_bridge.py. These warnings occurred when async functions were called without being properly awaited or when they weren't integrated correctly with FastAPI's event loop system.\r\n\r\n## The Cost\r\n- Runtime warnings cluttering the console output\r\n- Potential resource leaks from improperly handled async operations\r\n- Unpredictable behavior in WebSocket connections and API endpoints\r\n- Difficulty debugging real issues due to noise from async warnings\r\n\r\n## The Rule\r\n1. **Proper Await Usage**: All async functions must be awaited when called within async contexts\r\n   ```python\r\n   # Correct\r\n   await add_log_entry(\"source\", \"type\", \"message\")\r\n   \r\n   # Incorrect\r\n   add_log_entry(\"source\", \"type\", \"message\")  # Creates unawaited coroutine\r\n   ```\r\n\r\n2. **Event Loop Integration**: When scheduling tasks at module level, ensure they run within an active event loop:\r\n   ```python\r\n   # Correct - in startup event\r\n   async def startup_event():\r\n       await add_log_entry(\"System\", \"info\", \"Service started\")\r\n   \r\n   # Incorrect - at module level before event loop starts\r\n   # asyncio.create_task(add_log_entry(...))  # Will cause warning\r\n   ```\r\n\r\n3. **FastAPI Event Handlers**: Use FastAPI's event system (`@app.on_event(\"startup\")`) for initialization tasks that require async operations\r\n\r\n4. **Background Tasks**: For fire-and-forget async operations, use FastAPI's BackgroundTasks or properly scheduled asyncio tasks within request handlers\r\n\r\n5. **WebSocket Cleanup**: Always ensure proper cleanup of async resources in WebSocket exception handlers to prevent resource leaks\r\n\r\n6. **Exception Handling**: Wrap async operations in try/catch blocks that properly handle async exceptions and clean up resources"
    size: 1905
    tokens: 719
  - path: specs\standards\10-ARCH\014-gpu-resource-availability.md
    content: "# Standard 014: GPU Resource Availability Management\r\n\r\n## What Happened?\r\nThe system was returning 503 (Service Unavailable) errors for `/v1/memory/search` and other endpoints when GPU resources were busy or locked by another process. Users encountered errors like \"Ghost Engine Disconnected\" even when the Ghost Engine was running but GPU resources were temporarily unavailable.\r\n\r\n## The Cost\r\n- 2+ hours spent troubleshooting \"disconnected\" errors that were actually GPU resource contention issues\r\n- Misleading error messages suggesting connection problems when the real issue was resource availability\r\n- Poor user experience with hard failures instead of graceful queuing\r\n- Multiple failed requests during peak GPU usage periods\r\n\r\n## The Rule\r\n1. **GPU Queuing Protocol**: All GPU-dependent operations must use the `/v1/gpu/lock` and `/v1/gpu/unlock` endpoints to acquire/release resources safely\r\n2. **Graceful Degradation**: When GPU resources are unavailable, return informative messages about queue status rather than 503 errors\r\n3. **Resource Status Checks**: Use `/v1/gpu/status` to check availability before attempting GPU operations\r\n4. **Timeout Handling**: Implement proper timeouts (60s+) for GPU resource acquisition with clear user feedback\r\n5. **Queue Position Reporting**: Inform users of their position in the GPU queue when applicable\r\n6. **Fallback Strategies**: For non-critical operations, implement CPU-based fallbacks when GPU is heavily queued"
    size: 1475
    tokens: 572
  - path: specs\standards\10-ARCH\023-anchor-lite-simplification.md
    content: "# Standard 023: Anchor Lite Architecture\r\n\r\n## 1. What Happened\r\nThe system became overly complex with multiple database views (DB Builder, Memory Builder) and experimental chat interfaces, causing data synchronization issues and user confusion.\r\n\r\n## 2. The Cost\r\n- Loss of trust in retrieval (\"Jade\" not found).\r\n- High maintenance overhead.\r\n\r\n## 3. The Rule\r\n**Single Pipeline Architecture:**\r\n```mermaid\r\ngraph LR\r\n    BS[File System] -->|Watchdog| DB[Ghost Engine]\r\n    DB -->|WebSocket| UI[Context Console]\r\n    note[Single Source of Truth] -.-> BS\r\n```\r\n1. **Source:** File System (`context/` folder) is the Single Source of Truth.\r\n2. **Ingest:** `watchdog.py` monitors files and pushes to the Engine.\r\n3. **Index:** `ghost.html` (Headless CozoDB) maintains the index.\r\n4. **Retrieve:** `context.html` is the sole interface for search.\r\n"
    size: 846
    tokens: 326
  - path: specs\standards\10-ARCH\031-ghost-engine-stability-fix.md
    content: |-
      # Fix for Ghost Engine CozoDB Schema Issues

      ## Problem
      The Ghost Engine in ghost.html is experiencing crashes due to schema creation failures:
      - "Schema creation failed: undefined"
      - "Test query failed: undefined"
      - Browser crashes when reloading database

      ## Root Cause
      The issue is in the `ensureSchema()` function in ghost.html. The schema creation query attempts to create both the main table and the FTS (Full Text Search) index in a single operation:

      ```javascript
      const schemaQuery = `
      :create memory {
          id: String =>
          timestamp: Int,
          content: String,
          source: String,
          type: String
      } if not exists;

      ::fts create memory:content_fts {
          extractor: content,
          tokenizer: Simple,
          filters: [Lowercase]
      } if not exists;
      `;
      ```

      If the FTS creation fails (which can happen with certain CozoDB WASM builds), the entire schema creation fails.

      ## Solution
      Modify the `ensureSchema()` function to separate schema and FTS creation:

      ```javascript
      // First, create the basic schema
      async function ensureSchema() {
          // Create basic table first
          const basicSchemaQuery = `
          :create memory {
              id: String =>
              timestamp: Int,
              content: String,
              source: String,
              type: String
          } if not exists;
          `;

          try {
              const result = await db.run(basicSchemaQuery, "{}");
              const jsonResult = JSON.parse(result);

              if (jsonResult.ok) {
                  log("SUCCESS", "Basic schema created successfully");
              } else {
                  log(
                      "ERROR",
                      "Basic schema creation failed: " +
                          JSON.stringify(jsonResult.error),
                  );
                  return false;
              }
          } catch (e) {
              log("ERROR", "Basic schema operation failed: " + e.message);
              return false;
          }

          // Then, try to create FTS separately
          const ftsQuery = `
          ::fts create memory:content_fts {
              extractor: content,
              tokenizer: Simple,
              filters: [Lowercase]
          } if not exists;
          `;

          try {
              const ftsResult = await db.run(ftsQuery, "{}");
              const ftsJsonResult = JSON.parse(ftsResult);

              if (ftsJsonResult.ok) {
                  log("SUCCESS", "FTS index created successfully");
              } else {
                  log(
                      "WARNING",
                      "FTS creation failed (search will be limited): " +
                          JSON.stringify(ftsJsonResult.error),
                  );
                  // Don't return false here - basic functionality still works
              }
          } catch (e) {
              log(
                  "WARNING", 
                  "FTS creation failed (search will be limited): " + e.message
              );
              // Don't return false - basic functionality still works
          }

          return true;
      }
      ```

      ## Additional Improvements
      1. Better error handling in testQuery function
      2. More robust database initialization with fallbacks
      3. Improved error messages that don't return "undefined"

      ## Status
      This fix needs to be manually applied to ghost.html in the tools/ directory.
    size: 3029
    tokens: 1090
  - path: specs\standards\10-ARCH\032-ghost-engine-initialization-flow.md
    content: |-
      # Standard 032: Ghost Engine Initialization and Ingestion Flow

      ## What Happened?
      The Ghost Engine was experiencing race conditions where memory ingestion requests were being processed before the database was fully initialized. This caused errors like "Cannot read properties of null (reading 'run')" and inconsistent ingestion behavior between the Bridge API logs and the Ghost Engine logs.

      ## The Cost
      - Database ingestion failures when Ghost Engine connected to Bridge before database initialization completed
      - Inconsistent logging between Bridge and Ghost Engine (Bridge showing success, Ghost Engine showing failures)
      - Race conditions where ingestion requests arrived before database was ready
      - Poor user experience with failed memory operations
      - Confusing error messages in the UI

      ## The Rule
      1. **Sequential Initialization**: The Ghost Engine must initialize the database completely before signaling readiness to the Bridge.

      2. **Database Readiness Checks**: All ingestion and search operations must verify that the database object is properly initialized before attempting operations.

      3. **Proper Error Handling**: When database is not ready, the Ghost Engine must return appropriate error messages to the Bridge instead of failing silently.

      4. **Synchronous Connection Flow**: WebSocket connection must follow: Connect → Initialize Database → Signal Ready → Process Requests.

      5. **Graceful Degradation**: If database initialization fails, the Ghost Engine must report the error to the Bridge and not attempt to process requests.

      6. **Message Type Handling**: The system must properly handle all message types including `engine_error` responses.

      ## Implementation
      - Modified WebSocket connection flow to initialize database before signaling readiness
      - Added database readiness checks in `handleIngest` and `handleSearch` functions
      - Implemented proper error responses when database is not ready
      - Added support for `engine_error` message type handling
      - Enhanced error logging with fallbacks to prevent "undefined" messages
      - Ensured sequential processing: Connect → DB Init → Ready Signal → Process Requests
    size: 2129
    tokens: 811
  - path: specs\standards\10-ARCH\034-nodejs-monolith-migration.md
    content: |-
      # Standard 034: Node.js Monolith Migration

      ## What Happened?
      The system was migrated from a Python/Browser Bridge architecture (V2) to a Node.js Monolith architecture (V3). This involved:
      - Archiving legacy Python infrastructure (webgpu_bridge.py, anchor_watchdog.py, etc.)
      - Creating a new Node.js server with CozoDB integration
      - Implementing autonomous execution protocols
      - Converting Python scripts to JavaScript equivalents

      ## The Cost
      - Fragile headless browser architecture with WebGPU dependencies
      - Complex Python/JavaScript bridge with multiple failure points
      - Resource-intensive browser processes
      - Platform compatibility issues (especially on ARM/Android)
      - Complex deployment and dependency management

      ## The Rule
      1. **Node.js Monolith**: Use Node.js as the primary runtime environment for the Context Engine.

      2. **CozoDB Integration**: Integrate CozoDB directly using `cozo-node` for persistent storage.

      3. **Autonomous Execution**: Implement Protocol 001 for detached service execution with proper logging and verification.

      4. **File Watchdog**: Use `chokidar` for efficient file system monitoring and automatic ingestion.

      5. **API Endpoints**: Implement standardized endpoints:
         - `POST /v1/ingest` - Content ingestion
         - `POST /v1/query` - CozoDB query execution
         - `GET /health` - Service health verification

      6. **Legacy Archival**: Archive all V2 Python infrastructure to `archive/v2_python_bridge/`.

      7. **JavaScript Conversion**: Convert Python utility scripts to JavaScript equivalents for consistency.

      8. **Termux Compatibility**: Ensure architecture works on Termux/Linux environments.

      ## Implementation
      - Created Node.js server in `server/` directory
      - Implemented CozoDB with RocksDB backend
      - Added file watching functionality with chokidar
      - Created migration script for legacy session data
      - Converted read_all.py to read_all.js in both locations
      - Added proper error handling and logging
      - Implemented Protocol 001 for safe service execution
    size: 1989
    tokens: 747
  - path: specs\standards\20-DATA\017-file-ingestion-debounce-hash-checking.md
    content: |-
      # Standard 017: File Ingestion Debounce and Hash Checking

      ## What Happened?
      The Watchdog service was triggering excessive memory ingestion when modern editors (VS Code, Obsidian) would autosave files frequently. This caused "Memory Churn" in CozoDB with duplicate content being ingested repeatedly, fragmenting the database and spiking CPU usage.

      ## The Cost
      - High CPU usage from repeated ingestion of unchanged content
      - Database fragmentation from duplicate entries
      - Poor performance during active editing sessions
      - 2+ hours spent implementing debounce and hash checking to prevent "Autosave Flood"

      ## The Rule
      1. **Debounce File Events**: Implement a debounce mechanism that waits for a period of silence before processing file changes:
         ```python
         # Wait for debounce period before processing
         debounce_time = 2.0  # seconds
         ```

      2. **Content Hash Verification**: Calculate MD5 hash of file content before ingestion and compare with previously ingested version:
         ```python
         import hashlib
         current_hash = hashlib.md5(content).hexdigest()
         if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:
             # Skip ingestion - content hasn't changed
             return
         ```

      3. **Cancel Pending Operations**: Cancel any existing debounce timer when a new file event occurs for the same file:
         ```python
         if file_path in self.debounce_timers:
             self.debounce_timers[file_path].cancel()
         ```

      4. **Proper Cleanup**: Clean up debounce timer references after processing:
         ```python
         if file_path in self.debounce_timers:
             del self.debounce_timers[file_path]
         ```

      This standard prevents excessive database writes from autosave features while ensuring all actual changes are captured.
    size: 1746
    tokens: 657
  - path: specs\standards\20-DATA\019-code-file-ingestion-comprehensive-context.md
    content: |-
      # Standard 019: Code File Ingestion for Comprehensive Context

      ## What Happened?
      The Watchdog service was only monitoring text files (.txt, .md, .markdown) but ignoring code files which represent a significant portion of developer context. This created an "Ingestion Blind Spot" where the system was blind to codebase context.

      ## The Cost
      - Limited context ingestion for developers
      - Missing important code-related information
      - 30 minutes spent updating watchdog.py to include code extensions

      ## The Rule
      1. **Expand File Extensions**: Include common programming language extensions in file monitoring:
         ```python
         enabled_extensions = {".txt", ".md", ".markdown", ".py", ".js", ".html", ".css", 
                               ".json", ".yaml", ".yml", ".sh", ".bat", ".ts", ".tsx", 
                               ".jsx", ".xml", ".sql", ".rs", ".go", ".cpp", ".c", ".h", ".hpp"}
         ```

      2. **Comprehensive Coverage**: Monitor all relevant text-based file types that contain context

      3. **Maintain Performance**: Ensure file size limits still apply to prevent performance issues with large code files

      This standard ensures that developer context is fully captured by including code files in passive ingestion.
    size: 1207
    tokens: 455
  - path: specs\standards\20-DATA\021-chat-session-persistence-context-continuity.md
    content: |-
      # Standard 021: Chat Session Persistence for Context Continuity

      ## What Happened?
      The anchor.py CLI client maintained conversation history only in memory. If the terminal was closed or the CLI crashed, the entire conversation history was lost. This created a "Lost Context" risk where valuable conversation history was not preserved.

      ## The Cost
      - Loss of conversation history on CLI crashes or termination
      - Broken loop between active chatting and long-term memory
      - 45 minutes spent implementing chat session persistence to context folder

      ## The Rule
      1. **Auto-Save Sessions**: Automatically save each chat message to a session file:
         ```python
         def save_message_to_session(role, content):
             # Create timestamped session file in context/sessions/
             # Append each message as it occurs
         ```

      2. **Session Directory**: Create a dedicated `context/sessions/` directory for chat logs:
         ```python
         SESSIONS_DIR = os.path.join(CONTEXT_DIR, "sessions")
         os.makedirs(SESSIONS_DIR, exist_ok=True)
         ```

      3. **Markdown Format**: Save conversations in markdown format for easy reading and processing:
         ```python
         # Format: ## Role\nContent\n\n for each message
         ```

      4. **Loop Closure**: Ensure that chat sessions become text files that the Watchdog service can monitor and ingest into long-term memory automatically.

      This standard ensures conversation history survives CLI crashes and becomes part of the persistent context.
    size: 1448
    tokens: 558
  - path: specs\standards\20-DATA\022-text-file-source-of-truth-cross-machine-sync.md
    content: |-
      # Standard 022: Text-File Source of Truth for Cross-Machine Sync

      ## What Happened?
      The CozoDB database lives in IndexedDB inside the headless browser profile, making it impossible to sync between machines. Chat history and learned connections were trapped in the browser instance and lost when switching laptops. The system needed a "Text-File Source of Truth" approach where the database is treated as a cache and all important data is stored in text files.

      ## The Cost
      - Lost conversation history when switching between machines
      - Inability to sync learned connections and context across devices
      - 1 hour spent implementing daily session files and text-file persistence

      ## The Rule
      1. **Database is Cache**: Treat CozoDB as a cache, not the source of truth:
         ```python
         # All important data must exist in text files first
         # Database is rebuilt from text files on each machine
         ```

      2. **Daily Session Files**: Create daily markdown files for chat persistence:
         ```python
         def ensure_session_file():
             date_str = datetime.now().strftime("%Y-%m-%d")
             filename = f"chat_{date_str}.md"
             # Creates daily consolidated session files
         ```

      3. **Text-File First**: All important information must be written to text files:
         ```python
         # Every chat message gets saved to markdown file
         # Files are automatically ingested by watchdog service
         # Creates infinite loop: Chat -> File -> Ingestion -> Memory -> Next Chat
         ```

      4. **Cross-Machine Sync**: Use Dropbox/Git for file synchronization:
         ```python
         # Text files sync automatically via Dropbox/Git
         # Database rebuilds from text files on each machine
         # Ensures consistent context across all devices
         ```

      5. **Timestamped Entries**: Format messages with timestamps for tracking:
         ```python
         # Format: ### ROLE [HH:MM:SS]\n{content}\n\n
         ```

      This standard ensures that all important context is persisted in text files that can be synced across machines, making the database truly portable.
    size: 1995
    tokens: 779
  - path: specs\standards\20-DATA\024-context-ingestion-pipeline-fix.md
    content: |-
      # Standard 024: Context Ingestion Pipeline - Field Name Alignment Protocol

      ## What Happened?
      The context ingestion pipeline was failing silently due to field name mismatches between the watchdog service and the ghost engine. The watchdog was sending `filetype` but the memory ingestion endpoint expected `file_type`, and the ghost engine was looking for `msg.filetype` instead of `msg.file_type`. This caused the database to appear empty even though files were being processed, resulting in failed context searches for terms like "Dory" or "Coda".

      ## The Cost
      - 2+ hours spent debugging why context files weren't appearing in the database
      - Confusion from "Database appears empty!" messages in ghost engine logs
      - Failed context searches returning no results despite files existing in context directory
      - Misleading "Ingested" messages in watchdog logs that masked the actual field name mismatch
      - Users experiencing broken context retrieval functionality

      ## The Rule
      1. **Field Name Consistency**: All components in the ingestion pipeline must use consistent field names:
         - Watchdog sends: `file_type`, `source`, `content`, `filename`
         - Bridge expects: `file_type`, `source`, `content`, `filename`
         - Ghost engine receives: `file_type`, `source`, `content`, `filename`

      2. **Payload Validation**: Always validate that field names match across the entire pipeline:
         ```javascript
         // In ghost.html handleIngest function
         await runQuery(query, { 
             data: [[id, ts, msg.content, msg.source || msg.filename, msg.file_type || "text"]] 
         });
         ```

      3. **Source Identification**: The watchdog must send a proper source identifier instead of relying on default "unknown" values

      4. **Error Reporting**: Include detailed error messages when ingestion fails to help with debugging
    size: 1796
    tokens: 687
  - path: specs\standards\20-DATA\029-consolidated-data-aggregation.md
    content: |-
      # Standard 029: Consolidated Data Aggregation with YAML Support

      ## What Happened?
      The system had multiple scripts performing similar functions for data aggregation and migration:
      - `migrate_history.py` - Legacy session migration to YAML
      - `read_all.py` in context directory - Data aggregation to JSON
      - Multiple overlapping data processing scripts

      This created redundancy and confusion about which script to use for data aggregation. The functionality has been consolidated into a single authoritative script: `context/Coding-Notes/Notebook/read_all.py` which now supports all three output formats (text, JSON, YAML).

      ## The Cost
      - Multiple scripts with overlapping functionality
      - Confusion about which script to use for data aggregation
      - Maintenance burden of multiple similar scripts
      - Inconsistent output formats across scripts
      - Redundant code that needed to be updated in multiple places

      ## The Rule
      1. **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation from the context directory.

      2. **Multi-Format Output**: The script must generate three output formats:
         - `combined_text.txt` - Human-readable text corpus
         - `combined_memory.json` - Structured JSON for database ingestion
         - `combined_memory.yaml` - Structured YAML for easier processing and migration

      3. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.

      4. **Encoding Handling**: The script must handle various file encodings using chardet for reliable processing.

      5. **Recursive Processing**: The script must process all subdirectories while respecting exclusion rules.

      6. **Metadata Preservation**: File metadata (path, timestamp) must be preserved in structured outputs.

      ## Implementation
      - Consolidated migrate_history.py functionality into read_all.py
      - Moved migrate_history.py to archive/tools/
      - Updated read_all.py to generate YAML output with proper multiline formatting
      - Used yaml.dump() with custom representer for multiline strings
      - Maintained all existing functionality while adding YAML support
      - Preserved the same exclusion rules and file type filtering
    size: 2229
    tokens: 848
  - path: specs\standards\20-DATA\030-multi-format-output.md
    content: |-
      # Standard 030: Multi-Format Output for Project Aggregation

      ## What Happened?
      The `read_all.py` script in the root directory was only generating text and JSON outputs for project aggregation. To improve compatibility with various processing tools and follow the documentation policy of supporting YAML format, the script was updated to also generate a YAML version of the memory records.

      ## The Cost
      - Limited output format options for downstream processing
      - Inconsistency with the documentation policy that prefers YAML for configuration and data exchange
      - Missing opportunity to provide easily readable structured data in YAML format
      - Users had to convert JSON to YAML if they needed that format

      ## The Rule
      1. **Multi-Format Output**: The `read_all.py` script must generate both JSON and YAML versions of memory records.

      2. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.

      3. **Consistent Naming**: Output files should follow consistent naming patterns:
         - `combined_text.txt` - Aggregated text content
         - `combined_memory.json` - Structured JSON memory records
         - `combined_text.yaml` - Structured YAML memory records

      4. **Custom Representers**: Use custom YAML representers to handle multiline strings appropriately with the `|` indicator.

      5. **Encoding Handling**: Ensure proper UTF-8 encoding for both input and output operations.

      ## Implementation
      - Updated `read_all.py` to import and use the `yaml` module
      - Added custom string representer for multiline content
      - Created separate YAML output file with proper formatting
      - Maintained all existing functionality while adding YAML support
      - Used `yaml.dump()` with appropriate parameters for clean output
    size: 1784
    tokens: 690
  - path: specs\standards\20-DATA\033-cozodb-syntax-compliance.md
    content: |-
      # Standard 033: CozoDB Schema and Query Syntax Compliance

      ## What Happened?
      The Ghost Engine was experiencing continued ingestion failures due to incorrect CozoDB query syntax. Issues included:
      - Incorrect schema creation syntax with improper line breaks
      - Wrong insertion query syntax (`:put` vs `:insert`)
      - Improper parameter formatting for data insertion
      - Schema validation that didn't properly propagate failure status

      ## The Cost
      - Persistent ingestion failures despite initialization flow fixes
      - Incorrect CozoDB query syntax causing "Unknown error" messages
      - Failed schema creation preventing proper database operations
      - Misleading success messages when operations were actually failing
      - Continued inconsistency between Bridge and Ghost Engine logs

      ## The Rule
      1. **Schema Creation Syntax**: Use proper CozoDB schema creation syntax without line breaks in the schema definition: `:create memory {id: String => timestamp: Int, content: String, source: String, type: String} if not exists;`

      2. **FTS Creation Syntax**: Use proper CozoDB FTS creation syntax: `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`

      3. **Insert Query Syntax**: Use `:insert` or `:replace` with proper parameter binding syntax: `:insert memory {id, timestamp, content, source, type} <- $data`

      4. **Parameter Formatting**: Format parameters correctly as nested arrays for bulk operations: `{data: [[id, timestamp, content, source, type]]}`

      5. **Schema Validation**: Properly propagate schema creation success/failure status to prevent operations on uninitialized database

      6. **Error Propagation**: Ensure all database operations properly handle and report errors to maintain consistency between Bridge and Ghost Engine

      ## Implementation
      - Fixed schema creation queries to use correct CozoDB syntax without line breaks
      - Updated insertion queries from `:put` to `:insert` with proper syntax
      - Corrected parameter formatting for data insertion operations
      - Enhanced schema validation to properly return success/failure status
      - Improved error propagation throughout database operations
      - Maintained backward compatibility while fixing syntax issues
    size: 2200
    tokens: 830
  - path: specs\standards\20-DATA\037-database-hydration-snapshot-portability.md
    content: "# Standard 037: Database Hydration & Snapshot Portability\r\n\r\n**Status:** Active | **Domain:** DATA | **Number:** 037\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nLarge context libraries (89MB+) were difficult to move between machines because the \"Filesystem as Source of Truth\" required re-ingesting thousands of files. This process was slow, resource-intensive, and prone to file watcher errors or race conditions during initial indexing.\r\n\r\n### 2. The Cost\r\nHours of developer time lost to \"ingestion loops,\" \"missing context\" errors after migration, and inconsistent database states across different environments. The system was \"brittle\" during the first 10 minutes of startup on a new machine.\r\n\r\n### 3. The Rule\r\nUse **YAML Snapshots** as the primary portable artifact for the database state.\r\n- **Eject**: Export the current database state to a single YAML file for backup or migration.\r\n- **Auto-Hydrate**: On startup, the engine automatically checks if the database is empty. If it is, it picks the **latest** snapshot from the `backups/` folder and performs a bulk, idempotent restore.\r\n- **Manual Control**: Users can manually move old snapshots to a separate folder to prevent them from being picked, or delete the `engine/context.db` folder to force a fresh hydration from the latest snapshot.\r\n- **Persistence**: Once hydrated, the data lives in the persistent `engine/context.db` (RocksDB). The original source files are no longer required for the engine to function, enabling a \"Docker-style\" build-and-ship workflow.\r\n\r\n---\r\n*Verified by Architecture Council. Created after 89MB Context Migration.*\r\n"
    size: 1624
    tokens: 642
  - path: specs\standards\30-OPS\001-windows-console-encoding.md
    content: "# Standard 001: Windows Console Encoding\r\n\r\n## What Happened?\r\nThe Python Bridge (`webgpu_bridge.py`) crashed immediately upon launch on Windows 11. The error was `UnicodeEncodeError: 'charmap' codec can't encode character...`.\r\n\r\n## The Cost\r\n- 3 failed integration attempts.\r\n- \"Integration Hell\" state requiring full manual intervention.\r\n- Bridge stability compromised during demos.\r\n\r\n## The Rule\r\n1. **Explicit Encoding:** All Python scripts outputting to stdout must explicitly handle encoding.\r\n2. **The Fix:** Include this snippet at the top of all entry points:\r\n   ```python\r\n   import sys\r\n   if sys.platform == \"win32\":\r\n       sys.stdout.reconfigure(encoding='utf-8')\r\n   ```\r\n\r\n3. **Validation:** CI/CD or startup scripts must verify the bridge launches without encoding errors."
    size: 793
    tokens: 301
  - path: specs\standards\30-OPS\011-comprehensive-testing-verification.md
    content: "# Standard 011: Comprehensive Testing and Verification Protocol\r\n\r\n## What Happened?\r\nThe Anchor Core system required a comprehensive testing approach to prevent issues like missing endpoints, function syntax errors, model loading failures, and data pipeline problems. Previously, these issues were discovered reactively during development or deployment, causing delays and debugging overhead.\r\n\r\n## The Cost\r\n- Hours spent debugging missing endpoints after deployment\r\n- Time wasted on syntax errors in critical files\r\n- Model loading failures discovered during user testing\r\n- Data pipeline issues found late in the development cycle\r\n- Lack of systematic verification leading to inconsistent quality\r\n\r\n## The Rule\r\n1. **Dedicated Test Directory**: All test files must be organized in a dedicated `tests/` directory in the project root\r\n    ```bash\r\n    tests/\r\n    ├── comprehensive_test_suite.py\r\n    ├── endpoint_syntax_verification.py\r\n    ├── test_model_loading.py\r\n    ├── test_model_availability.py\r\n    ├── test_gpu_fixes.py\r\n    ├── test_orchestrator.py\r\n    ├── model_test.html\r\n    └── README.md\r\n    ```\r\n\r\n2. **Comprehensive Test Coverage**: Tests must cover:\r\n   - Model loading functionality\r\n   - Data pipeline verification\r\n   - Endpoint accessibility\r\n   - Missing endpoint detection\r\n   - Function syntax error detection\r\n   - System health verification\r\n\r\n3. **Endpoint Verification Protocol**: All critical endpoints must be tested for accessibility:\r\n   ```python\r\n   # Example endpoint test pattern\r\n   critical_endpoints = [\r\n       (\"/health\", \"GET\", 200),\r\n       (\"/v1/chat/completions\", \"POST\", 400),  # Expected 400 due to missing body\r\n       (\"/v1/gpu/status\", \"GET\", 200),\r\n       # ... add all critical endpoints\r\n   ]\r\n   ```\r\n\r\n4. **Syntax Verification**: Critical Python files must be checked for syntax errors:\r\n   ```python\r\n   # Use AST parsing to verify syntax\r\n   import ast\r\n   with open(file_path, 'r') as f:\r\n       source_code = f.read()\r\n   ast.parse(source_code)  # Will raise SyntaxError if invalid\r\n   ```\r\n\r\n5. **Test Documentation**: All test files must be documented in `tests/README.md` with:\r\n   - Purpose of each test file\r\n   - How to run the tests\r\n   - Test coverage details\r\n   - Expected outputs\r\n\r\n6. **Pre-Deployment Verification**: Before any deployment, run the comprehensive test suite:\r\n   ```bash\r\n   python tests/comprehensive_test_suite.py\r\n   ```\r\n\r\n7. **Continuous Verification**: Implement automated testing in CI/CD pipelines to catch issues early\r\n\r\n## Implementation Example\r\n\r\n### Running the Comprehensive Test Suite:\r\n```bash\r\n# Basic test run\r\npython tests/comprehensive_test_suite.py\r\n\r\n# With custom parameters\r\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output report.json\r\n\r\n# Endpoint and syntax verification only\r\npython tests/endpoint_syntax_verification.py\r\n```\r\n\r\n### Expected Test Coverage:\r\n- Model loading: 100% coverage of model files and configurations\r\n- API endpoints: 100% verification of all documented endpoints\r\n- Syntax: 100% verification of critical Python files\r\n- Data pipeline: End-to-end verification of data flow\r\n- System health: Verification of all core services\r\n\r\n## Verification Checklist\r\n- [ ] All test files organized in `tests/` directory\r\n- [ ] Comprehensive test suite covers all major components\r\n- [ ] Endpoint verification tests all critical endpoints\r\n- [ ] Syntax verification tests all critical Python files\r\n- [ ] Tests are documented in `tests/README.md`\r\n- [ ] Test suite runs without errors\r\n- [ ] Test reports are generated and reviewed"
    size: 3620
    tokens: 1329
  - path: specs\standards\30-OPS\013-universal-log-collection.md
    content: "# Standard 013: Universal Log Collection System\r\n\r\n## What Happened?\r\nThe system had fragmented logging across multiple sources (browser console, Python stdout, WebSocket events) making debugging difficult. Users had to check multiple places to understand system behavior.\r\n\r\n## The Cost\r\n- 4+ hours spent debugging connection issues by checking browser console, Python terminal, and WebSocket messages separately\r\n- Inefficient troubleshooting workflow requiring multiple monitoring tools\r\n- Missed error correlations between different system components\r\n- Poor visibility into system-wide operation\r\n\r\n## The Rule\r\n1. **Universal Collection**: All system logs (Python, JavaScript, WebSocket, browser, model loading, GPU status) must be aggregated in a single location: `tools/log-viewer.html`\r\n2. **Broadcast Channel Protocol**: All components must use the `sovereign-logs` or `coda_logs` BroadcastChannel to send messages to the log viewer:\r\n   ```javascript\r\n   // From browser components\r\n   const logChannel = new BroadcastChannel('sovereign-logs');\r\n   logChannel.postMessage({\r\n       source: 'component-name',\r\n       type: 'info|success|error|warning|debug',\r\n       time: new Date().toISOString(),\r\n       msg: 'message content'\r\n   });\r\n   ```\r\n\r\n3. **Python Integration**: Python scripts must send log data via API endpoints that feed into the log viewer\r\n4. **Centralized Access**: The single point of truth for all system diagnostics is `http://localhost:8000/log-viewer.html`\r\n5. **File-based Logging**: Each component must also write to its own log file in the `logs/` directory for persistent storage\r\n6. **Log Truncation**: Individual log files must be truncated to last 1000 lines to prevent disk space issues\r\n7. **GPU Resource Queuing**: All GPU operations must use the queuing system (`/v1/gpu/lock`, `/v1/gpu/unlock`) to prevent resource conflicts\r\n8. **Source Tagging**: All log entries must be clearly tagged with their source for easy identification"
    size: 1976
    tokens: 757
  - path: specs\standards\30-OPS\016-process-management-auto-resurrection.md
    content: |-
      # Standard 016: Process Management and Auto-Resurrection for Browser Engines

      ## What Happened?
      The Ghost Engine (headless browser) would sometimes crash or hang, leaving zombie processes that would prevent new browser instances from starting. When the ResurrectionManager tried to launch a new browser, it would fail because the previous process was still holding onto resources like the remote debugging port (9222).

      ## The Cost
      - 2+ hours spent debugging "Zombie Process" risk where browser resurrection would fail
      - Multiple failed attempts to restart the Ghost Engine
      - System becoming unresponsive when browser processes hung
      - Users experiencing "Total System Failure" when new browsers couldn't start due to port conflicts

      ## The Rule
      1. **Process Cleanup First**: Before launching a new browser process, always kill any existing browser processes:
         ```python
         async def kill_existing_browsers(self):
             import psutil
             for proc in psutil.process_iter(['pid', 'name']):
                 if proc.info['name'].lower() in ['msedge.exe', 'chrome.exe', 'chromium-browser']:
                     proc.kill()
         ```

      2. **Explicit Port Assignment**: Always specify a consistent remote debugging port to avoid conflicts:
         ```python
         # Add to browser launch command
         "--remote-debugging-port=9222"
         ```

      3. **Wait for Full Initialization**: Increase wait time after launching browser to ensure full initialization before checking connection:
         ```python
         await asyncio.sleep(5)  # Increased from 3 seconds
         ```

      4. **Proper Process Termination**: Ensure browser processes are fully terminated before launching new ones:
         ```python
         # Always call terminate() then wait() or kill() if needed
         process.terminate()
         process.wait(timeout=5)
         ```

      This standard ensures that browser resurrection works reliably by preventing port conflicts and zombie processes.
    size: 1880
    tokens: 716
  - path: specs\standards\30-OPS\020-browser-profile-management-cleanup.md
    content: |-
      # Standard 020: Browser Profile Management and Cleanup

      ## What Happened?
      Repeatedly launching and killing modern browsers (Edge/Chrome) could leave orphaned child processes or fill up the temporary directory with user data profiles. This created a "Memory Leak" risk where temporary browser profiles could accumulate over time and crash the host OS.

      ## The Cost
      - Potential disk space issues from accumulated temporary browser profiles
      - Risk of system instability from orphaned processes
      - 1 hour spent implementing proper browser profile management and cleanup

      ## The Rule
      1. **Unique Profile Directories**: Use unique temporary directories for each browser instance:
         ```python
         import tempfile
         temp_dir = tempfile.gettempdir()
         f"--user-data-dir={temp_dir}/anchor_ghost_{int(time.time())}"
         ```

      2. **Performance Optimization Flags**: Include performance optimization flags to reduce resource usage:
         ```python
         # Add these flags to browser launch command
         "--disable-background-timer-throttling",
         "--disable-backgrounding-occluded-windows", 
         "--disable-renderer-backgrounding",
         "--disable-ipc-flooding-protection",
         "--disable-background-media-suspend"
         ```

      3. **Cleanup Old Profiles**: Implement automatic cleanup of old temporary profiles:
         ```python
         async def _cleanup_old_profiles(self):
             # Remove directories older than 1 day
             cutoff_time = datetime.now() - timedelta(days=1)
             # Implementation to remove old temporary directories
         ```

      4. **Proper Process Termination**: Ensure browser processes are fully terminated before launching new ones

      This standard prevents disk space issues and system instability from temporary browser profiles.
    size: 1709
    tokens: 635
  - path: specs\standards\30-OPS\024-detached-logging-standard.md
    content: "# Standard 024: Detached Logging and Process Management for LLM Systems\r\n\r\n## What Happened?\r\nThe system had issues with scripts blocking execution and logs not being properly managed, causing system instability and difficulty in debugging. Scripts were running in attached mode causing blocking operations, and log files were growing without bounds, consuming excessive disk space.\r\n\r\n## The Cost\r\n- System instability due to blocking operations\r\n- Excessive disk space consumption from unbounded log files\r\n- Difficulty in debugging due to missing or improperly managed logs\r\n- Process conflicts and hanging operations\r\n\r\n## The Rule\r\n1. **Detached Mode Execution**: All scripts, especially those related to LLM models, must run in detached mode to prevent blocking operations:\r\n   ```python\r\n   # For Python scripts launched from batch files\r\n   start \"Process Name\" /min cmd /c \"python script.py > logs/script_output.log 2>&1\"\r\n   ```\r\n\r\n2. **Universal Log Output**: All scripts must output logs to the designated logs directory (`logs/`) with proper file naming conventions:\r\n   - Python scripts: `python_stdout.log`, `python_stderr.log`\r\n   - Custom components: `{component_name}.log`\r\n   - Error logs: `{component_name}_error.log`\r\n\r\n3. **Log Truncation**: All log files must implement automatic truncation to prevent excessive disk usage:\r\n   - Truncate after 5000 lines OR 10000 characters, whichever comes first\r\n   - Keep most recent entries when truncating\r\n   - Implement rotation if needed for high-volume logs\r\n\r\n4. **Process Management**: Scripts must properly manage child processes and ensure cleanup:\r\n   - Use proper subprocess management with error handling\r\n   - Implement graceful shutdown procedures\r\n   - Monitor and terminate orphaned processes\r\n\r\n5. **Error Handling**: All scripts must implement proper error handling and logging:\r\n   - Catch exceptions and log them appropriately\r\n   - Use structured logging with timestamps and severity levels\r\n   - Ensure logs are written even during error conditions\r\n\r\n6. **Resource Management**: Scripts must manage system resources efficiently:\r\n   - Close file handles properly\r\n   - Release memory when possible\r\n   - Monitor resource usage and implement limits\r\n\r\nThis standard ensures that all system components run reliably in detached mode while maintaining proper logging practices for debugging and monitoring."
    size: 2387
    tokens: 905
  - path: specs\standards\30-OPS\025-script-logging-protocol.md
    content: |-
      # Standard 025: Script Logging Protocol for LLM Development

      ## What Happened?
      The system needed a standardized approach for running scripts that generate data for the context engine. Previously, scripts were run directly in the terminal, making it difficult to track their execution, capture their output, and ensure they could run reliably in production environments. The documentation policy specifies that LLM dev scripts should never run in non-detached mode and should output to log files in the logs/ directory.

      ## The Cost
      - Difficulty tracking script execution and debugging issues
      - Lack of persistent logs for script runs
      - Scripts not designed to run in detached mode as required by the architecture
      - Inconsistent logging approaches across different scripts

      ## The Rule
      1. **Detached Mode Only**: All scripts that process data for the context engine must be designed to run in detached mode, not requiring user interaction.

      2. **Log File Output**: All scripts must write their execution logs to files in the `logs/` directory with timestamped names following the pattern `scriptname_YYYYMMDD_HHMMSS.log`.

      3. **Log Format**: Script logs must follow the same format as other system logs:
         ```
         [YYYY-MM-DD HH:MM:SS] [LEVEL] Message content
         ```
         Where LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG

      4. **Self-Contained Logging**: Scripts must handle their own logging setup, creating the logs directory if needed and writing to their own log files.

      5. **Progress Tracking**: Long-running scripts should log progress indicators at regular intervals to show they are still active.

      6. **Error Handling**: Scripts must log errors and exceptions to their log files and continue or exit gracefully as appropriate.

      7. **File Path Detection**: Scripts that depend on system executables (like browsers) should implement robust path detection with fallbacks to standard installation locations.

      ## Implementation Example:
      ```python
      import datetime
      import os
      from pathlib import Path

      def setup_logging(script_name):
          logs_dir = Path(__file__).parent.parent / "logs"
          logs_dir.mkdir(exist_ok=True)
          timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
          log_file = logs_dir / f"{script_name}_{timestamp}.log"
          return log_file

      def log_message(log_file, level, message):
          timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          log_entry = f"[{timestamp}] [{level}] {message}\n"
          with open(log_file, 'a', encoding='utf-8') as f:
              f.write(log_entry)

      def detect_executable_path(executable_name, possible_paths):
          """
          Detect the path of an executable with fallbacks to standard installation locations.

          Args:
              executable_name: Name of the executable as fallback (e.g., 'msedge')
              possible_paths: List of possible installation paths to check

          Returns:
              Path to the executable if found, otherwise the fallback name
          """
          for path in possible_paths:
              if os.path.exists(path):
                  return path
          return executable_name  # fallback
      ```

      ## Migration Tool Compliance
      The `migrate_history.py` tool follows this standard by:
      - Creating timestamped log files in the logs/ directory
      - Using the standard log format
      - Running in detached mode without requiring user interaction
      - Providing detailed progress information during execution
      - Handling errors gracefully and continuing operation when possible
      - Implementing robust file path detection with multiple fallback options
    size: 3494
    tokens: 1350
  - path: specs\standards\30-OPS\035-never-attached-mode.md
    content: |-
      # Standard 035: Never Attached Mode for Long-Running Services

      ## What Happened?
      Long-running services and scripts were being executed in attached mode, causing command-line interfaces to block for extended periods (sometimes hours). This happened during server startup when `npm start` was run directly in the terminal, causing the process to hang and occupy the command line for extended periods.

      ## The Cost
      - Command-line interfaces blocked for hours preventing other operations
      - Resource waste from keeping terminals open unnecessarily
      - Poor developer experience with unresponsive command prompts
      - Risk of accidental process termination when closing terminals
      - Violation of the principle that long-running services should operate independently

      ## The Rule
      1. **Detached Execution Only**: All long-running services (servers, daemons, watchers) must be started in detached mode using appropriate backgrounding techniques.

      2. **No Attached Mode**: Never run services like `npm start`, `python server.py`, or similar long-running processes directly in an attached terminal session.

      3. **Proper Logging**: All detached processes must log to the designated `logs/` directory for monitoring and debugging.

      4. **Platform-Specific Detaching**:
         - *Linux/Mac:* Use `nohup command > logs/output.log 2>&1 &` or systemd services
         - *Windows:* Use `start /min cmd /c "command > ..\logs\output.log 2>&1"` or similar backgrounding
         - *Cross-platform:* Use process managers like pm2 or nodemon with background options

      5. **Verification Method**: After starting a service in detached mode, verify it's running by checking logs or attempting to connect to its interface, not by waiting for terminal output.

      6. **Documentation Requirement**: All startup procedures must specify detached execution methods, never attached execution.

      ## Implementation
      - Updated all documentation to specify detached execution methods
      - Created background startup scripts where appropriate
      - Added proper error handling and logging to detached processes
      - Established monitoring procedures for detached services
      - Educated team members on detached vs attached execution differences
    size: 2165
    tokens: 836
  - path: specs\standards\30-OPS\036-log-file-management-protocol.md
    content: |-
      # Standard 036: Log File Management Protocol

      **Status:** Active | **Category:** Operations (30-OPS) | **Authority:** Human-Locked

      ## The Triangle of Pain

      ### 1. What Happened
      LLM agents and developers were unable to monitor long-running processes effectively because output was directed to terminal sessions that became unresponsive. Without proper log file management, debugging and monitoring of detached processes became impossible, leading to system state uncertainty.

      ### 2. The Cost
      - **Inability to Monitor:** No visibility into detached process status
      - **Debugging Difficulty:** Impossible to troubleshoot stuck processes
      - **Resource Management:** Unclear which processes were running or failing
      - **System State Confusion:** No clear way to verify process completion or errors

      ### 3. The Rule
      **All detached processes MUST write to specific log files in the `logs/` directory with proper naming conventions and rotation.**

      #### Specific Requirements:
      - **Centralized Logging:** All process output goes to `logs/` directory at project root
      - **Descriptive Naming:** Log files named after the process with optional timestamp: `process_name.log`
      - **Format Consistency:** All logs must be human and machine readable (text format)
      - **Size Management:** Implement log rotation or truncation to prevent infinite growth
      - **Access Path:** Standard path `logs/process_name.log` for all processes

      #### Examples:
      ```bash
      # CORRECT: Proper logging
      node server.js > logs/server.log 2>&1 &
      python data_process.py > logs/data_process.log 2>&1 &

      # Log file structure
      logs/
      ├── server.log
      ├── data_process.log
      ├── context_read.log
      └── backup_operation.log
      ```

      #### Verification:
      - Log files must be created before process execution
      - Log files must be accessible and writable
      - Log content must reflect actual process output
      - Log files must be monitored instead of terminal sessions

      ## Cross-References
      - Standard 035: Script Running Protocol - Detached Execution with Logging
      - Related to: Standard 013 (Universal Logging)

      ## Implementation
      - Created `logs/` directory in project root
      - All startup scripts now direct output to log files
      - Log files are monitored for process status verification
      - Documentation updated to reference log file checking

      ---
      *Verified by Architecture Council. Edited by Humans Only.*
    size: 2323
    tokens: 885
  - path: specs\standards\40-BRIDGE\010-bridge-redirect-implementation.md
    content: "# Standard 010: Bridge Redirect Implementation - Smart Model Loading\r\n\r\n## What Happened?\r\nThe Anchor Core system had inconsistent model loading behavior where some models worked and others didn't. The issue was that the bridge was only serving local files and returning 404 errors when files were missing, instead of providing fallback to online sources.\r\n\r\nThe browser components (like chat.html) were requesting model files from the local bridge (localhost:8000), but if the model hadn't been downloaded locally, they would fail with 404 errors instead of falling back to online loading.\r\n\r\n## The Cost\r\n- Models failing to load when not downloaded locally\r\n- Inconsistent behavior across different model requests\r\n- User frustration when models appear unavailable\r\n- Complex debugging to understand the local vs online loading pathway\r\n\r\n## The Rule\r\n1. **Smart Redirect Pattern**: Implement the following pattern for model file requests:\r\n   - **Check Local First**: When receiving a request for `/models/{file_path}`, first check if the file exists in the local models directory\r\n   - **Serve Local**: If found locally, serve the file with proper no-cache headers to prevent browser caching issues\r\n   - **Redirect Online**: If not found locally, redirect to the corresponding HuggingFace URL with HTTP 302 status\r\n\r\n2. **NoCache Headers**: When serving local files, ensure proper cache-control headers are applied:\r\n   ```python\r\n   # Headers to apply to local file responses\r\n   Cache-Control: no-store, no-cache, must-revalidate\r\n   Pragma: no-cache\r\n   Expires: 0\r\n   ```\r\n\r\n3. **Request Method Handling**: Handle both GET and HEAD requests appropriately:\r\n   - **GET**: Return the file content or redirect\r\n   - **HEAD**: Return headers with file size if local file exists, or redirect if missing\r\n\r\n4. **Logging**: Log when files are not found locally and redirected to HuggingFace for debugging purposes\r\n\r\n5. **Resilience**: The system must never fail to provide model files when they exist online, regardless of local download status\r\n\r\n## Implementation Example\r\n```python\r\n@app.get(\"/models/{file_path:path}\")\r\nasync def models_redirect(file_path: str):\r\n    \"\"\"Smart redirect: Check for local file first, redirect to HuggingFace if missing\"\"\"\r\n    import os\r\n    from fastapi.responses import FileResponse, RedirectResponse\r\n    \r\n    # Construct path to local model file\r\n    models_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\r\n    local_path = os.path.join(models_dir, file_path)\r\n    \r\n    # Check if the file exists locally\r\n    if os.path.exists(local_path) and os.path.isfile(local_path):\r\n        # Serve the local file with no-cache headers\r\n        return NoCacheFileResponse(local_path)\r\n    else:\r\n        # File doesn't exist locally, redirect to HuggingFace\r\n        print(f\"⚠️ File not found locally, redirecting to HuggingFace: {file_path}\")\r\n        hf_url = f\"https://huggingface.co/{file_path}\"\r\n        return RedirectResponse(url=hf_url, status_code=302)\r\n```\r\n\r\nThis ensures the system provides maximum resilience by falling back to online sources when local files are missing."
    size: 3145
    tokens: 1214
  - path: specs\standards\40-BRIDGE\015-browser-control-center.md
    content: "# Standard 015: Browser-Based Control Center Architecture\r\n\r\n## What Happened?\r\nThe system had fragmented interfaces requiring users to interact with both browser UI and terminal commands. Context retrieval required switching between interfaces, and vision processing required separate Python scripts. This created inefficient workflows and poor user experience.\r\n\r\n## The Cost\r\n- 6+ hours spent switching between browser and terminal for different operations\r\n- Fragmented workflow requiring multiple interfaces for simple tasks\r\n- Poor visibility into system state across different components\r\n- Inefficient context retrieval and vision processing workflows\r\n- Users had to remember multiple endpoints and interfaces\r\n\r\n## The Rule\r\n1. **Unified Browser Interface**: All primary operations (context retrieval, vision processing, memory search) must be accessible through browser-based UI at `http://localhost:8000/sidecar`\r\n2. **Dual-Tab Architecture**: Interface must have separate tabs for \"Retrieve\" (context search) and \"Observe\" (vision processing) to prevent workflow interference\r\n3. **File-Based Logging**: All system components must write to individual log files in the `logs/` directory with automatic truncation to 1000 lines\r\n4. **Centralized Log Access**: All logs must be accessible via `/logs/recent` endpoint and consolidated in `log-viewer.html`\r\n5. **Python VLM Integration**: Vision processing must be handled by dedicated Python module (`vision_engine.py`) with Ollama backend support\r\n6. **Endpoint Consistency**: All UI components must use consistent endpoint patterns (`/v1/namespace/action`)\r\n7. **Error Handling**: All operations must provide clear, actionable error messages to the user interface\r\n8. **State Visibility**: System state (engine status, GPU availability, memory status) must be visible in the UI"
    size: 1838
    tokens: 704
  - path: specs\standards\40-BRIDGE\018-streaming-cli-client-responsive-ux.md
    content: |-
      # Standard 018: Streaming CLI Client for Responsive UX

      ## What Happened?
      The anchor.py CLI client was using `stream=False` for API requests, causing the terminal to hang during long responses from 7B models. This created a poor user experience where the terminal would freeze for 10-20 seconds during long answers, breaking the "fluid" feeling of a memory assistant.

      ## The Cost
      - Poor user experience with hanging terminal during long responses
      - 1+ hours spent updating anchor.py to use streaming for better UX
      - Users experiencing "frozen" terminal during model responses
      - Break in the conversational flow of the memory assistant

      ## The Rule
      1. **Enable Streaming**: Always use `stream=True` when making chat completion requests from CLI clients:
         ```python
         response = requests.post(
             f"{BRIDGE_URL}/v1/chat/completions",
             json={
                 "messages": history,
                 "stream": True  # Enable streaming for better UX
             },
             stream=True,  # Enable streaming at request level
             timeout=120
         )
         ```

      2. **Process Streaming Response**: Handle the streaming response line by line to provide immediate feedback:
         ```python
         for line in response.iter_lines(decode_unicode=True):
             if line and line.startswith("data: "):
                 data_str = line[6:]  # Remove "data: " prefix
                 if data_str.strip() == "[DONE]":
                     break
                 # Process and display content as it arrives
         ```

      3. **Real-time Character Display**: Print characters as they arrive to provide immediate feedback:
         ```python
         print(content, end="", flush=True)
         ```

      4. **Maintain Conversation Context**: Properly accumulate streamed content to maintain conversation history:
         ```python
         ai_text += content  # Accumulate content for history
         history.append({"role": "assistant", "content": ai_text})
         ```

      This standard ensures CLI clients provide responsive feedback during long model responses while maintaining proper conversation context.
    size: 1996
    tokens: 751
  - path: specs\standards\40-BRIDGE\026-ghost-engine-connection-management.md
    content: |-
      # Standard 026: Ghost Engine Connection Management and Resilience

      ## What Happened?
      The Ghost Engine (headless browser running CozoDB WASM) was frequently showing "disconnected" status in logs, causing 503 errors for memory ingestion and search requests. The logs showed repeated "Ghost Engine Disconnected" messages followed by successful connections when the engine eventually came online. This created a frustrating user experience where the system appeared broken even though it was working correctly once the Ghost Engine connected.

      ## The Cost
      - Multiple 503 errors in memory API when Ghost Engine was not yet connected
      - User confusion about system status and functionality
      - Inconsistent behavior during system startup
      - 2+ hours spent debugging connection timing issues
      - Reduced reliability of memory ingestion during startup phases

      ## The Rule
      1. **Connection Readiness Protocol**: All memory operations (ingest/search) must check Ghost Engine connection status before attempting operations:
         ```python
         if not workers["chat"] or workers["chat"].closed:
             return JSONResponse(status_code=503, content={"error": "Ghost Engine Disconnected"})
         ```

      2. **Graceful Degradation**: When Ghost Engine is disconnected, systems should:
         - Return informative 503 errors with clear messaging
         - Continue operating other non-memory functions
         - Implement auto-retry mechanisms for critical operations
         - Log connection status changes for debugging

      3. **Connection Monitoring**: Implement WebSocket connection monitoring with:
         - Automatic reconnection attempts
         - Status indicators in logs
         - Health check endpoints to verify connection state

      4. **User Experience**: When Ghost Engine is disconnected:
         - UI should show clear status indicators
         - Operations should queue for processing when connection resumes
         - Users should be informed of the connection requirement

      5. **Resurrection Protocol**: The ResurrectionManager should:
         - Kill existing browser processes before launching new ones
         - Use proper executable path detection
         - Implement retry logic with exponential backoff
         - Log detailed status during resurrection attempts

      ## Implementation Notes
      The system now properly handles Ghost Engine disconnections by returning clear error messages and automatically reconnecting when the engine comes online. The watchdog continues to operate and queue files for ingestion, which are processed once the Ghost Engine connects.
    size: 2473
    tokens: 932
  - path: specs\standards\README.md
    content: |-
      # The Sovereign Engineering Code (SEC)

      This is the authoritative reference manual for the External Context Engine (ECE) project. Standards are organized by domain to facilitate navigation and understanding.

      ## Domain 00: CORE (Philosophy & Invariants)
      Philosophy, Privacy, and "Local-First" invariants that govern the fundamental principles of the system.

      ### Standards:
      - [012-context-utility-manifest.md](00-CORE/012-context-utility-manifest.md) - Context utility manifest and philosophical foundations
      - [027-no-resurrection-mode.md](00-CORE/027-no-resurrection-mode.md) - Manual control via NO_RESURRECTION_MODE flag
      - [028-default-no-resurrection-mode.md](00-CORE/028-default-no-resurrection-mode.md) - Default behavior for Ghost Engine resurrection

      ## Domain 10: ARCH (System Architecture)
      Node.js Monolith, CozoDB, Termux, Hardware limits, and system architecture decisions.

      ### Standards:
      - [003-webgpu-initialization-stability.md](10-ARCH/003-webgpu-initialization-stability.md) - WebGPU initialization stability
      - [004-wasm-memory-management.md](10-ARCH/004-wasm-memory-management.md) - WASM memory management
      - [014-async-best-practices.md](10-ARCH/014-async-best-practices.md) - Async/await patterns for system integration
      - [014-gpu-resource-availability.md](10-ARCH/014-gpu-resource-availability.md) - GPU resource availability
      - [023-anchor-lite-simplification.md](10-ARCH/023-anchor-lite-simplification.md) - Anchor Lite architectural simplification
      - [031-ghost-engine-stability-fix.md](10-ARCH/031-ghost-engine-stability-fix.md) - CozoDB schema FTS failure handling
      - [032-ghost-engine-initialization-flow.md](10-ARCH/032-ghost-engine-initialization-flow.md) - Database initialization race condition prevention
      - [034-nodejs-monolith-migration.md](10-ARCH/034-nodejs-monolith-migration.md) - Migration to Node.js monolith architecture

      ## Domain 20: DATA (Data, Memory, Filesystem)
      Source of Truth, File Ingestion, Schemas, YAML Snapshots, and all data-related concerns.

      ### Standards:
      - [017-file-ingestion-debounce-hash-checking.md](20-DATA/017-file-ingestion-debounce-hash-checking.md) - File ingestion with debouncing and hash checking
      - [019-code-file-ingestion-comprehensive-context.md](20-DATA/019-code-file-ingestion-comprehensive-context.md) - Comprehensive context for code file ingestion
      - [021-chat-session-persistence-context-continuity.md](20-DATA/021-chat-session-persistence-context-continuity.md) - Chat session persistence and context continuity
      - [022-text-file-source-of-truth-cross-machine-sync.md](20-DATA/022-text-file-source-of-truth-cross-machine-sync.md) - Text files as source of truth with cross-machine synchronization
      - [024-context-ingestion-pipeline-fix.md](20-DATA/024-context-ingestion-pipeline-fix.md) - Context ingestion pipeline fixes
      - [029-consolidated-data-aggregation.md](20-DATA/029-consolidated-data-aggregation.md) - Consolidated data aggregation approach
      - [030-multi-format-output.md](20-DATA/030-multi-format-output.md) - JSON, YAML, and text output support
      - [033-cozodb-syntax-compliance.md](20-DATA/033-cozodb-syntax-compliance.md) - CozoDB syntax compliance requirements
      - [037-database-hydration-snapshot-portability.md](20-DATA/037-database-hydration-snapshot-portability.md) - Database hydration and snapshot portability workflow

      ## Domain 30: OPS (Protocols, Safety, Debugging)
      Agent Safety (Protocol 001), Logging, Async handling, and operational procedures.

      ### Standards:
      - [001-windows-console-encoding.md](30-OPS/001-windows-console-encoding.md) - Windows console encoding handling
      - [011-comprehensive-testing-verification.md](30-OPS/011-comprehensive-testing-verification.md) - Comprehensive testing and verification
      - [013-universal-log-collection.md](30-OPS/013-universal-log-collection.md) - Universal log collection system
      - [016-process-management-auto-resurrection.md](30-OPS/016-process-management-auto-resurrection.md) - Process management and auto-resurrection
      - [020-browser-profile-management-cleanup.md](30-OPS/020-browser-profile-management-cleanup.md) - Browser profile management and cleanup
      - [024-detached-logging-standard.md](30-OPS/024-detached-logging-standard.md) - Detached execution with logging
      - [025-script-logging-protocol.md](30-OPS/025-script-logging-protocol.md) - Script logging protocol (Protocol 001)
      - [035-never-attached-mode.md](30-OPS/035-never-attached-mode.md) - Never run services in attached mode (Detached Execution)
      - [036-log-file-management-protocol.md](30-OPS/036-log-file-management-protocol.md) - Log file management and rotation

      ## Domain 40: BRIDGE (APIs, Extensions, UI)
      Extensions, Ports, APIs, and all interface-related concerns.

      ### Standards:
      - [010-bridge-redirect-implementation.md](40-BRIDGE/010-bridge-redirect-implementation.md) - Bridge redirect implementation
      - [015-browser-control-center.md](40-BRIDGE/015-browser-control-center.md) - Unified browser control center
      - [018-streaming-cli-client-responsive-ux.md](40-BRIDGE/018-streaming-cli-client-responsive-ux.md) - Responsive UX for streaming CLI clients
      - [026-ghost-engine-connection-management.md](40-BRIDGE/026-ghost-engine-connection-management.md) - Ghost Engine connection management
    size: 5196
    tokens: 2022
  - path: specs\tasks.md
    content: "# Context-Engine Implementation Tasks\r\n\r\n## Current Work Queue (Unified Anchor Architecture)\r\n\r\n### Phase 5: Unified Anchor (Completed)\r\n- [x] **Consolidation**: Merge File Server and Bridge into `webgpu_bridge.py` (Port 8000).\r\n- [x] **Renaming**: `model-server-chat` -> `chat.html`, `neural-terminal` -> `terminal.html`.\r\n- [x] **Cleanup**: Archive legacy startup scripts.\r\n- [x] **Launcher**: Create `start-anchor.bat`.\r\n- [x] **Native Shell Spawning**: Implement `/v1/system/spawn_shell` endpoint.\r\n- [x] **Dashboard Integration**: Add Anchor Shell spawn button to `index.html`.\r\n- [x] **Native Client**: Create `anchor.py` for PowerShell terminal spawning.\r\n- [x] **Architecture Documentation**: Create Anchor Core specification.\r\n- [x] **Testing Suite**: Create `test_model_loading.py` and `model_test.html` for endpoint verification.\r\n- [x] **Troubleshooting Documentation**: Add model loading troubleshooting standard.\r\n\r\n### Phase 5.1: Model Loading Fixes (Completed)\r\n- [x] **URL Construction Fix**: Implement `/models/{model}/resolve/main/{file}` redirect endpoint for MLC-LLM compatibility.\r\n- [x] **File Renaming**: Rename `root-mic.html` -> `anchor-mic.html`, `root-dreamer.html` -> `memory-builder.html`, `sovereign-db-builder.html` -> `db_builder.html`.\r\n- [x] **UI Layout Fix**: Add proper margins to prevent elements from being cut off at top of browser window.\r\n- [x] **Server Stability**: Fix server hanging issues caused by problematic path parameter syntax.\r\n- [x] **Endpoint Verification**: Ensure all documented endpoints are accessible and responding properly.\r\n\r\n### Phase 5.2: Search Enhancement (Completed)\r\n- [x] **BM25 Implementation**: Replace regex-based search with CozoDB FTS using BM25 algorithm in `tools/chat.html`.\r\n- [x] **Index Creation**: Add FTS index creation in `memory-builder.html`, `db_builder.html`, and `chat.html` initialization.\r\n- [x] **Hybrid Search**: Maintain vector search alongside BM25 for semantic + lexical retrieval.\r\n- [x] **Fallback Mechanism**: Implement regex fallback if FTS index is unavailable.\r\n- [x] **Stemming Support**: Enable English stemming for better word variation matching.\r\n\r\n### Phase 6: Text-Only + Watchdog Architecture (Completed)\r\n- [x] **Vision Removal**: Remove brittle Vision/Ollama dependencies to increase survival rate.\r\n- [x] **Watchdog Service**: Create `tools/watchdog.py` for passive text file monitoring.\r\n- [x] **File Ingestion**: Implement `/v1/memory/ingest` endpoint in bridge for text ingestion.\r\n- [x] **Debounce & Hash Check**: Add debounce and content hash checking to prevent duplicate ingestion.\r\n- [x] **Auto-Resurrection**: Enhance `ResurrectionManager` to kill existing browser processes before launching new ones.\r\n- [x] **Streaming CLI**: Update `anchor.py` to use streaming for better UX.\r\n- [x] **Documentation Update**: Update architecture specs and standards to reflect new approach.\r\n\r\n### Phase 7: Context Expansion & Persistence (Completed)\r\n- [x] **Code File Support**: Expand watchdog to monitor code extensions (.py, .js, .html, etc.)\r\n- [x] **Browser Profile Cleanup**: Add unique temp profiles and cleanup for browser processes\r\n- [x] **Chat Session Persistence**: Auto-save conversations to context/sessions/ directory\r\n- [x] **Ingestion Loop Closure**: Ensure chat sessions become ingested context automatically\r\n- [x] **Memory Leak Prevention**: Implement profile cleanup to prevent disk space issues\r\n- [x] **Documentation Update**: Create new standards 019-021 for new features\r\n\r\n### Phase 8: Session Recorder & Text-File Source of Truth (Completed)\r\n- [x] **Daily Session Files**: Create `chat_YYYY-MM-DD.md` files for each day's conversations\r\n- [x] **Text-File Source of Truth**: Implement \"Database is Cache\" philosophy\r\n\r\n### Phase 9: Node.js Monolith & Snapshot Portability (Completed)\r\n- [x] **Migration**: Move from Python/Browser Bridge to Node.js Monolith (Standard 034).\r\n- [x] **FTS Optimization**: Implement native CozoDB BM25 search.\r\n- [x] **Operational Safety**: Implement detached execution and logging protocols (Standard 035/036).\r\n- [x] **Snapshot Portability**: Create \"Eject\" (Backup) and \"Hydrate\" (Restore) workflow (Standard 037).\r\n- [x] **Search Hardening**: Implement result truncation for large context files.\r\n\r\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access\r\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat\r\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking\r\n- [x] **Session Tracking**: Add session file path display in CLI startup\r\n\r\n### Completed - Root Refactor ✅\r\n- [x] **Kernel**: Implement `tools/modules/sovereign.js`.\r\n- [x] **Mic**: Refactor `root-mic.html` to use Kernel.\r\n- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.\r\n- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).\r\n- [x] **Docs**: Update all specs to reflect Root Architecture.\r\n\r\n### Completed - Hardware Optimization 🐉\r\n- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.\r\n- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.\r\n- [x] **Crash Prevention**: Context clamping for constrained drivers.\r\n- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.\r\n- [x] **Consciousness Semaphore**: Implemented resource arbitration in `sovereign.js`.\r\n\r\n### Completed - The Subconscious ✅\r\n- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.\r\n- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.\r\n- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).\r\n- [x] **Memory Hygiene**: Implemented \"Forgetting Curve\" in `root-dreamer.html`.\r\n\r\n### Completed - Active Cognition ✅\r\n- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.\r\n- [x] **User Control**: Add \"Auto-Save\" toggle to System Controls.\r\n- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.\r\n- [x] **Multimodal**: Add Drag-and-Drop Image support to Console.\r\n\r\n### Phase 4.1: The Neural Shell (Completed) 🚧\r\n**Objective:** Decouple Intelligence (Chat) from Agency (Terminal).\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 3.5:** Ghost Auto-Ignition (Headless auto-start with ?headless=true flag).\r\n- [x] **Phase 4:** Migration to C++ Native Runtime (Removing Chrome entirely).\r\n- [x] **Bridge Repair**: Debug and stabilize `extension-bridge` connectivity.\r\n- [x] **Neural Shell Protocol**: Implement `/v1/shell/exec` in `webgpu_bridge.py`.\r\n- [x] **The \"Coder\" Model**: Add `Qwen2.5-Coder-1.5B` to Model Registry.\r\n- [x] **Terminal UI**: Create `tools/neural-terminal.html` for natural language command execution.\r\n\r\n### Phase 4.2: Agentic Expansion (Deferred)\r\n- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.\r\n- [ ] **Voice Output**: Add TTS to Console.\r\n\r\n## Phase 5: The Specialist Array\r\n- [ ] **Dataset Generation**: Samsung TRM / Distillation.\r\n- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.\r\n- [ ] **Model Merging**: FrankenMoE construction.\r\n\r\n## Phase 6: GPU Resource Management (Completed)\r\n- [x] **GPU Queuing System**: Implement `/v1/gpu/lock`, `/v1/gpu/unlock`, and `/v1/gpu/status` endpoints with automatic queuing\r\n- [x] **Resource Conflict Resolution**: Eliminate GPU lock conflicts with proper queue management\r\n- [x] **503 Error Resolution**: Fix \"Service Unavailable\" errors by implementing proper resource queuing\r\n- [x] **Sidecar Integration**: Add GPU status monitoring to sidecar interface\r\n- [x] **Log Integration**: Add GPU resource management logs to centralized logging system\r\n- [x] **Documentation**: Update specs and standards to reflect GPU queuing system\r\n\r\n## Phase 7: Async/Await Best Practices (Completed)\r\n- [x] **Coroutine Fixes**: Resolve \"coroutine was never awaited\" warnings in webgpu_bridge.py\r\n- [x] **Event Loop Integration**: Properly integrate async functions with FastAPI's event loop\r\n- [x] **Startup Sequence**: Ensure logging system initializes properly with application lifecycle\r\n- [x] **Resource Management**: Fix resource cleanup in WebSocket handlers to prevent leaks\r\n- [x] **Error Handling**: Enhance async error handling with proper cleanup procedures\r\n- [x] **Documentation**: Create Standard 014 for async/await best practices\r\n\r\n## Phase 8: Browser-Based Control Center (Completed)\r\n- [x] **Sidecar UI**: Implement `tools/sidecar.html` with dual tabs for retrieval and vision\r\n- [x] **Context UI**: Implement `tools/context.html` for manual context retrieval\r\n- [x] **Vision Engine**: Create `tools/vision_engine.py` for Python-powered image analysis\r\n- [x] **Bridge Integration**: Update `webgpu_bridge.py` to serve UI and handle vision endpoints\r\n- [x] **Endpoint Implementation**: Add `/v1/vision/ingest`, `/v1/memory/search`, `/logs/recent` endpoints\r\n- [x] **File-based Logging**: Implement persistent logging to `logs/` directory with truncation\r\n- [x] **Documentation**: Update specs and standards to reflect new architecture\r\n\r\n### Phase 9: Anchor Lite Refactor (Completed)\r\n- [x] **Consolidation**: Simplified system to Single Source of Truth (`context/`) -> Single Index (CozoDB) -> Single UI (`context.html`).\r\n- [x] **Cleanup**: Archived unused tools (`db_builder`, `memory-builder`, `sidecar`, `mobile-chat`).\r\n- [x] **Engine Refactor**: Created headless `ghost.html` engine with WebSocket bridge.\r\n- [x] **Launch Logic**: Unified startup in `start-anchor.bat` and `webgpu_bridge.py`.\r\n- [x] **Standard 023**: Documented \"Anchor Lite\" architecture and \"Triangle of Pain\".\r\n\r\n### Phase 10: Context Ingestion Pipeline Fixes (Completed)\r\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\r\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\r\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\r\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\r\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\r\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\r\n\r\n### Phase 11: Script Running Protocol Implementation (Completed)\r\n- [x] **Protocol Creation**: Created `SCRIPT_PROTOCOL.md` with guidelines to prevent getting stuck in long-running loops\r\n- [x] **System Optimization**: Fixed database paths and search queries for better performance\r\n- [x] **Documentation Update**: Updated doc_policy to include protocol as allowed root document\r\n- [x] **Standards Creation**: Created Standards 035 and 036 for detached execution and log management\r\n- [x] **Startup Scripts**: Created proper detached startup scripts with logging\r\n\r\n## Backlog\r\n- [ ] **Federation Protocol**: P2P sync.\r\n- [ ] **Android App**: Wrapper for Root Coda."
    size: 11184
    tokens: 4210
  - path: start-anchor.bat
    content: "@echo off\r\necho ⚓ STARTING ANCHOR SYSTEM IN BACKGROUND...\r\n\r\nREM Check for low-resource mode\r\nif \"%LOW_RESOURCE_MODE%\"==\"true\" (\r\n    echo 📱 Low-Resource Mode Enabled\r\n) else (\r\n    if \"%CPU_ONLY_MODE%\"==\"true\" (\r\n        echo 💻 CPU-Only Mode Enabled\r\n    )\r\n)\r\n\r\nREM 1. Start the Unified Server (Truly Background - no window)\r\nif \"%LOW_RESOURCE_MODE%\"==\"true\" (\r\n    start \"Anchor Core\" /min cmd /c \"cd tools && set LOW_RESOURCE_MODE=true && python webgpu_bridge.py\"\r\n) else (\r\n    if \"%CPU_ONLY_MODE%\"==\"true\" (\r\n        start \"Anchor Core\" /min cmd /c \"cd tools && set CPU_ONLY_MODE=true && python webgpu_bridge.py\"\r\n    ) else (\r\n        start \"Anchor Core\" /min cmd /c \"cd tools && python webgpu_bridge.py\"\r\n    )\r\n)\r\n\r\nREM 1.5 Start Watchdog (File Monitor)\r\nstart \"Anchor Watchdog\" /min cmd /c \"cd tools && python anchor_watchdog.py\"\r\n\r\nREM 2. Wait for Server to initialize\r\necho Waiting for server to initialize...\r\ntimeout /t 5 /nobreak >nul\r\n\r\nREM 3. Launch the Ghost Engine\r\nREM Handled automatically by the Bridge (Resurrection Manager)\r\necho 👻 Ghost Engine managed by Bridge...\r\n\r\necho.\r\necho ✅ Anchor System Started in Background\r\necho    Open http://localhost:8000 in your browser when ready\r\necho    Services will start automatically when you access the UI\r\necho.\r\necho 💡 For low-resource devices: set LOW_RESOURCE_MODE=true before running\r\necho 💡 For CPU-only: set CPU_ONLY_MODE=true before running\r\necho.\r\necho 🔄 Ghost Engine should connect automatically to enable chat and memory search"
    size: 1510
    tokens: 581
  - path: start-low-resource.bat
    content: "@echo off\r\necho 📱 STARTING ANCHOR SYSTEM IN LOW-RESOURCE MODE...\r\n\r\necho 📋 Configuration:\r\necho   - GPU Buffer: 64MB (conservative)\r\necho   - Single-threaded operations\r\necho   - Small model defaults (Phi-3.5-mini)\r\necho   - Reduced cache sizes\r\necho   - Longer timeouts for stability\r\n\r\nREM Set environment variables for low-resource mode\r\nset LOW_RESOURCE_MODE=true\r\n\r\nREM 1. Start the Unified Server with low-resource settings\r\necho Starting Anchor Core...\r\nstart \"Anchor Core\" /min cmd /c \"cd tools && set LOW_RESOURCE_MODE=true && python webgpu_bridge.py\"\r\n\r\nREM 2. Wait for Server to initialize\r\necho Waiting for server to initialize...\r\ntimeout /t 5 /nobreak >nul\r\n\r\nREM 3. Launch the Ghost Engine with conservative GPU settings (FIXED: JavaScript Enabled)\r\necho 👻 Launching Ghost Engine...\r\nstart \"Ghost Engine\" /min cmd /c \"msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222 --no-first-run --no-default-browser-check --disable-extensions --disable-plugins --disable-images --disable-web-security --user-data-dir=%TEMP%\\anchor_ghost --max-active-webgl-contexts=1 --max-webgl-contexts-per-group=1 --disable-gpu-memory-buffer-compositor-resources --force-gpu-mem-available-mb=64 --force-low-power-gpu --disable-gpu-driver-workarounds --disable-gpu-sandbox --disable-features=VizDisplayCompositor --disable-gpu-memory-buffer-video-frames --disable-gpu-memory-buffer-compositor-resources\"\r\n\r\necho.\r\necho ✅ Anchor System Started in Low-Resource Mode\r\necho    Open http://localhost:8000 in your browser when ready\r\necho    Services will start automatically when you access the UI\r\necho.\r\necho 💡 Tips for low-resource devices:\r\necho   - Use Phi-3.5-mini or smaller models\r\necho   - Expect slower response times\r\necho   - Close other GPU-intensive applications\r\necho   - Consider using CPU-only mode if GPU crashes persist"
    size: 1881
    tokens: 739
  - path: start_engine.bat
    content: |-
      @echo off
      REM ECE_Core Engine Startup Script with Logging Protocol
      REM Follows SCRIPT_PROTOCOL.md standards for detached execution

      echo Starting ECE_Core Engine with logging protocol...

      REM Create logs directory if it doesn't exist
      if not exist "..\logs" mkdir "..\logs"

      REM Check if server is already running by checking for the log file lock
      echo Checking for existing server instance...
      if exist "..\logs\server.log" (
          echo Warning: server.log exists, checking if server is already running...
          REM Try to copy the file to see if it's locked
          copy "..\logs\server.log" "..\logs\temp_check.log" >nul 2>&1
          if errorlevel 1 (
              echo Server appears to be running already. Please stop it first with: taskkill /f /im node.exe
              echo Or check the existing logs at logs/server.log
              pause
              exit /b 1
          ) else (
              del "..\logs\temp_check.log" >nul 2>&1
          )
      )

      REM Start the engine in background with logging
      cd /d "%~dp0\engine"
      start /b cmd /c "node src/index.js > ../logs/server.log 2>&1"

      REM Wait a moment for the server to start (Windows compatible)
      choice /c yn /d y /t 3 > nul

      REM Verify the server is running by checking the log
      echo Checking server status...
      if exist "../logs/server.log" (
          powershell -Command "Get-Content ../logs/server.log -Tail 5"
      ) else (
          echo Warning: Log file not found. Server may not have started.
      )

      echo.
      echo Server started in background. Check logs/server.log for output.
      echo Access the interface at: http://localhost:3000
    size: 1515
    tokens: 622
  - path: start_engine.ps1
    content: |-
      # ECE_Core Engine Startup Script with Logging Protocol
      # Follows SCRIPT_PROTOCOL.md standards for detached execution

      Write-Host "Starting ECE_Core Engine with logging protocol..." -ForegroundColor Green

      # Create logs directory if it doesn't exist
      if (!(Test-Path "../logs")) {
          New-Item -ItemType Directory -Path "../logs" | Out-Null
          Write-Host "Created logs directory" -ForegroundColor Cyan
      }

      # Change to engine directory
      Set-Location -Path "$PSScriptRoot\engine"

      # Start the engine in background with logging
      $process = Start-Process -FilePath "node" -ArgumentList "src/index.js" -RedirectStandardOutput "../logs/server.log" -RedirectStandardError "../logs/server_error.log" -PassThru -WindowStyle Hidden

      Write-Host "Server process started with PID: $($process.Id)" -ForegroundColor Green

      # Wait a moment for the server to start
      Start-Sleep -Seconds 3

      # Verify the server is running by checking the log
      if (Test-Path "../logs/server.log") {
          Write-Host "Last 5 lines of server log:" -ForegroundColor Cyan
          Get-Content -Path "../logs/server.log" -Tail 5
      } elseif (Test-Path "../logs/server_error.log") {
          Write-Host "Last 5 lines of error log:" -ForegroundColor Red
          Get-Content -Path "../logs/server_error.log" -Tail 5
      } else {
          Write-Host "Warning: Log files not found. Server may not have started." -ForegroundColor Yellow
      }

      Write-Host ""
      Write-Host "Server started in background. Check logs/server.log for output." -ForegroundColor Green
      Write-Host "Access the interface at: http://localhost:3000" -ForegroundColor Green
    size: 1555
    tokens: 597
  - path: tests\comprehensive_test_suite.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nComprehensive Test Suite for Anchor Core\r\n\r\nThis script provides a complete test suite covering:\r\n1. Model loading functionality\r\n2. Data pipeline verification\r\n3. Endpoint accessibility\r\n4. Missing endpoint detection\r\n5. Function syntax error detection\r\n\"\"\"\r\n\r\nimport requests\r\nimport sys\r\nimport time\r\nimport json\r\nfrom urllib.parse import urljoin\r\nfrom pathlib import Path\r\nimport subprocess\r\nimport importlib.util\r\nfrom typing import Dict, List, Tuple, Any\r\n\r\n\r\nclass ComprehensiveTestSuite:\r\n    def __init__(self, base_url: str = \"http://localhost:8000\", token: str = \"sovereign-secret\"):\r\n        self.base_url = base_url\r\n        self.token = token\r\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n        self.results = {\r\n            \"model_loading\": {},\r\n            \"data_pipeline\": {},\r\n            \"endpoint_verification\": {},\r\n            \"syntax_check\": {},\r\n            \"overall\": {\"passed\": 0, \"failed\": 0, \"total\": 0}\r\n        }\r\n\r\n    def run_all_tests(self) -> bool:\r\n        \"\"\"Run all test categories\"\"\"\r\n        print(\"🚀 Running Comprehensive Test Suite for Anchor Core...\")\r\n        print(f\"Testing against: {self.base_url}\")\r\n        print(\"=\" * 80)\r\n\r\n        # Run all test categories\r\n        model_loading_ok = self.test_model_loading()\r\n        data_pipeline_ok = self.test_data_pipeline()\r\n        endpoint_ok = self.test_endpoint_verification()\r\n        syntax_ok = self.test_syntax_verification()\r\n\r\n        # Summary\r\n        print(\"\\n\" + \"=\" * 80)\r\n        print(\"📊 COMPREHENSIVE TEST SUITE SUMMARY\")\r\n        print(\"=\" * 80)\r\n        \r\n        print(f\"Model Loading Tests: {'✅ PASS' if model_loading_ok else '❌ FAIL'}\")\r\n        print(f\"Data Pipeline Tests: {'✅ PASS' if data_pipeline_ok else '❌ FAIL'}\")\r\n        print(f\"Endpoint Verification: {'✅ PASS' if endpoint_ok else '❌ FAIL'}\")\r\n        print(f\"Syntax Verification: {'✅ PASS' if syntax_ok else '❌ FAIL'}\")\r\n\r\n        overall_success = all([model_loading_ok, data_pipeline_ok, endpoint_ok, syntax_ok])\r\n        print(f\"\\n🎯 Overall Result: {'✅ ALL TESTS PASSED' if overall_success else '❌ SOME TESTS FAILED'}\")\r\n        \r\n        print(f\"\\n📈 Test Statistics:\")\r\n        print(f\"  Total Tests: {self.results['overall']['total']}\")\r\n        print(f\"  Passed: {self.results['overall']['passed']}\")\r\n        print(f\"  Failed: {self.results['overall']['failed']}\")\r\n\r\n        return overall_success\r\n\r\n    def test_model_loading(self) -> bool:\r\n        \"\"\"Test model loading functionality\"\"\"\r\n        print(\"\\n🔍 Testing Model Loading...\")\r\n        print(\"-\" * 40)\r\n\r\n        # Test model availability\r\n        models_to_test = [\r\n            \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\r\n        ]\r\n\r\n        all_models_ok = True\r\n        for model_name in models_to_test:\r\n            print(f\"  Testing model: {model_name}\")\r\n            \r\n            # Test model config files accessibility\r\n            config_files = [\r\n                f\"models/{model_name}/resolve/main/ndarray-cache.json\",\r\n                f\"models/{model_name}/resolve/main/mlc-chat-config.json\",\r\n                f\"models/{model_name}/resolve/main/tokenizer.json\",\r\n                f\"models/{model_name}/resolve/main/tokenizer_config.json\"\r\n            ]\r\n\r\n            model_ok = True\r\n            for config_file in config_files:\r\n                try:\r\n                    url = urljoin(self.base_url, config_file)\r\n                    response = requests.head(url, timeout=10)\r\n                    status_ok = response.status_code in [200, 404]  # 404 is expected if file doesn't exist locally\r\n                    print(f\"    {'✅' if status_ok else '❌'} {config_file} -> {response.status_code}\")\r\n                    \r\n                    test_key = f\"model_{model_name}_{config_file.replace('/', '_')}\"\r\n                    self.results[\"model_loading\"][test_key] = {\r\n                        \"url\": url,\r\n                        \"status\": response.status_code,\r\n                        \"success\": status_ok\r\n                    }\r\n                    \r\n                    if not status_ok:\r\n                        model_ok = False\r\n                        all_models_ok = False\r\n                        \r\n                except Exception as e:\r\n                    print(f\"    ❌ {config_file} -> ERROR: {e}\")\r\n                    test_key = f\"model_{model_name}_{config_file.replace('/', '_')}\"\r\n                    self.results[\"model_loading\"][test_key] = {\r\n                        \"url\": urljoin(self.base_url, config_file),\r\n                        \"status\": \"ERROR\",\r\n                        \"success\": False,\r\n                        \"error\": str(e)\r\n                    }\r\n                    model_ok = False\r\n                    all_models_ok = False\r\n\r\n            if model_ok:\r\n                print(f\"  ✅ Model {model_name} is accessible\")\r\n            else:\r\n                print(f\"  ❌ Model {model_name} has issues\")\r\n\r\n        # Update overall counters\r\n        total_model_tests = len(self.results[\"model_loading\"])\r\n        passed_model_tests = sum(1 for r in self.results[\"model_loading\"].values() if r[\"success\"])\r\n        self.results[\"overall\"][\"total\"] += total_model_tests\r\n        self.results[\"overall\"][\"passed\"] += passed_model_tests\r\n        self.results[\"overall\"][\"failed\"] += total_model_tests - passed_model_tests\r\n\r\n        return all_models_ok\r\n\r\n    def test_data_pipeline(self) -> bool:\r\n        \"\"\"Test data pipeline functionality\"\"\"\r\n        print(\"\\n PIPELINE Testing Data Pipeline...\")\r\n        print(\"-\" * 40)\r\n\r\n        pipeline_tests = [\r\n            (\"/health\", \"GET\", 200, \"Health Check\"),\r\n            (\"/v1/gpu/status\", \"GET\", 200, \"GPU Status\"),\r\n            (\"/v1/system/spawn_shell\", \"POST\", 400, \"Spawn Shell (expects 400 due to missing body)\"),\r\n        ]\r\n\r\n        all_pipeline_ok = True\r\n        for endpoint, method, expected_status, description in pipeline_tests:\r\n            print(f\"  Testing {description}: {method} {endpoint}\")\r\n            try:\r\n                url = urljoin(self.base_url, endpoint)\r\n                if method == \"GET\":\r\n                    response = requests.get(url, headers=self.headers, timeout=10)\r\n                elif method == \"POST\":\r\n                    response = requests.post(url, headers=self.headers, timeout=10)\r\n                \r\n                status_ok = response.status_code == expected_status\r\n                print(f\"    {'✅' if status_ok else '❌'} Status: {response.status_code} (expected {expected_status})\")\r\n                \r\n                test_key = f\"pipeline_{endpoint.replace('/', '_')}\"\r\n                self.results[\"data_pipeline\"][test_key] = {\r\n                    \"url\": url,\r\n                    \"method\": method,\r\n                    \"status\": response.status_code,\r\n                    \"expected\": expected_status,\r\n                    \"success\": status_ok\r\n                }\r\n                \r\n                if not status_ok:\r\n                    all_pipeline_ok = False\r\n                    \r\n            except Exception as e:\r\n                print(f\"    ❌ ERROR: {e}\")\r\n                test_key = f\"pipeline_{endpoint.replace('/', '_')}\"\r\n                self.results[\"data_pipeline\"][test_key] = {\r\n                    \"url\": urljoin(self.base_url, endpoint),\r\n                    \"method\": method,\r\n                    \"status\": \"ERROR\",\r\n                    \"expected\": expected_status,\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                all_pipeline_ok = False\r\n\r\n        # Update overall counters\r\n        total_pipeline_tests = len(self.results[\"data_pipeline\"])\r\n        passed_pipeline_tests = sum(1 for r in self.results[\"data_pipeline\"].values() if r[\"success\"])\r\n        self.results[\"overall\"][\"total\"] += total_pipeline_tests\r\n        self.results[\"overall\"][\"passed\"] += passed_pipeline_tests\r\n        self.results[\"overall\"][\"failed\"] += total_pipeline_tests - passed_pipeline_tests\r\n\r\n        return all_pipeline_ok\r\n\r\n    def test_endpoint_verification(self) -> bool:\r\n        \"\"\"Test endpoint accessibility and detect missing endpoints\"\"\"\r\n        print(\"\\n🔍 Testing Endpoint Verification...\")\r\n        print(\"-\" * 40)\r\n\r\n        # Define critical endpoints that should exist\r\n        critical_endpoints = [\r\n            (\"/health\", \"GET\", 200),\r\n            (\"/v1/chat/completions\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/embeddings\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/shell/exec\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/lock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/unlock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/status\", \"GET\", 200),\r\n            (\"/v1/gpu/reset\", \"POST\", 200),\r\n            (\"/v1/gpu/force-release-all\", \"POST\", 200),\r\n            (\"/v1/system/spawn_shell\", \"POST\", 200),\r\n            (\"/v1/models/pull\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/models/pull/status\", \"GET\", 400),  # Expected to fail with 400 due to missing id param\r\n        ]\r\n\r\n        all_endpoints_ok = True\r\n        missing_endpoints = []\r\n\r\n        for endpoint, method, expected_status in critical_endpoints:\r\n            print(f\"  Testing endpoint: {method} {endpoint}\")\r\n            try:\r\n                url = urljoin(self.base_url, endpoint)\r\n                if method == \"GET\":\r\n                    response = requests.get(url, headers=self.headers, timeout=10)\r\n                elif method == \"POST\":\r\n                    response = requests.post(url, headers=self.headers, timeout=10)\r\n                \r\n                # For endpoints that are expected to fail due to missing body/params, \r\n                # we consider them accessible if they return 400/404/422 rather than 404/405\r\n                accessible = response.status_code != 404 and response.status_code != 405\r\n                success = accessible  # We consider it a success if the endpoint exists\r\n                \r\n                status_msg = f\"Status: {response.status_code}\"\r\n                if not accessible:\r\n                    status_msg += \" (MISSING/INACCESSIBLE)\"\r\n                    missing_endpoints.append(f\"{method} {endpoint}\")\r\n                    all_endpoints_ok = False\r\n                \r\n                print(f\"    {'✅' if accessible else '❌'} {status_msg}\")\r\n                \r\n                test_key = f\"endpoint_{endpoint.replace('/', '_')}\"\r\n                self.results[\"endpoint_verification\"][test_key] = {\r\n                    \"url\": url,\r\n                    \"method\": method,\r\n                    \"status\": response.status_code,\r\n                    \"expected\": expected_status,\r\n                    \"accessible\": accessible,\r\n                    \"success\": success\r\n                }\r\n                \r\n            except Exception as e:\r\n                print(f\"    ❌ ERROR: {e}\")\r\n                missing_endpoints.append(f\"{method} {endpoint}\")\r\n                test_key = f\"endpoint_{endpoint.replace('/', '_')}\"\r\n                self.results[\"endpoint_verification\"][test_key] = {\r\n                    \"url\": urljoin(self.base_url, endpoint),\r\n                    \"method\": method,\r\n                    \"status\": \"ERROR\",\r\n                    \"expected\": expected_status,\r\n                    \"accessible\": False,\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                all_endpoints_ok = False\r\n\r\n        if missing_endpoints:\r\n            print(f\"\\n  ⚠️  Missing/Inaccessible Endpoints:\")\r\n            for ep in missing_endpoints:\r\n                print(f\"    - {ep}\")\r\n        else:\r\n            print(f\"\\n  ✅ All critical endpoints are accessible!\")\r\n\r\n        # Update overall counters\r\n        total_endpoint_tests = len(self.results[\"endpoint_verification\"])\r\n        passed_endpoint_tests = sum(1 for r in self.results[\"endpoint_verification\"].values() if r[\"success\"])\r\n        self.results[\"overall\"][\"total\"] += total_endpoint_tests\r\n        self.results[\"overall\"][\"passed\"] += passed_endpoint_tests\r\n        self.results[\"overall\"][\"failed\"] += total_endpoint_tests - passed_endpoint_tests\r\n\r\n        return all_endpoints_ok\r\n\r\n    def test_syntax_verification(self) -> bool:\r\n        \"\"\"Test for function syntax errors in critical files\"\"\"\r\n        print(\"\\n🔍 Testing Syntax Verification...\")\r\n        print(\"-\" * 40)\r\n\r\n        # Define critical Python files to check for syntax errors\r\n        critical_files = [\r\n            \"tools/webgpu_bridge.py\",\r\n            \"tools/anchor.py\",\r\n            \"tools/orchestrator.py\",\r\n            \"tests/test_model_loading.py\",\r\n            \"tests/test_model_availability.py\",\r\n            \"tests/test_gpu_fixes.py\",\r\n            \"tests/test_orchestrator.py\",\r\n        ]\r\n\r\n        all_syntax_ok = True\r\n        syntax_errors = []\r\n\r\n        for file_path in critical_files:\r\n            print(f\"  Checking syntax: {file_path}\")\r\n            try:\r\n                file_abs_path = Path(file_path)\r\n                if file_abs_path.exists():\r\n                    # Use Python's built-in compile to check syntax\r\n                    with open(file_abs_path, 'r', encoding='utf-8') as f:\r\n                        source_code = f.read()\r\n                    \r\n                    compile(source_code, str(file_abs_path), 'exec')\r\n                    print(f\"    ✅ Syntax OK\")\r\n                    \r\n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                    self.results[\"syntax_check\"][test_key] = {\r\n                        \"file\": str(file_abs_path),\r\n                        \"success\": True,\r\n                        \"error\": None\r\n                    }\r\n                else:\r\n                    print(f\"    ⚠️  File not found\")\r\n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                    self.results[\"syntax_check\"][test_key] = {\r\n                        \"file\": str(file_abs_path),\r\n                        \"success\": False,\r\n                        \"error\": \"File not found\"\r\n                    }\r\n                    syntax_errors.append(f\"{file_path}: File not found\")\r\n                    all_syntax_ok = False\r\n                    \r\n            except SyntaxError as e:\r\n                print(f\"    ❌ Syntax Error: {e}\")\r\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\r\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                self.results[\"syntax_check\"][test_key] = {\r\n                    \"file\": str(file_abs_path),\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                all_syntax_ok = False\r\n            except Exception as e:\r\n                print(f\"    ❌ Error checking file: {e}\")\r\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\r\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                self.results[\"syntax_check\"][test_key] = {\r\n                    \"file\": str(file_abs_path),\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                all_syntax_ok = False\r\n\r\n        if syntax_errors:\r\n            print(f\"\\n  ❌ Syntax errors found:\")\r\n            for error in syntax_errors:\r\n                print(f\"    - {error}\")\r\n        else:\r\n            print(f\"\\n  ✅ All critical files have valid syntax!\")\r\n\r\n        # Update overall counters\r\n        total_syntax_tests = len(self.results[\"syntax_check\"])\r\n        passed_syntax_tests = sum(1 for r in self.results[\"syntax_check\"].values() if r[\"success\"])\r\n        self.results[\"overall\"][\"total\"] += total_syntax_tests\r\n        self.results[\"overall\"][\"passed\"] += passed_syntax_tests\r\n        self.results[\"overall\"][\"failed\"] += total_syntax_tests - passed_syntax_tests\r\n\r\n        return all_syntax_ok\r\n\r\n    def generate_detailed_report(self) -> Dict[str, Any]:\r\n        \"\"\"Generate a detailed test report\"\"\"\r\n        report = {\r\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n            \"base_url\": self.base_url,\r\n            \"results\": self.results,\r\n            \"summary\": {\r\n                \"total_tests\": self.results[\"overall\"][\"total\"],\r\n                \"passed_tests\": self.results[\"overall\"][\"passed\"],\r\n                \"failed_tests\": self.results[\"overall\"][\"failed\"],\r\n                \"success_rate\": self.results[\"overall\"][\"passed\"] / max(self.results[\"overall\"][\"total\"], 1) * 100\r\n            }\r\n        }\r\n        return report\r\n\r\n\r\ndef main():\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser(description=\"Run Comprehensive Test Suite for Anchor Core\")\r\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\r\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\r\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\r\n                       help=\"Authentication token (default: sovereign-secret)\")\r\n    parser.add_argument(\"--output\",\r\n                       help=\"Output file for detailed test report (JSON format)\")\r\n\r\n    args = parser.parse_args()\r\n\r\n    # Run the comprehensive test suite\r\n    test_suite = ComprehensiveTestSuite(base_url=args.url, token=args.token)\r\n    success = test_suite.run_all_tests()\r\n\r\n    # Generate and save detailed report if requested\r\n    if args.output:\r\n        report = test_suite.generate_detailed_report()\r\n        with open(args.output, 'w', encoding='utf-8') as f:\r\n            json.dump(report, f, indent=2, ensure_ascii=False)\r\n        print(f\"\\n📄 Detailed test report saved to: {args.output}\")\r\n\r\n    # Exit with appropriate code\r\n    sys.exit(0 if success else 1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
    size: 18130
    tokens: 6102
  - path: tests\endpoint_syntax_verification.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nEndpoint and Syntax Verification Tests for Anchor Core\r\n\r\nThis script specifically tests for:\r\n1. Missing endpoints\r\n2. Function syntax errors\r\n3. API endpoint accessibility\r\n4. System health verification\r\n\"\"\"\r\n\r\nimport requests\r\nimport sys\r\nimport json\r\nimport subprocess\r\nimport ast\r\nfrom urllib.parse import urljoin\r\nfrom pathlib import Path\r\nfrom typing import Dict, List, Tuple\r\n\r\n\r\nclass EndpointAndSyntaxTester:\r\n    def __init__(self, base_url: str = \"http://localhost:8000\", token: str = \"sovereign-secret\"):\r\n        self.base_url = base_url\r\n        self.token = token\r\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n        self.results = {\r\n            \"endpoints\": {},\r\n            \"syntax\": {},\r\n            \"system_health\": {}\r\n        }\r\n\r\n    def test_all_endpoints(self) -> Dict[str, bool]:\r\n        \"\"\"Test all defined endpoints for accessibility\"\"\"\r\n        print(\"🔍 Testing All Endpoints for Accessibility...\")\r\n        print(\"-\" * 50)\r\n\r\n        # Define all expected endpoints with their methods and expected status codes\r\n        endpoints = [\r\n            # Core endpoints\r\n            (\"/\", \"GET\", [200, 404]),  # Root may serve UI or return 404\r\n            (\"/health\", \"GET\", [200]),\r\n            \r\n            # API endpoints\r\n            (\"/v1/chat/completions\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/embeddings\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/shell/exec\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            \r\n            # GPU management endpoints\r\n            (\"/v1/gpu/lock\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/unlock\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/status\", \"GET\", [200]),\r\n            (\"/v1/gpu/reset\", \"POST\", [200, 401]),\r\n            (\"/v1/gpu/force-release-all\", \"POST\", [200, 401]),\r\n            \r\n            # System endpoints\r\n            (\"/v1/system/spawn_shell\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            \r\n            # Model endpoints\r\n            (\"/v1/models/pull\", \"POST\", [400, 401, 422]),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/models/pull/status\", \"GET\", [400, 401, 404]),  # Expected to fail with 400 due to missing id param\r\n            \r\n            # Model file endpoints (these should return 404 if files don't exist locally, but endpoint should be accessible)\r\n            (\"/models/test-model/resolve/main/ndarray-cache.json\", \"GET\", [200, 404]),\r\n            (\"/models/test-model/resolve/main/mlc-chat-config.json\", \"GET\", [200, 404]),\r\n            (\"/models/test-model/resolve/main/tokenizer.json\", \"GET\", [200, 404]),\r\n        ]\r\n\r\n        missing_endpoints = []\r\n        accessible_endpoints = []\r\n\r\n        for endpoint, method, expected_statuses in endpoints:\r\n            print(f\"  Testing: {method} {endpoint}\")\r\n            \r\n            try:\r\n                url = urljoin(self.base_url, endpoint)\r\n                \r\n                if method == \"GET\":\r\n                    response = requests.get(url, headers=self.headers, timeout=10)\r\n                elif method == \"POST\":\r\n                    # Send minimal payload to avoid 400 errors due to missing body\r\n                    if endpoint in [\"/v1/chat/completions\", \"/v1/embeddings\", \"/v1/shell/exec\"]:\r\n                        response = requests.post(url, headers=self.headers, timeout=10, json={})\r\n                    else:\r\n                        response = requests.post(url, headers=self.headers, timeout=10)\r\n                else:\r\n                    response = requests.request(method, url, headers=self.headers, timeout=10)\r\n                \r\n                status_ok = response.status_code in expected_statuses\r\n                accessible = response.status_code != 404 and response.status_code != 405  # 404 = not found, 405 = method not allowed\r\n                \r\n                status_icon = \"✅\" if status_ok else \"❌\"\r\n                print(f\"    {status_icon} Status: {response.status_code} (expected: {expected_statuses})\")\r\n                \r\n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\r\n                self.results[\"endpoints\"][test_key] = {\r\n                    \"url\": url,\r\n                    \"method\": method,\r\n                    \"status\": response.status_code,\r\n                    \"expected\": expected_statuses,\r\n                    \"accessible\": accessible,\r\n                    \"status_ok\": status_ok,\r\n                    \"success\": accessible  # Endpoint exists if not 404/405\r\n                }\r\n                \r\n                if accessible:\r\n                    accessible_endpoints.append(f\"{method} {endpoint}\")\r\n                else:\r\n                    missing_endpoints.append(f\"{method} {endpoint}\")\r\n                    \r\n            except requests.exceptions.ConnectionError:\r\n                print(f\"    ❌ Connection Error - Server may not be running\")\r\n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\r\n                self.results[\"endpoints\"][test_key] = {\r\n                    \"url\": url,\r\n                    \"method\": method,\r\n                    \"status\": \"CONNECTION_ERROR\",\r\n                    \"expected\": expected_statuses,\r\n                    \"accessible\": False,\r\n                    \"status_ok\": False,\r\n                    \"success\": False\r\n                }\r\n                missing_endpoints.append(f\"{method} {endpoint}\")\r\n            except Exception as e:\r\n                print(f\"    ❌ Error: {e}\")\r\n                test_key = f\"endpoint_{endpoint.replace('/', '_').replace('-', '_')}\"\r\n                self.results[\"endpoints\"][test_key] = {\r\n                    \"url\": urljoin(self.base_url, endpoint),\r\n                    \"method\": method,\r\n                    \"status\": \"ERROR\",\r\n                    \"expected\": expected_statuses,\r\n                    \"accessible\": False,\r\n                    \"status_ok\": False,\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                missing_endpoints.append(f\"{method} {endpoint}\")\r\n\r\n        print(f\"\\n  Summary:\")\r\n        print(f\"    ✅ Accessible Endpoints: {len(accessible_endpoints)}\")\r\n        print(f\"    ❌ Missing/Inaccessible: {len(missing_endpoints)}\")\r\n        \r\n        if missing_endpoints:\r\n            print(f\"\\n  Missing Endpoints:\")\r\n            for ep in missing_endpoints:\r\n                print(f\"    - {ep}\")\r\n        \r\n        endpoint_success = len(missing_endpoints) == 0\r\n        return {\"success\": endpoint_success, \"missing\": missing_endpoints, \"accessible\": accessible_endpoints}\r\n\r\n    def test_syntax_in_files(self) -> Dict[str, bool]:\r\n        \"\"\"Test syntax in critical Python files\"\"\"\r\n        print(\"\\n🔍 Testing Syntax in Critical Files...\")\r\n        print(\"-\" * 50)\r\n\r\n        # Define critical files to check for syntax errors\r\n        critical_files = [\r\n            \"tools/webgpu_bridge.py\",\r\n            \"tools/anchor.py\", \r\n            \"tools/orchestrator.py\",\r\n            \"tests/comprehensive_test_suite.py\",\r\n            \"tests/test_model_loading.py\",\r\n            \"tests/test_model_availability.py\",\r\n            \"tests/test_gpu_fixes.py\",\r\n            \"tests/test_orchestrator.py\",\r\n        ]\r\n\r\n        syntax_errors = []\r\n        valid_files = []\r\n\r\n        for file_path in critical_files:\r\n            print(f\"  Checking: {file_path}\")\r\n            \r\n            try:\r\n                path_obj = Path(file_path)\r\n                if not path_obj.exists():\r\n                    print(f\"    ⚠️  File not found\")\r\n                    test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                    self.results[\"syntax\"][test_key] = {\r\n                        \"file\": str(path_obj),\r\n                        \"exists\": False,\r\n                        \"valid_syntax\": False,\r\n                        \"success\": False,\r\n                        \"error\": \"File not found\"\r\n                    }\r\n                    syntax_errors.append(f\"{file_path}: File not found\")\r\n                    continue\r\n\r\n                # Read and parse the file to check for syntax errors\r\n                with open(path_obj, 'r', encoding='utf-8') as f:\r\n                    source_code = f.read()\r\n                \r\n                # Parse the AST to check for syntax errors\r\n                ast.parse(source_code)\r\n                \r\n                print(f\"    ✅ Valid syntax\")\r\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                self.results[\"syntax\"][test_key] = {\r\n                    \"file\": str(path_obj),\r\n                    \"exists\": True,\r\n                    \"valid_syntax\": True,\r\n                    \"success\": True,\r\n                    \"error\": None\r\n                }\r\n                valid_files.append(file_path)\r\n                \r\n            except SyntaxError as e:\r\n                print(f\"    ❌ Syntax Error: {e}\")\r\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                self.results[\"syntax\"][test_key] = {\r\n                    \"file\": str(path_obj),\r\n                    \"exists\": True,\r\n                    \"valid_syntax\": False,\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\r\n            except Exception as e:\r\n                print(f\"    ❌ Error: {e}\")\r\n                test_key = f\"syntax_{file_path.replace('/', '_').replace('.', '_')}\"\r\n                self.results[\"syntax\"][test_key] = {\r\n                    \"file\": str(path_obj),\r\n                    \"exists\": True,\r\n                    \"valid_syntax\": False,\r\n                    \"success\": False,\r\n                    \"error\": str(e)\r\n                }\r\n                syntax_errors.append(f\"{file_path}: {str(e)}\")\r\n\r\n        print(f\"\\n  Summary:\")\r\n        print(f\"    ✅ Valid Syntax: {len(valid_files)}\")\r\n        print(f\"    ❌ Syntax Errors: {len(syntax_errors)}\")\r\n        \r\n        if syntax_errors:\r\n            print(f\"\\n  Files with Syntax Errors:\")\r\n            for error in syntax_errors:\r\n                print(f\"    - {error}\")\r\n        \r\n        syntax_success = len(syntax_errors) == 0\r\n        return {\"success\": syntax_success, \"errors\": syntax_errors, \"valid\": valid_files}\r\n\r\n    def test_system_health(self) -> Dict[str, bool]:\r\n        \"\"\"Test overall system health\"\"\"\r\n        print(\"\\n🔍 Testing System Health...\")\r\n        print(\"-\" * 50)\r\n\r\n        health_checks = []\r\n\r\n        # Test health endpoint\r\n        try:\r\n            response = requests.get(urljoin(self.base_url, \"/health\"), timeout=10)\r\n            health_ok = response.status_code == 200\r\n            health_checks.append((\"Health Endpoint\", health_ok, response.status_code))\r\n            print(f\"  Health Endpoint: {'✅' if health_ok else '❌'} Status {response.status_code}\")\r\n        except Exception as e:\r\n            health_checks.append((\"Health Endpoint\", False, str(e)))\r\n            print(f\"  Health Endpoint: ❌ Error {e}\")\r\n\r\n        # Test if server is responding\r\n        try:\r\n            response = requests.get(self.base_url, timeout=10)\r\n            server_ok = response.status_code in [200, 404]  # 200 = UI served, 404 = no root handler\r\n            health_checks.append((\"Server Response\", server_ok, response.status_code))\r\n            print(f\"  Server Response: {'✅' if server_ok else '❌'} Status {response.status_code}\")\r\n        except Exception as e:\r\n            health_checks.append((\"Server Response\", False, str(e)))\r\n            print(f\"  Server Response: ❌ Error {e}\")\r\n\r\n        # Test authentication\r\n        try:\r\n            response = requests.get(urljoin(self.base_url, \"/v1/gpu/status\"), timeout=10)\r\n            auth_ok = response.status_code in [200, 401, 403]  # 401/403 = auth required, 200 = success\r\n            auth_msg = \"Auth OK\" if auth_ok else \"Unexpected status\"\r\n            health_checks.append((\"Authentication\", auth_ok, auth_msg))\r\n            print(f\"  Authentication: {'✅' if auth_ok else '❌'} {auth_msg}\")\r\n        except Exception as e:\r\n            health_checks.append((\"Authentication\", False, str(e)))\r\n            print(f\"  Authentication: ❌ Error {e}\")\r\n\r\n        all_healthy = all(check[1] for check in health_checks)\r\n        return {\"success\": all_healthy, \"checks\": health_checks}\r\n\r\n    def run_all_tests(self) -> bool:\r\n        \"\"\"Run all verification tests\"\"\"\r\n        print(\"🚀 Running Endpoint and Syntax Verification Tests...\")\r\n        print(f\"Testing against: {self.base_url}\")\r\n        print(\"=\" * 80)\r\n\r\n        # Run all test categories\r\n        endpoint_results = self.test_all_endpoints()\r\n        syntax_results = self.test_syntax_in_files()\r\n        health_results = self.test_system_health()\r\n\r\n        # Summary\r\n        print(\"\\n\" + \"=\" * 80)\r\n        print(\"📊 VERIFICATION TEST RESULTS\")\r\n        print(\"=\" * 80)\r\n        \r\n        print(f\"Endpoint Accessibility: {'✅ PASS' if endpoint_results['success'] else '❌ FAIL'}\")\r\n        print(f\"Syntax Verification: {'✅ PASS' if syntax_results['success'] else '❌ FAIL'}\")\r\n        print(f\"System Health: {'✅ PASS' if health_results['success'] else '❌ FAIL'}\")\r\n\r\n        overall_success = all([\r\n            endpoint_results['success'],\r\n            syntax_results['success'],\r\n            health_results['success']\r\n        ])\r\n\r\n        print(f\"\\n🎯 Overall Result: {'✅ ALL VERIFICATIONS PASSED' if overall_success else '❌ SOME VERIFICATIONS FAILED'}\")\r\n\r\n        if not endpoint_results['success']:\r\n            print(f\"\\n  Missing Endpoints: {len(endpoint_results['missing'])}\")\r\n        \r\n        if not syntax_results['success']:\r\n            print(f\"  Syntax Errors: {len(syntax_results['errors'])}\")\r\n\r\n        return overall_success\r\n\r\n    def generate_report(self) -> Dict:\r\n        \"\"\"Generate a detailed verification report\"\"\"\r\n        report = {\r\n            \"timestamp\": __import__('datetime').datetime.now().isoformat(),\r\n            \"base_url\": self.base_url,\r\n            \"results\": self.results,\r\n            \"summary\": {\r\n                \"endpoints\": {\r\n                    \"total\": len(self.results[\"endpoints\"]),\r\n                    \"accessible\": len([r for r in self.results[\"endpoints\"].values() if r[\"success\"]]),\r\n                    \"missing\": len([r for r in self.results[\"endpoints\"].values() if not r[\"success\"]])\r\n                },\r\n                \"syntax\": {\r\n                    \"total\": len(self.results[\"syntax\"]),\r\n                    \"valid\": len([r for r in self.results[\"syntax\"].values() if r[\"success\"]]),\r\n                    \"errors\": len([r for r in self.results[\"syntax\"].values() if not r[\"success\"]])\r\n                }\r\n            }\r\n        }\r\n        return report\r\n\r\n\r\ndef main():\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser(description=\"Verify Endpoints and Syntax for Anchor Core\")\r\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\",\r\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\r\n    parser.add_argument(\"--token\", default=\"sovereign-secret\",\r\n                       help=\"Authentication token (default: sovereign-secret)\")\r\n    parser.add_argument(\"--output\",\r\n                       help=\"Output file for verification report (JSON format)\")\r\n\r\n    args = parser.parse_args()\r\n\r\n    # Run the verification tests\r\n    tester = EndpointAndSyntaxTester(base_url=args.url, token=args.token)\r\n    success = tester.run_all_tests()\r\n\r\n    # Generate and save report if requested\r\n    if args.output:\r\n        report = tester.generate_report()\r\n        with open(args.output, 'w', encoding='utf-8') as f:\r\n            json.dump(report, f, indent=2, ensure_ascii=False)\r\n        print(f\"\\n📄 Verification report saved to: {args.output}\")\r\n\r\n    sys.exit(0 if success else 1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
    size: 16237
    tokens: 5536
  - path: tests\model_test.html
    content: "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Anchor Core Model Test Suite</title>\r\n    <style>\r\n        body {\r\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\r\n            max-width: 1200px;\r\n            margin: 0 auto;\r\n            padding: 20px;\r\n            background-color: #f5f5f5;\r\n        }\r\n        .container {\r\n            background: white;\r\n            border-radius: 8px;\r\n            padding: 20px;\r\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\r\n        }\r\n        h1 {\r\n            color: #2c3e50;\r\n            border-bottom: 2px solid #3498db;\r\n            padding-bottom: 10px;\r\n        }\r\n        .test-section {\r\n            margin: 20px 0;\r\n            padding: 15px;\r\n            border: 1px solid #ddd;\r\n            border-radius: 5px;\r\n            background-color: #fafafa;\r\n        }\r\n        .test-result {\r\n            padding: 5px 10px;\r\n            margin: 5px 0;\r\n            border-radius: 3px;\r\n            font-family: monospace;\r\n        }\r\n        .success {\r\n            background-color: #d4edda;\r\n            color: #155724;\r\n            border: 1px solid #c3e6cb;\r\n        }\r\n        .error {\r\n            background-color: #f8d7da;\r\n            color: #721c24;\r\n            border: 1px solid #f5c6cb;\r\n        }\r\n        .info {\r\n            background-color: #d1ecf1;\r\n            color: #0c5460;\r\n            border: 1px solid #bee5eb;\r\n        }\r\n        button {\r\n            background-color: #3498db;\r\n            color: white;\r\n            border: none;\r\n            padding: 10px 15px;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            margin: 5px;\r\n        }\r\n        button:hover {\r\n            background-color: #2980b9;\r\n        }\r\n        button:disabled {\r\n            background-color: #bdc3c7;\r\n            cursor: not-allowed;\r\n        }\r\n        .model-list {\r\n            display: flex;\r\n            flex-wrap: wrap;\r\n            gap: 10px;\r\n            margin: 10px 0;\r\n        }\r\n        .model-item {\r\n            background: #e3f2fd;\r\n            padding: 8px 12px;\r\n            border-radius: 4px;\r\n            cursor: pointer;\r\n            border: 1px solid #bbdefb;\r\n        }\r\n        .model-item:hover {\r\n            background: #bbdefb;\r\n        }\r\n    </style>\r\n</head>\r\n<body>\r\n    <div class=\"container\">\r\n        <h1>🧪 Anchor Core Model Test Suite</h1>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>🌐 System Information</h2>\r\n            <div id=\"system-info\" class=\"test-result info\">Loading system information...</div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>🔍 API Endpoint Tests</h2>\r\n            <button id=\"test-health\">Test Health Endpoint</button>\r\n            <button id=\"test-gpu-status\">Test GPU Status</button>\r\n            <button id=\"test-spawn\">Test Spawn Endpoint</button>\r\n            <div id=\"api-results\"></div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>📦 Model Path Tests</h2>\r\n            <p>Click on a model to test its accessibility:</p>\r\n            <div class=\"model-list\">\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\">Qwen2.5-Coder-1.5B</div>\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\">Qwen2.5-Coder-7B</div>\r\n                <div class=\"model-item\" data-model=\"Qwen2.5-7B-Instruct-q4f16_1-MLC\">Qwen2.5-7B</div>\r\n                <div class=\"model-item\" data-model=\"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\">DeepSeek-R1-Qwen-7B</div>\r\n                <div class=\"model-item\" data-model=\"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\">DeepSeek-R1-Llama-8B</div>\r\n            </div>\r\n            <div id=\"model-results\"></div>\r\n        </div>\r\n        \r\n        <div class=\"test-section\">\r\n            <h2>📈 Test Summary</h2>\r\n            <div id=\"summary\">Run tests to see summary...</div>\r\n        </div>\r\n    </div>\r\n\r\n    <script>\r\n        // Update system information\r\n        document.getElementById('system-info').textContent = \r\n            `Current Origin: ${window.location.origin} | Protocol: ${window.location.protocol} | Host: ${window.location.host}`;\r\n\r\n        // API Test Functions\r\n        async function testHealth() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing health endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/health');\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `✅ Health: ${JSON.stringify(data)}` \r\n                    : `❌ Health: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">❌ Health: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        async function testGpuStatus() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing GPU status endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/v1/gpu/status', {\r\n                    headers: { 'Authorization': 'Bearer sovereign-secret' }\r\n                });\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `✅ GPU Status: ${JSON.stringify(data)}` \r\n                    : `❌ GPU Status: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">❌ GPU Status: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        async function testSpawn() {\r\n            const resultDiv = document.getElementById('api-results');\r\n            resultDiv.innerHTML = '<div class=\"test-result info\">Testing spawn endpoint...</div>';\r\n            \r\n            try {\r\n                const response = await fetch('/v1/system/spawn_shell', {\r\n                    method: 'POST',\r\n                    headers: { \r\n                        'Content-Type': 'application/json',\r\n                        'Authorization': 'Bearer sovereign-secret'\r\n                    },\r\n                    body: JSON.stringify({})\r\n                });\r\n                const data = await response.json();\r\n                \r\n                const status = response.status === 200 ? 'success' : 'error';\r\n                const message = response.status === 200 \r\n                    ? `✅ Spawn: ${JSON.stringify(data)}` \r\n                    : `❌ Spawn: ${response.status} - ${response.statusText}`;\r\n                \r\n                resultDiv.innerHTML = `<div class=\"test-result ${status}\">${message}</div>`;\r\n            } catch (error) {\r\n                resultDiv.innerHTML = `<div class=\"test-result error\">❌ Spawn: Network Error - ${error.message}</div>`;\r\n            }\r\n        }\r\n\r\n        // Model Test Functions\r\n        async function testModel(modelName) {\r\n            const resultDiv = document.getElementById('model-results');\r\n            resultDiv.innerHTML = `<div class=\"test-result info\">Testing model: ${modelName}...</div>`;\r\n\r\n            const modelPath = `/models/${modelName}`;\r\n            const configFiles = [\r\n                `${modelPath}/ndarray-cache.json`,\r\n                `${modelPath}/tokenizer.json`,\r\n                `${modelPath}/mlc-chat-config.json`,\r\n                `${modelPath}/params.json`,\r\n                `${modelPath}/tokenizer_config.json`\r\n            ];\r\n\r\n            let results = [];\r\n            let allAvailable = true;\r\n\r\n            for (const configFile of configFiles) {\r\n                try {\r\n                    const response = await fetch(configFile, { method: 'HEAD' });\r\n                    const status = response.status;\r\n                    const statusText = response.statusText;\r\n\r\n                    const fileResult = {\r\n                        file: configFile,\r\n                        status: status,\r\n                        accessible: status === 200,\r\n                        statusText: statusText\r\n                    };\r\n\r\n                    if (status !== 200) {\r\n                        allAvailable = false;\r\n                    }\r\n\r\n                    results.push(fileResult);\r\n                } catch (error) {\r\n                    results.push({\r\n                        file: configFile,\r\n                        status: 'ERROR',\r\n                        accessible: false,\r\n                        statusText: error.message\r\n                    });\r\n                    allAvailable = false;\r\n                }\r\n            }\r\n\r\n            // Display results\r\n            let html = `<h4>Results for ${modelName}:</h4>`;\r\n            for (const result of results) {\r\n                const statusClass = result.status === 200 ? 'success' :\r\n                                  result.status === 'ERROR' ? 'error' : 'info';\r\n\r\n                const statusIcon = result.status === 200 ? '✅' :\r\n                                 result.status === 'ERROR' ? '❌' : 'ℹ️';\r\n\r\n                html += `<div class=\"test-result ${statusClass}\">${statusIcon} ${result.file} - ${result.status} ${result.statusText}</div>`;\r\n            }\r\n\r\n            // Add availability summary\r\n            const availabilityClass = allAvailable ? 'success' : 'error';\r\n            const availabilityIcon = allAvailable ? '🎉' : '⚠️';\r\n            const availabilityText = allAvailable ?\r\n                'Model is fully available for loading!' :\r\n                'Model is NOT available - download required before loading';\r\n\r\n            html += `<div class=\"test-result ${availabilityClass}\"><strong>${availabilityIcon} AVAILABILITY: ${availabilityText}</strong></div>`;\r\n\r\n            // Add download suggestion if not available\r\n            if (!allAvailable) {\r\n                html += `<div class=\"test-result info\">💡 To download: POST to /v1/models/pull with model_id: \"mlc-ai/${modelName}\"</div>`;\r\n            }\r\n\r\n            resultDiv.innerHTML = html;\r\n        }\r\n\r\n        // Event Listeners\r\n        document.getElementById('test-health').addEventListener('click', testHealth);\r\n        document.getElementById('test-gpu-status').addEventListener('click', testGpuStatus);\r\n        document.getElementById('test-spawn').addEventListener('click', testSpawn);\r\n        \r\n        // Add event listeners to model items\r\n        document.querySelectorAll('.model-item').forEach(item => {\r\n            item.addEventListener('click', () => {\r\n                const modelName = item.getAttribute('data-model');\r\n                testModel(modelName);\r\n            });\r\n        });\r\n\r\n        // Initial test of health endpoint\r\n        window.addEventListener('load', () => {\r\n            testHealth();\r\n        });\r\n    </script>\r\n</body>\r\n</html>"
    size: 11548
    tokens: 3869
  - path: tests\README.md
    content: "# Test Suite for Anchor Core\r\n\r\nThis directory contains all test files for the Anchor Core system, organized to verify functionality across different components.\r\n\r\n## Test Files\r\n\r\n### Python Tests\r\n- `comprehensive_test_suite.py` - Main test suite covering model loading, data pipeline, endpoint verification, and syntax checking\r\n- `endpoint_syntax_verification.py` - Specific tests for endpoint accessibility and syntax verification\r\n- `test_new_endpoints.py` - Tests for new sidecar, context, and vision endpoints\r\n- `test_model_loading.py` - Tests for model loading functionality and endpoint accessibility\r\n- `test_model_availability.py` - Tests for model availability and download capability\r\n- `test_gpu_fixes.py` - Tests for GPU resource management and lock functionality\r\n- `test_orchestrator.py` - Unit tests for the orchestrator component\r\n\r\n### HTML Tests\r\n- `model_test.html` - Interactive web-based test suite for model and endpoint verification\r\n\r\n## Running Tests\r\n\r\n### Comprehensive Test Suite\r\n```bash\r\npython tests/comprehensive_test_suite.py\r\n```\r\n\r\nWith custom parameters:\r\n```bash\r\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output test_report.json\r\n```\r\n\r\n### Individual Test Files\r\n```bash\r\npython tests/test_model_loading.py\r\npython tests/test_model_availability.py\r\npython tests/test_gpu_fixes.py\r\n```\r\n\r\n### Interactive Web Tests\r\nStart the Anchor Core server and navigate to:\r\n```\r\nhttp://localhost:8000/tests/model_test.html\r\n```\r\n\r\n## Test Coverage\r\n\r\nThe test suite covers:\r\n\r\n1. **Model Loading**: Verifies model availability and accessibility\r\n2. **Data Pipeline**: Tests API endpoints and data flow\r\n3. **Endpoint Verification**: Checks for missing or inaccessible endpoints\r\n4. **Syntax Verification**: Validates Python syntax in critical files\r\n5. **GPU Management**: Tests GPU lock, unlock, and resource management\r\n6. **System Integration**: Verifies end-to-end functionality\r\n\r\n## Test Categories\r\n\r\n### Model Tests\r\n- Model file accessibility\r\n- Configuration file verification\r\n- Download capability testing\r\n\r\n### API Tests\r\n- Health endpoint\r\n- GPU management endpoints\r\n- Shell execution endpoints\r\n- Model pull endpoints\r\n\r\n### System Tests\r\n- Bridge functionality\r\n- WebSocket connections\r\n- Authentication"
    size: 2308
    tokens: 843
  - path: tests\test_gpu_fixes.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nTest script to verify GPU resource management fixes including model loading serialization\r\n\"\"\"\r\n\r\nimport time\r\nimport requests\r\nimport threading\r\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\r\nimport json\r\n\r\ndef test_gpu_status():\r\n    \"\"\"Test GPU status endpoint\"\"\"\r\n    try:\r\n        response = requests.get(\"http://localhost:8000/v1/gpu/status\",\r\n                              headers={\"Authorization\": \"Bearer sovereign-secret\"})\r\n        if response.status_code == 200:\r\n            status = response.json()\r\n            print(f\"✅ GPU Status: {status}\")\r\n            return True\r\n        else:\r\n            print(f\"❌ GPU Status request failed: {response.status_code}\")\r\n            return False\r\n    except Exception as e:\r\n        print(f\"❌ Error getting GPU status: {e}\")\r\n        return False\r\n\r\ndef test_lock_acquisition(agent_id: str, timeout: int = 120):  # Increased to 120s\r\n    \"\"\"Test GPU lock acquisition\"\"\"\r\n    try:\r\n        print(f\"⏳ Agent {agent_id} requesting GPU lock...\")\r\n        start_time = time.time()\r\n\r\n        response = requests.post(\"http://localhost:8000/v1/gpu/lock\",\r\n                                headers={\"Authorization\": \"Bearer sovereign-secret\"},\r\n                                json={\"id\": agent_id},\r\n                                timeout=timeout)\r\n        \r\n        elapsed = time.time() - start_time\r\n        \r\n        if response.status_code == 200:\r\n            result = response.json()\r\n            print(f\"✅ Agent {agent_id} acquired lock in {elapsed:.2f}s: {result.get('token', 'no-token')}\")\r\n            \r\n            # Release the lock\r\n            release_response = requests.post(\"http://localhost:8000/v1/gpu/unlock\",\r\n                                          headers={\"Authorization\": \"Bearer sovereign-secret\"},\r\n                                          json={\"id\": agent_id})\r\n            if release_response.status_code == 200:\r\n                print(f\"✅ Agent {agent_id} released lock\")\r\n            else:\r\n                print(f\"⚠️  Agent {agent_id} failed to release lock: {release_response.status_code}\")\r\n            \r\n            return True\r\n        else:\r\n            print(f\"❌ Agent {agent_id} failed to acquire lock: {response.status_code} - {response.text}\")\r\n            return False\r\n    except Exception as e:\r\n        print(f\"❌ Agent {agent_id} error: {e}\")\r\n        return False\r\n\r\ndef test_concurrent_access():\r\n    \"\"\"Test concurrent GPU access with different priority agents\"\"\"\r\n    print(\"\\n🧪 Testing concurrent GPU access...\")\r\n    \r\n    agents = [\r\n        (\"Root-Mic\", 5),  # High priority\r\n        (\"Root-Console-Init\", 10),  # Medium priority\r\n        (\"Dreamer-Init\", 15),  # Lower priority\r\n        (\"Test-Agent-4\", 20),  # Even lower priority\r\n    ]\r\n    \r\n    with ThreadPoolExecutor(max_workers=4) as executor:\r\n        futures = []\r\n        for agent_id, delay in agents:\r\n            # Add small delay to ensure proper ordering\r\n            future = executor.submit(test_lock_acquisition, agent_id)\r\n            futures.append(future)\r\n            time.sleep(0.5)  # Stagger the requests\r\n        \r\n        # Wait for all to complete\r\n        for future in as_completed(futures):\r\n            future.result()\r\n\r\ndef test_force_release():\r\n    \"\"\"Test force release functionality\"\"\"\r\n    print(\"\\n🔧 Testing force release functionality...\")\r\n    \r\n    # First, acquire a lock manually\r\n    response = requests.post(\"http://localhost:8000/v1/gpu/lock\",\r\n                            headers={\"Authorization\": \"Bearer sovereign-secret\"},\r\n                            json={\"id\": \"test-force-release\"})\r\n    \r\n    if response.status_code == 200:\r\n        print(\"✅ Acquired test lock\")\r\n        \r\n        # Now force release all locks\r\n        force_response = requests.post(\"http://localhost:8000/v1/gpu/force-release-all\",\r\n                                     headers={\"Authorization\": \"Bearer sovereign-secret\"})\r\n        \r\n        if force_response.status_code == 200:\r\n            print(\"✅ Force release executed successfully\")\r\n        else:\r\n            print(f\"❌ Force release failed: {force_response.status_code}\")\r\n    else:\r\n        print(f\"❌ Failed to acquire test lock: {response.status_code}\")\r\n\r\ndef run_comprehensive_test():\r\n    \"\"\"Run comprehensive tests\"\"\"\r\n    print(\"🚀 Running comprehensive GPU resource management tests...\\n\")\r\n    \r\n    # Test 1: Basic status check\r\n    print(\"1️⃣ Testing GPU status endpoint...\")\r\n    status_ok = test_gpu_status()\r\n    \r\n    # Test 2: Force release\r\n    print(\"\\n2️⃣ Testing force release functionality...\")\r\n    test_force_release()\r\n    \r\n    # Test 3: Concurrent access\r\n    print(\"\\n3️⃣ Testing concurrent access patterns...\")\r\n    test_concurrent_access()\r\n    \r\n    # Test 4: Status after tests\r\n    print(\"\\n4️⃣ Checking final GPU status...\")\r\n    final_status_ok = test_gpu_status()\r\n    \r\n    print(\"\\n✅ Comprehensive testing completed!\")\r\n    return status_ok and final_status_ok\r\n\r\nif __name__ == \"__main__\":\r\n    run_comprehensive_test()"
    size: 5051
    tokens: 1743
  - path: tests\test_model_availability.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nModel Availability Test Suite for Anchor Core\r\n\r\nThis script tests model availability by checking if required model files exist\r\nlocally before attempting to load them into the MLC-LLM engine.\r\n\"\"\"\r\n\r\nimport requests\r\nimport sys\r\nimport time\r\nfrom urllib.parse import urljoin\r\nimport json\r\nfrom pathlib import Path\r\n\r\n\r\nclass ModelAvailabilityTester:\r\n    def __init__(self, base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\r\n        self.base_url = base_url\r\n        self.token = token\r\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n        self.results = {}\r\n\r\n    def test_model_availability(self, model_name):\r\n        \"\"\"Test if a model is available for loading by checking required files\"\"\"\r\n        print(f\"Testing model availability: {model_name}\")\r\n\r\n        # Define the required model files for MLC-LLM using the resolve/main pattern\r\n        # This matches how the MLC-LLM library actually accesses files\r\n        required_files = [\r\n            f\"models/{model_name}/resolve/main/ndarray-cache.json\",\r\n            f\"models/{model_name}/resolve/main/tokenizer.json\",\r\n            f\"models/{model_name}/resolve/main/mlc-chat-config.json\",\r\n            f\"models/{model_name}/resolve/main/tokenizer_config.json\"\r\n        ]\r\n\r\n        # Note: MLC-LLM models use sharded parameter files (params_shard_*.bin) instead of params.json\r\n        # So we don't check for params.json which doesn't exist for these models\r\n\r\n        model_result = {\r\n            \"model_name\": model_name,\r\n            \"available\": True,\r\n            \"files\": {},\r\n            \"download_required\": False\r\n        }\r\n\r\n        missing_files = []\r\n\r\n        for file_path in required_files:\r\n            try:\r\n                url = urljoin(self.base_url, file_path)\r\n                response = requests.head(url, timeout=10)  # Short timeout for availability check\r\n\r\n                file_status = {\r\n                    \"url\": url,\r\n                    \"status_code\": response.status_code,\r\n                    \"exists\": response.status_code == 200,\r\n                    \"checked_at\": time.time()\r\n                }\r\n\r\n                model_result[\"files\"][file_path] = file_status\r\n\r\n                if response.status_code == 200:\r\n                    print(f\"  OK {file_path}\")\r\n                elif response.status_code == 404:\r\n                    print(f\"  MISSING {file_path} - NOT FOUND\")\r\n                    missing_files.append(file_path)\r\n                    model_result[\"available\"] = False\r\n                    model_result[\"download_required\"] = True\r\n                else:\r\n                    print(f\"  WARNING {file_path} - Status {response.status_code}\")\r\n                    model_result[\"available\"] = False\r\n\r\n            except requests.exceptions.RequestException as e:\r\n                file_status = {\r\n                    \"url\": urljoin(self.base_url, file_path),\r\n                    \"status_code\": \"ERROR\",\r\n                    \"exists\": False,\r\n                    \"error\": str(e),\r\n                    \"checked_at\": time.time()\r\n                }\r\n                model_result[\"files\"][file_path] = file_status\r\n                print(f\"  ❌ {file_path} - ERROR: {e}\")\r\n                model_result[\"available\"] = False\r\n\r\n        if missing_files:\r\n            print(f\"  INFO Missing files: {len(missing_files)} required files not found\")\r\n        else:\r\n            print(f\"  SUCCESS Model {model_name} is fully available for loading!\")\r\n\r\n        self.results[model_name] = model_result\r\n        return model_result[\"available\"]\r\n\r\n    def test_model_download_capability(self, model_id):\r\n        \"\"\"Test if the model download endpoint works for a given model\"\"\"\r\n        print(f\"\\nTesting download capability for: {model_id}\")\r\n        \r\n        try:\r\n            url = urljoin(self.base_url, \"/v1/models/pull\")\r\n            payload = {\r\n                \"model_id\": model_id,\r\n                \"url\": f\"https://huggingface.co/{model_id}\"\r\n            }\r\n            \r\n            # Make a quick test request without waiting for full download\r\n            response = requests.post(url, json=payload, headers=self.headers, timeout=5)\r\n            \r\n            if response.status_code in [200, 409, 400]:  # 409=already exists, 400=bad request (but endpoint works)\r\n                print(f\"  OK Download endpoint accessible for {model_id}\")\r\n                return True\r\n            else:\r\n                print(f\"  FAILED Download endpoint failed for {model_id}: {response.status_code}\")\r\n                return False\r\n\r\n        except requests.exceptions.RequestException as e:\r\n            print(f\"  FAILED Download endpoint error for {model_id}: {e}\")\r\n            return False\r\n\r\n    def run_model_availability_tests(self, model_list):\r\n        \"\"\"Run availability tests for a list of models\"\"\"\r\n        print(f\"Running Model Availability Tests against: {self.base_url}\")\r\n        print(\"-\" * 70)\r\n        \r\n        available_models = []\r\n        unavailable_models = []\r\n        \r\n        for model_name in model_list:\r\n            print(f\"\\nINFO Testing: {model_name}\")\r\n            is_available = self.test_model_availability(model_name)\r\n\r\n            if is_available:\r\n                available_models.append(model_name)\r\n                print(f\"  RESULT: AVAILABLE for loading\")\r\n            else:\r\n                unavailable_models.append(model_name)\r\n                print(f\"  RESULT: NOT AVAILABLE (download required)\")\r\n                \r\n                # Test if download is possible\r\n                huggingface_id = f\"mlc-ai/{model_name}\"\r\n                self.test_model_download_capability(huggingface_id)\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"TEST SUMMARY:\")\r\n        print(f\"  Available Models: {len(available_models)}\")\r\n        for model in available_models:\r\n            print(f\"    OK {model}\")\r\n\r\n        print(f\"  Unavailable Models: {len(unavailable_models)}\")\r\n        for model in unavailable_models:\r\n            print(f\"    MISSING {model}\")\r\n\r\n        print(\"\\nRECOMMENDATION:\")\r\n        if unavailable_models:\r\n            print(\"  - Download required models using the /v1/models/pull endpoint\")\r\n            print(\"  - Or ensure models are pre-loaded in the models/ directory\")\r\n        else:\r\n            print(\"  - All models are ready for immediate loading!\")\r\n            \r\n        return available_models, unavailable_models\r\n\r\n    def generate_report(self):\r\n        \"\"\"Generate a detailed test report\"\"\"\r\n        report = {\r\n            \"base_url\": self.base_url,\r\n            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n            \"results\": self.results,\r\n            \"summary\": {\r\n                \"total_models\": len(self.results),\r\n                \"available\": len([r for r in self.results.values() if r[\"available\"]]),\r\n                \"unavailable\": len([r for r in self.results.values() if not r[\"available\"]])\r\n            }\r\n        }\r\n        return report\r\n\r\n\r\ndef main():\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser(description=\"Test Model Availability for Anchor Core\")\r\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\", \r\n                       help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\r\n    parser.add_argument(\"--token\", default=\"sovereign-secret\", \r\n                       help=\"Authentication token (default: sovereign-secret)\")\r\n    parser.add_argument(\"--models\", nargs=\"+\", \r\n                       help=\"Specific models to test (default: predefined list)\")\r\n    parser.add_argument(\"--output\", \r\n                       help=\"Output file for test report (JSON format)\")\r\n\r\n    args = parser.parse_args()\r\n\r\n    tester = ModelAvailabilityTester(base_url=args.url, token=args.token)\r\n    \r\n    # Default model list if none provided\r\n    default_models = [\r\n        \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n        \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\", \r\n        \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\r\n        \"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\r\n        \"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\",\r\n        \"Phi-3.5-mini-instruct-q4f16_1-MLC\",\r\n        \"Qwen2.5-1.5B-Instruct-q4f16_1-MLC\"\r\n    ]\r\n    \r\n    models_to_test = args.models if args.models else default_models\r\n    \r\n    available, unavailable = tester.run_model_availability_tests(models_to_test)\r\n    \r\n    if args.output:\r\n        report = tester.generate_report()\r\n        with open(args.output, 'w') as f:\r\n            json.dump(report, f, indent=2)\r\n        print(f\"\\nTest report saved to: {args.output}\")\r\n\r\n    # Exit with error code if no models are available\r\n    if not available:\r\n        print(\"\\nERROR: No models are currently available for loading!\")\r\n        sys.exit(1)\r\n    else:\r\n        print(f\"\\nSUCCESS: {len(available)} model(s) are ready for loading!\")\r\n        sys.exit(0)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
    size: 8961
    tokens: 3110
  - path: tests\test_model_loading.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nModel Loading Test Suite for Anchor Core\r\n\r\nThis script tests the model loading functionality and verifies that all endpoints\r\nare accessible and working correctly with the unified Anchor Core architecture.\r\n\"\"\"\r\n\r\nimport requests\r\nimport sys\r\nimport os\r\nfrom urllib.parse import urljoin\r\nimport json\r\nfrom pathlib import Path\r\n\r\n\r\nclass ModelLoadingTester:\r\n    def __init__(self, base_url=\"http://localhost:8000\", token=\"sovereign-secret\"):\r\n        self.base_url = base_url\r\n        self.token = token\r\n        self.headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n        self.results = {}\r\n        \r\n    def test_endpoint_accessibility(self, endpoint, method=\"GET\", expected_status=200):\r\n        \"\"\"Test if an endpoint is accessible\"\"\"\r\n        try:\r\n            url = urljoin(self.base_url, endpoint)\r\n            response = requests.request(method, url, headers=self.headers)\r\n            success = response.status_code == expected_status\r\n            self.results[f\"endpoint_{endpoint.replace('/', '_')}\"] = {\r\n                \"url\": url,\r\n                \"method\": method,\r\n                \"status\": response.status_code,\r\n                \"expected\": expected_status,\r\n                \"success\": success,\r\n                \"message\": f\"Status {response.status_code}\" if success else f\"Expected {expected_status}, got {response.status_code}\"\r\n            }\r\n            return success\r\n        except Exception as e:\r\n            self.results[f\"endpoint_{endpoint.replace('/', '_')}\"] = {\r\n                \"url\": urljoin(self.base_url, endpoint),\r\n                \"method\": method,\r\n                \"status\": \"ERROR\",\r\n                \"expected\": expected_status,\r\n                \"success\": False,\r\n                \"message\": str(e)\r\n            }\r\n            return False\r\n    \r\n    def test_model_path_accessibility(self, model_path):\r\n        \"\"\"Test if a model path is accessible\"\"\"\r\n        try:\r\n            url = urljoin(self.base_url, model_path)\r\n            response = requests.head(url, headers=self.headers)\r\n            success = response.status_code in [200, 404]  # 404 means path exists but file doesn't (which is expected for model directories)\r\n            self.results[f\"model_{model_path.replace('/', '_')}\"] = {\r\n                \"url\": url,\r\n                \"status\": response.status_code,\r\n                \"success\": success,\r\n                \"message\": f\"Model path accessible\" if success else f\"Model path not accessible: {response.status_code}\"\r\n            }\r\n            return success\r\n        except Exception as e:\r\n            self.results[f\"model_{model_path.replace('/', '_')}\"] = {\r\n                \"url\": urljoin(self.base_url, model_path),\r\n                \"status\": \"ERROR\",\r\n                \"success\": False,\r\n                \"message\": str(e)\r\n            }\r\n            return False\r\n    \r\n    def test_model_config_accessibility(self, model_name):\r\n        \"\"\"Test if model config files are accessible\"\"\"\r\n        config_files = [\r\n            f\"models/{model_name}/ndarray-cache.json\",\r\n            f\"models/{model_name}/tokenizer.json\",\r\n            f\"models/{model_name}/mlc-chat-config.json\",\r\n            f\"models/{model_name}/params.json\"\r\n        ]\r\n        \r\n        results = []\r\n        for config_file in config_files:\r\n            try:\r\n                url = urljoin(self.base_url, config_file)\r\n                response = requests.head(url)\r\n                success = response.status_code in [200, 404]  # Allow 404 as files may not exist yet\r\n                results.append({\r\n                    \"file\": config_file,\r\n                    \"url\": url,\r\n                    \"status\": response.status_code,\r\n                    \"success\": success\r\n                })\r\n            except Exception as e:\r\n                results.append({\r\n                    \"file\": config_file,\r\n                    \"url\": urljoin(self.base_url, config_file),\r\n                    \"status\": \"ERROR\",\r\n                    \"success\": False,\r\n                    \"message\": str(e)\r\n                })\r\n        \r\n        self.results[f\"model_configs_{model_name}\"] = results\r\n        return results\r\n    \r\n    def run_comprehensive_test(self):\r\n        \"\"\"Run comprehensive tests for all endpoints and model paths\"\"\"\r\n        print(\"Running Model Loading Test Suite...\")\r\n        print(f\"Testing against: {self.base_url}\")\r\n        print(\"-\" * 60)\r\n        \r\n        # Test core API endpoints\r\n        api_endpoints = [\r\n            (\"/health\", \"GET\", 200),\r\n            (\"/v1/gpu/status\", \"GET\", 200),\r\n            (\"/v1/gpu/lock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/gpu/unlock\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/shell/exec\", \"POST\", 400),  # Expected to fail with 400 due to missing body\r\n            (\"/v1/system/spawn_shell\", \"POST\", 200),\r\n        ]\r\n        \r\n        print(\"Testing API Endpoints:\")\r\n        for endpoint, method, expected in api_endpoints:\r\n            success = self.test_endpoint_accessibility(endpoint, method, expected)\r\n            status_icon = \"[PASS]\" if success else \"[FAIL]\"\r\n            key = f\"endpoint_{endpoint.replace('/', '_')}\"\r\n            print(f\"  {status_icon} {method} {endpoint} -> {self.results[key]['message']}\")\r\n        \r\n        print()\r\n        \r\n        # Test model paths\r\n        model_names = [\r\n            \"Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC\",\r\n            \"Qwen2.5-7B-Instruct-q4f16_1-MLC\",\r\n            \"DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC\",\r\n            \"DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC\"\r\n        ]\r\n        \r\n        print(\"Testing Model Paths:\")\r\n        for model_name in model_names:\r\n            success = self.test_model_path_accessibility(f\"/models/{model_name}\")\r\n            status_icon = \"[PASS]\" if success else \"[FAIL]\"\r\n            key = f\"model_/models/{model_name}\".replace(\"/\", \"_\")\r\n            if key in self.results:\r\n                print(f\"  {status_icon} /models/{model_name} -> {self.results[key]['message']}\")\r\n            else:\r\n                print(f\"  [FAIL] /models/{model_name} -> Key not found in results\")\r\n        \r\n        print()\r\n        \r\n        # Test model config files\r\n        print(\"Testing Model Config Files:\")\r\n        for model_name in model_names[:2]:  # Test just the first 2 to avoid too much output\r\n            configs = self.test_model_config_accessibility(model_name)\r\n            print(f\"  Model: {model_name}\")\r\n            for config in configs:\r\n                status_icon = \"[PASS]\" if config['success'] else \"[FAIL]\"\r\n                status_symbol = \"OK\" if config['success'] else \"X\"\r\n                print(f\"    [{status_symbol}] {config['file']} -> Status: {config['status']}\")\r\n        \r\n        print()\r\n        \r\n        # Summary\r\n        total_tests = len([r for r in self.results.values() if isinstance(r, dict) and 'success' in r]) + \\\r\n                     sum(len(r) for r in self.results.values() if isinstance(r, list))\r\n        passed_tests = 0\r\n        \r\n        for key, value in self.results.items():\r\n            if isinstance(value, dict) and 'success' in value:\r\n                if value['success']:\r\n                    passed_tests += 1\r\n            elif isinstance(value, list):\r\n                for item in value:\r\n                    if item.get('success'):\r\n                        passed_tests += 1\r\n        \r\n        print(\"-\" * 60)\r\n        print(f\"Test Summary: {passed_tests}/{total_tests} tests passed\")\r\n\r\n        if passed_tests == total_tests:\r\n            print(\"All tests passed! The Anchor Core is properly configured.\")\r\n            return True\r\n        else:\r\n            print(\"Some tests failed. Check the output above for details.\")\r\n            return False\r\n\r\n    def generate_test_report(self):\r\n        \"\"\"Generate a detailed test report\"\"\"\r\n        report = {\r\n            \"base_url\": self.base_url,\r\n            \"timestamp\": str(__import__('datetime').datetime.now()),\r\n            \"results\": self.results,\r\n            \"summary\": {\r\n                \"total_tests\": 0,\r\n                \"passed_tests\": 0,\r\n                \"failed_tests\": 0\r\n            }\r\n        }\r\n        \r\n        # Count tests\r\n        for key, value in self.results.items():\r\n            if isinstance(value, dict) and 'success' in value:\r\n                report[\"summary\"][\"total_tests\"] += 1\r\n                if value['success']:\r\n                    report[\"summary\"][\"passed_tests\"] += 1\r\n                else:\r\n                    report[\"summary\"][\"failed_tests\"] += 1\r\n            elif isinstance(value, list):\r\n                report[\"summary\"][\"total_tests\"] += len(value)\r\n                for item in value:\r\n                    if item.get('success'):\r\n                        report[\"summary\"][\"passed_tests\"] += 1\r\n                    else:\r\n                        report[\"summary\"][\"failed_tests\"] += 1\r\n        \r\n        return report\r\n\r\n\r\ndef main():\r\n    import argparse\r\n    \r\n    parser = argparse.ArgumentParser(description=\"Test Anchor Core Model Loading Functionality\")\r\n    parser.add_argument(\"--url\", default=\"http://localhost:8000\", help=\"Base URL of the Anchor Core (default: http://localhost:8000)\")\r\n    parser.add_argument(\"--token\", default=\"sovereign-secret\", help=\"Authentication token (default: sovereign-secret)\")\r\n    parser.add_argument(\"--output\", help=\"Output file for test report (JSON format)\")\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    tester = ModelLoadingTester(base_url=args.url, token=args.token)\r\n    success = tester.run_comprehensive_test()\r\n    \r\n    if args.output:\r\n        report = tester.generate_test_report()\r\n        with open(args.output, 'w') as f:\r\n            json.dump(report, f, indent=2)\r\n        print(f\"📄 Test report saved to: {args.output}\")\r\n    \r\n    sys.exit(0 if success else 1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
    size: 10021
    tokens: 3460
  - path: tests\test_new_endpoints.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nTest script to verify the new sidecar and vision endpoints are working\r\n\"\"\"\r\n\r\nimport requests\r\nimport time\r\n\r\ndef test_endpoints():\r\n    base_url = \"http://localhost:8000\"\r\n    \r\n    print(\"Testing new Anchor Core endpoints...\")\r\n    print(f\"Base URL: {base_url}\")\r\n    print(\"-\" * 50)\r\n    \r\n    # Test sidecar endpoint\r\n    try:\r\n        response = requests.get(f\"{base_url}/sidecar\", timeout=10)\r\n        print(f\"GET /sidecar -> Status: {response.status_code} {'✅' if response.status_code == 200 else '❌'}\")\r\n    except Exception as e:\r\n        print(f\"GET /sidecar -> Error: {e} ❌\")\r\n    \r\n    # Test context endpoint\r\n    try:\r\n        response = requests.get(f\"{base_url}/context\", timeout=10)\r\n        print(f\"GET /context -> Status: {response.status_code} {'✅' if response.status_code == 200 else '❌'}\")\r\n    except Exception as e:\r\n        print(f\"GET /context -> Error: {e} ❌\")\r\n    \r\n    # Test memory search endpoint (with a sample query)\r\n    try:\r\n        response = requests.post(\r\n            f\"{base_url}/v1/memory/search\",\r\n            json={\"query\": \"test\"},\r\n            timeout=10\r\n        )\r\n        print(f\"POST /v1/memory/search -> Status: {response.status_code} {'✅' if response.status_code == 200 else '❌'}\")\r\n        if response.status_code == 200:\r\n            data = response.json()\r\n            print(f\"  Response keys: {list(data.keys())}\")\r\n    except Exception as e:\r\n        print(f\"POST /v1/memory/search -> Error: {e} ❌\")\r\n    \r\n    # Test vision ingest endpoint (this will fail without a proper image, but should return 400 not 404)\r\n    try:\r\n        response = requests.post(f\"{base_url}/v1/vision/ingest\", timeout=10)\r\n        # Should return 400 (bad request) not 404 (not found)\r\n        is_ok = response.status_code in [400, 405]  # 400 = bad request (no image), 405 = method not allowed\r\n        print(f\"POST /v1/vision/ingest -> Status: {response.status_code} {'✅' if is_ok else '❌'} (should be 400 or 405 for valid endpoint)\")\r\n    except Exception as e:\r\n        print(f\"POST /v1/vision/ingest -> Error: {e} ❌\")\r\n    \r\n    print(\"-\" * 50)\r\n    print(\"Endpoint testing complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_endpoints()"
    size: 2208
    tokens: 815
  - path: tests\test_orchestrator.py
    content: "import unittest\r\nfrom unittest.mock import patch, MagicMock\r\nfrom orchestrator import Orchestrator, MLCConnectionError\r\n\r\nclass TestOrchestrator(unittest.TestCase):\r\n\r\n    def setUp(self):\r\n        self.orc = Orchestrator()\r\n\r\n    @patch('requests.get')\r\n    def test_load_mlc_model_success(self, mock_get):\r\n        # Mock successful bridge connection\r\n        mock_response = MagicMock()\r\n        mock_response.json.return_value = {\"data\": [{\"id\": \"webgpu-chat\"}]}\r\n        mock_response.status_code = 200\r\n        mock_get.return_value = mock_response\r\n\r\n        result = self.orc.load_mlc_model(\"my-model\")\r\n        self.assertTrue(result)\r\n        self.assertEqual(self.orc.active_model, \"my-model\")\r\n\r\n    @patch('requests.get')\r\n    def test_load_mlc_model_failure(self, mock_get):\r\n        # Mock connection error\r\n        mock_get.side_effect = Exception(\"Connection refused\")\r\n        \r\n        with self.assertRaises(MLCConnectionError):\r\n            self.orc.load_mlc_model(\"my-model\")\r\n\r\n    @patch('requests.post')\r\n    def test_invoke_mlc_inference_success(self, mock_post):\r\n        self.orc.active_model = \"test-model\"\r\n        \r\n        # Mock successful inference\r\n        mock_response = MagicMock()\r\n        mock_response.json.return_value = {\r\n            \"choices\": [{\"message\": {\"content\": \"Hello from MLC\"}}]\r\n        }\r\n        mock_response.status_code = 200\r\n        mock_post.return_value = mock_response\r\n\r\n        output = self.orc.invoke_mlc_inference(\"Hi\")\r\n        self.assertEqual(output, \"Hello from MLC\")\r\n\r\n    def test_invoke_without_load(self):\r\n        with self.assertRaises(ValueError):\r\n            self.orc.invoke_mlc_inference(\"Hi\")\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n"
    size: 1731
    tokens: 580
  - path: tools\anchor-mic.html
    content: "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <title>Root Mic 🎙️</title>\r\n    <style>\r\n        :root {\r\n            --bg-color: #0f0f11;\r\n            --surface-color: #1a1a1d;\r\n            --primary-color: #00ff88;\r\n            --accent-color: #00ccff;\r\n            --text-color: #eeeeee;\r\n            --danger-color: #ff4444;\r\n        }\r\n\r\n        body {\r\n            background-color: var(--bg-color);\r\n            color: var(--text-color);\r\n            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\r\n            margin: 0;\r\n            display: flex;\r\n            flex-direction: column;\r\n            align-items: center;\r\n            justify-content: center;\r\n            min-height: 100vh;\r\n            padding: 8px; /* Add padding to prevent cutoff */\r\n            overflow: auto; /* Allow scrolling if needed */\r\n        }\r\n\r\n        .container {\r\n            text-align: center;\r\n            width: 100%;\r\n            max-width: 500px;\r\n            padding: 20px;\r\n        }\r\n\r\n        h1 {\r\n            font-weight: 300;\r\n            letter-spacing: 2px;\r\n            margin-bottom: 30px;\r\n            text-transform: uppercase;\r\n            font-size: 1.5rem;\r\n            color: var(--accent-color);\r\n            text-shadow: 0 0 10px rgba(0, 204, 255, 0.3);\r\n        }\r\n\r\n        #mic-btn {\r\n            width: 150px;\r\n            height: 150px;\r\n            border-radius: 50%;\r\n            background: radial-gradient(circle at 30% 30%, #444, #222);\r\n            border: 4px solid #333;\r\n            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.5), 0 0 0 4px var(--bg-color);\r\n            cursor: pointer;\r\n            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);\r\n            display: flex;\r\n            align-items: center;\r\n            justify-content: center;\r\n            margin: 0 auto 30px;\r\n            position: relative;\r\n            outline: none;\r\n            -webkit-tap-highlight-color: transparent;\r\n        }\r\n\r\n        #mic-btn::after {\r\n            content: '';\r\n            position: absolute;\r\n            top: -10px;\r\n            left: -10px;\r\n            right: -10px;\r\n            bottom: -10px;\r\n            border-radius: 50%;\r\n            border: 2px solid var(--primary-color);\r\n            opacity: 0;\r\n            transform: scale(0.8);\r\n            transition: all 0.3s;\r\n        }\r\n\r\n        #mic-btn:hover {\r\n            transform: scale(1.05);\r\n            background: radial-gradient(circle at 30% 30%, #555, #333);\r\n        }\r\n\r\n        #mic-btn.active {\r\n            background: radial-gradient(circle at 30% 30%, #ff5555, #aa0000);\r\n            box-shadow: 0 0 30px rgba(255, 68, 68, 0.6);\r\n            border-color: #ff4444;\r\n            transform: scale(0.95);\r\n        }\r\n\r\n        #mic-btn.active::after {\r\n            animation: pulse 1.5s infinite;\r\n            opacity: 1;\r\n        }\r\n\r\n        #mic-icon {\r\n            font-size: 64px;\r\n            color: #888;\r\n            transition: color 0.3s;\r\n        }\r\n\r\n        #mic-btn.active #mic-icon {\r\n            color: white;\r\n        }\r\n\r\n        #clarify-btn {\r\n            background: transparent;\r\n            color: var(--accent-color);\r\n            border: 1px solid var(--accent-color);\r\n            padding: 8px 16px;\r\n            border-radius: 20px;\r\n            cursor: pointer;\r\n            font-size: 0.9rem;\r\n            margin-bottom: 20px;\r\n            transition: all 0.3s;\r\n            text-transform: uppercase;\r\n            letter-spacing: 1px;\r\n        }\r\n\r\n        #clarify-btn:hover:not(:disabled) {\r\n            background: rgba(0, 204, 255, 0.1);\r\n            transform: translateY(-2px);\r\n        }\r\n\r\n        #clarify-btn:disabled {\r\n            opacity: 0.3;\r\n            cursor: not-allowed;\r\n            border-color: #555;\r\n            color: #555;\r\n        }\r\n\r\n        #status {\r\n            font-size: 1.2rem;\r\n            margin-bottom: 20px;\r\n            height: 1.5em;\r\n            color: #888;\r\n        }\r\n\r\n        .visualizer {\r\n            width: 100%;\r\n            height: 60px;\r\n            background: var(--surface-color);\r\n            border-radius: 12px;\r\n            margin-bottom: 20px;\r\n            position: relative;\r\n            overflow: hidden;\r\n            border: 1px solid #333;\r\n        }\r\n\r\n        .bar-container {\r\n            display: flex;\r\n            align-items: center;\r\n            justify-content: center;\r\n            height: 100%;\r\n            gap: 2px;\r\n        }\r\n\r\n        .bar {\r\n            width: 4px;\r\n            background: var(--primary-color);\r\n            border-radius: 2px;\r\n            height: 4px;\r\n            transition: height 0.1s ease;\r\n        }\r\n\r\n        #output {\r\n            background: var(--surface-color);\r\n            padding: 20px;\r\n            border-radius: 12px;\r\n            border: 1px solid #333;\r\n            min-height: 100px;\r\n            text-align: left;\r\n            font-family: 'Courier New', monospace;\r\n            font-size: 0.9rem;\r\n            color: #ccc;\r\n            position: relative;\r\n            overflow-y: auto;\r\n            max-height: 200px;\r\n        }\r\n\r\n        #copy-toast {\r\n            position: absolute;\r\n            top: 20px;\r\n            right: 20px;\r\n            background: var(--primary-color);\r\n            color: #000;\r\n            padding: 8px 16px;\r\n            border-radius: 20px;\r\n            font-weight: bold;\r\n            opacity: 0;\r\n            transform: translateY(-20px);\r\n            transition: all 0.3s;\r\n            pointer-events: none;\r\n        }\r\n\r\n        #copy-toast.show {\r\n            opacity: 1;\r\n            transform: translateY(0);\r\n        }\r\n\r\n        footer {\r\n            margin-top: 40px;\r\n            font-size: 0.7rem;\r\n            color: #444;\r\n        }\r\n\r\n        @keyframes pulse {\r\n            0% {\r\n                transform: scale(1);\r\n                opacity: 1;\r\n                border-color: var(--danger-color);\r\n            }\r\n\r\n            100% {\r\n                transform: scale(1.5);\r\n                opacity: 0;\r\n                border-color: var(--danger-color);\r\n            }\r\n        }\r\n\r\n        #loading-overlay {\r\n            position: fixed;\r\n            top: 0;\r\n            left: 0;\r\n            width: 100%;\r\n            height: 100%;\r\n            background: rgba(0, 0, 0, 0.9);\r\n            display: flex;\r\n            flex-direction: column;\r\n            align-items: center;\r\n            justify-content: center;\r\n            z-index: 1000;\r\n            transition: opacity 0.5s;\r\n        }\r\n\r\n        .loader {\r\n            width: 48px;\r\n            height: 48px;\r\n            border: 5px solid #FFF;\r\n            border-bottom-color: var(--primary-color);\r\n            border-radius: 50%;\r\n            display: inline-block;\r\n            box-sizing: border-box;\r\n            animation: rotation 1s linear infinite;\r\n        }\r\n\r\n        .progress-text {\r\n            margin-top: 20px;\r\n            font-family: monospace;\r\n            color: var(--primary-color);\r\n        }\r\n\r\n        @keyframes rotation {\r\n            0% {\r\n                transform: rotate(0deg);\r\n            }\r\n\r\n            100% {\r\n                transform: rotate(360deg);\r\n            }\r\n        }\r\n    </style>\r\n</head>\r\n<body>\r\n    <div id=\"loading-overlay\">\r\n        <span class=\"loader\"></span>\r\n        <div id=\"loading-text\" class=\"progress-text\">Initializing Neural Engines...</div>\r\n    </div>\r\n    <div id=\"copy-toast\">Copied to Clipboard!</div>\r\n    <div class=\"container\">\r\n        <h1>Root Mic 🎙️</h1>\r\n        <div class=\"visualizer\"><div class=\"bar-container\" id=\"bars\"></div></div>\r\n        <button id=\"mic-btn\"><div id=\"mic-icon\">🎙️</div></button>\r\n        <button id=\"clarify-btn\" disabled>Refine Text</button>\r\n        <div id=\"status\">Ready</div>\r\n        <div id=\"output\">...</div>\r\n        <footer>Running Locally: Whisper-Tiny (Audio) + Qwen2.5-1.5B (Text)</footer>\r\n    </div>\r\n\r\n    <script type=\"module\">\r\n        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\r\n        import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n        // THE NEW KERNEL\r\n        import { AnchorLogger, createStore, getWebGPUConfig, GPUController } from './modules/anchor.js';\r\n\r\n        // Load hot reload functionality in development\r\n        if (location.hostname === 'localhost' || location.hostname === '127.0.0.1') {\r\n            import('./modules/gpu-hot-reloader.js').then(() => {\r\n                console.log('🔄 GPU Hot Reloader loaded for development');\r\n            }).catch(err => {\r\n                console.warn('⚠️ GPU Hot Reloader not available:', err);\r\n            });\r\n        }\r\n\r\n        const logger = new AnchorLogger('Root-Mic');\r\n        \r\n        // Reactive Store\r\n        const { state, subscribe } = createStore({\r\n            status: 'Ready',\r\n            output: '...',\r\n            isLoading: true,\r\n            loadingText: 'Initializing Neural Engines...',\r\n            isRecording: false\r\n        });\r\n\r\n        // UI Bindings\r\n        subscribe((prop, val) => {\r\n            if (prop === 'status') document.getElementById('status').innerText = val;\r\n            if (prop === 'output') {\r\n                document.getElementById('output').innerText = val;\r\n                // Enable clarify button if there is text (and not just placeholder/loading)\r\n                const btn = document.getElementById('clarify-btn');\r\n                if (val && val !== '...' && val.length > 10) {\r\n                    btn.disabled = false;\r\n                } else {\r\n                    btn.disabled = true;\r\n                }\r\n            }\r\n            if (prop === 'loadingText') document.getElementById('loading-text').innerText = val;\r\n            if (prop === 'isLoading') {\r\n                const ol = document.getElementById('loading-overlay');\r\n                ol.style.opacity = val ? '1' : '0';\r\n                setTimeout(() => ol.style.display = val ? 'flex' : 'none', 500);\r\n            }\r\n            if (prop === 'isRecording') {\r\n                const btn = document.getElementById('mic-btn');\r\n                if (val) btn.classList.add('active'); else btn.classList.remove('active');\r\n            }\r\n        });\r\n\r\n        let whisperWorker = null;\r\n        let llmEngine = null;\r\n        let mediaRecorder = null;\r\n        let audioChunks = [];\r\n        let audioContext = null;\r\n        let analyser = null;\r\n        let dataArray = null;\r\n        let animationId = null;\r\n        let silenceStart = 0;\r\n\r\n        async function init() {\r\n            try {\r\n                // 1. Whisper Init (Worker)\r\n                state.loadingText = \"Step 1/2: Initializing Whisper Worker...\";\r\n                whisperWorker = new Worker('./modules/whisper-worker.js', { type: 'module' });\r\n                \r\n                // Wait for worker init\r\n                await new Promise((resolve, reject) => {\r\n                    whisperWorker.onmessage = (e) => {\r\n                        if (e.data.type === 'init_done') resolve();\r\n                        if (e.data.type === 'error') reject(new Error(e.data.error));\r\n                    };\r\n                    whisperWorker.postMessage({ type: 'init' });\r\n                });\r\n                logger.success(\"Whisper Worker Ready\");\r\n\r\n                // 2. LLM Init (Using Kernel)\r\n                state.loadingText = \"Step 2/2: Config Qwen2.5 (Brain)...\";\r\n                await initLLM();\r\n\r\n                state.isLoading = false;\r\n                initVisualizer();\r\n                logger.success(\"Root Mic Online\");\r\n            } catch (e) {\r\n                logger.error(e.message);\r\n                state.loadingText = `Error: ${e.message}`;\r\n            }\r\n        }\r\n\r\n        // Clarify Logic\r\n        document.getElementById('clarify-btn').addEventListener('click', async () => {\r\n            if (!state.output || state.output === '...' || state.output.length < 5) return;\r\n            \r\n            const originalText = state.output;\r\n            state.status = \"Refining...\";\r\n            state.output = originalText + \"\\n\\n[Refining...]\";\r\n\r\n            try {\r\n                const reply = await llmEngine.chat.completions.create({\r\n                    messages: [\r\n                        { role: \"system\", content: \"You are a professional text editor. Your task is to refine the provided speech-to-text output into clear, coherent, and polished text suitable for reading or pasting. Fix grammar and flow, but keep the original meaning intact. Output ONLY the refined text.\" },\r\n                        { role: \"user\", content: `Refine this text:\\n\\n\"${originalText}\"` }\r\n                    ],\r\n                    temperature: 0.5,\r\n                    max_tokens: 512,\r\n                });\r\n\r\n                const summary = reply.choices[0].message.content;\r\n                state.output = summary; // Replace output with summary\r\n                state.status = \"Clarified\";\r\n                \r\n                if (document.hasFocus()) {\r\n                    navigator.clipboard.writeText(summary);\r\n                    const t = document.getElementById('copy-toast');\r\n                    t.innerText = \"Refined Text Copied!\";\r\n                    t.classList.add('show');\r\n                    setTimeout(() => { \r\n                        t.classList.remove('show');\r\n                        t.innerText = \"Copied to Clipboard!\";\r\n                    }, 2000);\r\n                }\r\n\r\n            } catch (e) {\r\n                logger.error(\"Clarify failed: \" + e.message);\r\n                state.status = \"Error\";\r\n                state.output = originalText; // Revert\r\n            }\r\n        });\r\n\r\n        async function initLLM() {\r\n            const modelId = \"mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC\";\r\n            const snapdragonId = \"snapdragon-mic-qwen\";\r\n\r\n            // 0. THE BLOCKER (Model Load Lock) - Serialize model loading\r\n            logger.info(\"Requesting Model Load Lock...\");\r\n\r\n            try {\r\n                await GPUController.withModelLoadLock(\"Root-Mic\", async () => {\r\n                    logger.success(\"Model Load Lock Acquired.\");\r\n\r\n                    // KERNEL CALL: Get safe GPU config\r\n                    const gpuConfig = await getWebGPUConfig('lite');\r\n\r\n                    if (gpuConfig.isConstrained) {\r\n                        logger.warn(`Clamping Buffer to ${Math.round(gpuConfig.maxBufferSize/1024/1024)}MB for Mobile/XPS compatibility.`);\r\n                    }\r\n\r\n                    // Create device explicitly with limits\r\n                    const device = await gpuConfig.adapter.requestDevice(gpuConfig.deviceConfig);\r\n\r\n                    const appConfig = {\r\n                        model_list: [{\r\n                            model: \"https://huggingface.co/\" + modelId + \"/resolve/main/\",\r\n                            model_id: snapdragonId,\r\n                            model_lib: \"https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/web-llm-models/v0_2_80/Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm\",\r\n                            vram_required_MB: 2000,\r\n                            low_resource_required: true,\r\n                            buffer_size_required_bytes: gpuConfig.maxBufferSize\r\n                        }]\r\n                    };\r\n\r\n                    llmEngine = await webllm.CreateMLCEngine(snapdragonId, {\r\n                        appConfig,\r\n                        device,\r\n                        initProgressCallback: (report) => {\r\n                            state.loadingText = `Loading Brain: ${Math.round(report.progress * 100)}%`;\r\n                        }\r\n                    });\r\n                });\r\n            } catch (error) {\r\n                state.loadingText = `Model Load Error: ${error.message}`;\r\n\r\n                // Try to check GPU status for more information\r\n                try {\r\n                    const status = await GPUController.checkStatus();\r\n                    if (status && status.locked) {\r\n                        logger.warn(`GPU currently locked by: ${status.owner || 'unknown'}`);\r\n                        if (status.queued && status.queued.length > 0) {\r\n                            logger.warn(`Queue: ${status.queued.join(', ')}`);\r\n                        }\r\n                    }\r\n                } catch (statusErr) {\r\n                    logger.warn(`Could not get GPU status: ${statusErr.message}`);\r\n                }\r\n\r\n                throw error;\r\n            }\r\n        }\r\n\r\n        function initVisualizer() {\r\n            const container = document.getElementById('bars');\r\n            for (let i = 0; i < 30; i++) {\r\n                const bar = document.createElement('div');\r\n                bar.className = 'bar';\r\n                container.appendChild(bar);\r\n            }\r\n        }\r\n\r\n        // Recording Logic\r\n        document.getElementById('mic-btn').addEventListener('click', async () => {\r\n            if (!state.isRecording) startRecording(); else stopRecording();\r\n        });\r\n\r\n        async function startRecording() {\r\n            try {\r\n                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\r\n                mediaRecorder = new MediaRecorder(stream);\r\n                audioChunks = [];\r\n                mediaRecorder.ondataavailable = e => audioChunks.push(e.data);\r\n                mediaRecorder.onstop = processAudio;\r\n\r\n                // Visualizer\r\n                audioContext = new (window.AudioContext || window.webkitAudioContext)();\r\n                const source = audioContext.createMediaStreamSource(stream);\r\n                analyser = audioContext.createAnalyser();\r\n                analyser.fftSize = 64;\r\n                source.connect(analyser);\r\n                dataArray = new Uint8Array(analyser.frequencyBinCount);\r\n                animateVisualizer();\r\n\r\n                mediaRecorder.start();\r\n                state.isRecording = true;\r\n                silenceStart = Date.now();\r\n                state.status = \"Listening...\";\r\n                state.output = \"...\";\r\n            } catch (e) { alert(e.message); }\r\n        }\r\n\r\n        function stopRecording() {\r\n            mediaRecorder.stop();\r\n            state.isRecording = false;\r\n            state.status = \"Processing...\";\r\n            cancelAnimationFrame(animationId);\r\n            if (audioContext) audioContext.close();\r\n            document.querySelectorAll('.bar').forEach(b => b.style.height = '4px');\r\n        }\r\n\r\n        function animateVisualizer() {\r\n            if (!state.isRecording) return;\r\n            animationId = requestAnimationFrame(animateVisualizer);\r\n            analyser.getByteFrequencyData(dataArray);\r\n            const bars = document.querySelectorAll('.bar');\r\n            let maxVol = 0;\r\n            for (let i = 0; i < bars.length; i++) {\r\n                const val = dataArray[i];\r\n                if (val > maxVol) maxVol = val;\r\n                bars[i].style.height = `${Math.max(4, (val / 255) * 50)}px`;\r\n            }\r\n            if (maxVol > 10) silenceStart = Date.now();\r\n            else if (Date.now() - silenceStart > 3000) state.status = \"⚠️ No Audio?\";\r\n        }\r\n\r\n        async function processAudio() {\r\n            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); // Default browser format\r\n\r\n            // 1. Decode to System Sample Rate (e.g., 48000Hz)\r\n            const audioCtx = new AudioContext();\r\n            const arrayBuffer = await audioBlob.arrayBuffer();\r\n            const decodedBuffer = await audioCtx.decodeAudioData(arrayBuffer);\r\n\r\n            // 2. Resample to 16000Hz (Required by Whisper)\r\n            const targetRate = 16000;\r\n            const offlineCtx = new OfflineAudioContext(1, decodedBuffer.duration * targetRate, targetRate);\r\n            const source = offlineCtx.createBufferSource();\r\n            source.buffer = decodedBuffer;\r\n            source.connect(offlineCtx.destination);\r\n            source.start(0);\r\n            \r\n            const resampledBuffer = await offlineCtx.startRendering();\r\n            let audioData = resampledBuffer.getChannelData(0);\r\n\r\n            // NORMALIZE / AMPLIFY with Noise Gate\r\n            let peak = 0;\r\n            for (let i = 0; i < audioData.length; i++) {\r\n                const val = Math.abs(audioData[i]);\r\n                if (val > peak) peak = val;\r\n            }\r\n\r\n            // Cap amplification to avoid boosting silence/hiss into \"Applause\"\r\n            // If peak is TOO low (silence), don't amplify at all.\r\n            let ampFactor = 1.0;\r\n            if (peak > 0.01 && peak < 0.5) {\r\n                ampFactor = Math.min(0.5 / peak, 5.0); // Max 5x boost\r\n                for (let i = 0; i < audioData.length; i++) {\r\n                    audioData[i] = audioData[i] * ampFactor;\r\n                }\r\n            } else if (peak <= 0.01) {\r\n                // Too quiet, likely silence. Don't send to Whisper or send silence.\r\n                state.status = \"Too quiet (Ignored)\";\r\n                return;\r\n            }\r\n\r\n            state.status = \"Transcribing...\";\r\n            \r\n            // 0. THE BLOCKER (GPU Lock)\r\n            await GPUController.withLock(\"Root-Mic-Process\", async () => {\r\n                // Offload to Worker\r\n                const rawText = await new Promise((resolve, reject) => {\r\n                    const reqId = Date.now();\r\n                    const handler = (e) => {\r\n                        if (e.data.id === reqId) {\r\n                            whisperWorker.removeEventListener('message', handler);\r\n                            if (e.data.type === 'transcribe_result') resolve(e.data.text);\r\n                            else reject(new Error(e.data.error));\r\n                        }\r\n                    };\r\n                    whisperWorker.addEventListener('message', handler);\r\n                    whisperWorker.postMessage({ type: 'transcribe', data: audioData, id: reqId });\r\n                });\r\n\r\n                // Hallucination Filter (Aggressive)\r\n                let cleanedText = rawText.trim();\r\n                const hallucinations = [\r\n                    '[Music]', '[BLANK_AUDIO]', 'Computed', '*sigh*', '*breathing*', \r\n                    'Applause', 'Thank you', 'Subtitles', 'Amara.org', 'Copyright', \r\n                    '©', 'Caption', 'Sovereign' \r\n                ];\r\n                \r\n                hallucinations.forEach(h => { \r\n                    const regex = new RegExp(h.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'), 'gi');\r\n                    cleanedText = cleanedText.replace(regex, '').trim();\r\n                });\r\n\r\n                // Filter single punctuation or very short junk\r\n                if (/^[.,?!;:]+$/.test(cleanedText) || cleanedText.length < 2) cleanedText = \"\";\r\n\r\n                if (!cleanedText || cleanedText.length < 1) {\r\n                    state.status = \"Heard nothing\";\r\n                    return;\r\n                }\r\n\r\n                state.output = `Raw: \"${cleanedText}\"\\n\\nCleaning...`;\r\n                state.status = \"Refining...\";\r\n\r\n                const reply = await llmEngine.chat.completions.create({\r\n                    messages: [\r\n                        { role: \"system\", content: \"You are a verbatim transcription corrector. Your ONLY task is to fix grammar, spelling, and punctuation. Do NOT answer questions. Do NOT add commentary. Output ONLY the corrected text.\" },\r\n                        { role: \"user\", content: `Correct this text: \"${cleanedText}\"` }\r\n                    ],\r\n                    temperature: 0.3,\r\n                    max_tokens: 512,\r\n                });\r\n\r\n                const finalText = reply.choices[0].message.content;\r\n                state.output = finalText;\r\n            }); // End Lock\r\n\r\n            state.status = \"Ready\";\r\n            \r\n            // Auto Copy (Handle focus requirement)\r\n            if (document.hasFocus()) {\r\n                navigator.clipboard.writeText(cleanText).then(() => {\r\n                    const t = document.getElementById('copy-toast');\r\n                    t.classList.add('show');\r\n                    setTimeout(() => t.classList.remove('show'), 2000);\r\n                }).catch(err => {\r\n                    console.warn(\"Clipboard write failed (focus lost?):\", err);\r\n                });\r\n            } else {\r\n                console.warn(\"Clipboard write skipped: Document not focused.\");\r\n                state.output += \"\\n(Copy skipped - Click to copy)\";\r\n            }\r\n        }\r\n\r\n        init();\r\n    </script>\r\n</body>\r\n</html>\r\n"
    size: 24727
    tokens: 8299
  - path: tools\anchor.py
    content: "import requests\r\nimport sys\r\nimport json\r\nimport os\r\nfrom datetime import datetime\r\n\r\n# Configuration\r\nBRIDGE_URL = \"http://localhost:8000\"\r\nCONTEXT_DIR = \"../context/sessions\"  # Path relative to tools/\r\n\r\ndef ensure_session_file():\r\n    \"\"\"Creates a daily session file if it doesn't exist\"\"\"\r\n    if not os.path.exists(CONTEXT_DIR):\r\n        os.makedirs(CONTEXT_DIR)\r\n\r\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\r\n    filename = f\"{CONTEXT_DIR}/chat_{date_str}.md\"\r\n\r\n    if not os.path.exists(filename):\r\n        with open(filename, \"w\", encoding=\"utf-8\") as f:\r\n            f.write(f\"# Chat Session: {date_str}\\n\\n\")\r\n\r\n    return filename\r\n\r\ndef append_to_log(role, text):\r\n    \"\"\"Writes the message to the daily markdown file\"\"\"\r\n    filename = ensure_session_file()\r\n    timestamp = datetime.now().strftime(\"%H:%M:%S\")\r\n\r\n    with open(filename, \"a\", encoding=\"utf-8\") as f:\r\n        # Format as Markdown\r\n        f.write(f\"### {role.upper()} [{timestamp}]\\n\")\r\n        f.write(f\"{text}\\n\\n\")\r\n\r\ndef check_connection():\r\n    try:\r\n        requests.get(f\"{BRIDGE_URL}/health\", timeout=1)\r\n        return True\r\n    except:\r\n        return False\r\n\r\ndef chat_loop():\r\n    print(\"\\n⚓ Anchor Terminal (Chat Mode)\")\r\n    print(\"--------------------------------\")\r\n    print(f\"📁 Session Log: {os.path.abspath(CONTEXT_DIR)}\")\r\n    print(\"Connecting to Ghost Engine...\")\r\n\r\n    if not check_connection():\r\n        print(f\"❌ Could not connect to {BRIDGE_URL}\")\r\n        print(\"   -> Run 'start-anchor.bat' first.\")\r\n        return\r\n\r\n    # Conversation History\r\n    history = [\r\n        {\"role\": \"system\", \"content\": \"You are Anchor, a helpful AI assistant running locally.\"}\r\n    ]\r\n\r\n    print(\"✅ Connected. Type 'exit' to quit, 'clear' to reset.\\n\")\r\n\r\n    while True:\r\n        try:\r\n            user_input = input(\"You: \").strip()\r\n            if not user_input: continue\r\n\r\n            if user_input.lower() in ['exit', 'quit']:\r\n                print(\"👋 Disconnecting.\")\r\n                break\r\n\r\n            if user_input.lower() == 'clear':\r\n                history = [history[0]]\r\n                print(\"--- Context Cleared ---\")\r\n                continue\r\n\r\n            # 1. Update History & Log\r\n            history.append({\"role\": \"user\", \"content\": user_input})\r\n            append_to_log(\"user\", user_input)\r\n\r\n            print(\"Anchor: \", end=\"\", flush=True)\r\n\r\n            # 2. Send to Bridge\r\n            try:\r\n                response = requests.post(\r\n                    f\"{BRIDGE_URL}/v1/chat/completions\",\r\n                    json={\r\n                        \"messages\": history,\r\n                        \"stream\": True\r\n                    },\r\n                    stream=True,\r\n                    timeout=120\r\n                )\r\n\r\n                if response.status_code == 200:\r\n                    ai_text = \"\"\r\n                    for line in response.iter_lines(decode_unicode=True):\r\n                        if line and line.startswith(\"data: \"):\r\n                            data_str = line[6:]\r\n                            if data_str.strip() == \"[DONE]\": break\r\n                            try:\r\n                                chunk = json.loads(data_str)\r\n                                if 'choices' in chunk and len(chunk['choices']) > 0:\r\n                                    content = chunk['choices'][0].get('delta', {}).get('content', '')\r\n                                    if content:\r\n                                        print(content, end=\"\", flush=True)\r\n                                        ai_text += content\r\n                            except: continue\r\n\r\n                    print(\"\\n\")\r\n                    # 3. Update History & Log Assistant Response\r\n                    history.append({\"role\": \"assistant\", \"content\": ai_text})\r\n                    append_to_log(\"assistant\", ai_text)\r\n\r\n                else:\r\n                    print(f\"❌ Error {response.status_code}: {response.text}\")\r\n\r\n            except Exception as e:\r\n                print(f\"❌ Request Failed: {e}\")\r\n\r\n        except KeyboardInterrupt:\r\n            print(\"\\nInterrupted.\")\r\n            break\r\n\r\nif __name__ == \"__main__\":\r\n    chat_loop()"
    size: 4183
    tokens: 1426
  - path: tools\anchor_watchdog.py
    content: "import sys, time, os, requests, hashlib, logging\r\nfrom watchdog.observers import Observer\r\nfrom watchdog.events import FileSystemEventHandler\r\n\r\nWATCH_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"context\"))\r\nBRIDGE_INGEST_URL = \"http://localhost:8000/v1/memory/ingest\"\r\nALLOWED = {'.md', '.txt', '.json', '.yaml', '.py', '.js', '.html', '.css'}\r\n\r\n# Setup Logging\r\nLOG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"logs\"))\r\nif not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\r\n\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s - %(message)s',\r\n    datefmt='%H:%M:%S',\r\n    handlers=[\r\n        logging.StreamHandler(),\r\n        logging.FileHandler(os.path.join(LOG_DIR, \"watchdog.log\"), mode='w', encoding='utf-8')\r\n    ]\r\n)\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass Handler(FileSystemEventHandler):\r\n    def __init__(self):\r\n        self.hashes = {}\r\n        self.last_mod = {}\r\n\r\n    def process(self, filepath):\r\n        _, ext = os.path.splitext(filepath)\r\n        if ext.lower() not in ALLOWED: return\r\n        \r\n        # Debounce & Hash Check\r\n        now = time.time()\r\n        if filepath in self.last_mod and now - self.last_mod[filepath] < 1.0: return\r\n        self.last_mod[filepath] = now\r\n        time.sleep(0.1)\r\n        \r\n        hasher = hashlib.md5()\r\n        try:\r\n            with open(filepath, 'rb') as f: hasher.update(f.read())\r\n            new_hash = hasher.hexdigest()\r\n        except: return\r\n        \r\n        if self.hashes.get(filepath) == new_hash: return\r\n        self.hashes[filepath] = new_hash\r\n\r\n        logger.info(f\"👀 Change: {os.path.basename(filepath)}\")\r\n        try:\r\n            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f: content = f.read()\r\n            payload = { \"filename\": os.path.relpath(filepath, WATCH_DIR), \"content\": content, \"filetype\": ext }\r\n            requests.post(BRIDGE_INGEST_URL, json=payload, timeout=5)\r\n            logger.info(f\"✅ Ingested\")\r\n        except Exception as e: \r\n            logger.error(f\"❌ Error ingesting {os.path.basename(filepath)}: {e}\")\r\n\r\n    def on_modified(self, event): \r\n        if not event.is_directory: self.process(event.src_path)\r\n    def on_created(self, event):\r\n        if not event.is_directory: self.process(event.src_path)\r\n\r\ndef wait_for_bridge():\r\n    \"\"\"Wait for the bridge to become available.\"\"\"\r\n    logger.info(\"⏳ Waiting for Bridge to come online...\")\r\n    url = \"http://localhost:8000/health\"\r\n    for i in range(30): # Wait up to 30 seconds\r\n        try:\r\n            requests.get(url, timeout=2)\r\n            logger.info(\"🟢 Bridge is Online!\")\r\n            return True\r\n        except:\r\n            time.sleep(1)\r\n    logger.error(\"❌ Bridge unreachable after 30s. Exiting.\")\r\n    return False\r\n\r\nif __name__ == \"__main__\":\r\n    if not os.path.exists(WATCH_DIR): os.makedirs(WATCH_DIR)\r\n    \r\n    # Wait for Bridge\r\n    if not wait_for_bridge():\r\n        sys.exit(1)\r\n\r\n    handler = Handler()\r\n    \r\n    # Initial Indexing\r\n    logger.info(\"🔍 Starting Initial Index Walk...\")\r\n    for root, dirs, files in os.walk(WATCH_DIR):\r\n        for file in files:\r\n            handler.process(os.path.join(root, file))\r\n    logger.info(\"✅ Initial Indexing Complete\")\r\n\r\n    obs = Observer()\r\n    obs.schedule(handler, WATCH_DIR, recursive=True)\r\n    obs.start()\r\n    logger.info(f\"🐕 Watchdog Active: {WATCH_DIR}\")\r\n    try:\r\n        while True: time.sleep(1)\r\n    except KeyboardInterrupt: obs.stop()\r\n    obs.join()\r\n"
    size: 3523
    tokens: 1290
  - path: tools\ask_memory.py
    content: "#!/usr/bin/env python3\r\n\"\"\"\r\nAnchor Memory Retriever\r\nUsage: python ask_memory.py \"What did I work on last week?\"\r\n\"\"\"\r\nimport sys\r\nimport requests\r\nimport argparse\r\nimport json\r\n\r\n# Configuration\r\nBRIDGE_URL = \"http://localhost:8000\"\r\n\r\ndef search_memory(query):\r\n    print(f\"⚓ Querying Anchor Memory for: '{query}'...\")\r\n    try:\r\n        response = requests.post(\r\n            f\"{BRIDGE_URL}/v1/memory/search\",\r\n            json={\"query\": query},\r\n            timeout=15\r\n        )\r\n        \r\n        if response.status_code == 200:\r\n            data = response.json()\r\n            if data.get('status') == 'success':\r\n                return data.get('result', '')\r\n            else:\r\n                print(f\"❌ Error from Engine: {data.get('error')}\")\r\n        elif response.status_code == 503:\r\n            print(f\"❌ Service unavailable: Ghost Engine not connected. Is chat.html open?\")\r\n        elif response.status_code == 400:\r\n            print(f\"❌ Bad request: {response.json().get('error', 'Invalid query')}\")\r\n        elif response.status_code == 504:\r\n            print(f\"❌ Gateway timeout: Search took too long\")\r\n        else:\r\n            print(f\"❌ Bridge Error ({response.status_code}): {response.text}\")\r\n            \r\n    except requests.exceptions.ConnectionError:\r\n        print(f\"❌ Could not connect to Anchor Bridge at {BRIDGE_URL}\")\r\n        print(\"   Is 'start-anchor.bat' running?\")\r\n    except Exception as e:\r\n        print(f\"❌ Unexpected error: {e}\")\r\n    \r\n    return None\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"Query your local Anchor Memory system.\")\r\n    parser.add_argument(\"query\", nargs=\"+\", help=\"The question or topic to search for.\")\r\n    args = parser.parse_args()\r\n    \r\n    full_query = \" \".join(args.query)\r\n    \r\n    context = search_memory(full_query)\r\n    \r\n    if context:\r\n        print(\"\\n\" + \"=\"*60)\r\n        print(\"📋 COPY THE TEXT BELOW TO WEBLLM CHAT\")\r\n        print(\"=\"*60 + \"\\n\")\r\n        print(context)\r\n        print(\"\\n\" + \"=\"*60)\r\n        \r\n        # Optional: Auto-copy to clipboard if user has pyperclip\r\n        try:\r\n            import pyperclip\r\n            pyperclip.copy(context)\r\n            print(\"✅ Copied to clipboard automatically!\")\r\n        except ImportError:\r\n            print(\"(Tip: Install 'pyperclip' to auto-copy: pip install pyperclip)\")\r\n    else:\r\n        print(\"\\n❌ Failed to retrieve memory context.\")\r\n        print(\"Make sure:\")\r\n        print(\"1. Anchor Core is running (start-anchor.bat)\")\r\n        print(\"2. The chat.html page is open and connected\")\r\n        print(\"3. The database has been initialized\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
    size: 2664
    tokens: 956
  - path: tools\CHANGELOG.md
    content: "# Tools Changelog\r\n\r\n## [2.4.0] - 2026-01-01 \"Anchor Lite Refactor\"\r\n### Changed\r\n- Renamed `chat.html` to `ghost.html` (Headless Engine).\r\n- Refactored `context.html` to be the primary Search Dashboard.\r\n### Removed\r\n- Archived `db_builder.html`, `memory-builder.html`, `mobile-chat.html`.\r\n- Removed auto-loading of WebLLM to save resources.\r\n\r\n\r\n## [Unreleased] - 2025-12-26\r\n\r\n### Added\r\n- **On-Demand Model Serving**: `model-server-chat.html` now checks `http://localhost:8080/models/{id}` before loading. If missing, it triggers a download via the Bridge.\r\n- **Quota Bypass**: Using `useIndexedDBCache: false` for large models to bypass browser storage limits, relying on the Bridge's local file server instead.\r\n\r\n### Fixed\r\n- **Model ID Mismatch**: Fixed logic in `loadModel` where the `mlc-ai/` prefix was being aggressively stripped, causing config lookups to fail.\r\n- **UI Progress**: Added real-time progress bars for server-side model downloads.\r\n\r\n - 2025-12-23\r\n\r\n### Added\r\n- **Orchestrator Model:** New `orchestrator.py` tool to programmatically interact with the MLC Bridge from Python.\r\n- **Health Endpoint:** Added `/health` to `webgpu_bridge.py` for extension connectivity checks.\r\n- **Audit Whitelist:** Added `/audit/server-logs` to auth whitelist in Bridge to fix Log Viewer 401 errors.\r\n\r\n### Changed\r\n- **Bridge Port:** Moved standard bridge port from `8000` to `8080` to avoid conflicts with `http.server`.\r\n- **Launch Scripts:** Updated `start-sovereign-console.bat` and `launch-chromium-d3d12.bat` to respect new port `8080` and correct paths.\r\n- **Root Dreamer:**\r\n  - Robustified `init()` to handle non-Error objects during crash.\r\n  - Improved JSON parsing to strip markdown code blocks before parsing.\r\n  - Added strict engine readiness check to `dreamLoop` to prevent race conditions.\r\n- **Root Console:**\r\n  - Added \"High Performance (Small)\" models (Qwen 2.5 1.5B, TinyLlama) to the dropdown.\r\n  - Updated JS mapper to handle new small model paths.\r\n  - Fixed crash in `executeR1Loop` where `genErr.message` could be undefined.\r\n\r\n### Fixed\r\n- **WebGPU Crash:** Mitigated `DXGI_ERROR_DEVICE_REMOVED` by advising single-tab usage and providing smaller model options for constrained profiles.\r\n- **Extension Connection:** Fixed CORS and Port mismatch preventing the Chrome Extension from connecting to the Bridge.\r\n"
    size: 2349
    tokens: 906
metadata:
  total_files: 93
  total_tokens: 162915
  token_limit: 200000
  token_limit_reached: false
