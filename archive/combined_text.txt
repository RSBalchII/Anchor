--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\README.md (Section: ROOT_PROJECT) ---

# Context-Engine: Infinite Context Pipeline

> **Executive Cognitive Enhancement (ECE)** - An infinite context system and cognitive augmentation toolkit that solves the traditional LLM context window limitation.

**Philosophy**: Your mind, augmented. Your data, sovereign. Your tools, open. **Infinite context, unlimited conversations.**

---

## Architecture: Three Components, One System

Context-Engine consists of three primary components working in harmony:

### üß† **The Core** - The Brain
**Role**: Memory system, reasoning engine, and cognitive orchestration
**Location**: `backend/`

- **Memory Architecture**: Neo4j (primary graph storage) + Redis (hot cache)
- **Cognitive Agents**: Verifier (truth-checking), Archivist (maintenance), Distiller (summarization)
- **Retrieval**: Graph-R1 + Markovian reasoning for context-aware memory recall
- **Tool Integration**: Plugin-based architecture via UTCP (Simple Tool Mode)
- **API**: FastAPI server on port 8000

**Key Features**:
- ‚úÖ Graph-based memory with relationship traversal
- ‚úÖ Empirical Distrust verification (primary source supremacy)
- ‚úÖ Automatic memory distillation and summarization
- ‚úÖ Traceability & rollback for automated repairs
- ‚úÖ Circuit breakers and graceful degradation

### ü§ñ **Anchor** - The Body
**Role**: Terminal interface and interaction layer
**Location**: `anchor/`

- **Interface**: Lightweight CLI with streaming responses
- **Integration**: Connects to ECE_Core API for memory-enhanced conversations
- **Tool Execution**: Pattern-based tool mode for reliable execution
- **Deployment**: Can be packaged as standalone executable

**Key Features**:
- ‚úÖ Real-time streaming responses
- ‚úÖ Memory-enhanced conversations
- ‚úÖ Simple tool mode for reliable tool execution
- ‚úÖ Security hardening (whitelist, confirmation flows)

### üåâ **Extension** - The Bridge
**Role**: Browser integration and active context injection
**Location**: `extension/`

- **Interface**: Chrome Side Panel with persistent chat
- **Integration**: Connects to ECE_Core API for context-aware browsing
- **Ingestion**: "Save to Memory" capability to archive web content and chat transcripts directly to the knowledge graph.
- **Capabilities**:
  - üëÅÔ∏è **Sight**: Reads active page content on demand
  - üó£Ô∏è **Voice**: Streaming chat interface
  - ‚úã **Hands**: Executes JavaScript actions on the page

---

## Infinite Context Pipeline: Hardware + Logic

### üîß **Phase 1: Hardware Foundation**
- **64k Context Window**: All servers now boot with 65,536 token capacity
- **GPU Optimization**: Full layer offload (99) with Q8 quantized KV cache
- **Flash Attention**: Enabled for optimal performance with long contexts

### üß© **Phase 2: Context Rotation Protocol** 
- **Context Shifting**: Automatic rotation when context approaches 55k tokens
- **Intelligent Distillation**: Old context compressed to "Narrative Gists" using Distiller
- **Persistent Storage**: Gists stored in Neo4j as `:ContextGist` nodes with chronological links

### üß† **Phase 3: Graph-R1 Reasoning Enhancement**
- **Gist Retrieval**: GraphReasoner now searches `:ContextGist` nodes for historical context
- **Continuity Maintenance**: Maintains reasoning flow across context rotations
- **Smart Querying**: Enhanced retrieval logic with historical context awareness

---

## Data Flow

```
User Input (Anchor CLI / Extension)
    ‚Üì
ECE_Core API (:8000)
    ‚Üì
‚îú‚îÄ Redis: Check active session cache
‚îú‚îÄ Neo4j: Graph traversal + semantic search + ContextGist retrieval
‚îî‚îÄ LLM: Generate response with full context (including historical gists)
    ‚Üì
Cognitive Agents (optional)
‚îú‚îÄ Verifier: Fact-check via Empirical Distrust
‚îú‚îÄ Distiller: Summarize and extract entities + Context Rotation
‚îî‚îÄ Archivist: Maintain freshness, schedule repairs
    ‚Üì
Response ‚Üí Anchor ‚Üí User
```

---

## Memory Architecture

### Current (Production)
- **Neo4j** (port 7687) - PRIMARY STORAGE
  - All memories, summaries, relationships
  - ContextGist nodes for historical context
  - Graph-based retrieval with Graph-R1 optimization
- **Redis** (port 6379) - ACTIVE SESSION CACHE
  - Hot cache for active conversations (24h TTL)
  - Graceful fallback to Neo4j if unavailable

### Deprecated
- ~~**SQLite**~~ - Fully removed 2025-11-13, migrated to Neo4j

---

## Quick Start

### Prerequisites
- Python 3.11+
- Neo4j database (local or remote)
- Redis server (optional, but recommended)
- llama.cpp server (will be started by our scripts)

### Clean Architecture - 3 Main Scripts
This project now uses a simplified 3-script architecture:

1. **`python start_llm_server.py`** - Interactive LLM server with model selection (64k window)
2. **`python start_ece.py`** - ECE Core with integrated MCP memory system
3. **`python start_embedding_server.py`** - Auto-selects gemma-300m embedding server

### Three-Terminal Startup

**Terminal 1 - LLM Server:**
```bash
python start_llm_server.py  # Interactive model selection, 64k window
```

**Terminal 2 - ECE_Core (The Brain):**
```bash
python start_ece.py  # Includes MCP endpoints at port 8000
```

**Terminal 3 - Embedding Server (optional):**
```bash
python start_embedding_server.py  # Auto-selects gemma-300m
```

### Quick Reka Configuration
For Reka Flash 3 21B optimized settings:
```bash
start-reka.bat  # Starts all services with RTX 4090 optimized parameters
```

### Alternative: All-in-one Safe Startup

To start all services with conservative defaults using the new Python architecture:
```bash
python start_all_safe.py  # Python version (recommended)
# OR
start_all_safe_simple.bat  # Batch wrapper
```

### Health Checks
```bash
# Verify LLM
curl http://localhost:8080/v1/models

# Verify ECE_Core
curl http://localhost:8000/health
```

### Troubleshooting: Proxy & Missing Dependencies

If your `start-openai-stream-proxy.*` script returns an error such as:

```
ModuleNotFoundError: No module named 'sse_starlette'
```

then your Python environment is missing the `sse-starlette` package. The proxy requires `sse-starlette` for SSE support and `uvicorn` to run.

Quick fix (Windows PowerShell):
```pwsh
cd C:\Users\rsbiiw\Projects\Context-Engine\ece-core
.\.venv\Scripts\Activate  # Or whichever venv you use
pip install -r requirements.txt
```

Quick fix (Unix/macOS):
```bash
cd /path/to/Context-Engine\ece-core
source .venv/bin/activate
pip install -r requirements.txt
```

After installing dependencies, re-run the proxy script or the `start-all-safe.*` wrapper.

If you see "The filename, directory name, or volume label syntax is incorrect.", ensure you're running the start script from the repo root and that there are no stray quotes or illegal characters in your path. The wrapper assumes `start-openai-stream-proxy.*` will be run from the repo root (so `%~dp0` works as expected).

## All-in-one Safe Startup (recommended for devs)

To start all core services in a single command using conservative defaults that reduce OOMs and contention, use the safe startup scripts.

This wrapper starts (in order):
- LLaMa safe server (lower ubatch & single parallel-slot)
- LLaMa embedding server (optional)
- LLM Server with Reka-optimized settings
- ECE Core with MCP enabled
- MCP server (integrated into ECE Core when enabled)

Usage:
```bash
# Windows (Python version - recommended)
python start_all_safe.py

# Windows (Batch wrapper)
start_all_safe_simple.bat

# Windows (Reka-optimized)
start-reka.bat
```

The scripts perform basic health checks and will wait for LLaMa and ECE Core to be reachable before starting dependent components.

## Configuration

### ECE_Core Configuration
- **Primary config**: `ece-core/.env` (from `.env.example`)
- **LLM settings**: Context size, GPU layers, model path
- **Memory settings**: Redis/Neo4j connection strings
- **Agent settings**: Enable/disable Verifier, Archivist, Distiller

### Anchor Configuration
- **Primary config**: `anchor/.env` (from `.env.example`)
- **ECE connection**: `ECE_URL=http://localhost:8000`
- **Tool settings**: `PLUGINS_ENABLED=true` to enable tools

---

## Documentation

### Core Specs (Single Source of Truth)
- `specs/spec.md` - Technical architecture
- `specs/plan.md` - Vision, roadmap, ADRs
- `specs/tasks.md` - Implementation backlog
- `specs/TROUBLESHOOTING.md` - Operational debugging

### Component Specs
- `backend/specs/spec.md` - Backend technical specs
- `backend/specs/plan.md` - Backend roadmap
- `backend/specs/tasks.md` - Backend tasks
- `anchor/specs/spec.md` - Anchor technical specs
- `anchor/specs/plan.md` - Anchor roadmap
- `anchor/specs/tasks.md` - Anchor tasks

### Supplementary
- `CHANGELOG.md` - Complete project history
- `archive/README.md` - Archived code explanation

---

## Tool Architecture

**Current**: Plugin-based UTCP (Simple Tool Mode) and MCP Integration

Tools are discovered via multiple methods:
- **Plugin-based UTCP**: `ece-core/plugins/` directory
  - `web_search` - DuckDuckGo search
  - `filesystem_read` - File and directory operations
  - `shell_execute` - Shell command execution (with safety checks)
  - `mgrep` - Semantic code search
- **MCP Integration**: Memory tools via `/mcp` endpoints
  - `add_memory` - Add to Neo4j memory graph
  - `search_memories` - Search memory graph with relationships
  - `get_summaries` - Get session summaries

**Note**: MCP (Model Context Protocol) is now integrated into the main ECE server when `mcp.enabled: true` in config.

---

## Cognitive Architecture: Agents

ECE_Core implements an agent-based architecture for memory hygiene and cognitive enhancement:

### Verifier Agent
- **Role**: Truth-checking via Empirical Distrust
- **Method**: Provenance-aware scoring (primary sources > summaries)
- **Goal**: Reduce hallucinations, increase factual accuracy

### Distiller Agent
- **Role**: Memory summarization and compression + Context Rotation
- **Method**: LLM-assisted distillation with salience scoring
- **Goal**: Maintain high-value context, prune noise, enable infinite context

### Archivist Agent
- **Role**: Knowledge base maintenance and freshness + Context Management
- **Method**: Scheduled verification, stale node detection, context rotation oversight
- **Goal**: Keep memory graph current and trustworthy, manage context windows

### Memory Weaver (Maintenance Engine)
- **Role**: Automated relationship repair
- **Method**: Embedding-based similarity with audit trail
- **Goal**: Maintain graph integrity with full traceability

---

## Small Model Considerations

**Tool Usage**:
- ‚ö†Ô∏è Models < 14B parameters are **unreliable** for structured tool protocols
- ‚úÖ Use "Simple Tool Mode" (pattern-based execution) for 4B-8B models
- ‚úÖ Use 14B+ models (DeepSeek-R1, Qwen2.5-14B) for full tool support
- ‚úÖ MCP Integration works with any model for memory operations

**Recommended Models**:
- **Gemma-3 4B** - Best for speed (chat only, tools unreliable)
- **Qwen3-8B** - Best for reasoning (Simple Tool Mode works)
- **DeepSeek-R1-14B** - Best for tools (full structured protocol support)
- **Reka Flash 3 21B** - Best for reasoning (use start-reka.bat)

---

## Development

### Install Dependencies
```bash
# ECE_Core
cd ece-core
pip install -e .

# Anchor
cd anchor
pip install -e .
```

### Run Tests
```bash
# ECE_Core tests
cd ece-core
python -m pytest tests/

# Anchor tests
cd anchor
python -m pytest tests/
```

### Package Distribution
```bash
# ECE_Core wheel
cd ece-core
python -m build

# Anchor standalone executable
cd anchor
pyinstaller anchor.spec
```

---

## Project Status

**Current Phase**: Infinite Context Implementation (Phase 5)
**Version**: Context-Engine 1.0.0, ECE_Core 1.0.0, Anchor 0.1.0-alpha
**Last Updated**: 2025-12-08

### ‚úÖ Completed
- Neo4j + Redis architecture (SQLite removed)
- Plugin-based tool system (UTCP)
- MCP integration into main ECE server
- Cognitive agents (Verifier, Archivist, Distiller)
- Traceability & rollback for automated repairs
- Security hardening (API auth, audit logs)
- PyInstaller packaging
- **NEW: Infinite Context Pipeline** (64k windows, context rotation, Graph-R1 integration)

### üîÑ In Progress
- Vector adapter + C2C hot-replica for semantic retrieval
- Compressed summaries + passage recall (EC-T-133)
- SLM benchmarking and ALScore measurements

### üìÖ Planned
- CLI wrapper for script operations (`ece-cli`)
- Increase test coverage to 80%+
- Developer onboarding (`docker-compose.dev.yaml`)

---

## Target Users

### Primary: Developers with Executive Function Challenges
**Pain Points**: Memory decay, context switching, project knowledge retention
**Solution**: Persistent external memory with automatic retrieval

### Secondary: Privacy-Conscious Developers
**Pain Points**: Cloud dependency, data sovereignty, vendor lock-in
**Solution**: 100% local, zero telemetry, your data stays yours

### Tertiary: AI Power Users
**Pain Points**: Need long-term memory, tool integration, customization
**Solution**: Memory-enhanced workflows, extensible architecture, open source

---

## Research Foundation

- **Graph-R1**: Memory retrieval patterns (https://arxiv.org/abs/2507.21892)
- **Markovian Reasoning**: Chunked thinking (https://arxiv.org/abs/2506.21734)
- **Hierarchical Reasoning Model (HRM)**: Multi-level context processing
- **Empirical Distrust**: Primary source supremacy for verification

See `ece-core/specs/references.md` for complete bibliography.

---

## License

MIT - Use, modify, and distribute freely.

---

## Acknowledgments

Built for neurodivergent hackers who need their tools to work reliably.

**"Your mind, augmented. Your data, sovereign. Your tools, open."**

---

## Need Help?

- **Operational Issues**: See `specs/TROUBLESHOOTING.md`
- **Architecture Questions**: See `specs/spec.md`
- **Implementation Tasks**: See `specs/tasks.md`
- **Project History**: See `CHANGELOG.md`

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\README.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\CHANGELOG.md (Section: ROOT_PROJECT) ---

# Context-Engine Changelog

## [1.0.0] - 2025-12-08 "Infinite Context Pipeline"

### Added
- **Phase 1: Hardware Foundation**: All LLM servers now boot with 65,536 context window and Flash Attention enabled
- **Phase 2: Context Rotation Protocol**: ContextManager automatically rotates context when exceeding 55k tokens
- **Phase 3: Graph-R1 Enhancement**: GraphReasoner now retrieves ContextGist memories for historical continuity
- **ContextGist Nodes**: Neo4j storage for compressed historical context summaries with chronological links
- **Context Shifting Logic**: Intelligent distillation of old content using Distiller agent with gist creation
- **Documentation Structure**: Organized specs/ directories at root, backend, and anchor levels with spec.md, plan.md, tasks.md
- **Infinite Context Pipeline**: Complete end-to-end implementation enabling unlimited context window management

### Changed
- **Upgraded Context Windows**: All start scripts now default to 64k context for infinite work capability
- **Enhanced Memory Architecture**: Neo4j now stores both active memories and ContextGist historical summaries
- **Improved ContextManager**: Added check_and_rotate_context() logic with automatic gist creation and storage
- **Extended GraphReasoner**: Updated retrieval queries to include ContextGist nodes alongside regular memories
- **Optimized Distiller Integration**: Enhanced _chunk_and_distill functionality for context rotation use cases
- **Refined Archivist Agent**: Now coordinates context rotation and gist management operations

### Fixed
- **Context Limit Elimination**: Fixed issue where systems would crash when reaching context limits
- **Memory Continuity**: Resolved problems with historical context access across conversation boundaries
- **Performance Optimization**: Fixed inefficiencies in large context handling with 64k window support
- **Rotation Logic**: Fixed issues with context preservation during rotation cycles

---

## [0.9.0] - 2025-12-07 "Reka & Local Proxy"

### Added
- **Reka Configuration**: Full support for Reka-Flash-3-21B (Q4_K_S) with 16k context, stop tokens, and optimized LLaMa server flags.
- **Local API Proxy**: Added `scripts/local_api_proxy.py` to enforce static API keys for local LLaMa instances (fixes Cline extension "OpenAI API Key" requirement).
- **VS Code Integration**: Added `.vscode/settings.json` template and `VSCODE_CLINE_SETUP.md` for seamless local development.
- **MCP Health**: Added `/health` endpoint to Unified Launcher for better compatibility.

### Fixed
- **MCP Routing**: Resolved duplicate `/mcp` prefix in Unified Launcher routes (`/mcp/tools` is now accessible).
- **LLM Client**: Added `stop` token support to API payloads and local GGUF generation.

## [0.8.0] - 2025-12-06 "Archivist Protocol"

### Added
- **Archivist Ingestion**: Implemented `POST /archivist/ingest` endpoint to accept live data from the browser.
- **Memory Schema**: Enforced **Directive INJ-A1** (`PlaintextMemory`) for immutable "Page-Store" records.
- **Modular DOM Adapters**:
    - `GeminiAdapter`: Clean extraction for Google Gemini.
    - `ChatGPTAdapter`: Clean extraction for ChatGPT.
    - `ClaudeAdapter`: Clean extraction for Claude.ai.
    - `GenericAdapter`: Universal fallback for any webpage.
- **Extension UI**: Added **[Save to Memory]** button to the Side Panel for manual ingestion.

### Fixed
- **Encoding Crash**: Resolved Windows `charmap` error by enforcing `PYTHONIOENCODING='utf-8'`.
- **Server Stability**: Fixed startup crashes caused by `MemoryWeaver` resource contention.

## [0.7.0] - 2025-12-06 "Operation Concrete"

### Added
- **Browser Bridge**: A Chrome Extension (MV3) capable of:
    - **Voice**: Streaming chat interface via Side Panel.
    - **Sight**: Context injection (reading active tab).
    - **Hands**: JavaScript execution on active pages (User-ratified).
- **Backend Architecture**: Migrated from monolithic scripts to **Modular Recipes** (MAX Agentic Cookbook standard).
    - `CodaChatRecipe`: Handles orchestration, context, and tool execution.
- **Persistence**: Side panel now saves chat history to local storage.
- **Markdown Support**: Chat interface renders code blocks and syntax highlighting.

### Changed
- **Identity**: System formally renamed from "Sybil" to **"Coda"**.
- **Documentation**: Adopted `specs/` based documentation policy.

### Fixed
- **Audit Logger**: Patched critical `NameError` in streaming endpoints.
- **Security**: Hardened extension execution via `world: "MAIN"` to bypass strict CSP on some sites.

---

## [0.6.0] - 2025-11-30 "Operation MCP Integrated"

### Added
- **MCP Integration**: Complete integration of MCP server into main ECE Core server
- **Unified Endpoint**: All MCP functionality now available at `/mcp` on main server (port 8000)
- **Memory Tools**: Enhanced MCP tools for memory operations:
    - `add_memory` - Add to Neo4j memory graph
    - `search_memories` - Search memory graph with relationships
    - `get_summaries` - Get session summaries
- **Configuration**: New `mcp_enabled` setting in config.yaml to toggle integration
- **Authentication**: MCP endpoints now inherit main server authentication settings

### Changed
- **Architecture**: MCP server no longer runs as separate process, now integrated into main ECE server
- **Endpoints**: MCP tools now accessed via `/mcp/tools` and `/mcp/call` instead of separate server
- **Deployment**: Simplified deployment - no need to start separate MCP service
- **Resources**: Reduced memory footprint by eliminating duplicate server processes

### Fixed
- **Connection Issues**: Resolved intermittent connection failures between ECE and external MCP server
- **Latency**: Reduced tool call latency by eliminating inter-service communication overhead
- **Synchronization**: Fixed race conditions in concurrent tool executions

---

## [0.5.1] - 2025-11-29 "Memory Weaver Security Audit"

### Added
- **Security Hardening**: Added input validation for all GraphReasoner queries
- **Audit Trail**: Enhanced logging for all automated relationship repairs
- **Circuit Breakers**: Added fail safes for Weaver operations

### Changed
- **Weaver Engine**: Refactored to use parameterized queries, preventing Cypher injection
- **Permission Model**: Strengthened access controls for relationship modification operations

### Fixed
- **Cypher Injection**: Patched vulnerability in Neo4j relationship queries
- **Race Conditions**: Fixed concurrency issues in automated repair operations
- **Resource Exhaustion**: Added limits to prevent DoS via excessive repair requests

---

## [0.5.0] - 2025-11-28 "Memory Weaver (Automated Repair)"

### Added
- **Memory Weaver Engine**: Automated system for detecting and repairing broken relationships in Neo4j
- **Similarity Detection**: Embedding-based relationship discovery for linking related memories
- **Audit System**: Complete traceability for all automated repairs with `auto_commit_run_id`
- **Rollback Capability**: Deterministic reversal of automated changes via `rollback_commits_by_run.py`
- **Scheduler**: Background maintenance tasks for continuous graph integrity

### Changed
- **Graph Maintenance**: Automated relationship repair now runs as background process
- **Quality Assurance**: Enhanced relationship validation with similarity scoring
- **Traceability**: All automated changes now logged with unique run identifiers

### Fixed
- **Orphaned Nodes**: Automatically discovers and connects isolated memories
- **Broken Links**: Repairs missing relationships between related concepts
- **Data Drift**: Corrects inconsistent metadata across related nodes

---

## [0.4.0] - 2025-11-25 "Graph-R1 Implementation"

### Added
- **Graph Reasoner**: Iterative "Think → Query → Retrieve → Rethink" reasoning engine
- **Q-Learning Retrieval**: Reinforcement learning for optimized memory access patterns
- **Markovian Reasoning**: Chunked thinking with state preservation across context shifts
- **Multi-Hop Queries**: Complex graph traversal for answering compound questions
- **Cognitive Agents**: Plugin architecture for specialized reasoning tasks

### Changed
- **Retrieval Method**: Replaced simple vector search with Graph-R1 retrieval
- **Memory Access**: Graph-based traversal now primary method for context assembly
- **Agent Architecture**: Modular cognitive agents for specialized tasks
- **Context Building**: Enhanced context with relationship-aware retrieval

### Fixed
- **Context Relevance**: Improved precision of memory retrieval
- **Chain of Thought**: Better preservation of reasoning pathways
- **Memory Decay**: Reduced loss of historical context in long conversations

---

## [0.3.1] - 2025-11-20 "Security Hardening"

### Added
- **API Authentication**: Token-based authentication for all endpoints
- **Rate Limiting**: Request throttling to prevent abuse
- **Input Sanitization**: Enhanced validation for all user inputs
- **Audit Logging**: Comprehensive logging of all sensitive operations
- **Secure Defaults**: Safe configuration presets for common deployment scenarios

### Changed
- **Security Model**: Implemented zero-trust architecture
- **Credential Handling**: Secure storage and transmission of API keys
- **Access Controls**: Granular permissions for different API endpoints

### Fixed
- **Authentication Bypass**: Patched critical vulnerability in API access
- **Data Exposure**: Resolved information disclosure in error messages
- **Injection Attacks**: Fixed potential SQL injection in Neo4j queries

---

## [0.3.0] - 2025-11-15 "Neo4j Migration Complete"

### Added
- **Neo4j Integration**: Complete migration from SQLite to Neo4j graph database
- **Redis Cache**: Hot cache layer for active session management
- **Graph Schema**: Formal schema definition for memory relationships
- **Migration Tools**: Scripts to migrate existing SQLite data to Neo4j
- **Backup System**: Automated graph backup and restoration procedures

### Changed
- **Storage Architecture**: Tiered storage (Redis hot cache + Neo4j persistent)
- **Query Language**: Cypher queries for graph operations
- **Relationship Modeling**: Graph-based connections between memories
- **Indexing Strategy**: Graph-based indices for faster retrieval

### Fixed
- **Performance**: Significantly improved query performance for complex relationships
- **Scalability**: Better handling of large-scale memory graphs
- **Consistency**: Stronger data integrity with ACID-compliant transactions

---

## [0.2.0] - 2025-10-30 "Cognitive Agents"

### Added
- **Verifier Agent**: Fact-checking via empirical distrust protocol
- **Archivist Agent**: Memory maintenance and staleness detection
- **Distiller Agent**: Content summarization and extraction
- **Agent Framework**: Plugin system for extensible cognitive capabilities
- **Truth Scoring**: Provenance-aware fact-checking with primary source priority

### Changed
- **Memory Hygiene**: Automated maintenance of memory quality
- **Verification Process**: Evidence-based fact-checking system
- **Quality Assurance**: Continuous assessment of memory reliability
- **Maintenance Schedule**: Regular memory grooming operations

### Fixed
- **Hallucinations**: Reduced false information in responses
- **Stale Information**: Automatic detection and updating of outdated memories
- **Data Quality**: Improved content validation and cleaning procedures

---

## [0.1.0] - 2025-09-15 "Initial Architecture"

### Added
- **Core Backend**: Initial ECE_Core with SQLite memory system
- **Anchor Interface**: Terminal interface for user interaction
- **Basic Memory**: Text-based memory storage and retrieval
- **LLM Integration**: Support for various local LLM servers
- **Plugin System**: Extensible tool architecture (UTCP)

### Changed
- **Foundation**: Established core architecture patterns
- **API Design**: Defined RESTful API structure for components

### Fixed
- **Basic Functionality**: Initial implementation of core features

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\CHANGELOG.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\spec.md (Section: ROOT_SPECS) ---

# Context-Engine Specification

## Identity
- **Name**: Context-Engine
- **Role**: Executive Cognitive Enhancement (ECE) System
- **Philosophy**: Local-first, user-sovereign, agentic.

## Architecture Overview

The system follows a **Bridge-Core** architecture with three main components:

### 1. The Core (Brain) - backend/
- **Type**: Python FastAPI service
- **Logic**: Modular recipes (CodaChatRecipe for orchestration)
- **Memory**:
  - **Neo4j** (primary graph storage) - All memories, summaries, relationships
  - **Redis** (hot cache) - Active session management with 24h TTL
- **Agents**: Verifier (truth), Archivist (maintenance), Distiller (compression)
- **API**: FastAPI on port 8000

### 2. The Body (Interface) - anchor/
- **Type**: Terminal interface 
- **Communication**: HTTP/SSE to `localhost:8000`
- **State**: Local session persistence
- **Safety**: T-101 protocols with human confirmation flows

### 3. The Bridge (Browser Integration) - extension/
- **Type**: Chrome Extension (MV3)
- **Communication**: HTTP/SSE to `localhost:8000` 
- **State**: Local Storage (Persistence)
- **Capabilities**:
  - **Voice**: Streaming chat via Side Panel
  - **Sight**: Context injection (reading active tab)
  - **Memory**: **[Save to Memory]** button for Neo4j ingestion 
  - **Hands**: JavaScript execution (User-ratified)

## Infinite Context Pipeline Architecture

### Phase 1: Hardware Foundation
- **64k Context Window**: All LLM servers boot with 65,536 token capacity
- **GPU Optimization**: Full layer offload with Q8 quantized KV cache
- **Flash Attention**: Enabled for optimal performance with long contexts

### Phase 2: Context Rotation Protocol
- **Context Shifting**: Automatic rotation when context approaches 55k tokens
- **Intelligent Distillation**: Old context compressed to "Narrative Gists" using Distiller
- **Persistent Storage**: Gists stored in Neo4j as `:ContextGist` nodes with chronological links

### Phase 3: Graph-R1 Reasoning Enhancement
- **Gist Retrieval**: GraphReasoner searches `:ContextGist` nodes for historical context
- **Continuity Maintenance**: Maintains reasoning flow across context rotations
- **Smart Querying**: Enhanced retrieval with historical context awareness

## Memory Architecture (Current - Production)

### Neo4j Graph Database (port 7687) - PRIMARY
- **Purpose**: Permanent storage of memories, summaries, and relationships
- **Structure**: 
  - `(:Memory)` nodes with content, timestamp, importance, tags
  - `(:Summary)` nodes for distilled content
  - `[:RELATED_TO]`, `[:CAUSED_BY]`, `[:MENTIONS]` relationships for semantic connections
  - `[:NEXT_IN_SERIES]` relationships for chronological context gists
- **Features**: Full-text search, graph traversal, semantic queries

### Redis Cache (port 6379) - HOT CACHE  
- **Purpose**: Active session state and recent conversation cache
- **TTL**: 24-hour expiration for hot data
- **Content**: Recent exchanges, temporary context, session variables
- **Behavior**: Falls back to Neo4j when unavailable

## Cognitive Architecture: Agent System

### Verifier Agent
- **Role**: Truth-checking via Empirical Distrust
- **Method**: Provenance-aware scoring (primary sources > summaries)
- **Goal**: Reduce hallucinations, increase factual accuracy

### Distiller Agent  
- **Role**: Memory summarization and compression + Context Rotation
- **Method**: LLM-assisted distillation with salience scoring + context gist creation
- **Goal**: Maintain high-value context, prune noise, enable infinite context

### Archivist Agent
- **Role**: Knowledge base maintenance and freshness + Context Management
- **Method**: Scheduled verification, stale node detection, context rotation oversight
- **Goal**: Keep memory graph current and trustworthy, manage context windows

### Memory Weaver (Maintenance Engine)
- **Role**: Automated relationship repair
- **Method**: Embedding-based similarity with audit trail (`auto_commit_run_id`)
- **Goal**: Maintain graph integrity with full traceability

## Tool Integration Architecture

### UTCP (Simple Tool Mode) - Current
- **Discovery**: Plugin-based via `backend/plugins/` directory
- **Plugins**: 
  - `web_search` - DuckDuckGo search
  - `filesystem_read` - File and directory operations  
  - `shell_execute` - Shell command execution (with safety checks)
  - `mgrep` - Semantic code search
- **Execution**: Pattern-based for <14B models, structured for >14B models

### MCP Integration - Now Part of Main Server
- **Location**: Integrated into main ECE server when `mcp.enabled: true`
- **Endpoints**: `/mcp/tools`, `/mcp/call` on main port (8000)
- **Tools**:
  - `add_memory` - Add to Neo4j graph
  - `search_memories` - Graph search with relationships  
  - `get_summaries` - Session summary retrieval

## API Interface

### Core Endpoints (Port 8000)
- `POST /chat/stream` - Streaming conversation with full memory context
- `POST /archivist/ingest` - Ingest content to Neo4j memory graph
- `GET /health` - Server health check
- `GET /models` - Available models information
- `POST /mcp/call` - Memory tool operations (when MCP enabled)

### Security
- **API Keys**: Optional token authentication for all endpoints
- **Rate Limiting**: Request throttling to prevent abuse
- **Input Sanitization**: Validation for all user inputs

## Development & Deployment

### Requirements
- **Python**: 3.11+ 
- **Neo4j**: Graph database (local or remote)
- **Redis**: Cache server (recommended)
- **llama.cpp**: Server for local LLMs

### Startup Architecture
- **3-Script Model**:
  1. `python start_llm_server.py` - Interactive LLM with 64k context
  2. `python start_ece.py` - ECE Core with MCP and cognitive agents  
  3. `python start_embedding_server.py` - Optional embedding server

## Technology Stack

### Backend (Python)
- **Framework**: FastAPI
- **Database**: Neo4j (graph), Redis (cache)
- **Models**: llama.cpp server integration
- **Tools**: UTCP plugin system

### Frontend Components
- **Anchor**: Pure Python CLI with streaming
- **Extension**: Manifest V3 Chrome Extension with Side Panel UI

## Small Model Considerations

### Tool Usage
- ⚠️ Models < 14B: Use "Simple Tool Mode" (pattern-based execution)
- ✅ Models ≥ 14B: Full structured protocol support
- ✅ MCP Tools: Work with any model for memory operations

### Recommended Models
- **Gemma-3 4B** - Speed (chat only, tools unreliable)
- **Qwen3-8B** - Reasoning (Simple Tool Mode works)  
- **DeepSeek-R1-14B** - Tools (full structured protocol support)
- **Reka Flash 3 21B** - Reasoning (use start-reka.bat)

## Performance Optimization

### Context Windows
- **64k Context**: Full capacity for infinite work capability
- **Rotation Threshold**: 55k tokens triggers automatic context rotation
- **Gist Creation**: Old content compressed to maintain continuity

### Memory Management
- **Hot Cache**: Redis for active sessions (24h TTL)
- **Cold Storage**: Neo4j for permanent memories
- **Automatic Cleanup**: Scheduled pruning of expired sessions

---

## Research Foundation

- **Graph-R1**: Memory retrieval patterns (https://arxiv.org/abs/2507.21892)  
- **Markovian Reasoning**: Chunked thinking (https://arxiv.org/abs/2506.21734)
- **Hierarchical Reasoning Model (HRM)**: Multi-level context processing
- **Empirical Distrust**: Primary source supremacy for verification
- **Infinite Context Pipeline**: Hardware-software context rotation protocol

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\spec.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\plan.md (Section: ROOT_SPECS) ---

# Context-Engine Implementation Plan

## Vision Statement

Create a truly **infinite context** cognitive augmentation system that never runs out of memory space while maintaining perfect continuity of thought across conversations. Transform the user's mind from "limited by the current context window" to "augmented by an ever-growing, searchable, and contextually-aware memory system."

## Strategic Objectives

### Primary Goal: Infinite Context Pipeline
Build a cognitive system that can theoretically process infinite amounts of text while maintaining contextual awareness and reasoning continuity.

### Secondary Goals:
1. **Local Sovereignty**: Complete data ownership and processing on personal hardware
2. **Cognitive Enhancement**: Assist with memory, reasoning, and executive function tasks
3. **Seamless Integration**: Natural interaction through CLI, browser extension, and tools

## Implementation Phases

### Phase 1-4: Foundation (COMPLETED)
- Core Memory System (Neo4j/Redis tiered architecture)
- Basic Chat API with context management
- Cognitive agents (Verifier, Archivist, Distiller)
- MCP integration into main server

### Phase 5: Infinite Context Pipeline (CURRENT)
- **Hardware Foundation**: 64k context windows across all servers (COMPLETED)
- **Context Rotation Protocol**: Automatic rotation when approaching limits (COMPLETED)
- **Graph-R1 Enhancement**: Historical context retrieval (COMPLETED)
- **Continuity Maintenance**: Seamless transitions across rotations (COMPLETED)

### Phase 6: Consolidation (COMPLETED)
- Documentation reset to `specs/` policy
- Directory cleanup and artifact packaging
- Extension integration with Core API

### Phase 7: Optimization (IN PROGRESS) 
- Vector adapter + C2C hot-replica for semantic retrieval
- Compressed summaries + passage recall (EC-T-133)
- SLM benchmarking and ALScore measurements

### Phase 8: Expansion (FUTURE)
- **Tooling**: Deep integration with OS-level tools
- **Multimodal**: Vision and Audio input capabilities  
- **Federation**: Connecting multiple Context-Engine instances
- **Mobile**: Native mobile applications
- **Edge Deployment**: Optimized for embedded devices

## Technical Implementation Priorities

### Current Focus (Phase 7): Optimization & Enhancement
1. **Vector Adapter Integration**
   - Implement VectorDB interface for semantic search
   - Hot-replica capability for instant context availability
   - Cross-modal embedding alignment (text-to-image, etc.)

2. **Compressed Memory Architecture** 
   - Compressed summary generation and storage
   - Passage-level recall from compressed representations
   - Lossless reconstruction algorithms

3. **Performance Measurement**
   - SLM benchmarking with standardized tasks
   - ALScore implementation for algorithmic latency measurement
   - Memory retrieval accuracy assessments

### Future Considerations (Phase 8+)
- **Distributed Architecture**: Horizontal scaling capabilities
- **Privacy-Preserving Computation**: Homomorphic encryption for sensitive data
- **Advanced Reasoning**: Multi-agent collaboration and debate protocols
- **Quantum-Ready Architecture**: Preparing for hybrid classical/quantum systems

## Research Foundation

### Core Concepts Being Validated
- **Graph-R1 Reasoning**: Whether iterative graph traversal improves memory recall (VALIDATED)
- **Markovian Memory**: Whether chunked thinking with state preservation works (VALIDATED)  
- **Empirical Distrust**: Whether provenance-aware verification reduces hallucinations (VALIDATED)
- **Infinite Context Pipeline**: Whether hardware + software context rotation enables unlimited work (VALIDATED)

### New Research Directions
- **Continuous Learning**: How to update knowledge graph while system is active
- **Cross-Modal Memory**: Associating text, images, audio in unified memory space
- **Quantum-Inspired Retrieval**: Quantum-like superposition in memory search
- **Distributed Consciousness**: Multi-node cognitive architecture

## Business Model Alignment

### Primary Users: Executive Function Support
- Developers with ADHD/autism who need external memory systems
- Researchers needing to track complex, long-term projects
- Writers building elaborate, interconnected story worlds

### Value Propositions  
1. **Never Lose Context**: Infinite conversation and document processing
2. **Perfect Memory**: Every interaction preserved and searchable
3. **Sovereign Data**: Your thoughts remain your property
4. **Local Processing**: Works offline, no cloud dependency

### Market Positioning
- **Alternative to**: Cloud-based AI assistants (ChatGPT, Claude)
- **Differentiation**: Local-first with infinite memory, not limited by context windows
- **Complement to**: Any workflow requiring long-term memory and reasoning

## Risk Assessment

### Technical Risks
- **Memory Corruption**: Mitigated by audit trails and rollback capabilities
- **Performance Degradation**: Managed through active maintenance and pruning
- **Hardware Limits**: Addressed through context rotation and compression

### Adoption Risks  
- **Complexity**: Mitigated through excellent documentation and CLI automation
- **Privacy Concerns**: Addressed by 100% local processing default
- **Tool Reliability**: Managed through safety layers and human confirmation

### Competitive Risks
- **Cloud AI Services**: Differentiated through local sovereignty and infinite context
- **Other Local Solutions**: Ahead with Graph-R1 and infinite context pipeline
- **Enterprise Solutions**: Positioned for individual knowledge workers, not corporations

## Success Metrics

### Technical Metrics
- **Context Window**: Achieved 64k effective capacity with infinite rotation
- **Memory Accuracy**: >95% retrieval accuracy for stored information
- **Response Latency**: <2s for context-rich queries
- **System Uptime**: >99% availability for local deployment

### User Experience Metrics
- **Session Length**: Users engaging in conversations >1 hour continuously
- **Memory Retention**: Users successfully retrieving information from weeks/months ago
- **Productivity Impact**: Measurable improvement in task completion and context management
- **Privacy Satisfaction**: 100% of data remaining local to user's device

## Timeline & Milestones

### Phase 5 Milestones (Infinite Context Pipeline) - COMPLETED
- [x] 64k context windows on all servers - Dec 2025
- [x] Context rotation protocol implementation - Dec 2025  
- [x] Graph-R1 historical context retrieval - Dec 2025
- [x] Continuity maintenance across rotations - Dec 2025

### Phase 7 Milestones (Optimization) - IN PROGRESS
- [ ] Vector adapter integration - Jan 2026
- [ ] Compressed summary architecture - Feb 2026
- [ ] SLM benchmarking framework - Mar 2026

### Phase 8 Milestones (Expansion) - PLANNED
- [ ] Tooling integration framework - Q2 2026
- [ ] Mobile applications - Q3 2026
- [ ] Federation protocol - Q4 2026

## Strategic Partnerships

### Potential Collaborations
- **Hardware Manufacturers**: Optimization for specific GPUs and NPUs
- **Research Institutions**: Validation of Graph-R1 and Markovian reasoning
- **Open Source Communities**: Tool integration and model development
- **Neuroscience Labs**: Cognitive architecture validation studies

## Ethical Framework

### Core Principles
1. **User Sovereignty**: All data belongs to and remains with the user
2. **Cognitive Liberty**: Enhancement without control or manipulation
3. **Transparency**: Clear visibility into how the system processes information
4. **Autonomy**: Tools that enhance human decision-making, not replace it

### Implementation Guidelines
- Open source codebase with MIT license
- Local processing by default, no telemetry
- Clear audit trail for all automated operations
- Human confirmation for all autonomous changes

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\plan.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\tasks.md (Section: ROOT_SPECS) ---

# Context-Engine Implementation Tasks

## Current Work Queue (Phase 7: Optimization)

### Active Development - Vector Adapter & C2C Replication
- [ ] Implement VectorAdapter interface for semantic search (EC-V-101)
  - [ ] Define abstract base class with required methods
  - [ ] Implement Redis VectorAdapter with HNSW indexing
  - [ ] Implement FAISS VectorAdapter for local deployment
  - [ ] Performance benchmarking against graph-only retrieval
  - [ ] Integration testing with ContextManager

- [ ] C2C (Context-to-Context) Hot-Replica System (EC-C2C-102)  
  - [ ] Define hot-replica synchronization protocol
  - [ ] Implement real-time vector index updates
  - [ ] Cross-validation between graph and vector retrieval
  - [ ] Automatic failover from vector to graph when needed

### Active Development - Compressed Summaries
- [ ] Compressed Summary Architecture (EC-CS-133)
  - [ ] Implement summary generation pipeline with salience scoring
  - [ ] Design passage recall mechanism from compressed representations
  - [ ] Optimize compression ratios vs. information retention
  - [ ] Integration with ContextGist rotation system

### Active Development - SLM Benchmarking
- [ ] SLM (Small Language Model) Benchmark Suite (EC-BM-155)
  - [ ] Implement ALScore (Augmentation Latency Score) measurement
  - [ ] Standardized benchmarks for memory-augmented tasks
  - [ ] Performance comparison across model architectures (Gemma, Qwen, Llama)
  - [ ] Optimization recommendations for different hardware configurations

## Upcoming Priorities (Phase 8: Expansion)

### Tooling Integration Framework
- [ ] OS-Level Tool Integration (EC-TI-201) 
  - Define standardized interfaces for filesystem, clipboard, window management
  - Security hardening for native tool execution
  - Performance optimization for frequent small operations

### Multimodal Capabilities  
- [ ] Vision Input System (EC-VIS-202)
  - Image embedding and storage in Neo4j
  - Visual context injection for conversations
  - OCR integration for document processing

- [ ] Audio Processing Module (EC-AUD-203)
  - Speech-to-text for voice input
  - Audio embedding for multimodal memory
  - Text-to-speech for voice output

### Federation Protocol
- [ ] Distributed Context Engine Network (EC-FED-204)
  - Secure peer-to-peer communication protocol
  - Cross-instance memory sharing with privacy controls
  - Conflict resolution for concurrent modifications

## Backlog (Future Considerations)

### Mobile Deployment
- [ ] Android Application (EC-MOB-301)
- [ ] iOS Application (EC-MOB-302) 
- [ ] Cross-platform UI framework evaluation (React Native vs. Flutter)

### Advanced Reasoning
- [ ] Multi-Agent Collaboration (EC-MA-303)
- [ ] Debate Protocols (EC-DEB-304)
- [ ] Metacognitive Awareness (EC-MET-305)

### Privacy & Security
- [ ] Homomorphic Encryption for Sensitive Data (EC-PRIV-306)
- [ ] Zero-Knowledge Proofs for Verification (EC-ZKP-307)
- [ ] Differential Privacy for Statistical Queries (EC-DP-308)

## Completed Recently (Phase 5: Infinite Context Pipeline)

### ✅ Hardware Foundation (EC-HW-101)
- [x] Upgrade LLM servers to 64k context window (Dec 2025)
- [x] Flash Attention optimization for long contexts (Dec 2025)
- [x] KV cache optimization with Q8 quantization (Dec 2025)

### ✅ Context Rotation Protocol (EC-CRP-102)
- [x] ContextManager monitoring of 55k token threshold (Dec 2025)
- [x] Distiller integration for content compression (Dec 2025)
- [x] Neo4j storage for ContextGist nodes (Dec 2025)
- [x] Chronological linking of gists with [:NEXT_GIST] (Dec 2025)

### ✅ Graph-R1 Enhancement (EC-GR1-103)
- [x] GraphReasoner retrieval of ContextGist nodes (Dec 2025)
- [x] Historical context integration in reasoning loop (Dec 2025)
- [x] Continuity maintenance across rotations (Dec 2025)

### ✅ System Integration & Testing
- [x] End-to-end testing of infinite context pipeline (Dec 2025)
- [x] Performance benchmarking with 30k+ token inputs (Dec 2025)
- [x] Memory continuity verification across rotation boundaries (Dec 2025)

## Maintenance Tasks

### Ongoing
- [ ] Security audit of all HTTP endpoints and API calls
- [ ] Performance monitoring of Neo4j queries and Redis operations
- [ ] Documentation updates for new features and APIs
- [ ] Dependency updates and vulnerability scans

### Monthly
- [ ] Review and clean up old ContextGist nodes to prevent unbounded growth
- [ ] Verify backup and recovery procedures for Neo4j and Redis
- [ ] Update HuggingFace model references and fallback URLs
- [ ] Test with latest llama.cpp builds for new features and optimizations

## Known Issues & Technical Debt

### Performance
- [ ] Neo4j query optimization for large graph traversal (EC-PERF-001)
- [ ] Redis memory usage monitoring and cleanup (EC-PERF-002)
- [ ] Context rotation timing optimization to minimize disruption (EC-PERF-003)

### Reliability
- [ ] Fallback mechanisms when Neo4j is temporarily unavailable (EC-REL-001)
- [ ] Retry logic for failed ContextGist creations during high load (EC-REL-002)
- [ ] Graceful degradation when ContextGist retrieval fails (EC-REL-003)

### Usability
- [ ] Progress indicators during large context rotation operations (EC-USAB-001)
- [ ] User notifications about automatic context rotation events (EC-USAB-002)
- [ ] Configurable rotation thresholds based on model capabilities (EC-USAB-003)

## Research Tasks

### Active Research
- [ ] Evaluation of different compression algorithms for ContextGist generation (EC-RES-001)
- [ ] Comparison of rotation strategies (oldest-first vs. least-relevant-first) (EC-RES-002)
- [ ] Investigation of hybrid retrieval (graph + vector + keyword) effectiveness (EC-RES-003)

### Planned Research
- [ ] Long-term memory stability testing over 6+ month periods (EC-RES-004)
- [ ] Cognitive load measurement with infinite vs. finite context systems (EC-RES-005)
- [ ] User productivity impact assessment with comprehensive usage analytics (EC-RES-006)

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\specs\tasks.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\README.md (Section: BACKEND) ---

# ECE_Core Backend

> **Executive Cognitive Enhancement (ECE)** - Backend component for the Context-Engine

**Philosophy**: Your mind, augmented. Your data, sovereign. Your tools, open.

---

## Architecture: The Brain

The ECE_Core backend is the cognitive engine of the Context-Engine system. It manages the memory system, reasoning engine, and cognitive orchestration.

### Memory Architecture
- **Neo4j** (port 7687) - PRIMARY STORAGE
  - All memories, summaries, relationships in graph format
  - ContextGist nodes for historical compressed context
  - Node types: `(:Memory)`, `(:Event)`, `(:Person)`, `(:Idea)`, `(:Code)`, `(:ContextGist)`
  - Relationship types: `[:RELATED_TO]`, `[:MENTIONS]`, `[:CAUSED_BY]`, `[:NEXT_GIST]`
- **Redis** (port 6379) - ACTIVE SESSION CACHE
  - Hot cache for active conversations (24h TTL)
  - Graceful fallback to Neo4j if unavailable

### Cognitive Architecture
- **Verifier Agent**: Truth-checking via Empirical Distrust
- **Distiller Agent**: Memory summarization, compression + Context Rotation Protocol
- **Archivist Agent**: Knowledge base maintenance and freshness + Context Continuity
- **Memory Weaver**: Automated relationship repair with full traceability

### Reasoning Architecture
- **Graph-R1 Reasoning Pattern**: "Think → Query → Retrieve → Rethink" iteration
- **Markovian Reasoner**: Infinite-length task handling with state preservation
- **Hybrid Retrieval**: Vector + Graph + Full-text search with ContextGist integration

---

## Quick Start

### Installation
```bash
cd backend
pip install -e .
```

### Configuration
- **Primary Config**: `backend/.env` (from `.env.example`)
- **LLM Settings**: Context size, GPU layers, model path
- **Memory Settings**: Redis/Neo4j connection strings
- **Agent Settings**: Enable/disable Verifier, Archivist, Distiller

### Run Server
```bash
# Start ECE_Core server
python launcher.py
# Server runs on http://localhost:8000
```

---

## Key Features

### ✅ Infinite Context Pipeline
- **64k Context Windows**: All servers configured with 65,536 token capacity
- **Context Rotation Protocol**: Automatic rotation when context approaches 55k tokens
- **Intelligent Distillation**: Old context compressed to "Narrative Gists" using Distiller agent
- **Historical Continuity**: ContextGist nodes maintain reasoning across rotations

### ✅ Cognitive Agents
- **Truth Verification**: Provenance-aware fact-checking
- **Memory Hygiene**: Automatic summarization and maintenance
- **Relationship Repair**: Automated graph integrity maintenance with audit trail

### ✅ Tool Architecture
- **Plugin System**: UTCP-based tool system in `plugins/` directory
- **Safety Layers**: Whitelist/blacklist with human confirmation for dangerous operations
- **MCP Integration**: Memory tools via integrated MCP endpoints

---

## Development

### Run Tests
```bash
python -m pytest tests/
```

### Package Distribution
```bash
python -m build
```

---

## Documentation

- `specs/spec.md` - Technical architecture and design
- `specs/plan.md` - Vision, roadmap, and strategic priorities
- `specs/tasks.md` - Implementation backlog and current tasks
- `specs/TROUBLESHOOTING.md` - Operational debugging and error resolution

---

## Research Foundation

- **Graph-R1**: Memory retrieval patterns with iterative graph traversal
- **Markovian Reasoning**: Chunked thinking with state preservation
- **Hierarchical Reasoning Model (HRM)**: Multi-level context processing
- **Empirical Distrust**: Primary source supremacy for verification

---

## Target Users

- **Cognitive Enhancement**: Users needing personal external memory systems
- **Privacy-Conscious**: Users wanting 100% local, zero-telemetry systems
- **AI Developers**: Users needing extensible, memory-enhanced workflows

---

## Acknowledgments

Built for the cognitive architecture that bridges human and machine intelligence.

**"Your data, sovereign. Your tools, open. Your mind, augmented."**

---

## Need Help?

- **Architecture Questions**: See `specs/spec.md`
- **Vision & Roadmap**: See `specs/plan.md`
- **Current Tasks**: See `specs/tasks.md`
- **Troubleshooting**: See `specs/TROUBLESHOOTING.md`

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\README.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\app_factory.py (Section: BACKEND_PYTHON) ---

"""Small wrapper to provide a clean app import for tests and tools.
This module avoids pulling in legacy `src.main` content and provides a
single `app` object to import in tests or external tooling.
"""
from src.bootstrap import create_app
from src.config import settings
import logging

logging.basicConfig(level=getattr(logging, settings.ece_log_level), format='%(asctime)s - %(levelname)s - %(message)s')

def create_app_with_routers():
	"""Create the FastAPI app and include all API routers.

	Use this factory to create an app instance after pytest autouse fixtures have run
	to ensure that patches (fake LLM, fake Redis) are applied before the app lifecycle
	creates real clients.
	"""
	app = create_app()
	from src.api import (
	memory_router,
	reason_router,
	health_router,
	# openai_router,
	plugins_router,
	audit_router,
    plan_router,
	)

	# Import browser bridge plugin
	from src.plugins.browser_bridge.plugin import router as browser_bridge_router
	# Import Coda Chat Recipe
	from src.recipes.coda_chat import router as coda_chat_router
	# Import Archivist Recipe
	from src.recipes.archivist import router as archivist_router

	from fastapi import Depends
	from src.security import verify_api_key

	app.include_router(health_router)  # Public
	app.include_router(health_router)  # Public
	# app.include_router(openai_router, dependencies=[Depends(verify_api_key)])
	app.include_router(reason_router, dependencies=[Depends(verify_api_key)])
	app.include_router(plugins_router, dependencies=[Depends(verify_api_key)])
	app.include_router(audit_router, dependencies=[Depends(verify_api_key)])
	app.include_router(plan_router, dependencies=[Depends(verify_api_key)])

	# Include browser bridge router
	app.include_router(browser_bridge_router)

	# Temporarily remove dependencies to debug 403
	app.include_router(coda_chat_router, prefix="/chat") #, dependencies=[Depends(verify_api_key)])

	# Include Archivist Recipe
	app.include_router(archivist_router, prefix="/archivist", dependencies=[Depends(verify_api_key)])

	return app


# Backwards compatible app instance for import-time use (rare)
app = None


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\app_factory.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\bootstrap.py (Section: BACKEND_PYTHON) ---

"""Application bootstrap: provide create_app() with lifecycle initialization.

This module centralizes the app startup and shutdown lifecycle so `main.py`
becomes a thin routing module while the heavy initialization logic lives
here. The components are stored in `app.state` to be accessible from routes.
"""
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, Any
from src.config import settings
from src.security import audit_logger
from src.memory import TieredMemory
from src.context import ContextManager
from src.intelligent_chunker import IntelligentChunker
from src.distiller import Distiller
from src.graph import GraphReasoner, MarkovianReasoner
from src.agents import VerifierAgent, ArchivistAgent
from src.agents.planner import PlannerAgent
try:
    from plugins.manager import PluginManager
except Exception:
    PluginManager = None
from src.tool_call_models import ToolCallParser, ToolCallValidator
from src.tools import ToolExecutor

logger = logging.getLogger(__name__)


def create_app() -> FastAPI:
    """Create FastAPI app with initialized components stored in app.state."""
    app = FastAPI(title="ECE_Core", version=settings.ece_version)

    @asynccontextmanager
    async def lifespan(app: FastAPI):
        logger.info("Starting ECE_Core with Markovian reasoning... (bootstrap)")
        memory = TieredMemory()
        await memory.initialize()

        mem_status = []
        if memory.redis and memory.redis.redis:
            mem_status.append("Redis")
        if memory.neo4j and memory.neo4j.neo4j_driver:
            mem_status.append("Neo4j")
        mem_str = " + ".join(mem_status) if mem_status else "No backends connected!"
        logger.info(f"Memory initialized ({mem_str})")

        # Import LLMClient here so unit tests can patch `src.llm.LLMClient` before booting the app
        from src.llm import LLMClient
        llm = LLMClient()
        logger.info("LLM client ready")
        context_mgr = ContextManager(memory, llm)
        logger.info("Context manager ready")
        chunker = IntelligentChunker(llm)
        logger.info("Intelligent chunker ready")
        distiller = Distiller(llm)
        logger.info("Distiller ready")
        graph_reasoner = GraphReasoner(memory, llm)
        logger.info("Graph reasoner ready (memory retrieval)")
        markov_reasoner = MarkovianReasoner(llm)
        logger.info("Markovian reasoner ready (chunked processing)")
        verifier_agent = VerifierAgent(memory, llm)
        logger.info("Verifier agent ready (Empirical Distrust)")
        archivist_agent = ArchivistAgent(memory, verifier_agent)
        # Start archivist background loop
        await archivist_agent.start()
        logger.info("Archivist agent ready (Maintenance Loop)")

        tool_parser = ToolCallParser()
        logger.info("Tool call parser ready (Pydantic validation)")
        tool_validator = None
        mcp_client = None
        plugin_manager = None

        # Initialize plugin manager (preferred) or MCP client for tools
        if PluginManager:
            plugin_manager = PluginManager(settings.__dict__)
            discovered = plugin_manager.discover()
            if discovered:
                logger.info(f"Plugin manager loaded plugins: {', '.join(discovered)}")
                tools = plugin_manager.list_tools()
                if tools:
                    tools_dict = {tool['name']: tool for tool in tools}
                    tool_validator = ToolCallValidator(tools_dict)
                    logger.info("Tool validator ready (via plugins)")
            else:
                logger.warning("Plugin manager enabled but no plugins discovered (tools disabled)")
        else:
            logger.warning("PluginManager not available (tools disabled)")

        # Initialize MCP client if configured
        if settings.mcp_enabled:
            try:
                from src.mcp_client import MCPClient as _MCPClient
                mcp_client = _MCPClient()
                logger.info("MCP client initialized for %s", mcp_client.base_url)
            except Exception as e:
                logger.warning("MCP client could not be initialized: %s", e)

        # Store components in app.state
        app.state.memory = memory
        app.state.llm = llm
        app.state.context_mgr = context_mgr
        app.state.chunker = chunker
        app.state.distiller = distiller
        app.state.graph_reasoner = graph_reasoner
        app.state.markov_reasoner = markov_reasoner
        app.state.verifier_agent = verifier_agent
        app.state.archivist_agent = archivist_agent
        app.state.plugin_manager = plugin_manager
        app.state.audit_logger = audit_logger
        app.state.tool_parser = tool_parser
        app.state.tool_validator = tool_validator
        # Planner agent
        planner_agent = PlannerAgent(llm)
        app.state.planner = planner_agent

        logger.info(f"ECE_Core running at http://{settings.ece_host}:{settings.ece_port}")
        try:
            yield
        finally:
            logger.info("Shutting down (bootstrap)...")
            try:
                await archivist_agent.stop()
            except Exception:
                pass
            try:
                await memory.close()
            except Exception:
                pass
            try:
                await llm.close()
            except Exception:
                pass

    app = FastAPI(title="ECE_Core", version=settings.ece_version, lifespan=lifespan)

    # DEBUG: Log all requests
    @app.middleware("http")
    async def log_requests(request, call_next):
        logger.info(f"Incoming request: {request.method} {request.url}")
        logger.info(f"Headers: {request.headers}")
        response = await call_next(request)
        logger.info(f"Response status: {response.status_code}")
        return response

    # Configure CORS - Permissive for Debugging
    app.add_middleware(
        CORSMiddleware,
        allow_origin_regex='.*',  # Allow ANY origin matching this regex
        allow_credentials=True,   # Allow cookies/auth headers
        allow_methods=["*"],
        allow_headers=["*"],
    )

    return app


def get_components(app: FastAPI) -> dict:
    """Return a dict of initialized components for convenience in routes.
    """
    return {
        "memory": getattr(app.state, "memory", None),
        "llm": getattr(app.state, "llm", None),
        "context_mgr": getattr(app.state, "context_mgr", None),
        "chunker": getattr(app.state, "chunker", None),
        "distiller": getattr(app.state, "distiller", None),
        "graph_reasoner": getattr(app.state, "graph_reasoner", None),
        "markov_reasoner": getattr(app.state, "markov_reasoner", None),
        "verifier_agent": getattr(app.state, "verifier_agent", None),
        "archivist_agent": getattr(app.state, "archivist_agent", None),
        "plugin_manager": getattr(app.state, "plugin_manager", None),
        "tool_parser": getattr(app.state, "tool_parser", None),
        "tool_validator": getattr(app.state, "tool_validator", None),
        "audit_logger": getattr(app.state, "audit_logger", None),
    }


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\bootstrap.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\chat_templates.py (Section: BACKEND_PYTHON) ---

"""
Chat template management for different LLM models.
Supports various chat formats including Qwen3, Gemma3, and standard OpenAI format.
"""
from typing import List, Dict, Optional
from jinja2 import Template


class ChatTemplate:
    """Base class for chat templates"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        raise NotImplementedError()


class OpenAIChatTemplate(ChatTemplate):
    """Standard OpenAI chat format"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # For API compatibility, we just return the messages as-is with system prompt added
        formatted_messages = []
        if system_prompt:
            formatted_messages.append({"role": "system", "content": system_prompt})
        formatted_messages.extend(messages)
        return formatted_messages


class Qwen3ChatTemplate(ChatTemplate):
    """Qwen3 chat template format - direct implementation"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Add system prompt to messages if provided
        all_messages = []
        if system_prompt and not (messages and messages[0].get("role") == "system"):
            all_messages.append({"role": "system", "content": system_prompt})
        all_messages.extend(messages)

        result = []

        if tools:
            # Include tools in the system message
            result.append("SYSTEM")
            if all_messages and all_messages[0].get("role") == "system":
                result.append(all_messages[0]["content"])
                result.append("")  # Empty line
            result.append("In this environment you have access to a set of tools you can use to answer the user's question. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.")
            result.append("")
            result.append("Tool Use Rules")
            result.append("Here are the rules you should always follow to solve your task:")
            result.append("1. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.")
            result.append("2. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.")
            result.append("3. If no tool call is needed, just answer the question directly.")
            result.append("4. Never re-do a tool call that you previously did with the exact same parameters.")
            result.append("5. For tool use, MARK SURE use XML tag format as shown in the examples above. Do not use any other format.")
            result.append("Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.")
            result.append("")
            result.append("# Tools")
            result.append("")
            result.append("You may call one or more functions to assist with the user query.")
            result.append("")
            result.append("You are provided with function signatures within <tools></tools> XML tags:")
            result.append("<tools>")
            for tool in tools:
                import json
                result.append(json.dumps(tool))
            result.append("</tools>")
            result.append("")
            result.append("For each function call, return a json object with function name and arguments within ")
            result.append(" XML tags:")
            result.append("")
            result.append("")


            # Process remaining messages (skip the system message we already handled)
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for i, message in enumerate(all_messages[start_idx:]):
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user" or (role == "system" and i > 0):  # Additional system messages after first
                    result.append(f"USER")
                    result.append(content)
                    result.append("")
                elif role == "assistant":
                    result.append("ASSISTANT")
                    if message.get("tool_calls"):
                        # Add tool calls in the specified format
                        for tool_call in message["tool_calls"]:
                            function = tool_call.get("function", tool_call)
                            # Ensure arguments are properly formatted as JSON string
                            args = function["arguments"]
                            if isinstance(args, str):
                                # If arguments are already a string, use-as-is
                                formatted_args = args
                            else:
                                # If arguments are a dict/object, convert to JSON string
                                import json
                                formatted_args = json.dumps(args)
                            result.append(f"")
                            result.append(f'{{"name": "{function["name"]}", "arguments": {formatted_args}}}')
                        result.append(content)  # Add content after tool calls
                    else:
                        result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append("USER")
                    result.append(content)
                    result.append("")
        else:
            # No tools version
            if all_messages and all_messages[0].get("role") == "system":
                result.append("SYSTEM")
                result.append(all_messages[0]["content"])
                result.append("USER")
            else:
                result.append("SYSTEM")
                result.append("You are a helpful assistant.")
                result.append("USER")

            # Process remaining messages
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for message in all_messages[start_idx:]:
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user":
                    result.append(content)
                    result.append("ASSISTANT")
                elif role == "assistant":
                    result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append(content)
                    result.append("USER")

        return "\n".join(result).strip()


class Qwen3ThinkingChatTemplate(ChatTemplate):
    """Qwen3 chat template with explicit thinking token support"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Add system prompt to messages if provided
        all_messages = []
        if system_prompt and not (messages and messages[0].get("role") == "system"):
            all_messages.append({"role": "system", "content": system_prompt})
        all_messages.extend(messages)

        # Build the formatted conversation with explicit thinking tokens
        result = []

        if tools:
            # Include tools in the system message
            result.append("SYSTEM")
            if all_messages and all_messages[0].get("role") == "system":
                result.append(all_messages[0]["content"])
                result.append("")  # Empty line
            result.append("In this environment you have access to a set of tools you can use to answer the user's question. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.")
            result.append("")
            result.append("Tool Use Rules")
            result.append("Here are the rules you should always follow to solve your task:")
            result.append("1. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.")
            result.append("2. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.")
            result.append("3. If no tool call is needed, just answer the question directly.")
            result.append("4. Never re-do a tool call that you previously did with the exact same parameters.")
            result.append("5. For tool use, MARK SURE use XML tag format as shown in the examples above. Do not use any other format.")
            result.append("Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.")
            result.append("")
            result.append("# Tools")
            result.append("")
            result.append("You may call one or more functions to assist with the user query.")
            result.append("")
            result.append("You are provided with function signatures within <tools></tools> XML tags:")
            result.append("<tools>")
            for tool in tools:
                import json
                result.append(json.dumps(tool))
            result.append("</tools>")
            result.append("")
            result.append("For each function call, return a json object with function name and arguments within ")
            result.append(" XML tags:")
            result.append("")


            # Process remaining messages (skip the system message we already handled)
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for i, message in enumerate(all_messages[start_idx:]):
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user" or (role == "system" and i > 0):  # Additional system messages after first
                    result.append(f"USER")
                    result.append(content)
                    result.append("")
                elif role == "assistant":
                    result.append("ASSISTANT")
                    if message.get("tool_calls"):
                        # Add tool calls in the specified format
                        for tool_call in message["tool_calls"]:
                            function = tool_call.get("function", tool_call)
                            # Ensure arguments are properly formatted as JSON string
                            args = function["arguments"]
                            if isinstance(args, str):
                                # If arguments are already a string, use-as-is
                                formatted_args = args
                            else:
                                # If arguments are a dict/object, convert to JSON string
                                import json
                                formatted_args = json.dumps(args)
                            result.append(f"")
                            result.append(f'{{"name": "{function["name"]}", "arguments": {formatted_args}}}')
                        result.append(content)  # Add content after tool calls
                    else:
                        result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append("USER")
                    result.append(content)
                    result.append("")
        else:
            # No tools version - with explicit thinking token support
            if all_messages and all_messages[0].get("role") == "system":
                result.append("SYSTEM")
                result.append(all_messages[0]["content"])
                result.append("USER")
            else:
                result.append("SYSTEM")
                result.append("You are a helpful assistant.")
                result.append("USER")

            # Process remaining messages
            start_idx = 1 if all_messages and all_messages[0].get("role") == "system" else 0
            for message in all_messages[start_idx:]:
                role = message.get("role", "")
                content = message.get("content", "")

                if role == "user":
                    result.append(content)
                    result.append("ASSISTANT")
                elif role == "assistant":
                    result.append(content)
                    result.append("")
                elif role == "tool":
                    result.append(content)
                    result.append("USER")

        return "\n".join(result).strip()


class Gemma3ChatTemplate(ChatTemplate):
    """Gemma-3 chat template for creative writing"""

    def format_messages(self, messages: List[Dict[str, str]], system_prompt: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str:
        # Gemma uses the <start_of_turn> and <end_of_turn> format
        result = []

        # Add system prompt if provided
        if system_prompt:
            result.append(f"<start_of_turn>system")
            result.append(system_prompt)
            result.append(f"<end_of_turn>")

        # Process messages
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")

            if role == "user":
                result.append(f"<start_of_turn>user")
                result.append(content)
                result.append(f"<end_of_turn>")
            elif role == "assistant":
                result.append(f"<start_of_turn>model")
                result.append(content)
                result.append(f"<end_of_turn>")
            elif role == "system" and not system_prompt:  # Additional system messages
                result.append(f"<start_of_turn>system")
                result.append(content)
                result.append(f"<end_of_turn>")

        # Add final model turn marker for generation
        result.append(f"<start_of_turn>model")

        return "\n".join(result)


class ChatTemplateManager:
    """Manager for different chat templates"""

    def __init__(self):
        self.templates = {
            "openai": OpenAIChatTemplate(),
            "qwen3": Qwen3ChatTemplate(),
            "qwen3-thinking": Qwen3ThinkingChatTemplate(),  # Enhanced template with thinking token support
            "gemma3": Gemma3ChatTemplate(),  # Added Gemma 3 template
        }

    def get_template(self, template_name: str) -> ChatTemplate:
        return self.templates.get(template_name, self.templates["openai"])

    def register_template(self, name: str, template: ChatTemplate):
        self.templates[name] = template


# Global template manager instance
chat_template_manager = ChatTemplateManager()

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\chat_templates.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\circuit_breaker.py (Section: BACKEND_PYTHON) ---

"""
Circuit breaker pattern for ECE_Core external dependencies.
Prevents cascading failures when Neo4j or Redis are slow/down.
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Optional, Callable, Any
from enum import Enum

logger = logging.getLogger(__name__)

class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"  # Normal operation
    OPEN = "open"  # Failing - reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    """
    Circuit breaker for external service calls.
    
    Usage:
        breaker = CircuitBreaker(failure_threshold=5, timeout=60)
        result = await breaker.call(my_async_function, arg1, arg2)
    """
    
    def __init__(
        self,
        failure_threshold: int = 5,
        timeout: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = CircuitState.CLOSED
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to try resetting."""
        if self.last_failure_time is None:
            return True
        
        return datetime.now() > self.last_failure_time + timedelta(seconds=self.timeout)
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute function with circuit breaker protection.
        
        Args:
            func: Async function to call
            *args, **kwargs: Arguments to pass to function
        
        Returns:
            Function result
        
        Raises:
            CircuitBreakerError: If circuit is open
            Exception: Original exception if circuit allows call
        """
        # If circuit is OPEN, check if we should try again
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logger.info("Circuit breaker transitioning to HALF_OPEN")
            else:
                raise CircuitBreakerError("Circuit breaker is OPEN")
        
        try:
            # Attempt the call
            result = await func(*args, **kwargs)
            
            # Success - reset failure count
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
                logger.info("Circuit breaker reset to CLOSED")
            
            self.failure_count = 0
            return result
        
        except self.expected_exception as e:
            # Track failure
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            
            logger.warning(f"Circuit breaker failure {self.failure_count}/{self.failure_threshold}: {e}")
            
            # Open circuit if threshold reached
            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                logger.error(f"Circuit breaker opened after {self.failure_count} failures")
            
            raise

class CircuitBreakerError(Exception):
    """Raised when circuit breaker is open."""
    pass

# ============================================================================
# PRE-CONFIGURED CIRCUIT BREAKERS
# ============================================================================

neo4j_breaker = CircuitBreaker(
    failure_threshold=5,
    timeout=60,
    expected_exception=Exception
)

redis_breaker = CircuitBreaker(
    failure_threshold=3,
    timeout=30,
    expected_exception=Exception
)

llm_breaker = CircuitBreaker(
    failure_threshold=10,
    timeout=120,
    expected_exception=Exception
)

# ============================================================================
# DECORATOR FOR EASY USAGE
# ============================================================================

def circuit_breaker(breaker: CircuitBreaker):
    """
    Decorator to apply circuit breaker to async functions.
    
    Usage:
        @circuit_breaker(neo4j_breaker)
        async def query_neo4j(...):
            ...
    """
    def decorator(func: Callable):
        async def wrapper(*args, **kwargs):
            return await breaker.call(func, *args, **kwargs)
        return wrapper
    return decorator


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\circuit_breaker.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\config.py (Section: BACKEND_PYTHON) ---

"""
Comprehensive configuration management for ECE_Core.
Organized by component/file for easy maintenance.
"""
from pydantic_settings import BaseSettings
from pydantic import ConfigDict, model_validator
from pathlib import Path
try:
    import yaml
except Exception:
    yaml = None
from dotenv import load_dotenv
from typing import Optional
import json
from urllib.parse import urlparse
import os

try:
    from importlib.metadata import version as _get_pkg_version
    try:
        _pkg_version = _get_pkg_version("ece-core")
    except Exception:
        _pkg_version = None
except Exception:
    _pkg_version = None

class Settings(BaseSettings):
    """
    Configuration organized by component.
    All settings can be overridden via environment variables or .env file.
    """
    
    # ============================================================
    # LLM_CLIENT.PY - Local GGUF Model Settings
    # ============================================================
    llm_api_base: str = "http://localhost:8080/v1"  # llama.cpp server
    # Optional: specify a distinct base URL for embeddings (useful when embedding server runs separately on port 8081)
    llm_embeddings_api_base: Optional[str] = "http://127.0.0.1:8081/v1"
    # Optional: specific model name for embeddings (embedding-capable model like qwen3-embedding-4b)
    llm_embeddings_model_name: Optional[str] = ""
    # Control whether a local GGUF model should be used as a fallback for embeddings
    llm_embeddings_local_fallback_enabled: bool = False
    # Embeddings chunk tuning
    llm_embeddings_chunk_size_default: int = 2048  # default char-based chunk size for long docs (reduced to avoid embedding server 500s)
    llm_embeddings_min_chunk_size: int = 128  # smallest allowed chunk size
    # Sequence of backoff chunk sizes to try when server reports input too large
    llm_embeddings_chunk_backoff_sequence: list[int] = [2048, 1024, 512, 256, 128]
    # Enable adaptive backoff for embeddings (parse server messages and try smaller chunk sizes automatically)
    llm_embeddings_adaptive_backoff_enabled: bool = True
    # Default batch size (number of documents per embeddings API request). Lower default to avoid server overloads.
    llm_embeddings_default_batch_size: int = 2
    # Model selection (name/path used by the LLM client and helper script)
    # Default production-tuned LLM settings for OpenAI-20B on 16GB RTX 4090
    llm_model_name: str = "OpenAI-20B-NEOPlus-Uncensored-IQ4_NL.gguf"  # Model name/path used by launcher
    llm_model_path: str = ""  # Optional local model path (GGUF) — leave blank to avoid loading if not present
    # Optimized for 16GB RTX 4090 with 64k context window to maximize memory utilization
    # 64k context results in ~8GB KV cache which fits comfortably with model weight on RTX 4090
    llm_context_size: int = 65536  # OpenAI-20B-NEOPlus IQ4_NL - 64k context window (synchronized with hardware capabilities)
    # Tuned to keep responses snappy while avoiding large GPU occupancy
    llm_max_tokens: int = 4096  # Increased generation length for long reasoning chains
    llm_temperature: float = 1.0  # Reka default: higher temperature to support novel reasoning
    llm_top_p: float = 0.95  # Nucleus sampling threshold tuned for Reka
    llm_timeout: int = 300  # Request timeout seconds
    # Offload all layers to GPU for maximum inference speed on RTX 4090
    llm_gpu_layers: int = -1  # Use -1 to pin all layers to GPU (where supported)
    llm_threads: int = 12  # CPU threads
    llm_concurrency: int = 4  # How many concurrent LLM calls to allow
    llm_local_embeddings: bool = True  # Load local model with embeddings enabled when used as fallback
    # Character->token heuristics for chunk sizing; tokens ~ 4 chars, useful for server token limits
    llm_chars_per_token: int = 4
    # Fraction of server context to use for chunking (e.g., 0.5 = use half of the model context for chunk size)
    llm_chunk_context_ratio: float = 0.5
    # Stop tokens configuration for Reka model
    llm_stop_tokens: Optional[list] = ["< sep >", ""]  # Added Reka stop token
    # Chat template configuration - specify which template to use for formatting conversations
    llm_chat_template: str = "qwen3-thinking"  # Options: "openai", "qwen3", "qwen3-thinking", "gemma3". Switched to qwen3-thinking for better tool support and thinking token handling
    # Batch size optimizations for 16GB VRAM to maximize context window
    llm_batch_size: int = 1024  # Optimized to conserve VRAM/RAM during processing
    llm_ubatch_size: int = 1024 # Optimized to conserve VRAM/RAM during processing

    @property
    def resolved_chat_template(self) -> str:
        """Auto-detect chat template based on model name if set to 'auto'"""
        if self.llm_chat_template.lower() == 'auto':
            model_name = getattr(self, 'llm_model_name', '').lower()
            if 'gemma' in model_name:
                return 'gemma3'
            elif 'qwen' in model_name:
                return 'qwen3-thinking'
            elif 'llama' in model_name or 'llm' in model_name:
                # Llama models usually use standard chatml format
                return 'openai'  # Using openai format as standard for Llama
            elif 'moe' in model_name or '4x' in model_name or 'california' in model_name:
                # For your specific MoE model, use standard format since it's llama3.2 based
                return 'openai'  # Llama3.2 based models use standard chat format
            else:
                return 'openai'  # Default fallback
        return self.llm_chat_template
    # (duplicate 'llm_chars_per_token' removed)

    # ============================================================
    # LLAMA.CPP SERVER & EMBEDDING SERVER RUNTIME CONFIG
    # Path to the built llama-server executable (explicit path overrides auto-detection)
    llama_server_exe_path: Optional[str] = None
    # Default ports for API and embedding servers
    llama_server_default_port: int = 8080
    llama_embed_server_default_port: int = 8081
    # Allow interactive model selection via `select_model.py` if model not configured in settings
    llama_allow_select_model: bool = True
    # Additional server runtime tuning flags exposed for convenience
    llama_server_cont_batching: bool = True
    llama_server_flash_attn: str = "auto"
    llama_server_cache_type_k: str = "f16"
    llama_server_cache_type_v: str = "f16"
    llama_server_repeat_penalty: float = 1.1
    # llama.cpp server batch tuning
    # Batch tuning: set a larger logical batch size to allow batching while keeping micro batches small
    # Defaults tuned for RTX 4090 16GB VRAM: large logical batch, small ubatch to avoid VRAM spikes
    llama_server_batch_size: int = 2048  # logical max batch size (llama-server --batch-size)
    llama_server_ubatch_size: int = 2048  # physical ubatch size (llama-server --ubatch-size) - raised for RTX 4090 stability
    llama_server_parallel: int = 1  # number of parallel sequences/slots (llama-server --parallel)
    # Optional cap for UBATCH to avoid allocating more memory than GPU can handle
    llama_server_ubatch_max: Optional[int] = None
    # Optional: configure prompt cache ram in MiB; default 0 disables prompt cache (good for VRAM-constrained setups)
    llama_cache_ram: int = 0
    # Optional stop tokens to instruct the model to terminate completions
    llm_stop_tokens: Optional[list[str]] = None
    
    # ============================================================
    # MEMORY.PY - Tiered Memory Settings
    # ============================================================
    # Redis (Hot/Working Memory)
    redis_url: str = "redis://localhost:6379"
    redis_ttl: int = 3600  # Session TTL in seconds
    redis_max_tokens: int = 32000  # Max tokens in Redis before flush (increased from 16000 for larger buffer)
    
    # Memory thresholds
    max_context_tokens: int = 60000  # Max tokens in total context (synchronized with 64k hardware window, leaving 5k buffer for output)
    summarize_threshold: int = 48000  # Trigger summarization when Redis exceeds this (allowing much longer conversations before forcing rotation)
    
    # ============================================================
    # CONTEXT_MANAGER.PY - Context Assembly
    # ============================================================
    # Archivist settings (summarization)
    archivist_enabled: bool = True
    archivist_chunk_size: int = 8000  # Tokens per chunk for summarization (increased to preserve more detail with 64k context)
    archivist_overlap: int = 500  # Overlap between chunks (increased for better continuity with larger chunks)
    archivist_compression_ratio: float = 0.5  # Target 50% of original size (reduced aggressiveness from 0.3)
    
    # Context tiers
    context_recent_turns: int = 50  # Recent conversation turns to include (increased from 10 to support 50+ exchanges)
    context_summary_limit: int = 20  # Max historical summaries to include (increased from 8)
    context_entity_limit: int = 50  # Max entity-based memories (increased from 15)

    # Defaults for memory provenance & freshness
    memory_default_provenance_score: float = 0.5  # Default provenance when metadata is unknown (0.0-1.0)
    memory_default_freshness_score: float = 1.0  # Freshness score at creation (1.0 = brand new)
    # Distiller caching settings
    memory_distill_cache_enabled: bool = True
    memory_distill_cache_ttl: int = 86400  # in seconds

    # ============================================================
    # ARCHIVIST - Auto-Purge (Janitor) Settings
    # ============================================================
    # When enabled, Archivist will periodically scan for and optionally delete
    # contaminated memory nodes that match the configured markers.
    archivist_auto_purge_enabled: bool = False
    archivist_auto_purge_interval_seconds: int = 600  # default 10 minutes
    archivist_auto_purge_dry_run: bool = True  # default to dry-run to avoid accidental deletes
    # A set of markers used to detect contaminated nodes. Lower-cased.
    archivist_auto_purge_markers: list[str] = [
        "thinking_content",
        "combined_text",
        "prompt-logs",
        "prompt_logs",
        "calibration_run",
        "dry-run",
        "dry_run",
        "[planner]",
        "--- start of file:",
        "(anchor) ps ",
        "[info] http",
        '"thinking_content":'
    ]
    
    # ============================================================
    # QLEARNING_RETRIEVER.PY - Graph Retrieval
    # ============================================================
    qlearning_enabled: bool = True
    qlearning_learning_rate: float = 0.1
    qlearning_discount_factor: float = 0.9
    qlearning_epsilon: float = 0.3  # Exploration rate
    qlearning_max_hops: int = 3  # Graph traversal depth
    qlearning_max_paths: int = 5  # Max paths to explore
    qlearning_save_interval: int = 10  # Save Q-table every N queries
    qlearning_table_path: str = "./q_table.json"
    
    # ============================================================
    # EXTRACT_ENTITIES.PY - Entity Extraction
    # ============================================================
    entity_extraction_batch_size: int = 20  # Process N turns at a time
    entity_extraction_delay: float = 0.1  # Delay between LLM calls (rate limiting)
    entity_min_confidence: float = 0.5  # Min confidence for entity extraction
    entity_types: list[str] = ["PERSON", "CONCEPT", "PROJECT", "CONDITION", "SKILL"]  # Specify entity types explicitly
    
    # ============================================================
    # NEO4J - Knowledge Graph (Optional)
    # ============================================================
    neo4j_enabled: bool = True  # Enable Neo4j for memory storage and retrieval
    neo4j_uri: str = "bolt://localhost:7687"  # Neo4j connection URI
    neo4j_user: str = "neo4j"  # Neo4j username
    neo4j_password: str = os.getenv("NEO4J_PASSWORD", "password")  # Neo4j password from environment variable
    neo4j_max_connection_pool_size: int = 50  # Max connection pool size
    neo4j_connection_timeout: int = 30  # Connection timeout in seconds
    # ============================================================
    # NEO4J RECONNECT (resilience settings for critical DB errors)
    # If Neo4j has a critical error at startup, attempt to reconnect in background.
    neo4j_reconnect_enabled: bool = True
    neo4j_reconnect_initial_delay: int = 5  # seconds before first reconnect attempt
    neo4j_reconnect_max_attempts: int = 6  # attempts before stopping
    neo4j_reconnect_backoff_factor: float = 2.0  # exponential backoff multiplier

    # ============================================================
    # VECTOR DB (Optional)
    # ============================================================
    vector_enabled: bool = False  # Enable vector DB usage for semantic search
    vector_adapter_name: str = "redis"  # Name of adapter to use (redis, faiss, pinecone)
    vector_auto_embed: bool = False  # Autogenerate embeddings for new memories (via llm_client)
    
    # ============================================================
    # MAIN.PY - ECE Server
    # ============================================================
    ece_host: str = "127.0.0.1"
    ece_port: int = 8000
    ece_log_level: str = "INFO"
    # Optional full URL form for MCP server (e.g., http://localhost:8008)
    mcp_url: Optional[str] = None
    # Versioning
    # Detect the package version if installed; otherwise allow ECE_VERSION env var or fallback to 'dev'
    ece_version: str = os.getenv('ECE_VERSION', _pkg_version or 'dev')
    ece_cors_origins: list[str] = ["*"]  # CORS allowed origins
    
    # ============================================================
    # SECURITY - API Authentication
    # ============================================================
    ece_api_key: Optional[str] = None
    ece_require_auth: bool = False
    # ------------------------------------------------------------------
    # MCP Server configuration
    # ------------------------------------------------------------------
    mcp_enabled: bool = False
    mcp_host: str = "127.0.0.1"
    mcp_port: int = 8421
    # Optional: support YAML `server.host` / `server.port` semantics via SERVER_HOST / SERVER_PORT env vars
    server_host: Optional[str] = None
    server_port: Optional[int] = None
    # Per-protocol API key for MCP (if required). Fallback to ece_api_key if not set.
    mcp_api_key: Optional[str] = None
    
    # ============================================================
    # SECURITY - Audit Logging
    # ============================================================
    audit_log_enabled: bool = True
    audit_log_path: str = "./logs/audit.log"
    audit_log_tool_calls: bool = True
    audit_log_memory_access: bool = False
    
    # ============================================================
    # ANCHOR - CLI Client
    # ============================================================
    anchor_session_id: str = "anchor-session"
    anchor_timeout: int = 300
    
    # ============================================================
    # GRAPH_REASONER.PY - Graph-R1 Reasoning
    # ============================================================
    reasoning_max_iterations: int = 5  # Markovian thinking iterations
    reasoning_enabled: bool = True
    # ============================================================
    # TOOL EXECUTION - Maximum iterations when processing tool calls
    # This controls how many tool-execute/regenerate cycles are allowed
    # Default kept in sync with ToolExecutor default (3)
    # New recommended setting name: tool_max_iterations (for non-MCP generic use)
    tool_max_iterations: int = 3
    # Backward compatibility: keep the old MCP-prefixed name in place
    mcp_max_tool_iterations: int = 3
    
    # ============================================================
    # Computed Properties
    # ============================================================
    @property
    def archivist_max_summary_tokens(self) -> int:
        """Target summary size based on compression ratio"""
        return int(self.summarize_threshold * self.archivist_compression_ratio)
    
    @property
    def llm_model(self) -> str:
        """Model name for API calls (backward compatibility)"""
        return self.llm_model_name
    
    # ============================================================
    # Pydantic Config
    # ============================================================
    # Pydantic v2 model config: ignore extra environment variables to avoid validation errors
    model_config = ConfigDict(extra="ignore", env_file=".env", env_file_encoding="utf-8", case_sensitive=False)

    @model_validator(mode='after')
    def _post_init(self):
        """After validation, derive host/port settings from URL fields if present.

        This allows the YAML config to specify `mcp: { url: "http://localhost:8008" }` and
        `server: { host: "0.0.0.0", port: 8000 }` while preserving backward-compatible
        `mcp_host`/`mcp_port` and `ece_host`/`ece_port` fields.
        """
        # Parse mcp_url if present
        if getattr(self, 'mcp_url', None):
            try:
                parsed = urlparse(self.mcp_url)
                if parsed.hostname:
                    object.__setattr__(self, 'mcp_host', parsed.hostname)
                if parsed.port:
                    object.__setattr__(self, 'mcp_port', parsed.port)
            except Exception:
                pass

        # Apply server_host/server_port to ece_host/ece_port if provided
        if getattr(self, 'server_host', None):
            object.__setattr__(self, 'ece_host', self.server_host)
        if getattr(self, 'server_port', None):
            try:
                object.__setattr__(self, 'ece_port', int(self.server_port))
            except Exception:
                pass

        # Parse possible stringified list for llm_stop_tokens from environment/YAML fallback.
        try:
            val = getattr(self, 'llm_stop_tokens', None)
            if isinstance(val, str):
                s = val.strip()
                if s.startswith('[') and s.endswith(']'):
                    try:
                        # Convert single quotes to double quotes if needed, then parse JSON
                        j = s.replace("'", '"')
                        parsed = json.loads(j)
                        if isinstance(parsed, list):
                            object.__setattr__(self, 'llm_stop_tokens', parsed)
                    except Exception:
                        # Fall through to comma-splitting
                        items = [i.strip() for i in s.strip('[]').split(',') if i.strip()]
                        object.__setattr__(self, 'llm_stop_tokens', items)
                else:
                    # Comma-separated values
                    items = [i.strip() for i in s.split(',') if i.strip()]
                    object.__setattr__(self, 'llm_stop_tokens', items)
        except Exception:
            pass

        return self

    # ============================================================
    # MEMORY WEAVER (AUTONOMOUS REPAIR)
    # ============================================================
    weaver_enabled: bool = True
    # We recommend enabling real commits after a short observation period
    # for production readiness; default to enabled for Sovereign Brain mode.
    weaver_dry_run_default: bool = False
    weaver_threshold: float = 0.55
    weaver_delta: float = 0.05
    weaver_time_window_hours: int = 24
    weaver_max_commit: int = 50
    weaver_prefer_same_app: bool = True
    weaver_commit_enabled: bool = True
    # When present, nodes containing this tag in m.tags will be excluded from weaver runs
    weaver_exclude_tag: Optional[str] = '#corrupted'
    # Defaults related to weaver and repair scripts
    weaver_candidate_limit: int = 200  # candidate limit per summary in repair runs
    weaver_batch_size_default: int = 2  # default batch size used by repair/weaver when not overridden
    # Backwards compatible alias and convenience env name (WEAVER_BATCH_SIZE)
    # If set, this value takes precedence over `weaver_batch_size_default`.
    weaver_batch_size: int | None = None
    # Sleep seconds to wait between batches for safe GPU breathing room
    weaver_sleep_between_batches: float = 1.0

    # Matrix / Factory Configuration - spawn worker processes for heavy async tasks
    matrix_worker_count: int = 8  # Number of worker processes for Matrix/Weaver tasks

# Global settings instance
def _load_config_fallbacks() -> None:
    """Load configs from the recommended `configs/` directory and .env files.

    Behavior:
    - Loads configs/config.yaml if present, flattening keys into environment variables
      so that Pydantic BaseSettings picks them up.`
    - Loads .env from `configs/.env` if present, else root `.env`.
    - Does not override existing environment variables.
    """
    repo_root = Path(__file__).resolve().parents[1]
    configs_dir = repo_root / "configs"
    # 1) Load .env file from config dir if present (otherwise root .env). Don't override existing env vars.
    candidate_envs = [configs_dir / ".env", repo_root / ".env"]
    for envf in candidate_envs:
        if envf.exists():
            try:
                load_dotenv(dotenv_path=str(envf), override=False)
            except Exception:
                pass
            break
    # 2) Load YAML defaults and set environment variables for missing keys
    if yaml is None:
        # PyYAML not installed; skip YAML defaults. If a YAML config exists, print a helpful hint.
        repo_root = Path(__file__).resolve().parents[1]
        configs_dir = repo_root / "configs"
        candidates = [configs_dir / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
        config_yaml = next((p for p in candidates if p.exists()), None)
        if config_yaml is not None:
            try:
                print(f"⚠️  PyYAML is not installed; found YAML config at {config_yaml}. Install PyYAML (pip install PyYAML) to load YAML-based defaults.")
            except Exception:
                # If printing fails in constrained env, just pass
                pass
        return
    # Support multiple locations for config.yaml to preserve compatibility with existing scripts
    candidates = [configs_dir / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
    config_yaml = next((p for p in candidates if p.exists()), None)
    if config_yaml is not None:
        try:
            raw = yaml.safe_load(config_yaml.read_text()) or {}
        except Exception:
            raw = {}
        def _flatten(cfg: dict, prefix: str = None):
            for k, v in (cfg or {}).items():
                key = f"{(prefix + '_') if prefix else ''}{k}".upper()
                if isinstance(v, dict):
                    yield from _flatten(v, key)
                else:
                    yield key, v
        for ek, ev in _flatten(raw):
            # Only set env var if not already present
            if os.environ.get(ek) is None and ev is not None:
                os.environ[ek] = str(ev)

_load_config_fallbacks()

try:
    settings = Settings()
except Exception:
    # During tests or in constrained environments, extra env vars can cause validation errors.
    # Fall back to a constructed default settings object to avoid blocking collection.
    settings = Settings.construct()

# Legacy exports for backward compatibility


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\config.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\config_loader.py (Section: BACKEND_PYTHON) ---

"""
Configuration Loader for ECE_Core and Anchor

Loads configuration from YAML files with environment variable substitution.
Provides typed configuration objects with validation.
"""
try:
    import yaml
except Exception:
    yaml = None
import os
import re
import logging
from pathlib import Path
from src.utils.config_finder import find_config_path
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field, validator

logger = logging.getLogger(__name__)


class ConfigLoader:
    """
    Configuration loader with environment variable substitution.
    
    Supports ${ENV_VAR} syntax in YAML files.
    Example: password: "${NEO4J_PASSWORD}"
    """
    
    def __init__(self, config_path: Optional[Path] = None):
        """
        Initialize config loader.
        
        Args:
            config_path: Path to config.yaml. If None, looks in current directory.
        """
        if config_path is None:
            # Ask the config_finder for the canonical location if present
            candidate = find_config_path()
            if candidate:
                config_path = Path(candidate)
            else:
                # Look for a default in the ece-core directory for backward compatibility
                config_path = Path(__file__).parent.parent / "config.yaml"
        
        self.config_path = Path(config_path)
        self._config: Optional[Dict[str, Any]] = None
    
    def load(self) -> Dict[str, Any]:
        """
        Load configuration from YAML file.
        
        Returns:
            Dictionary with configuration
        """
        if not self.config_path.exists():
            logger.warning(f"Config file not found: {self.config_path}")
            logger.warning("Using default configuration")
            return {}
        
        try:
            if yaml is None:
                logger.warning("PyYAML not installed; skipping YAML-based config loading")
                return {}
            with open(self.config_path, 'r') as f:
                content = f.read()
            
            # Substitute environment variables
            content = self._substitute_env_vars(content)
            
            # Parse YAML
            config = yaml.safe_load(content)
            
            logger.info(f"Loaded configuration from {self.config_path}")
            self._config = config
            return config
            
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return {}
    
    def _substitute_env_vars(self, content: str) -> str:
        """
        Substitute ${ENV_VAR} patterns with environment variable values.
        
        Args:
            content: YAML content with potential ${VAR} patterns
            
        Returns:
            Content with substitutions made
        """
        def replace_env(match):
            env_var = match.group(1)
            value = os.getenv(env_var)
            
            if value is None:
                # Check for default value: ${VAR:-default}
                if ':-' in env_var:
                    var_name, default = env_var.split(':-', 1)
                    value = os.getenv(var_name, default)
                else:
                    logger.warning(f"Environment variable {env_var} not set")
                    # Keep placeholder for optional values
                    return match.group(0)
            
            return value
        
        # Replace ${VAR} and ${VAR:-default}
        pattern = re.compile(r'\$\{([^}]+)\}')
        return pattern.sub(replace_env, content)
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        Get configuration value by dot-notation key.
        
        Args:
            key: Dot-notation key (e.g., "server.port")
            default: Default value if key not found
            
        Returns:
            Configuration value or default
        """
        if self._config is None:
            self.load()
        
        if self._config is None:
            return default
        
        # Navigate nested dict with dot notation
        parts = key.split('.')
        value = self._config
        
        for part in parts:
            if isinstance(value, dict) and part in value:
                value = value[part]
            else:
                return default
        
        return value
    
    def reload(self) -> Dict[str, Any]:
        """
        Reload configuration from file.
        
        Returns:
            Updated configuration dictionary
        """
        return self.load()
    
    def print_config(self, hide_secrets: bool = True):
        """
        Print current configuration (for debugging).
        
        Args:
            hide_secrets: Whether to hide password fields
        """
        if self._config is None:
            self.load()
        
        config = self._config.copy() if self._config else {}
        
        if hide_secrets:
            # Redact sensitive fields
            config = self._redact_secrets(config)
        
        print("=" * 60)
        print("Current Configuration:")
        print("=" * 60)
        print(yaml.dump(config, default_flow_style=False, sort_keys=False))
        print("=" * 60)
    
    def _redact_secrets(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Redact sensitive configuration values.
        
        Args:
            config: Configuration dictionary
            
        Returns:
            Config with secrets redacted
        """
        redacted = {}
        secret_keys = {'password', 'secret', 'token', 'key', 'api_key'}
        
        for key, value in config.items():
            if isinstance(value, dict):
                redacted[key] = self._redact_secrets(value)
            elif any(secret in key.lower() for secret in secret_keys):
                redacted[key] = "***REDACTED***"
            else:
                redacted[key] = value
        
        return redacted


# Global config loader instance
_loader: Optional[ConfigLoader] = None


def get_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    Get configuration (singleton pattern).
    
    Args:
        config_path: Optional path to config file
        
    Returns:
        Configuration dictionary
    """
    global _loader
    
    if _loader is None:
        _loader = ConfigLoader(config_path)
        _loader.load()
    
    return _loader._config or {}


def reload_config() -> Dict[str, Any]:
    """
    Reload configuration from file.
    
    Returns:
        Updated configuration dictionary
    """
    global _loader
    
    if _loader is None:
        return get_config()
    
    return _loader.reload()


def get_value(key: str, default: Any = None) -> Any:
    """
    Get configuration value by key.
    
    Args:
        key: Dot-notation key (e.g., "server.port")
        default: Default value if not found
        
    Returns:
        Configuration value
    """
    global _loader
    
    if _loader is None:
        get_config()
    
    return _loader.get(key, default) if _loader else default


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\config_loader.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\content_utils.py (Section: BACKEND_PYTHON) ---

import html
import json
import re

EMOJI_REGEX = re.compile(
    "[\U0001F300-\U0001F6FF\U0001F900-\U0001F9FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]",
    flags=re.UNICODE,
)

JSON_LIKE_PATTERNS = [re.compile(p) for p in [r"\{\s*\".*\"\s*:\s*", r"\[\s*\{", r'"response_content"', r'"timestamp"']]
HTML_LIKE_PATTERNS = [re.compile(p) for p in [r'<\s*\/?\w+[^>]*>', r'<a\s+href=', r'<script\b', r'<div\b', r'<p\b']]

SPAM_KEYWORDS = ['erotik', 'click here', 'buy now', 'free', 'cheap', 'subscribe now']

TECHNICAL_KEYWORDS = ['error', 'exception', 'traceback', 'sudo', 'apt-get', 'npm', 'pip', 'docker', 'cargo', 'journal', 'systemd', 'kernel', 'trace', 'failed', 'stacktrace']


def is_json_like(text: str) -> bool:
    if not text:
        return False
    for p in JSON_LIKE_PATTERNS:
        if p.search(text):
            return True
    return False


def is_html_like(text: str) -> bool:
    if not text:
        return False
    for p in HTML_LIKE_PATTERNS:
        if p.search(text):
            return True
    return False


def remove_html_tags(text: str) -> str:
    return re.sub(r'<[^>]+>', ' ', text)


def strip_emojis(text: str) -> str:
    return EMOJI_REGEX.sub('', text)


def extract_text_from_json(content: str) -> str:
    try:
        obj = json.loads(content)
        if isinstance(obj, dict):
            for k in ('response_content', 'content', 'text', 'message', 'response'):
                if k in obj and isinstance(obj[k], str):
                    return obj[k]
            values = []
            for v in obj.values():
                if isinstance(v, str):
                    values.append(v)
            return ' '.join(values)
        if isinstance(obj, list):
            texts = []
            for el in obj:
                if isinstance(el, dict):
                    for k in ('response_content', 'content', 'text'):
                        if k in el and isinstance(el[k], str):
                            texts.append(el[k])
                elif isinstance(el, str):
                    texts.append(el)
            return ' '.join(texts)
    except Exception:
        return content
    return content


# ---------------------- Technical Normalization ----------------------
ANSI_ESCAPE_RE = re.compile(r"\x1b\[[0-9;]*[A-Za-z]")
WINDOWS_PATH_RE = re.compile(r"[A-Za-z]:\\\\")
UNIX_PATH_RE = re.compile(r"/(?:[\w\-\.@]+/)+[\w\-\.@]+")
HEXDUMP_RE = re.compile(r"(?:0x[0-9a-fA-F]{2,}|[0-9A-Fa-f]{2,}(?:\s+[0-9A-Fa-f]{2,}){4,})")


def contains_ansi_codes(text: str) -> bool:
    return bool(ANSI_ESCAPE_RE.search(text))


def contains_windows_path(text: str) -> bool:
    m = re.search(r"[A-Za-z]:", text)
    if not m:
        return False
    idx = m.end()
    if idx < len(text) and text[idx] in ('\\', '/'):
        return True
    return False


def contains_unix_path(text: str) -> bool:
    if '/usr/' in text or '/bin/' in text or '/home/' in text:
        return True
    return bool(UNIX_PATH_RE.search(text))


def contains_hex_dump(text: str) -> bool:
    return bool(HEXDUMP_RE.search(text))


def normalize_technical_content(text: str) -> str:
    """Normalize technical content by removing/annotating noisy artifacts while preserving semantic metadata.
    The function detects ANSI/color codes, OS paths, and hex dumps and inserts human-readable annotations
    such as [Context: Terminal Output], [OS: Linux], [OS: Windows], and [Binary Data Omitted].
    """
    if not text:
        return ''
    tags = []
    t = str(text)
    # ANSI sequences
    if contains_ansi_codes(t):
        t = ANSI_ESCAPE_RE.sub(' ', t)
        tags.append('[Context: Terminal Output]')
    # Windows paths
    if contains_windows_path(t):
        tags.append('[OS: Windows]')
        t = WINDOWS_PATH_RE.sub(lambda m: (m.group(0)[:60] + '...' if len(m.group(0)) > 70 else m.group(0)), t)
    # Unix paths
    if contains_unix_path(t):
        tags.append('[OS: Linux]')
        t = UNIX_PATH_RE.sub(lambda m: (m.group(0)[:80] + '...' if len(m.group(0)) > 80 else m.group(0)), t)
    # Hex dump / binary-like sequences
    if contains_hex_dump(t):
        tags.append('[Binary Data Omitted]')
        t = HEXDUMP_RE.sub('[binary_data]', t)
    # If there are HTML-like artifacts but we want a log context, annotate with [Context: HTML]
    if is_html_like(t):
        tags.append('[Context: HTML]')
        t = remove_html_tags(t)
    # Strip control characters
    t = re.sub(r'\x00|\x07|\x0b|\x0c', ' ', t)
    t = re.sub(r'\s+', ' ', html.unescape(t)).strip()
    if tags:
        annotation = ' '.join(sorted(set(tags))) + ' '
        t = annotation + t
    return t

# ---------------------- End Technical Normalization ----------------------


def clean_content(text: str, remove_emojis: bool = True, remove_non_ascii: bool = False, annotate_technical: bool = False) -> str:
    if not text:
        return ''
    t = text.strip()
    # If requested, apply technical normalization which will remove/annotate ANSI codes, paths, and hex dumps
    if annotate_technical:
        t = normalize_technical_content(t)
    if t.startswith('{') or t.startswith('[') or '"response_content"' in t:
        t2 = extract_text_from_json(t)
        if isinstance(t2, str) and t2:
            t = t2
    t = remove_html_tags(t)
    t = html.unescape(t)
    if remove_emojis:
        t = strip_emojis(t)
    if remove_non_ascii:
        t = ''.join([c for c in t if ord(c) < 128])
    t = re.sub(r'[^\w\s\.,;:\-\'"@#%\(\)\?\/\\]+', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def has_technical_signal(text: str) -> bool:
    """Detect strings that indicate the text is technical/log-like and should be preserved.
    This will detect shell prompts, package managers, version numbers, file paths, and error markers.
    """
    if not text:
        return False
    t = text.lower()
    # Quick patterns
    if 'sudo' in t or 'apt-get' in t or 'npm ' in t or 'pip ' in t or 'docker ' in t or 'cargo ' in t:
        return True
    # Version numbers
    if re.search(r'v\d+\.\d+(?:\.\d+)?', text):
        return True
    # File paths
    if re.search(r'\/\w+\/\w+', text) or re.search(r'[A-Za-z]:\\\\', text):
        return True
        # Allow square brackets so annotation tokens like [Context: Terminal Output] are preserved
        t = re.sub(r'[^\w\s\.,;:\-\'"@#%\(\)\[\]\?/\\]+', ' ', t)
    for k in TECHNICAL_KEYWORDS:
        if k in t:
            return True
    # Shell-like prompts or stack traces
    if re.search(r'\b(error|exception|traceback|failed)\b', t):
        return True
    if re.search(r'\b[\$#] ', text):
        return True
    return False


def is_token_soup(text: str, *, min_tokens: int = 3) -> bool:
    """Detect whether a piece of text is likely corrupted/garbled (token soup).

    Heuristics used:
    - High fraction of tokens that contain code-like characters (parentheses, ';', '{', '}', '->', etc.)
    - High fraction of tokens that are hexadecimal strings or long digit-only sequences
    - High fraction of tokens that are one-letter (indicative of binary tokens or low-quality text)
    - Low fraction of alphabetic words (few dictionary-like words)
    - Large runs of punctuation or non-letter characters
    These heuristics are intentionally conservative — we prefer to flag likely garbage and
    leave marginal cases untouched.
    """
    if not text or not text.strip():
        return False
    s = text.strip()
    # Quick filters
    if len(s) < 40:
        # short content unlikely to be long corrupted token soup
        return False
    tokens = re.findall(r"\S+", s)
    if len(tokens) < min_tokens:
        return False
    total = len(tokens)
    code_like = 0
    hex_like = 0
    one_letter = 0
    no_vowel = 0
    alpha_like = 0
    long_token = 0
    for t in tokens:
        if len(t) == 1:
            one_letter += 1
        if re.search(r'[(){}\[\]<>;=:\\|/\\\\@#%\$]', t):
            code_like += 1
        if re.fullmatch(r'0x[0-9a-fA-F]{8,}', t) or re.fullmatch(r'[A-Fa-f0-9]{16,}', t):
            hex_like += 1
        if re.search(r'[0-9]', t) and re.search(r'[A-Za-z]', t) is None and len(t) >= 8:
            hex_like += 1
        if len(t) >= 6 and not re.search(r'[aeiouAEIOU]', t):
            no_vowel += 1
        if re.fullmatch(r'[A-Za-z]+', t):
            alpha_like += 1
        if len(t) > 30:
            long_token += 1
    # Ratios
    code_like_ratio = code_like / total
    hex_ratio = hex_like / total
    one_letter_ratio = one_letter / total
    no_vowel_ratio = no_vowel / total
    alpha_ratio = alpha_like / total
    long_token_ratio = long_token / total
    # If we see many code-like tokens or hex-like tokens, it's probably corrupted
    if code_like_ratio > 0.25 or hex_ratio > 0.05 or one_letter_ratio > 0.25:
        return True
    # If very few alphabetic words and many tokens are long & vowel-free, flag
    if alpha_ratio < 0.25 and (no_vowel_ratio > 0.2 or long_token_ratio > 0.05):
        return True
    # If we see run of excessive punctuation
    punct_runs = re.findall(r'[^\w\s]{6,}', s)
    if len(punct_runs) > 0:
        return True
    return False


def sanitize_token_soup(text: str) -> str:
    """Return a sanitized version of a token-soup string. We aim to preserve any readable text
    but remove or collapse obvious code fragments, long hex/ids, and JSON-like containers.
    This is a best-effort function — it can't recover content that has been transformed beyond
    recognition, but it will remove obvious artifacts so downstream summarizers see cleaner input.
    """
    if not text:
        return ''
    t = text
    # Remove fenced code blocks
    t = re.sub(r'```.*?```', ' ', t, flags=re.DOTALL)
    # Remove common code patterns: function calls with arguments, memory copies
    t = re.sub(r'\b[A-Za-z_][A-Za-z0-9_]*\([^\)]*\)', ' ', t)
    # Remove long hex sequences
    t = re.sub(r'0x[0-9a-fA-F]{6,}', ' ', t)
    t = re.sub(r'\b[A-Fa-f0-9]{16,}\b', ' ', t)
    # Remove JSON-like structural content when large
    if len(t) > 200 and (t.strip().startswith('{') or t.strip().startswith('[')):
        t = extract_text_from_json(t)
    # Collapse multiple punctuation and whitespace
    t = re.sub(r'[<>\|\\]{1,}', ' ', t)
    t = re.sub(r'[^\w\s\.,;:\-\'"\(\)]+', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    # Truncate to a sensible length - we don't want to produce overly long sanitized strings
    if len(t) > 500:
        t = t[:500] + '...'
    return t


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\content_utils.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\context.py (Section: BACKEND_PYTHON) ---

"""Context Manager: Assembles context and manages overflow."""
import logging
from typing import Optional
from datetime import datetime, timezone
from src.memory import TieredMemory
from src.llm import LLMClient
from src.distiller import Distiller
from src.intelligent_chunker import IntelligentChunker
from src.config import settings

logger = logging.getLogger(__name__)

class ContextManager:
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        self.distiller = Distiller(llm)  # Context quality gate & extractor
        self.chunker = IntelligentChunker(llm)  # Large input processor
    
    async def build_context(self, session_id: str, user_input: str) -> str:
        """
        Build context from memory tiers with Archivist filtering.
        
    Tiers:
    1. Relevant memories (Neo4j) - Long-term memories relevant to the query
    2. Summaries (Neo4j) - Long-term compressed context
        3. Active (Redis) - Short-term recent conversation
        
        The Archivist filters and consolidates retrieved context to prevent bloat.
        If user_input is very large, IntelligentChunker processes it first.
        """
        # DEBUG: Log build_context entry
        logger.debug(f"=== build_context START for session {session_id} ===")
        logger.debug(f"User input length: {len(user_input)} chars")
        # logger.debug(f"User input: {user_input[:200]}...")
        
        # **NEW**: If user input is large, process it intelligently
        if len(user_input) > 4000:
            logger.info(f"Large input detected ({len(user_input):,} chars), processing with IntelligentChunker...")
            user_input_processed = await self.chunker.process_large_input(
                user_input=user_input,
                query_context=""  # First pass, no context yet
            )
            logger.info(f"Input compressed: {len(user_input):,} â†’ {len(user_input_processed):,} chars")
        else:
            user_input_processed = user_input
        
        # 1. Retrieve relevant long-term memories
        logger.debug("Retrieving relevant memories...")
        relevant_memories = await self._retrieve_relevant_memories(user_input_processed, limit=10)
        logger.debug(f"Retrieved {len(relevant_memories)} relevant memories")
        # logger.debug(f"Memories: {relevant_memories}")
        
        # Get summaries from Neo4j
        logger.debug("Retrieving summaries...")
        summaries = await self.memory.get_summaries(session_id, limit=8)
        logger.debug(f"Retrieved {len(summaries)} summaries")
        # logger.debug(f"Summaries: {summaries}")
        
        # 3. Get recent conversation
        logger.debug("Retrieving active context...")
        active_context = await self.memory.get_active_context(session_id)
        logger.debug(f"Active context length: {len(active_context)} chars")
        # logger.debug(f"Active context: {active_context[:200]}...")
        
        # 4. DISTILLER: Filter and format
        logger.debug("Running Distiller filter...")
        filtered = await self.distiller.filter_and_consolidate(
            query=user_input_processed,
            memories=relevant_memories,
            summaries=summaries,
            active_context=active_context
        )
        logger.debug(f"Distiller returned filtered context")
        # logger.debug(f"Filtered: {filtered}")
        
        # 5. Build final context from filtered results
        parts = []
        
        # Current datetime (temporal awareness)
        current_dt = datetime.now(timezone.utc)
        formatted_dt = current_dt.strftime("%B %d, %Y at %H:%M:%S UTC")
        parts.append(f"**Current Date & Time:** {formatted_dt}\n<current_datetime>{current_dt.isoformat()}</current_datetime>")
        
        # Note: We intentionally append summaries and relevant memories AFTER
        # the current question so they are placed *closest* to the model's generation
        # (reduce recency bias toward tool instruction). We'll include explicit tags
        # that the LLM can recognize and consult first for memory/recall questions.

        # Recent conversation (current session - always include for continuity)
        # Label clearly as the CURRENT session, not historical
        if filtered["active_context"]:
            recent_turns = "\n".join(filtered["active_context"].split("\n")[-100:])  # Preserve more turns (from 40 to 100)
            logger.debug(f"Adding current conversation ({len(recent_turns)} chars)")
            parts.append(f"# Current Conversation (This Session):\n{recent_turns}")
        
        # Current question: placed before memory blocks so the memory blocks are
        # *closer* to the model's generation position (reducing tool bias)
        parts.append(f"# What the User Just Said:\n{user_input}")

        # NEW: Context Rotation Protocol to maintain optimal window size for 64k limits
        # Check if active context is getting too large for efficient processing within 64k window
        active_context = filtered["active_context"]
        MAX_CONTEXT_BEFORE_ROTATION = 55000  # Leave buffer for new content and system prompt
        CONTEXT_GIST_THRESHOLD = 25000      # Size at which we consider rotating old context

        if len(active_context) > CONTEXT_GIST_THRESHOLD:
            logger.info(f"Active context ({len(active_context)} chars) exceeds threshold ({CONTEXT_GIST_THRESHOLD}), initiating rotation...")

            # Calculate safe rotation point that preserves recent context
            rotation_point = max(len(active_context) // 2, len(active_context) - 20000)  # Don't cut too aggressively
            old_portion = active_context[:rotation_point]
            recent_portion = active_context[rotation_point:]

            # Create a summary of the old portion using distillation
            try:
                # Distill the old conversation into a compact, meaningful summary
                gist_summary = await self.distiller.make_compact_summary(
                    memories=[{"content": old_portion}],
                    summaries=[],
                    active_context="",
                    new_input=""
                )

                if gist_summary and len(gist_summary.strip()) > 0:
                    logger.debug(f"Created gist summary ({len(gist_summary)} chars) from {len(old_portion)} chars of old context")

                    # Create "gist memory" in Neo4j for long-term reference with proper metadata
                    gist_memory_id = await self.memory.add_memory(
                        session_id=session_id,
                        content=gist_summary,
                        category="context_gist",
                        importance=8,  # High importance for context continuity
                        tags=["gist", "summary", "historical", "context_rotation"],
                        metadata={
                            "original_context_length": len(old_portion),
                            "type": "context_rotation_gist",
                            "rotation_point": rotation_point,
                            "timestamp": datetime.now(timezone.utc).isoformat()
                        }
                    )
                    logger.info(f"Stored context gist in memory with ID: {gist_memory_id}")

                    # Reconstruct context with gist + recent context
                    rotated_context = f"## Prior Context Summary:\n{gist_summary}\n\n## Recent Conversation:\n{recent_portion}"

                    # Update the memory with the rotated context
                    await self.memory.save_active_context(session_id, rotated_context)

                    # Update filtered active_context to reflect the rotated version
                    active_context = rotated_context
                    filtered["active_context"] = rotated_context

                    logger.info(f"Context successfully rotated: {len(active_context)} chars total")
                else:
                    logger.debug("Gist summary was empty, skipping context rotation")
            except Exception as e:
                logger.warning(f"Failed to create gist during context rotation: {e}")
                # Fall back to basic trimming if distillation fails
                if len(active_context) > MAX_CONTEXT_BEFORE_ROTATION:
                    # Simply trim to keep only the most recent portion that fits
                    trimmed_portion = active_context[-MAX_CONTEXT_BEFORE_ROTATION:]
                    await self.memory.save_active_context(session_id, trimmed_portion)
                    active_context = trimmed_portion
                    filtered["active_context"] = trimmed_portion
                    logger.info(f"Fallback: Trimmed context to {len(active_context)} chars")

        # Optional: Append a distiller compact summary to the active context for future turns
        try:
            compact_summary = await self.distiller.make_compact_summary(relevant_memories, summaries, active_context, user_input_processed)
            if compact_summary and len(compact_summary.strip()) > 0:
                # Append only if not already present in the active_context
                if compact_summary.strip() not in (active_context or "")[-1200:]:
                    new_active = (active_context or "") + "\n" + compact_summary
                    await self.memory.save_active_context(session_id, new_active)
                    # Update filtered active_context to include compact summary so prompt contains it
                    filtered["active_context"] = (filtered.get("active_context") or "") + "\n" + compact_summary
        except Exception as e:
            logger.debug(f"Failed to append distiller summary to active context: {e}")

        # Append Relevant Memories last (wrapped in explicit <memory> tags) so the model can
        # consult them as the freshest verified context prior to generation. We include provenance
        # attributes (id, source, status, date) to make it explicit which memories are factual.
        if filtered.get("relevant_memories"):
            mem_parts = []
            for mem in filtered['relevant_memories']:
                try:
                    mid = mem.get('id') or mem.get('memory_id') or ''
                    meta = mem.get('metadata') or {}
                    if isinstance(meta, str):
                        import json as _json
                        try:
                            meta = _json.loads(meta)
                        except Exception:
                            meta = {}
                    src = meta.get('source') or mem.get('source') or 'neo4j'
                    status = meta.get('status') or 'unverified'
                    date = mem.get('timestamp') or meta.get('created_at') or ''
                    content = mem.get('content') or ''
                    # Escape XML-sensitive characters in content
                    import xml.sax.saxutils as saxutils
                    esc_content = saxutils.escape(str(content))
                    mp = f'<memory id="{mid}" source="{saxutils.escape(str(src))}" status="{saxutils.escape(str(status))}" date="{saxutils.escape(str(date))}">{esc_content}</memory>'
                    mem_parts.append(mp)
                except Exception:
                    continue
            parts.append("<retrieved_memory>\n" + "\n".join(mem_parts) + "\n</retrieved_memory>")

        # Include historical summaries under a dedicated tag as <memory> entries with provenance
        if filtered.get("summaries"):
            hist_parts = []
            for s in filtered['summaries']:
                try:
                    import xml.sax.saxutils as saxutils
                    # summaries are simple text strings; for provenance, we treat them as 'summary' source
                    date = ''
                    mp = f'<memory id="" source="neo4j" status="verified" date="{date}">{saxutils.escape(str(s))}</memory>'
                    hist_parts.append(mp)
                except Exception:
                    continue
            parts.append('<historical_summaries>\n' + '\n'.join(hist_parts) + '\n</historical_summaries>')

        return "\n\n".join(parts)
    
    async def _retrieve_relevant_memories(self, query: str, limit: int = 15) -> list:
        """
        ENHANCED: Hybrid memory retrieval (Vector + Full-Text).
        
        1. Vector Search (Semantic) - Finds conceptually related memories
        2. Full-text search (Lexical) - Finds exact keyword matches
        3. Fallback to recent - If no matches found
        """
        # DEBUG: Log memory retrieval
        logger.debug(f"=== _retrieve_relevant_memories START ===")
        logger.debug(f"Query: {query[:100]}...")
        
        memories = []
        seen_ids = set()

        # Strategy 1: Vector Search (Semantic)
        if self.memory.vector_adapter and getattr(self.memory, "_vector_enabled", False):
            try:
                logger.debug("Attempting vector search...")
                # Generate embedding
                embeddings = await self.llm.get_embeddings(query)
                if embeddings and len(embeddings) > 0:
                    embedding = embeddings[0] if isinstance(embeddings[0], list) else embeddings
                    vector_results = await self.memory.vector_adapter.query_vector(embedding, top_k=limit)
                    
                    for res in vector_results:
                        # Convert vector result to memory dict format
                        mem = {
                            "id": res.get("node_id"),
                            "memory_id": res.get("node_id"),
                            "content": res.get("metadata", {}).get("content"),
                            "category": res.get("metadata", {}).get("category"),
                            "importance": res.get("metadata", {}).get("importance", 5),
                            "score": res.get("score"),
                            "source": "vector",
                            "metadata": res.get("metadata", {})
                        }
                        # Filter memory items by provenance / origin rules
                        if not self._memory_is_allowed(mem):
                            continue
                        if mem["id"] and mem["id"] not in seen_ids:
                            memories.append(mem)
                            seen_ids.add(mem["id"])
                    logger.debug(f"Vector search returned {len(vector_results)} results")
            except Exception as e:
                logger.warning(f"Vector search failed: {e}")

        # Strategy 2: Full-text search on actual content
        # Extract key terms (words longer than 3 chars)
        words = query.lower().split()
        keywords = [w.strip('.,!?;:()[]{}') for w in words if len(w) > 3]
        
        # Try each significant keyword with full-text search
        lexical_memories = []
        for keyword in keywords[:5]:  # Top 5 keywords
            results = await self.memory.search_memories_neo4j(
                query_text=keyword,
                limit=limit
            )
            lexical_memories.extend(results)
        
            for m in lexical_memories:
                # ensure metadata is loaded as a dict
                if isinstance(m.get('metadata'), str):
                    try:
                        import json as _json
                        m['metadata'] = _json.loads(m['metadata'])
                    except Exception:
                        m['metadata'] = {}
                m["source"] = "lexical"
                if not self._memory_is_allowed(m):
                    continue
                if m['id'] not in seen_ids:
                    seen_ids.add(m['id'])
                    memories.append(m)
        
        # If we found memories, re-rank/sort them
        if memories:
            # Simple scoring: Vector score (if present) vs Lexical score (if present)
            def score_mem(m: dict):
                base_score = float(m.get('score', 0) or 0)
                imp = float(m.get('importance', 5)) / 10.0
                return base_score + imp * 0.2

            scored_sorted = sorted(memories, key=score_mem, reverse=True)
            return scored_sorted[:limit]
        
        # Strategy 3: Retrieve ContextGist memories that may contain historical context
        # These are compressed summaries of old context that was rotated out
        try:
            logger.debug("Checking for relevant ContextGist memories...")
            context_gist_query = f"context gist {query}" if len(query.split()) < 10 else query
            gist_memories = await self.memory.search_memories_neo4j(
                query_text=context_gist_query,
                category="context_gist",
                limit=5
            )

            for gist in gist_memories:
                # ensure metadata is loaded as a dict
                if isinstance(gist.get('metadata'), str):
                    try:
                        import json as _json
                        gist['metadata'] = _json.loads(gist['metadata'])
                    except Exception:
                        gist['metadata'] = {}
                gist["source"] = "context_gist"
                if not self._memory_is_allowed(gist):
                    continue
                if gist['id'] not in seen_ids:
                    seen_ids.add(gist['id'])
                    memories.append(gist)
                    logger.debug(f"Included ContextGist memory: {gist['id']}")
        except Exception as e:
            logger.warning(f"Failed to retrieve ContextGist memories: {e}")

        # Strategy 4: Recent memories (Fallback)
        if not memories:
            logger.debug("No matches found, falling back to recent memories")
            all_recent = []
            for category in ['event', 'idea', 'task', 'person', 'code', 'general', 'genesis', 'context_gist']:
                recent = await self.memory.get_recent_by_category(category, limit=3)
                all_recent.extend(recent)

            # Sort by importance and recency
            memories = sorted(
                all_recent,
                key=lambda x: (x.get('importance', 0), x.get('created_at', '')),
                reverse=True
            )[:limit]

        # Filter items through same provenance filter
        filtered_memories = []
        for m in memories:
            if self._memory_is_allowed(m):
                filtered_memories.append(m)

        return filtered_memories

    def _memory_is_allowed(self, mem: dict) -> bool:
        """Return True if a memory should be used for injection into prompts.

        Rules:
        - Allow if metadata.status == 'committed'
        - Allow if metadata.app_id exists and metadata.source is not a blacklisted file-based generator
        - Block if content or metadata indicate 'thinking_content' or '[PLANNER]' or 'dry-run'
        - Block if metadata.source indicates combined_text/prompt_logs or simple dev file sources
        """
        try:
            # Conservative defaults
            md = mem.get('metadata') or {}
            if isinstance(md, str):
                import json as _json
                try:
                    md = _json.loads(md)
                except Exception:
                    md = {}
            # If explicit status exists and it's committed, allow
            if md.get('status') == 'committed':
                return True
            # If explicit status exists and not committed, block
            if md.get('status') and md.get('status') != 'committed':
                return False
            # Block obvious dev/test file sources
            src = (md.get('source') or md.get('path') or '')
            if isinstance(src, str):
                lc = src.lower()
                blacklisted = ['combined_text', 'combined_text2', 'prompt-logs', 'prompt_logs', 'calibration_run', 'dry-run', 'dry_run', 'tests/', 'weaver']
                for b in blacklisted:
                    if b in lc:
                        return False
            # Block content and metadata containing 'thinking_content' or planner markers
            cont = (mem.get('content') or '')
            if isinstance(cont, str) and ('thinking_content' in cont or '[planner]' in cont.lower()):
                return False
            if 'thinking_content' in str(md).lower():
                return False
            # If we have a valid app_id, we can generally allow it as a verified source
            if md.get('app_id'):
                return True
            # Otherwise conservatively block to avoid file/text provenance contamination
            return False
        except Exception:
            # If anything goes wrong, be conservative and block the memory from retrieval
            return False
    
    async def update_context(self, session_id: str, user_input: str, assistant_response: str):
        current = await self.memory.get_active_context(session_id)
        new_turn = f"User: {user_input}\nAssistant: {assistant_response}\n"
        updated_context = current + "\n" + new_turn

        token_count = self.memory.count_tokens(updated_context)

        if token_count > settings.summarize_threshold:
            summary = await self._summarize_context(updated_context)
            await self.memory.flush_to_neo4j(session_id, summary, original_tokens=token_count)
            recent_turns = "\n".join(updated_context.split("\n")[-25:])  # Keep more recent turns (from 10 to 25)
            await self.memory.save_active_context(session_id, recent_turns)
        else:
            await self.memory.save_active_context(session_id, updated_context)
    
    async def _summarize_context(self, context: str) -> str:
        """
        CHUNKED Markovian summarization with Distiller annotation.
        
        Instead of choking on large context, process in chunks:
        1. Split context into digestible chunks
        2. Distiller annotates meaning for each chunk
        3. Combine annotations into final summary
        """
        # Token budget: For 8K context model
        # System (200) + Output (1000) + Safety (300) = 1500 reserved
        # Available for input: ~6500 tokens
        CHUNK_SIZE = 5000  # tokens (~20,000 chars)
        
        # Rough token estimation: ~4 chars per token
        char_chunk_size = CHUNK_SIZE * 4
        
        # If context fits in one chunk, process directly
        if len(context) <= char_chunk_size:
            return await self._summarize_single_chunk(context)
        
        # Otherwise, chunk and process iteratively
        print(f"ðŸ§© Large context detected ({len(context)} chars) - chunking...")
        
        chunks = []
        start = 0
        while start < len(context):
            end = min(start + char_chunk_size, len(context))
            chunk_text = context[start:end]
            chunks.append(chunk_text)
            start = end
        
        print(f"   Split into {len(chunks)} chunks")
        
        # Process each chunk with Distiller
        annotated_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"   Processing chunk {i+1}/{len(chunks)}...")
            
            # Distiller annotates the chunk's meaning
            annotation = await self.distiller.annotate_chunk(chunk, chunk_number=i+1, total_chunks=len(chunks))
            annotated_chunks.append(f"[Chunk {i+1}] {annotation}")
        
        # Combine all annotations into a coherent summary
        combined = "\n\n".join(annotated_chunks)
        
        # Final compression pass
        final_summary = await self._compress_annotations(combined)
        
        print(f"âœ… Chunked summary complete")
        return final_summary
    
    async def _summarize_single_chunk(self, context: str) -> str:
        """Summarize a single chunk of context - preserve granular details."""
        system_prompt = """You are a memory summarizer. Create a comprehensive summary that preserves granular details.

Focus on:
- EXACT facts, numbers, and entity names (never generalize)
- All decisions and conclusions reached
- Different perspectives or options discussed
- Open questions and follow-ups
- Technical details, configurations, or specifications
- Specific code snippets or examples

Preserve specificity. This summary will be the ONLY memory of this conversation."""

        summary = await self.llm.generate(
            prompt=f"Summarize this conversation:\n\n{context}",
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=1200  # Increased from 1000 to allow more detail
        )
        return summary
    
    async def _compress_annotations(self, combined_annotations: str) -> str:
        """Compress multiple chunk annotations into a final summary while preserving details."""
        system_prompt = """You are synthesizing multiple memory annotations into one coherent summary.

Each annotation represents a chunk of a larger conversation. Synthesize them into:
- A unified narrative with all important facts preserved
- Key facts, numbers, and entities from ALL chunks (be exhaustive)
- Important patterns and recurring themes across the full conversation
- All decisions, conclusions, and open questions
- Technical details and specifications that shouldn't be lost

Preserve granularity and specificity across all chunks."""

        # If annotations are still too large, truncate to most recent
        max_chars = 6000 * 4  # Increased from 4000 to ~6000 tokens
        if len(combined_annotations) > max_chars:
            combined_annotations = "...[earlier annotations truncated]...\n\n" + combined_annotations[-max_chars:]
        
        final = await self.llm.generate(
            prompt=f"Synthesize these chunk annotations:\n\n{combined_annotations}",
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=1200  # Increased from 1000 to preserve more detail
        )
        return final



--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\context.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller.py (Section: BACKEND_PYTHON) ---

"""Thin wrapper for the canonical Distiller implementation.

This module intentionally re-exports the canonical implementation in
`core.distiller_impl` so that external callers can import `core.distiller`
without depending on a specific implementation file.
"""
from .distiller_impl import (
	DistilledEntity,
	DistilledMoment,
	Distiller,
	distill_moment,
	annotate_chunk,
	_safe_validate_moment,
	filter_and_consolidate,
	make_compact_summary,
)

__all__ = [
	"DistilledEntity",
	"DistilledMoment",
	"Distiller",
	"distill_moment",
	"annotate_chunk",
	"_safe_validate_moment",
	"filter_and_consolidate",
	"make_compact_summary",
]


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller_impl.py (Section: BACKEND_PYTHON) ---

"""Canonical Distiller implementation used across ECE_Core.

This implementation provides a compact Distiller API that mirrors the
legacy summarization and entity extraction methods used in earlier versions: `distill_moment`,
`annotate_chunk`, `filter_and_consolidate`, `make_compact_summary`, and
validation helpers. The file is intentionally single-copy, deterministic,
and has minimal dependencies to simplify testing.
"""
from __future__ import annotations

import asyncio
import json
import logging
import re
import uuid
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional
from collections import OrderedDict
import hashlib as _hashlib
import json as _json
from typing import Tuple

_redis_client = None
_redis_connect_lock = asyncio.Lock()

from pydantic import BaseModel, Field, ValidationError, validator
from src.config import settings
from src.content_utils import clean_content, has_technical_signal, is_token_soup, sanitize_token_soup, normalize_technical_content

logger = logging.getLogger(__name__)

# Simple in-memory distillation cache to avoid repeated LLM calls during ingestion
_distill_cache: "OrderedDict[str, Any]" = OrderedDict()
_distill_cache_limit = 4096
_llm_semaphore: Optional[asyncio.Semaphore] = None


class DistilledEntity(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    text: str
    type: Optional[str] = None
    score: Optional[float] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @validator("text")
    def not_empty_text(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("Entity text must be non-empty")
        return v.strip()


class DistilledMoment(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    text: str
    summary: Optional[str] = None
    entities: List[DistilledEntity] = Field(default_factory=list)
    score: float = Field(default=0.5, description="Salience score (0.0-1.0)")
    created_at: datetime = Field(default_factory=datetime.utcnow)

    @validator("text")
    def not_empty_text(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("Moment text must be non-empty")
        return v.strip()


def _normalize_entity_dict(e: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(e, dict):
        return {}
    if "text" not in e and "name" in e:
        e = dict(e)
        e["text"] = e.pop("name")
    return e


def _simple_entity_extraction(text: str, max_entities: int = 10) -> List[DistilledEntity]:
    # Add technical entity extraction if a technical signal exists
    from src.content_utils import has_technical_signal
    entities: List[DistilledEntity] = []
    seen = set()
    if has_technical_signal(text):
        # extract version numbers, file paths, package names, and error codes
        version_re = re.compile(r'v\d+\.\d+(?:\.\d+)?')
        path_re = re.compile(r'\b(?:[A-Za-z0-9\-_/\\]+\/[A-Za-z0-9\-_.]+)\b')
        pkg_re = re.compile(r'\b(?:npm|pip|apt-get|docker|cargo)\b', re.IGNORECASE)
        for m in version_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='version'))
        for m in path_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='path'))
        for m in pkg_re.findall(text):
            key = m.lower()
            if key not in seen:
                seen.add(key)
                entities.append(DistilledEntity(text=m, type='package'))
        # also fallback to proper nouns
        pattern = r"\b(?:[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b"
        matches = re.findall(pattern, text)
        for m in matches:
            k = m.strip().lower()
            if k in seen:
                continue
            seen.add(k)
            entities.append(DistilledEntity(text=m, type='proper_noun'))
        return entities[:max_entities]
    pattern = r"\b(?:[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b"
    matches = re.findall(pattern, text)
    out: List[DistilledEntity] = []
    for m in matches:
        key = m.strip().lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(DistilledEntity(text=m, type="proper_noun"))
        if len(out) >= max_entities:
            break
    return out


async def _maybe_await(v: Any) -> Any:
    if asyncio.iscoroutine(v):
        return await v
    return v


class Distiller:
    def __init__(self, llm_client: Optional[Any] = None):
        self.llm = llm_client

    async def _call_llm(self, text: str, skip_chunking: bool = False, max_entities: int = 10) -> Any:
        if not self.llm:
            raise RuntimeError("No LLM configured")
        global _llm_semaphore
        if _llm_semaphore is None:
            _llm_semaphore = asyncio.Semaphore(getattr(settings, 'llm_concurrency', 4))
        # Prefer the `generate` API when present (modern LLMs), but support `complete`
        # for legacy clients. Also ensure we use callable attributes (MagicMock
        # will report attributes even when not set). This prevents calling
        # auto-generated MagicMock attributes which return a MagicMock instance
        # rather than the configured AsyncMock return value.
        try:
            if hasattr(self.llm, "generate") and callable(getattr(self.llm, "generate", None)):
                # Allow LLM client to optionally force remote API usage; we rely on the LLM client
                # to raise ContextSizeExceededError when it determines the prompt would exceed server context
                async with _llm_semaphore:
                    return await _maybe_await(self.llm.generate(text))
            if hasattr(self.llm, "complete") and callable(getattr(self.llm, "complete", None)):
                return await _maybe_await(self.llm.complete(text))
        except Exception as e:
            # If the LLM indicates the context is too large and we have not yet chunked, perform chunking
            from src.llm import ContextSizeExceededError
            if isinstance(e, ContextSizeExceededError) and not skip_chunking:
                logger.debug("LLM reported ContextSizeExceeded; falling back to chunk-and-merge strategy")
                return await self._chunk_and_distill(text, max_entities=max_entities)
            raise
        # If we reach here, LLM didn't have expected interface
        raise RuntimeError("LLM missing expected method")
        raise RuntimeError("LLM missing expected method")

    async def distill_moment(self, text: str, chunk_index: Optional[int] = None, total_chunks: Optional[int] = None, max_entities: int = 10, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        if not text or not text.strip():
            raise ValueError("text must be non-empty")
        # Keep a copy of the raw text for potential normalizations
        raw_text = text
        # Clean text before distillation while preserving technical signals
        tech = has_technical_signal(text)
        if tech:
            # Preserve technical artifacts, but reduce obvious noise and annotate context
            text = clean_content(text, remove_emojis=False, remove_non_ascii=False, annotate_technical=True)
        else:
            text = clean_content(text, remove_emojis=True, remove_non_ascii=False, annotate_technical=False)
        # Detect token-soup/corrupted content, attempt normalization, then fallback to sanitize
        if is_token_soup(text):
            logger.warning("Detected token-soup content; attempting normalization to preserve technical context")
            # Attempt to normalize technical content (map ANSI, paths, and hex dumps to human tags) and retry
            try:
                normalized = normalize_technical_content(raw_text)
                cleaned_normalized = clean_content(normalized, remove_emojis=False, remove_non_ascii=False)
            except Exception as e:
                logger.debug(f"Normalization failed: {e}")
                cleaned_normalized = ''
            # If normalization produced a non-soup text, proceed with it
            if cleaned_normalized and not is_token_soup(cleaned_normalized):
                text = cleaned_normalized
            else:
                # Keep technical content if flagged; otherwise sanitize aggressively
                sanitized = text if tech else sanitize_token_soup(text)
                # Return a safe fallback summary and minimal entity extraction
                entities = _simple_entity_extraction(sanitized, max_entities=max_entities)
                summary = (sanitized[:300] + '...') if len(sanitized) > 300 else sanitized
                moment = DistilledMoment(text=sanitized, summary=summary, entities=entities, score=0.1)
                return moment.dict()
        entities: List[DistilledEntity] = []
        summary: Optional[str] = None
        if not self.llm:
            return {"summary": text[:200] + "...", "entities": []}
        # Check cache before calling LLM (avoid repeated distillations during ingestion)
        try:
            # Include metadata in the hash so that different metadata results can be cached separately
            content_hash = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
            cached = _distill_cache.get(content_hash)
            if cached:
                return cached
        except Exception:
            pass
        # Use metadata heuristics to avoid LLM calls for code/log-like artifacts
        try:
            md_path = (metadata or {}).get('path') if isinstance(metadata, dict) else None
            if md_path and isinstance(md_path, str):
                lower = md_path.lower()
                if lower.endswith(('.py', '.js', '.ts', '.java', '.go', '.rs', '.c', '.cpp', '.sh', '.md', '.log')) or '/logs/' in lower or lower.endswith('.log'):
                    entities = _simple_entity_extraction(text, max_entities=max_entities)
                    summary = (text[:400] + '...') if len(text) > 400 else text
                    moment = DistilledMoment(text=text, summary=summary, entities=entities, score=0.1)
                    return moment.dict()
        except Exception:
            pass
        try:
            raw = await self._call_llm(text, max_entities=max_entities)
            parsed = None
            if isinstance(raw, dict):
                parsed = raw
            elif isinstance(raw, str):
                try:
                    parsed = json.loads(raw)
                except Exception:
                    return {"summary": f"Error distilling chunk {chunk_index}. Raw: {str(raw)[:100]}...", "entities": []}
            if isinstance(parsed, dict):
                summary = parsed.get("summary") or parsed.get("title")
                score = float(parsed.get("score", 0.5))
                # Normalize score if 0-10
                if score > 1.0:
                    score = score / 10.0
                
                raw_entities = parsed.get("entities", [])
                for e in raw_entities[:max_entities]:
                    if isinstance(e, str):
                        entities.append(DistilledEntity(text=e))
                    elif isinstance(e, dict):
                        nd = _normalize_entity_dict(e)
                        try:
                            entities.append(DistilledEntity(**nd))
                        except ValidationError:
                            logger.debug("Invalid LLM entity: %s", e)
        except Exception:
            logger.exception("LLM call failed; falling back to simple extractor")
        if not entities:
            entities = _simple_entity_extraction(text, max_entities=max_entities)
            score = 0.5  # Default for fallback
            
        moment = DistilledMoment(text=text, summary=summary, entities=entities, score=score)
        # Write to in-memory cache with limited size
        try:
            _distill_cache[content_hash] = moment.dict()
            if len(_distill_cache) > _distill_cache_limit:
                # pop the oldest
                _distill_cache.popitem(last=False)
        except Exception:
            pass
        return moment.dict()

    async def _chunk_and_distill(self, text: str, max_entities: int = 10) -> Dict[str, Any]:
        """
        Chunk a large piece of text into smaller pieces and distill them individually, then combine summaries.
        This is a simple approach: summarize each chunk, collect summaries, then ask the LLM to summarize the summaries.
        """
        # Detect token-soup/corrupt content and attempt normalization before sanitizing
        if is_token_soup(text):
            logger.warning("Detected token-soup in _chunk_and_distill; attempting normalization")
            try:
                normalized = normalize_technical_content(text)
                cleaned_normalized = clean_content(normalized, remove_emojis=False, remove_non_ascii=False)
            except Exception as e:
                logger.debug(f"Normalization error in _chunk_and_distill: {e}")
                cleaned_normalized = ''
            if cleaned_normalized and not is_token_soup(cleaned_normalized):
                text = cleaned_normalized
            else:
                logger.warning("Normalization did not yield clean text; sanitizing token soup and returning fallback")
                sanitized = sanitize_token_soup(text)
                entities = _simple_entity_extraction(sanitized, max_entities=max_entities)
                summary = (sanitized[:300] + '...') if len(sanitized) > 300 else sanitized
                return {"summary": summary, "entities": [e.dict() for e in entities], "score": 0.1}

        # Estimate tokens and char conversion heuristic (approx 4 chars per token)
        # Prefer to use a chunk size that's smaller than the server context if we detected it
        chunk_tokens = settings.archivist_chunk_size
        if hasattr(self.llm, '_detected_server_context_size') and self.llm._detected_server_context_size:
            try:
                detected = int(self.llm._detected_server_context_size)
                # leave a buffer for prompt and final summary
                usable = max(256, detected - 512)
                if usable < chunk_tokens:
                    chunk_tokens = usable
            except Exception:
                pass
        overlap_tokens = settings.archivist_overlap
        chars_per_token = 4
        chunk_chars = chunk_tokens * chars_per_token
        overlap_chars = overlap_tokens * chars_per_token
        text_len = len(text)
        chunks = []
        start = 0
        while start < text_len:
            end = min(start + chunk_chars, text_len)
            # Try to split at newline within the window for semantic boundaries
            seg = text[start:end]
            if end < text_len:
                last_newline = seg.rfind('\n')
                if last_newline > int(chunk_chars * 0.5):
                    end = start + last_newline
                    seg = text[start:end]
            chunks.append(seg)
            # Advance, with overlap
            start = max(0, end - overlap_chars)
        logger.info(f"Chunked text into {len(chunks)} parts for distillation")
        # Distill each chunk
        chunk_summaries = []
        chunk_entities = []
        for i, c in enumerate(chunks):
            try:
                res = await self._call_llm(c, skip_chunking=True, max_entities=max_entities)
            except Exception as e:
                logger.warning(f"Failed to distill chunk {i} independently: {e}")
                continue
            parsed = None
            if isinstance(res, dict):
                parsed = res
            elif isinstance(res, str):
                try:
                    parsed = json.loads(res)
                except Exception:
                    parsed = {"summary": res}
            if isinstance(parsed, dict):
                chunk_summaries.append(parsed.get("summary") or parsed.get("text") or "")
                raw_entities = parsed.get("entities", []) or []
                for e in raw_entities:
                    if isinstance(e, dict):
                        nd = _normalize_entity_dict(e)
                        try:
                            chunk_entities.append(DistilledEntity(**nd))
                        except ValidationError:
                            logger.debug("Invalid LLM entity in chunk: %s", e)
                    elif isinstance(e, str):
                        chunk_entities.append(DistilledEntity(text=e))
        # Join summaries and ask for final summarization
        combined = "\n\n".join([s for s in chunk_summaries if s])
        # Build a compact instruction for final summarization
        final_prompt = f"Summarize the following chunk summaries into a concise JSON object with fields 'summary' and 'entities'. Summaries:\n\n{combined}"
        final_raw = await self._call_llm(final_prompt, skip_chunking=True, max_entities=max_entities)
        final_parsed = None
        if isinstance(final_raw, dict):
            final_parsed = final_raw
        elif isinstance(final_raw, str):
            try:
                final_parsed = json.loads(final_raw)
            except Exception:
                final_parsed = {"summary": final_raw}
        # Consolidate entities from chunk_entities and final_parsed entities
        entities = []
        if isinstance(final_parsed, dict):
            raw_entities = final_parsed.get("entities", []) or []
            for e in raw_entities:
                if isinstance(e, dict):
                    nd = _normalize_entity_dict(e)
                    try:
                        entities.append(DistilledEntity(**nd))
                    except ValidationError:
                        logger.debug("Invalid final entity: %s", e)
                elif isinstance(e, str):
                    entities.append(DistilledEntity(text=e))
        # Merge chunk_entities
        entities.extend(chunk_entities)
        entities = filter_and_consolidate(entities)
        summary = (final_parsed.get("summary") if isinstance(final_parsed, dict) else final_parsed.get("title") if isinstance(final_parsed, dict) else None) or (combined[:400] + '...')
        # Score: fallback average or default
        score = float(final_parsed.get("score", 0.5)) if isinstance(final_parsed, dict) and final_parsed.get("score") else 0.5
        return {"summary": summary, "entities": [e.dict() for e in entities], "score": score}

    async def annotate_chunk(self, text: str, chunk_number: Optional[int] = None, total_chunks: Optional[int] = None) -> str:
        moment = await self.distill_moment(text, chunk_index=chunk_number, total_chunks=total_chunks)
        entities = moment.get("entities", []) if isinstance(moment, dict) else moment.entities
        summary = moment.get("summary") if isinstance(moment, dict) else moment.summary
        ent_names = [e.get("text") if isinstance(e, dict) else e.text for e in entities]
        ent_str = ", ".join([n for n in ent_names if n])
        return (summary or text[:200]) + ("\n\nEntities: " + ent_str if ent_str else "")

    async def filter_and_consolidate(self, query: str, memories: List[Dict[str, Any]], summaries: List[Dict[str, Any]], active_turn: Optional[str] = None, active_context: Optional[str] = None) -> Dict[str, Any]:
        # Support both active_turn and active_context keywords (legacy vs new callers)
        active_turn = active_turn or active_context
        q_lower = (query or "").lower()
        # Also ensure we strip out test/dev/thinking content that may have slipped into memories
        def _is_memory_clean(m: dict) -> bool:
            if not m or not isinstance(m, dict):
                return False
            meta = m.get('metadata') or {}
            if isinstance(meta, str):
                try:
                    meta = json.loads(meta)
                except Exception:
                    meta = {}
            src = (meta.get('source') or meta.get('path') or '')
            if isinstance(src, str) and any(x in src.lower() for x in ('combined_text', 'prompt-logs', 'calibration_run', 'dry-run')):
                return False
            content = (m.get('content') or '')
            if isinstance(content, str) and ('thinking_content' in content or '[planner]' in content.lower()):
                return False
            return True

        relevant_memories = [m for m in (memories or []) if q_lower in (m.get("content", "") or "").lower() and _is_memory_clean(m)]
        # Preserve active context in output (maintain key for ContextManager)
        return {"summaries": summaries or [], "relevant_memories": relevant_memories, "active_context": active_turn or ""}

    async def make_compact_summary(self, memories: List[Dict[str, Any]], summaries: List[Dict[str, Any]], active_turn: Optional[str], new_input: Optional[str], max_sentences: int = 3) -> str:
        if new_input and new_input.strip():
            return new_input.strip()
        if summaries:
            texts = [s.get("summary") or s.get("text") for s in summaries]
            joined = " ".join([t for t in texts if t])
            sentences = re.split(r"(?<=[.!?])\s+", joined)
            return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])
        if memories:
            texts = [m.get("content") for m in memories if m.get("content")]
            joined = " ".join(texts)
            sentences = re.split(r"(?<=[.!?])\s+", joined)
            return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])
        return ""

    def _safe_validate_moment(self, moment_data: Dict[str, Any]) -> DistilledMoment:
        return DistilledMoment(**moment_data)


def filter_and_consolidate(entities: Iterable[DistilledEntity]) -> List[DistilledEntity]:
    by_key: Dict[str, DistilledEntity] = {}
    for e in entities:
        if not e or not e.text:
            continue
        key = e.text.strip().lower()
        existing = by_key.get(key)
        if not existing:
            by_key[key] = e
            continue
        if (existing.score or 0) < (e.score or 0):
            by_key[key] = e
    return list(by_key.values())


def make_compact_summary(moment: DistilledMoment, max_sentences: int = 3) -> str:
    if moment.summary and moment.summary.strip():
        return moment.summary.strip()
    sentences = re.split(r"(?<=[.!?])\s+", moment.text)
    return " ".join([s.strip() for s in sentences if s.strip()][:max_sentences])


_default_distiller = Distiller()


async def distill_moment(text: str, llm_client: Optional[Any] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Dict[str, Any]:
    """Global convenience function. Accepts metadata to help Distiller choose strategy.
    If a Redis cache is configured, the function will attempt to reuse distillation results.
    """
    d = _default_distiller if llm_client is None else Distiller(llm_client)
    # If Redis caching enabled, try to fetch first
    try:
        if getattr(settings, 'memory_distill_cache_enabled', False) and getattr(settings, 'redis_url', None):
            global _redis_client
            async with _redis_connect_lock:
                if _redis_client is None:
                    try:
                        import redis.asyncio as aioredis
                        _redis_client = aioredis.from_url(settings.redis_url, decode_responses=True)
                        await _redis_client.ping()
                    except Exception:
                        _redis_client = None
            if _redis_client is not None:
                key = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
                try:
                    val = await _redis_client.get(key)
                    if val:
                        # parse and return
                        return _json.loads(val)
                except Exception:
                    # Ignore Redis errors and fall back to in-memory cache
                    pass
    except Exception:
        pass
    result = await d.distill_moment(text, metadata=metadata, **kwargs)
    # Cache to Redis + in-memory cache if enabled
    try:
        content_hash = _hashlib.sha256((text + _json.dumps(metadata or {}, sort_keys=True)).encode('utf-8')).hexdigest()
        _distill_cache[content_hash] = result
        if len(_distill_cache) > _distill_cache_limit:
            _distill_cache.popitem(last=False)
        if getattr(settings, 'memory_distill_cache_enabled', False) and getattr(settings, 'redis_url', None) and _redis_client:
            try:
                await _redis_client.set(content_hash, _json.dumps(result), ex=getattr(settings, 'memory_distill_cache_ttl', 86400))
            except Exception:
                pass
    except Exception:
        pass
    return result


async def annotate_chunk(text: str, llm_client: Optional[Any] = None, **kwargs: Any) -> str:
    d = _default_distiller if llm_client is None else Distiller(llm_client)
    return await d.annotate_chunk(text, **kwargs)


def _safe_validate_moment(moment_data: Dict[str, Any]) -> DistilledMoment:
    return _default_distiller._safe_validate_moment(moment_data)


__all__ = [
    "DistilledEntity",
    "DistilledMoment",
    "Distiller",
    "distill_moment",
    "annotate_chunk",
    "_safe_validate_moment",
    "filter_and_consolidate",
    "make_compact_summary",
]


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\distiller_impl.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\exceptions.py (Section: BACKEND_PYTHON) ---

"""
Custom exceptions for ECE_Core and Anchor

Simple, focused exception hierarchy for better error handling.
"""


class ECEError(Exception):
    """Base exception for all ECE_Core errors"""
    pass


class ConfigurationError(ECEError):
    """Configuration loading or validation failed"""
    pass


class MemoryError(ECEError):
    """Memory system (Redis/Neo4j) errors"""
    pass


class LLMError(ECEError):
    """LLM communication errors"""
    pass


class ToolCallError(ECEError):
    """Tool call parsing or execution errors"""
    pass


class MCPError(ECEError):
    """MCP server connection or tool errors"""
    pass


class ValidationError(ECEError):
    """Input validation errors"""
    pass


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\exceptions.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\graph.py (Section: BACKEND_PYTHON) ---

"""
Graph Reasoner: Memory retrieval using Graph-R1 patterns
Implements: think → generate query → retrieve subgraph → rethink
Based on Graph-R1 paper: arxiv.org/abs/2507.21892

Note: This implements the paper's iterative graph traversal for MEMORY RETRIEVAL,
not complex reasoning. It helps find relevant memories by exploring graph connections.
"""
import json
from typing import List, Dict, Any, Optional
from src.llm import LLMClient
from src.memory import TieredMemory

class GraphReasoner:
    """
    Memory retrieval using Graph-R1 patterns.
    Implements iterative "think-query-retrieve-rethink" cycle for finding relevant memories.
    
    Note: "Reasoning" here means iterative graph traversal to find connected memories,
    not complex logical reasoning. It's a smart retrieval strategy.
    """
    
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        self.max_iterations = 5  # Markovian thinking: small fixed iterations
    
    async def reason(self, session_id: str, question: str) -> Dict[str, Any]:
        """
        Main reasoning loop: think → query → retrieve → rethink
        Returns final answer with reasoning trace
        """
        reasoning_trace = []
        current_thought = question
        retrieved_context = []
        
        for iteration in range(self.max_iterations):
            # Step 1: Think (high-level planning)
            thought = await self._think(current_thought, retrieved_context, iteration)
            reasoning_trace.append({
                "iteration": iteration,
                "thought": thought,
                "type": "planning"
            })
            
            # Step 2: Generate query from thought
            query = await self._generate_query(thought, question)
            reasoning_trace.append({
                "iteration": iteration,
                "query": query,
                "type": "query_generation"
            })
            
            # Step 3: Retrieve subgraph (from Neo4j memories)
            subgraph = await self._retrieve_subgraph(query, session_id)
            retrieved_context.append({
                "iteration": iteration,
                "subgraph": subgraph
            })
            reasoning_trace.append({
                "iteration": iteration,
                "retrieved": len(subgraph),
                "type": "retrieval"
            })
            
            # Step 4: Check if we can answer
            answer_attempt = await self._attempt_answer(
                question, 
                thought, 
                retrieved_context
            )
            
            if answer_attempt["confident"]:
                reasoning_trace.append({
                    "iteration": iteration,
                    "final_answer": answer_attempt["answer"],
                    "type": "answer"
                })
                return {
                    "answer": answer_attempt["answer"],
                    "reasoning_trace": reasoning_trace,
                    "iterations": iteration + 1,
                    "confidence": "high"
                }
            
            # Step 5: Rethink for next iteration
            current_thought = await self._rethink(
                thought, 
                retrieved_context, 
                question
            )
        
        # Max iterations reached - provide best attempt
        final_answer = await self._final_answer(question, retrieved_context)
        reasoning_trace.append({
            "iteration": self.max_iterations,
            "final_answer": final_answer,
            "type": "final_attempt"
        })
        
        return {
            "answer": final_answer,
            "reasoning_trace": reasoning_trace,
            "iterations": self.max_iterations,
            "confidence": "medium"
        }
    
    async def _think(self, current_thought: str, retrieved_context: List[Dict], iteration: int) -> str:
        """
        High-level planning step.
        Like HRM's abstract planning module.
        """
        context_summary = self._summarize_context(retrieved_context)
        
        prompt = f"""You are in iteration {iteration} of a reasoning process.

Current question/thought: {current_thought}

Retrieved context so far:
{context_summary}

What should you focus on next? Think step by step about:
1. What information is still missing?
2. What aspect of the question needs exploration?
3. What specific memory or knowledge would help?

Provide a concise plan (2-3 sentences)."""
        
        thought = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=150
        )
        
        return thought.strip()
    
    async def _generate_query(self, thought: str, original_question: str) -> str:
        """
        Generate specific query to retrieve relevant memories.
        """
        prompt = f"""Based on this reasoning step:
{thought}

And original question:
{original_question}

Generate a concise search query (keywords and concepts) to find relevant memories.
Query:"""
        
        query = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=50
        )
        
        return query.strip()
    
    async def _retrieve_subgraph(self, query: str, session_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve relevant memories from Neo4j (Moments + Entities + ContextGists).
        """
        # Extract potential categories and tags from query
        keywords = query.lower().split()

        # 1. Search Summaries (Legacy + New)
        summaries = await self.memory.get_summaries(session_id, limit=3)

        # 2. Search ContextGist memories (compressed historical context)
        # These contain summaries of old context that was rotated out
        context_gists = await self.memory.search_memories_neo4j(
            query_text=query,
            category="context_gist",
            limit=5
        )

        # 3. Search Moments & Entities via Cypher
        # We look for Moments with matching summary text OR linked to matching Entities
        cypher_query = """
        CALL db.index.fulltext.queryNodes("momentSearch", $query) YIELD node, score
        RETURN node.summary as content, score, "moment" as type, node.id as id
        UNION
        CALL db.index.fulltext.queryNodes("entitySearch", $query) YIELD node, score
        MATCH (m:Moment)-[:CONTAINS]->(node)
        RETURN m.summary as content, score, "moment_via_entity" as type, m.id as id
        ORDER BY score DESC
        LIMIT 5
        """

        # Fallback to simple keyword search if fulltext index fails or returns nothing
        # (Not implemented here for brevity, relying on index)

        try:
            records = await self.memory.execute_cypher(cypher_query, {"query": query})
        except Exception as e:
            # Fallback if index doesn't exist yet
            records = []

        # Combine results
        subgraph = []

        # Add ContextGists first (historical context)
        for gist in context_gists:
            subgraph.append({
                "type": "context_gist",
                "content": gist.get("content", gist.get("summary", "")),
                "id": gist.get("id", ""),
                "importance": gist.get("importance", 5),
                "metadata": gist.get("metadata", {})
            })

        # Add summaries (organized context)
        for summary in summaries:
            subgraph.append({
                "type": "summary",
                "content": summary["summary"],
                "timestamp": summary["timestamp"]
            })

        # Add direct moments and entities
        for rec in records:
            subgraph.append({
                "type": rec.get("type", "memory"),
                "content": rec.get("content", ""),
                "id": rec.get("id", ""),
                "score": rec.get("score", 0.5)
            })

        return subgraph
    
    async def _attempt_answer(
        self,
        question: str,
        current_thought: str,
        retrieved_context: List[Dict]
    ) -> Dict[str, Any]:
        """
        Attempt to answer based on current knowledge.
        Returns confidence and answer.
        """
        context_text = self._format_context(retrieved_context)

        # Check if we have ContextGist information that might be relevant
        has_context_gists = any(
            item.get("type") == "context_gist"
            for ctx in retrieved_context
            for item in ctx.get("subgraph", [])
        )

        prompt = f"""Question: {question}

Current reasoning: {current_thought}

Has historical context (ContextGists): {"YES" if has_context_gists else "NO"}

Retrieved context:
{context_text}

Can you answer the question with HIGH confidence based on this context?
If YES: Provide the answer.
If NO: Explain what information is still needed.

Format:
Confidence: [HIGH/LOW]
Answer or Reasoning: [your response]"""

        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=300
        )

        # Parse response
        lines = response.strip().split('\n')
        confident = "HIGH" in lines[0].upper() if lines else False
        answer = '\n'.join(lines[1:]).replace("Answer or Reasoning:", "").strip()

        return {
            "confident": confident,
            "answer": answer
        }
    
    async def _rethink(
        self,
        previous_thought: str,
        retrieved_context: List[Dict],
        original_question: str
    ) -> str:
        """
        Rethink based on what we've learned.
        Markovian: carry forward only essential state (textual summary).
        """
        context_summary = self._summarize_context(retrieved_context)

        # Check if we have ContextGist information that might provide historical context
        has_context_gists = any(
            item.get("type") == "context_gist"
            for ctx in retrieved_context
            for item in ctx.get("subgraph", [])
        )

        prompt = f"""Original question: {original_question}

Previous reasoning: {previous_thought}

Historical context available: {"YES" if has_context_gists else "NO"}

What we've learned:
{context_summary}

What should be the next focus?
- If historical context exists, consider if it might contain relevant background information
- If no historical context, focus on continuing to explore new information
Provide a refined thought (1-2 sentences)."""

        rethought = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=100
        )

        return rethought.strip()
    
    async def _final_answer(self, question: str, retrieved_context: List[Dict]) -> str:
        """
        Generate final answer after max iterations.
        """
        context_text = self._format_context(retrieved_context)
        
        prompt = f"""Question: {question}

All retrieved context:
{context_text}

Based on everything available, provide the best possible answer."""
        
        answer = await self.llm.generate(
            prompt=prompt,
            temperature=0.3,
            max_tokens=500
        )
        
        return answer.strip()
    
    def _summarize_context(self, retrieved_context: List[Dict]) -> str:
        """Create concise summary of retrieved context."""
        if not retrieved_context:
            return "No context retrieved yet."
        
        summary_parts = []
        for ctx in retrieved_context[-3:]:  # Last 3 iterations
            subgraph = ctx.get("subgraph", [])
            summary_parts.append(
                f"Iteration {ctx['iteration']}: Retrieved {len(subgraph)} items"
            )
        
        return "\n".join(summary_parts)
    
    def _format_context(self, retrieved_context: List[Dict]) -> str:
        """Format all retrieved context for prompts."""
        if not retrieved_context:
            return "No context available."
        
        formatted = []
        for ctx in retrieved_context:
            subgraph = ctx.get("subgraph", [])
            for item in subgraph:
                formatted.append(f"[{item['type']}] {item['content'][:200]}...")
        
        return "\n\n".join(formatted) if formatted else "No specific context."


class MarkovianReasoner:
    """
    Simpler Markovian-style reasoning without graph retrieval.
    Just: think → summarize → repeat with small context window.
    """
    
    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.max_chunks = 5
    
    async def reason(self, task: str, initial_context: str = "") -> str:
        """
        Chunked reasoning with textual carryover.
        Each chunk processes only previous summary + current task.
        """
        carryover = initial_context
        
        for chunk in range(self.max_chunks):
            # Process one reasoning chunk
            chunk_result = await self._process_chunk(task, carryover, chunk)
            
            # Check if task complete
            if chunk_result["complete"]:
                return chunk_result["answer"]
            
            # Carry forward only summary (Markovian property)
            carryover = chunk_result["summary"]
        
        # Final synthesis
        final = await self._synthesize(task, carryover)
        return final
    
    async def _process_chunk(
        self, 
        task: str, 
        carryover: str, 
        chunk_num: int
    ) -> Dict[str, Any]:
        """
        Process one reasoning chunk.
        Small context window: task + previous summary only.
        """
        prompt = f"""Task: {task}

Previous reasoning:
{carryover if carryover else 'Starting fresh.'}

Chunk {chunk_num+1}/{self.max_chunks}:
1. What's one key step toward solving this?
2. Is the task complete? (YES/NO)
3. Summary for next chunk (if incomplete):

Your response:"""
        
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.4,
            max_tokens=300
        )
        
        # Simple parsing
        lines = response.strip().split('\n')
        complete = any("YES" in line.upper() for line in lines[:5])
        
        return {
            "complete": complete,
            "answer": response if complete else None,
            "summary": response  # Entire response becomes carryover
        }
    
    async def _synthesize(self, task: str, final_carryover: str) -> str:
        """Final synthesis after all chunks."""
        prompt = f"""Task: {task}

Reasoning completed:
{final_carryover}

Provide final answer:"""
        
        answer = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=400
        )
        
        return answer.strip()


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\graph.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\intelligent_chunker.py (Section: BACKEND_PYTHON) ---

"""
Intelligent Chunker: Decides how to process each chunk.

Instead of blindly annotating everything, the Chunker:
1. Analyzes chunk content type (code, prose, logs, etc.)
2. Determines if annotation alone suffices or full detail needed
3. Routes to appropriate processing strategy
"""
from typing import List, Dict, Tuple, Literal
from src.llm import LLMClient
import re


ChunkStrategy = Literal["annotation_only", "distilled", "full_detail"]


class IntelligentChunker:
    """
    Analyzes chunks and routes them to the optimal processing strategy.
    
    This is the "decider" that makes chunking intelligent, not just mechanical.
    """
    
    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.chunk_size = 4000  # chars per chunk
        
    async def process_large_input(
        self, 
        user_input: str,
        query_context: str = ""
    ) -> str:
        """
        Main entry point for processing large user inputs.
        
        Returns a compressed context suitable for the LLM.
        """
        # Detect if input is large enough to warrant chunking
        if len(user_input) < self.chunk_size:
            return user_input  # No chunking needed
        
        # Split into semantic chunks
        chunks = self._split_semantic_chunks(user_input)
        
        # Process each chunk with appropriate strategy
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            strategy = await self._determine_strategy(chunk, query_context)
            processed = await self._process_chunk(chunk, i+1, len(chunks), strategy)
            processed_chunks.append(processed)
        
        # Combine processed chunks
        combined = self._combine_processed_chunks(processed_chunks)
        
        return combined
    
    def _split_semantic_chunks(self, text: str) -> List[str]:
        """
        Split text into chunks at semantic boundaries.
        
        Prefers to split at:
        1. Paragraph breaks (double newline)
        2. Code block boundaries (```)
        3. Section headers (##, ###)
        4. Sentence boundaries (. followed by newline)
        
        Avoids splitting mid-sentence or mid-code-block.
        """
        chunks = []
        current_chunk = ""
        
        # Split on paragraph boundaries first
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            # If adding this paragraph exceeds chunk size, save current chunk
            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # Add final chunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    async def _determine_strategy(
        self, 
        chunk: str, 
        query_context: str
    ) -> ChunkStrategy:
        """
        Decide processing strategy for this chunk.
        
        Returns:
        - "annotation_only": Just extract meaning, don't send full text
        - "distilled": Compress the chunk, send summary + key details
        - "full_detail": Send entire chunk (code, specs, novel info)
        """
        # Heuristic checks (fast, no LLM needed)
        
        # Code blocks always get full detail
        if "```" in chunk or "def " in chunk or "class " in chunk:
            return "full_detail"

        # If a file path indicating code is present, treat as full detail
        if re.search(r"\b[A-Za-z]:[\\/][\w\-\./\\]+\.py\b", chunk):
            return "full_detail"
        if "Traceback (most recent call last)" in chunk:
            return "distilled"
        
        # Error logs get distilled
        if "ERROR:" in chunk or "Traceback" in chunk or "Exception" in chunk:
            return "distilled"
        
        # Short, simple confirmations get annotation only
        if len(chunk) < 200 and any(word in chunk.lower() for word in 
                                     ["yes", "ok", "agree", "sure", "understood"]):
            return "annotation_only"
        
        # Terminal output (lots of technical info) gets distilled
        if any(marker in chunk for marker in ["INFO:", "WARNING:", "slot ", "srv "]):
            return "distilled"
        
        # For ambiguous cases, ask the LLM (slower but accurate)
        prompt = f"""Analyze this chunk and determine if it needs:
A) annotation_only - Simple, repetitive, or already-known context
B) distilled - Long but compressible (logs, verbose explanations)
C) full_detail - Code, specs, novel information requiring full context

Query context: {query_context[:200]}

Chunk preview:
{chunk[:500]}

Answer with just the letter (A, B, or C):"""
        
        response = await self.llm.generate(
            prompt=prompt,
            temperature=0.1,
            max_tokens=5
        )
        
        # Parse response
        response = response.strip().upper()
        if 'A' in response:
            return "annotation_only"
        elif 'B' in response:
            return "distilled"
        else:
            return "full_detail"
    
    async def _process_chunk(
        self,
        chunk: str,
        chunk_num: int,
        total_chunks: int,
        strategy: ChunkStrategy
    ) -> Dict[str, str]:
        """
        Process chunk according to determined strategy.
        """
        if strategy == "annotation_only":
            annotation = await self._annotate_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "annotation_only",
                "content": annotation,
                "original_length": len(chunk)
            }
        
        elif strategy == "distilled":
            summary = await self._distill_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "distilled",
                "content": summary,
                "original_length": len(chunk)
            }
        
        else:  # full_detail
            annotation = await self._annotate_chunk(chunk, chunk_num, total_chunks)
            return {
                "strategy": "full_detail",
                "content": chunk,
                "annotation": annotation,
                "original_length": len(chunk)
            }
    
    async def _annotate_chunk(self, chunk: str, chunk_num: int, total: int) -> str:
        """
        Extract meaning/themes from chunk without full content.
        """
        prompt = f"""Chunk {chunk_num}/{total} - Extract key meaning:

{chunk[:3000]}

In 2-3 sentences, state:
1. Main theme/topic
2. Key entities mentioned
3. Any decisions/insights

Be concise:"""
        
        annotation = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=150
        )
        
        return annotation.strip()
    
    async def _distill_chunk(self, chunk: str, chunk_num: int, total: int) -> str:
        """
        Compress chunk while preserving important details.
        """
        prompt = f"""Chunk {chunk_num}/{total} - Distill this down:

{chunk[:3000]}

Provide a compressed version that:
- Keeps critical facts, errors, decisions
- Removes verbose/repetitive content
- Stays under 300 words

Also rate the SALIENCE (importance) from 0.0 to 1.0.
- 1.0 = Critical architecture/decision
- 0.5 = Routine info
- 0.1 = Noise/logs

Format: JSON {{ "summary": "...", "score": 0.8 }}
Distilled version:"""
        
        distilled = await self.llm.generate(
            prompt=prompt,
            temperature=0.2,
            max_tokens=400
        )
        
        return distilled.strip()
    
    def _combine_processed_chunks(self, processed: List[Dict[str, str]]) -> str:
        """
        Combine processed chunks into final context.
        """
        parts = []
        
        for i, chunk_data in enumerate(processed):
            strategy = chunk_data['strategy']
            
            if strategy == "annotation_only":
                parts.append(f"[Chunk {i+1} summary] {chunk_data['content']}")
            
            elif strategy == "distilled":
                parts.append(f"[Chunk {i+1} distilled]\n{chunk_data['content']}")
            
            else:  # full_detail
                parts.append(
                    f"[Chunk {i+1} - FULL DETAIL]\n"
                    f"Note: {chunk_data['annotation']}\n"
                    f"Content:\n{chunk_data['content']}"
                )
        
        combined = "\n\n".join(parts)
        
        # Add metadata summary
        total_original = sum(c['original_length'] for c in processed)
        compression_ratio = len(combined) / total_original if total_original > 0 else 1
        
        header = f"""ðŸ§© Large context processed ({len(processed)} chunks)
Original: {total_original:,} chars â†’ Compressed: {len(combined):,} chars
Compression: {compression_ratio:.1%}

---

"""
        
        return header + combined


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\intelligent_chunker.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\llm.py (Section: BACKEND_PYTHON) ---

"""
Simple LLM client supporting both API servers and local GGUF models.

Supports:
- llama.cpp server (OpenAI-compatible API)
- Local GGUF files via llama-cpp-python
- MCP (Model Context Protocol) for reliable tool execution
"""
import httpx
import logging
import asyncio
from typing import Optional, List, Tuple, Dict
import math
from src.config import settings
import os
import re
import json
from src.chat_templates import chat_template_manager

logger = logging.getLogger(__name__)


class EmbeddingsAPIError(RuntimeError):
    """Raised when embedding API responds with an error. Contains parsed info if available.

    Attributes:
        status_code: numeric HTTP status code
        body: raw response body
        server_message: parsed message (if present)
        n_ctx: optional detected context size (tokens)
    """
    def __init__(self, message: str, status_code: Optional[int] = None, body: Optional[str] = None, server_message: Optional[str] = None, n_ctx: Optional[int] = None):
        super().__init__(message)
        self.status_code = status_code
        self.body = body
        self.server_message = server_message
        self.n_ctx = n_ctx


class LLMClient:
    """
    LLM client with automatic fallback:
    1. Try API server (llama.cpp server, LM Studio, etc.)
    2. Fall back to local GGUF loading if API unavailable
    """
    
    def __init__(self):
        # Normalize API base so we don't accidentally double-up /v1 segments
        api_base = settings.llm_api_base or ''
        api_base = api_base.rstrip('/')
        # If the configured base ended in /v1, strip it; we'll append /v1 endpoints below
        if api_base.endswith('/v1'):
            api_base = api_base[:-3]
        self.api_base = api_base
        # Embeddings may use a different base (embedding-only server on 8081). Prefer explicit embeddings base setting, fallback to api_base
        emb_base = getattr(settings, 'llm_embeddings_api_base', '') or settings.llm_api_base
        emb_base = str(emb_base).rstrip('/')
        if emb_base.endswith('/v1'):
            emb_base = emb_base[:-3]
        self.embeddings_base = emb_base
        # Resolve model name defensively to handle different possible config names
        # Some environments may provide either `llm_model_name` or `llm_model`.
        self.model = getattr(settings, 'llm_model_name', getattr(settings, 'llm_model', ''))
        self.model_path = settings.llm_model_path
        self.client = httpx.AsyncClient(timeout=settings.llm_timeout)

        # Chat template configuration
        # Use the resolved template which can auto-detect based on model name
        resolved_template = getattr(settings, 'resolved_chat_template', 'openai')
        self.chat_template_name = resolved_template
        self.chat_template = chat_template_manager.get_template(self.chat_template_name)
        
        # Lazy-load local model if needed
        self._local_llm = None
        self._use_local = False
        self._local_llm_embedding_enabled = False
        self._detected_model = None
        self._model_detection_attempted = False
        # Embeddings-specific detection
        self._detected_embeddings_model = None
        self._embeddings_model_detection_attempted = False
        # Detected server context size (tokens). May be populated by parsing API errors
        self._detected_server_context_size: Optional[int] = None
        # Force remote usage flag (skip local fallback)
        self.force_remote_api: bool = False
    
    async def detect_model(self) -> str:
        """
        Detect the actual model running on the API server.
        Makes a GET request to /v1/models endpoint.
        Returns: Model name or falls back to configured name.
        """
        if self._model_detection_attempted:
            return self._detected_model or self.model
        
        self._model_detection_attempted = True
        
        try:
            # Try to get models list from API
            response = await self.client.get(f"{self.api_base}/models")
            response.raise_for_status()
            result = response.json()
            
            if "data" in result and len(result["data"]) > 0:
                # Get first model or find best match
                models = result["data"]
                if isinstance(models[0], dict) and "id" in models[0]:
                    self._detected_model = models[0]["id"]
                    print(f"‚úÖ Detected model: {self._detected_model}")
                    try:
                        md = models[0]
                        context_keys = ['n_ctx_train', 'n_ctx', 'context', 'context_window', 'max_input_tokens', 'max_context_tokens', 'max_tokens']
                        for ck in context_keys:
                            if ck in md and isinstance(md[ck], int):
                                self._detected_server_context_size = md[ck]
                                print(f"üîé Detected server context size via model metadata: {self._detected_server_context_size}")
                                break
                    except Exception:
                        pass
                    return self._detected_model
        except Exception as e:
            print(f"‚ö†Ô∏è  Model detection failed: {e}")
        
        # Fallback to configured model
        self._detected_model = self.model
        print(f"üìã Using configured model: {self._detected_model}")
        return self._detected_model

    async def detect_embeddings_model(self) -> str:
        """
        Detect the model served by the embeddings base. Falls back to `settings.llm_embeddings_model_name` or general model.
        """
        if self._embeddings_model_detection_attempted:
            return self._detected_embeddings_model or settings.llm_embeddings_model_name or self.model
        self._embeddings_model_detection_attempted = True
        try:
            response = await self.client.get(f"{self.embeddings_base}/models")
            response.raise_for_status()
            result = response.json()
            if "data" in result and len(result["data"]) > 0:
                models = result["data"]
                if isinstance(models[0], dict) and "id" in models[0]:
                    self._detected_embeddings_model = models[0]["id"]
                    # Attempt to parse context window (tokens) from model metadata
                    md = models[0]
                    # Look for typical fields
                    context_keys = [
                        'n_ctx_train', 'n_ctx', 'context', 'context_window', 'max_input_tokens', 'max_context_tokens', 'max_tokens'
                    ]
                    for ck in context_keys:
                        if ck in md and isinstance(md[ck], int):
                            self._detected_server_context_size = md[ck]
                            print(f"üîé Detected embeddings model context tokens via model metadata: {self._detected_server_context_size}")
                            break
                    # If not found, attempt a details endpoint for the model
                    if not self._detected_server_context_size:
                        try:
                            model_detail_resp = await self.client.get(f"{self.embeddings_base}/models/{self._detected_embeddings_model}")
                            model_detail_resp.raise_for_status()
                            detail_json = model_detail_resp.json()
                            if isinstance(detail_json, dict):
                                for ck in context_keys:
                                    if ck in detail_json and isinstance(detail_json[ck], int):
                                        self._detected_server_context_size = detail_json[ck]
                                        print(f"üîé Detected embeddings model context tokens via model detail endpoint: {self._detected_server_context_size}")
                                        break
                        except Exception:
                            pass
                    print(f"‚úÖ Detected embeddings model: {self._detected_embeddings_model}")
                    return self._detected_embeddings_model
        except Exception as e:
            print(f"‚ö†Ô∏è  Embeddings model detection failed: {e}")
        # Fallback to configured embedding model or general model
        if settings.llm_embeddings_model_name:
            self._detected_embeddings_model = settings.llm_embeddings_model_name
            print(f"üìã Using configured embeddings model: {self._detected_embeddings_model}")
            return self._detected_embeddings_model
        self._detected_embeddings_model = self._detected_model or self.model
        print(f"üìã Using model for embeddings: {self._detected_embeddings_model}")
        return self._detected_embeddings_model
    
    def get_model_name(self) -> str:
        """Get the detected or configured model name (non-async version for printing)"""
        if self._detected_model:
            return self._detected_model
        return self.model
    
    def _init_local_model(self):
        """Initialize local GGUF model (lazy loading)"""
        if self._local_llm is not None:
            return
        
        try:
            from llama_cpp import Llama
            
            if not os.path.exists(self.model_path):
                print(f"‚ö†Ô∏è  Model not found: {self.model_path}")
                return
            
            print(f"üîß Loading local GGUF model: {self.model_path}")
            # Use setting to control whether the local model exposes embedding() API
            enable_embedding = getattr(settings, 'llm_local_embeddings', True)
            self._local_llm = Llama(
                model_path=self.model_path,
                n_ctx=settings.llm_context_size,
                n_gpu_layers=settings.llm_gpu_layers,
                n_threads=settings.llm_threads,
                verbose=False
                , embedding=enable_embedding
            )
            self._local_llm_embedding_enabled = enable_embedding
            print(f"‚úÖ Local model loaded")
        except ImportError:
            print("‚ö†Ô∏è  llama-cpp-python not installed. Install with: pip install llama-cpp-python")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load local model: {e}")

    def _parse_context_size_from_error(self, err_msg: str) -> Optional[int]:
        """
        Parse server error messages to extract numeric context size hints (n_ctx or n_ctx_slot).
        Returns numeric int context size if found, otherwise None.
        """
        if not err_msg:
            return None
        try:
            m = re.search(r"n_ctx_slot\s*[=:\s]\s*(\d+)", err_msg)
            if m:
                return int(m.group(1))
            m2 = re.search(r"context\s*(?:size|window)\s*:?\s*(\d+)", err_msg)
            if m2:
                return int(m2.group(1))
            m3 = re.search(r"task\.n_tokens\s*[=]\s*(\d+)", err_msg)
            if m3:
                return int(m3.group(1))
        except Exception:
            return None
        return None
    
    async def generate(self,
                      prompt: str,
                      max_tokens: Optional[int] = None,
                      temperature: float = None,
                      system_prompt: Optional[str] = None,
                      tools: Optional[List[Dict]] = None) -> str:
        """
        Generate completion using API server or local model.
        Automatically falls back to local if API fails.
        """
        temperature = temperature if temperature is not None else settings.llm_temperature
        max_tokens = max_tokens or settings.llm_max_tokens

        # Try API server first if not forced to use local
        api_exc = None
        if not self._use_local or self.force_remote_api:
            try:
                return await self._generate_api(prompt, max_tokens, temperature, system_prompt, tools)
            except Exception as e:
                api_exc = e
                print(f"‚ö†Ô∏è  API failed: {e}")
                # If we are forcing the remote API, do not fall back to local model
                if self.force_remote_api:
                    raise
                print(f"   Attempting fallback to local model...")

        # Try local model fallback (only if API failed or local use requested)
        try:
            return await self._generate_local(prompt, max_tokens, temperature, system_prompt)
        except Exception as local_exc:
            print(f"‚ö†Ô∏è  Local model failed: {local_exc}")
            # If there was also an API failure, raise a combined error for easier debugging
            if api_exc:
                raise RuntimeError(f"API error: {api_exc}; Local error: {local_exc}")
            # Otherwise, re-raise the local exception
            raise

    async def get_embeddings(self, texts: list[str] | str):
        """Return embeddings for a list of texts or single text using API or local model.

        Returns: List[List[float]] if input is list, else List[float] for single string.
        """
        if isinstance(texts, str):
            inputs = [texts]
        else:
            inputs = texts

        # Try using API
        try:
            # Determine the model for embeddings explicitly
            # 1) prefer detected embeddings model, 2) configured embedding model, 3) detected model for api_base, 4) fallback config
            model_for_embeddings = None
            if self._detected_embeddings_model:
                model_for_embeddings = self._detected_embeddings_model
            else:
                # try to detect embeddings model from embeddings base
                try:
                    model_for_embeddings = await self.detect_embeddings_model()
                except Exception:
                    model_for_embeddings = settings.llm_embeddings_model_name or self._detected_model or self.model

            payload = {"model": model_for_embeddings, "input": inputs}
            # Use dedicated embeddings base if available
            resp = await self.client.post(f"{self.embeddings_base}/v1/embeddings", json=payload)
            resp.raise_for_status()
            data = resp.json()
            # Data likely has structure {'data': [{'embedding': [...]}, ...]}
            if isinstance(data, dict) and "data" in data:
                embeddings = [d.get("embedding") for d in data["data"]]
                return embeddings
        except httpx.HTTPStatusError as http_ex:
            try:
                body_str = http_ex.response.text
            except Exception:
                body_str = "<no response body>"
            server_message = None
            try:
                body_json = http_ex.response.json()
                if isinstance(body_json, dict):
                    server_message = body_json.get('message') or body_json.get('error')
            except Exception:
                body_json = None
            # Try to parse n_ctx from the message if available
            n_ctx = None
            if server_message:
                n_ctx = self._parse_context_size_from_error(server_message)
            print(f"‚ö†Ô∏è  Embeddings API failed: {http_ex} (status={http_ex.response.status_code}): {body_str}")
            # Save any detected context size for adaptive chunking
            if n_ctx and not self._detected_server_context_size:
                self._detected_server_context_size = n_ctx
                print(f"üîé Detected server context size via embeddings API error: {n_ctx}")
            # If fallback is disabled, raise structured error for upstream re-chunking logic
            if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
                raise EmbeddingsAPIError("Embeddings API failed", status_code=http_ex.response.status_code, body=body_json or body_str, server_message=server_message, n_ctx=n_ctx)
        except Exception as e:
            # Generic exception - try to parse context size if possible and raise a structured error
            server_message = None
            n_ctx = None
            try:
                server_message = str(e)
                n_ctx = self._parse_context_size_from_error(server_message)
                if n_ctx and not self._detected_server_context_size:
                    self._detected_server_context_size = n_ctx
            except Exception:
                pass
            print(f"‚ö†Ô∏è  Embeddings API failed: {e}")
            if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
                raise EmbeddingsAPIError("Embeddings API failed", status_code=None, body=None, server_message=server_message, n_ctx=n_ctx)

        # Fallback to local model embeddings; llama-cpp-python may not expose embeddings method
        if not getattr(settings, 'llm_embeddings_local_fallback_enabled', False):
            raise RuntimeError("Embeddings API failed and local fallback for embeddings is disabled")
        try:
            # if we haven't initialized the local model yet, load it
            self._init_local_model()
            # Ensure local model is embedding-enabled when we want to call embed()
            if not self._local_llm_embedding_enabled:
                # try re-initializing with embeddings enabled
                print("üîß Reinitializing local model with embedding support")
                try:
                    # Force recreate with embedding enabled
                    # Destroy previous instance reference first
                    self._local_llm = None
                    # Call init to create with the setting in config
                    self._init_local_model()
                except Exception:
                    pass
            if self._local_llm is not None:
                # llama-cpp-python may expose an embeddings API in newer versions as embed()
                if hasattr(self._local_llm, "embed"):
                    res = self._local_llm.embed(inputs)
                    # Expect res to be list of embeddings
                    return res
        except Exception as e:
            print(f"‚ö†Ô∏è  Local embedding failed: {e}")

        raise RuntimeError("No embeddings method available (API or local model)")

    async def get_embeddings_for_documents(self, texts: list[str], chunk_size: Optional[int] = None, batch_size: int | None = None, min_batch: int = 1, delay: float = 0.15, max_retries: int = 3, chars_per_token: Optional[int] = None, max_chunk_tokens: Optional[int] = None):
        """
        Obtain a single embedding vector per document, even when documents are longer than `chunk_size`.
        Strategy:
          - Split each document into chunks up to `chunk_size` characters
          - Call get_embeddings() for all chunks in adaptive batches to avoid server 500s
          - Average embeddings for all chunks belonging to a document to obtain a final vector
        Returns: list[embedding] aligned to input `texts` (None where embedding failed)
        """
        if not texts:
            return []

        # Determine chunk_size in characters from token-based context detection if needed
        if chars_per_token is None:
            chars_per_token = getattr(settings, 'llm_chars_per_token', 4)
        detected_n_ctx = max_chunk_tokens or self._detected_server_context_size or getattr(settings, 'llm_context_size', None)
        if detected_n_ctx is None:
            detected_n_ctx = 4096
        if chunk_size is None:
            # Use a configurable fraction of the embeddings model context for chunking
            ratio = getattr(settings, 'llm_chunk_context_ratio', 0.5)
            tokens_per_chunk = max(64, int(math.floor(detected_n_ctx * float(ratio))))
            char_chunk_size = int(tokens_per_chunk * chars_per_token)
            # Cap by the configured embeddings default chunk size to avoid excessively large requests
            default_char_chunk = getattr(settings, 'llm_embeddings_chunk_size_default', 4096)
            chunk_size = min(default_char_chunk, char_chunk_size)
            print(f"üîß Computed chunk_size: {chunk_size} chars (tokens_per_chunk={tokens_per_chunk}, detected_n_ctx={detected_n_ctx}, chars_per_token={chars_per_token}, ratio={ratio})")

        # Build chunks per document
        docs_chunks = []  # list of lists
        chunk_to_doc_index = []  # flattened mapping index->doc_idx
        all_chunks = []
        for doc_idx, text in enumerate(texts):
            if text is None:
                docs_chunks.append([])
                continue
            if len(text) <= chunk_size:
                docs_chunks.append([text])
                all_chunks.append(text)
                chunk_to_doc_index.append(doc_idx)
            else:
                # split by whitespace preserving words across chunks to avoid cutting words (basic approach)
                chunks = []
                start = 0
                while start < len(text):
                    end = min(start + chunk_size, len(text))
                    # try to break on last whitespace if possible (avoid tokenization here)
                    if end < len(text):
                        wh = text.rfind(' ', start, end)
                        if wh > start:
                            end = wh
                    chunk = text[start:end]
                    chunks.append(chunk)
                    all_chunks.append(chunk)
                    chunk_to_doc_index.append(doc_idx)
                    start = end
                docs_chunks.append(chunks)

        # Resolve batch_size default from settings if None
        if batch_size is None:
            batch_size = getattr(settings, 'llm_embeddings_default_batch_size', 4)

        # Function to embed in batches with graceful shrinking on failure
        async def _split_text_into_smaller_chunks(text, target_size):
            # Splits `text` into chunks <= target_size by splitting on whitespace around midpoint.
            if len(text) <= target_size:
                return [text]
            # naive splitting by whitespace, try to preserve words
            mid = len(text) // 2
            left_break = text.rfind(' ', 0, mid)
            right_break = text.find(' ', mid, len(text))
            if left_break == -1 and right_break == -1:
                # no whitespace found; just split at mid
                left = text[:mid]
                right = text[mid:]
            else:
                # prefer nearest break
                if left_break == -1:
                    split_at = right_break
                elif right_break == -1:
                    split_at = left_break
                else:
                    # pick the break closer to mid
                    split_at = left_break if (mid - left_break) <= (right_break - mid) else right_break
                left = text[:split_at]
                right = text[split_at:].lstrip()
            res_left = await _split_text_into_smaller_chunks(left, target_size)
            res_right = await _split_text_into_smaller_chunks(right, target_size)
            return res_left + res_right

        # Use class-level context parser

        async def _embed_all_chunks(chunk_list, initial_batch=batch_size, min_batch=min_batch, delay_time=delay, max_retries_local=max_retries):
            n = len(chunk_list)
            results = [None] * n
            i = 0
            cur_batch = initial_batch
            while i < n:
                if cur_batch <= 0:
                    cur_batch = 1
                end = min(i + cur_batch, n)
                batch = chunk_list[i:end]
                try:
                    # call underlying get_embeddings which returns list of embeddings aligned with input
                    # Debug: show attempt
                    # print(f"üîç Embedding batch size {len(batch)} (starting idx {i})")
                    embs = await self.get_embeddings(batch)
                    if not embs:
                        # treat as failure to trigger shrinking
                        raise RuntimeError("Empty embeddings returned")
                    for j, emb in enumerate(embs):
                        results[i + j] = emb
                    i = end
                    await asyncio.sleep(delay_time)
                except Exception as e:
                    err_txt = str(e).lower()
                    # Attempt to parse server context size and record it to adaptively backoff
                    detected_ctx = None
                    if isinstance(e, EmbeddingsAPIError) and getattr(e, 'n_ctx', None):
                        detected_ctx = e.n_ctx
                    else:
                        detected_ctx = self._parse_context_size_from_error(str(e))
                    if detected_ctx and not self._detected_server_context_size:
                        self._detected_server_context_size = detected_ctx
                        print(f"üîé Detected server context size: {detected_ctx}")
                    # If the server returned a structured error saying the request is too large and provided n_ctx, resplit using tokens-based chunking
                    if isinstance(e, EmbeddingsAPIError) and getattr(e, 'n_ctx', None):
                        n_ctx = e.n_ctx
                        # Only proceed if the server-reported context is smaller than our current chunking
                        # Convert current chunk_size (chars) back to token-estimate to compare
                        cur_token_estimate = max(1, int(chunk_size / max(1, chars_per_token)))
                        if n_ctx < cur_token_estimate:
                            print(f"üîÅ Rebuilding chunks: server context {n_ctx} smaller than current chunk token estimate {cur_token_estimate}")
                            # compute new chunk_size based on n_ctx
                            new_tokens_per_chunk = max(64, int(math.floor(n_ctx * 0.8)))
                            new_chunk_chars = int(new_tokens_per_chunk * chars_per_token)
                            # Re-run with new chunk size
                            return await self.get_embeddings_for_documents(texts, chunk_size=new_chunk_chars, batch_size=initial_batch, min_batch=min_batch, delay=delay_time, max_retries=max_retries_local, chars_per_token=chars_per_token, max_chunk_tokens=n_ctx)
                    # If server returned a size-related error, try to re-chunk the offending content into smaller text chunks
                    if 'too large' in err_txt or 'increase the physical batch size' in err_txt or 'exceeds the available context size' in err_txt:
                        print(f"‚ö†Ô∏è  Embedding server indicates request too large: {e}. Attempting to split chunks further.")
                        # Re-chunk the current batch: replace each chunk with multiple smaller ones and try them individually
                        # Use configured backoff sequence if adaptive backoff is enabled
                        backoff_seq = getattr(settings, 'llm_embeddings_chunk_backoff_sequence', [4096, 2048, 1024, 512, 256, 128])
                        backoff_seq = sorted(list(dict.fromkeys(backoff_seq)), reverse=True)
                        # Ensure monotonic decreasing order and filter sizes smaller than current chunk_size
                        cur_chunk_size = chunk_size
                        # If server context size known, use that to cap backoff sizes
                        if self._detected_server_context_size and getattr(settings, 'llm_embeddings_adaptive_backoff_enabled', True):
                            # heuristically convert tokens to approximate chars using factor 4
                            approx_chars = max(128, int(self._detected_server_context_size * 4))
                            backoff_seq = [s for s in backoff_seq if s <= approx_chars]
                            if not backoff_seq:
                                backoff_seq = [max(128, approx_chars // 4)]

                        for j_local, original_chunk in enumerate(batch):
                            if not original_chunk:
                                continue
                            doc_idx_local = i + j_local
                            # attempt to split this chunk into smaller character-based subchunks using backoff sequence
                            smaller_chunks = None
                            for target_size in backoff_seq:
                                if target_size >= len(original_chunk):
                                    # no-op, chunk is small enough
                                    continue
                                try:
                                    smaller = await _split_text_into_smaller_chunks(original_chunk, target_size)
                                    if len(smaller) > 1:
                                        smaller_chunks = smaller
                                        break
                                except Exception:
                                    continue
                            if smaller_chunks is None:
                                # fall back to simple halving if we couldn't find a split in the sequence
                                try:
                                    smaller_chunks = await _split_text_into_smaller_chunks(original_chunk, max(128, len(original_chunk) // 2))
                                except Exception:
                                    smaller_chunks = [original_chunk]
                            if len(smaller_chunks) == 1:
                                # couldn't split, we'll let normal per-item retry handle
                                continue
                            # Try to embed these smaller chunks
                            # Note: we'll call get_embeddings directly with smaller chunks
                            try:
                                sub_embs = await self.get_embeddings(smaller_chunks)
                                # average back to a single vector
                                if sub_embs and len(sub_embs) > 0:
                                    vec_len = len(sub_embs[0])
                                    sum_vec = [0.0]*vec_len
                                    for sv in sub_embs:
                                        if not sv:
                                            continue
                                        for vi in range(len(sv)):
                                            sum_vec[vi] += sv[vi]
                                    avg_vec = [x/len(sub_embs) for x in sum_vec]
                                    # assign back
                                    results[doc_idx_local] = avg_vec
                                else:
                                    # fallthrough to shrinking behavior below
                                    pass
                            except Exception as esub:
                                # If the re-chunk attempt fails, we'll fall back to shrinking batch size
                                print(f"‚ö†Ô∏è  Subchunk embedding failed: {esub}")
                                pass
                        # Continue loop but shrink batch if necessary
                    # shrink batch if possible
                    if cur_batch > min_batch:
                        old_batch = cur_batch
                        cur_batch = max(min_batch, cur_batch // 2)
                        # double wait to be polite when we hit errors
                        await asyncio.sleep(delay_time * 2)
                        continue
                    # min-batch failing - try per item with retries
                    for j in range(i, end):
                        tries = 0
                        while tries < max_retries_local:
                            try:
                                em = await self.get_embeddings([chunk_list[j]])
                                results[j] = (em[0] if isinstance(em, list) and len(em) > 0 else None)
                                break
                            except Exception as e2:
                                tries += 1
                                await asyncio.sleep(delay_time * (tries + 1))
                        if tries >= max_retries_local and results[j] is None:
                            # give up this chunk
                            results[j] = None
                    i = end
            return results

        # Embed all chunks
        chunk_embeddings = await _embed_all_chunks(all_chunks, initial_batch=batch_size, min_batch=min_batch, delay_time=delay, max_retries_local=max_retries)

        # Now aggregate per document by averaging
        doc_embeddings = []
        # build list of lists per doc
        per_doc_embs = [[] for _ in range(len(texts))]
        for idx, emb in enumerate(chunk_embeddings):
            doc_idx = chunk_to_doc_index[idx]
            if emb is not None:
                per_doc_embs[doc_idx].append(emb)

        for doc_chunks_emb in per_doc_embs:
            if not doc_chunks_emb:
                doc_embeddings.append(None)
            else:
                # average the vectors elementwise
                length = len(doc_chunks_emb)
                # handle variable-length vectors unlikely but guard
                vec_len = len(doc_chunks_emb[0])
                sum_vec = [0.0] * vec_len
                for v in doc_chunks_emb:
                    if not v:
                        continue
                    for i in range(vec_len):
                        sum_vec[i] += v[i]
                avg_vec = [x / length for x in sum_vec]
                doc_embeddings.append(avg_vec)

        return doc_embeddings
    
    async def _generate_api(self,
                           prompt: str,
                           max_tokens: int,
                           temperature: float,
                           system_prompt: Optional[str],
                           tools: Optional[List[Dict]] = None) -> str:
        """Generate using API server (llama.cpp, LM Studio, etc.)"""

        # Detect model if not already done
        if not self._model_detection_attempted:
            await self.detect_model()

        # Use chat template to format the conversation
        messages = [{"role": "user", "content": prompt}]
        formatted_input = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)

        # Determine API endpoint and payload format based on chat template
        # For templates that generate raw prompts (not OpenAI-style messages), use /completions endpoint
        if self.chat_template_name in ["qwen3", "qwen3-thinking", "gemma", "gemma2", "gemma3", "llama", "llama2", "llama3", "mistral", "phi3", "chatml"]:
            # For these templates, use the formatted input as a single prompt
            payload = {
                "model": self._detected_model or self.model,
                "prompt": formatted_input,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p
            }
            # Use the basic /completions endpoint for raw prompt inputs
            api_endpoint = f"{self.api_base}/completions"
        else:
            # For standard OpenAI format (or when template is unknown/default), use messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self._detected_model or self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p
            }
            # Use the standard /chat/completions endpoint for message-based inputs
            api_endpoint = f"{self.api_base}/chat/completions"

        # If stop tokens are configured, include them in the API payload
        if getattr(settings, 'llm_stop_tokens', None):
            payload["stop"] = settings.llm_stop_tokens

        model_display = self._detected_model or self.model
        print(f"üîç Sending to LLM API:")
        print(f"   URL: {api_endpoint}")
        print(f"   Model: {model_display} (detected: {self._detected_model is not None})")
        print(f"   Template: {self.chat_template_name}")
        print(f"   Endpoint: {api_endpoint.split('/')[-1]}")
        if "messages" in payload:
            print(f"   Messages: {len(payload['messages'])} messages")
        else:
            print(f"   Prompt length: {len(payload['prompt'])} chars")
        print(f"   Payload: {payload}")

        try:
            response = await self.client.post(
            api_endpoint,
            json=payload
        )
            response.raise_for_status()
        except httpx.HTTPStatusError as http_err:
            # Detect llama.cpp style context error and raise a specialized exception
            # The server returns a 400 with a message like: "the request exceeds the available context size, try increasing it"
            txt = http_err.response.text or ""
            try:
                j = http_err.response.json()
                if isinstance(j, dict) and j.get("error"):
                    txt = j.get("error")
            except Exception:
                pass
            # Try to parse numbers like 'n_ctx_slot = 8192' and 'task.n_tokens = 10225'
            n_ctx = None
            m_ctx = re.search(r"n_ctx_slot\s*=\s*(\d+)", txt)
            if m_ctx:
                n_ctx = int(m_ctx.group(1))
                self._detected_server_context_size = n_ctx
            if "exceeds the available context size" in txt.lower() or "request exceeds the available context size" in txt.lower():
                raise ContextSizeExceededError(f"Context too large trying to use model; details: {txt}", n_ctx=n_ctx, server_message=txt)
            # If no special handling, re-raise
            raise

        try:
            result = response.json()
            print(f"üîç API Response: {result}")

            # Handle OpenAI format (gpt models)
            if "choices" in result and len(result["choices"]) > 0:
                choice = result["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"]
                    if content:
                        return content

            # If we get here, response was malformed
            print(f"‚ö†Ô∏è  Unexpected response format: {result}")
            return ""
        except (KeyError, ValueError) as e:
            print(f"‚ùå Failed to parse API response: {e}")
            print(f"   Raw response: {response.text}")
            return ""


    async def _generate_local(self,
                             prompt: str,
                             max_tokens: int,
                             temperature: float,
                             system_prompt: Optional[str],
                             tools: Optional[List[Dict]] = None) -> str:
        """Generate using local GGUF model"""
        self._init_local_model()

        if self._local_llm is None:
            raise RuntimeError("Neither API nor local model available")

        # Use chat template to format the conversation for local generation
        messages = [{"role": "user", "content": prompt}]
        if self.chat_template_name in ["qwen3"]:
            # For Qwen3 template with local model, format as a single prompt
            full_prompt = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)
        else:
            # Standard format for local model
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"

        # Generate (synchronous call, but we're in async context)
        # Note: llama-cpp-python is sync, so we just call it directly
        # In production, might want to use asyncio.to_thread()
        output = self._local_llm(
            full_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=settings.llm_top_p,
            stop=getattr(settings, 'llm_stop_tokens', None),
            echo=False
        )

        return output["choices"][0]["text"].strip()


    async def stream_generate(self,
                             prompt: str,
                             max_tokens: Optional[int] = None,
                             temperature: float = None,
                             system_prompt: Optional[str] = None,
                             tools: Optional[List[Dict]] = None):
        """Stream generation token-by-token using API server."""
        temperature = temperature if temperature is not None else settings.llm_temperature
        max_tokens = max_tokens or settings.llm_max_tokens

        if not self._model_detection_attempted:
            await self.detect_model()

        # Use chat template to format the conversation for streaming
        messages = [{"role": "user", "content": prompt}]
        formatted_input = self.chat_template.format_messages(messages, system_prompt=system_prompt, tools=tools)

        # Determine API endpoint and payload format based on chat template
        # For templates that generate raw prompts (not OpenAI-style messages), use /completions endpoint
        if self.chat_template_name in ["qwen3", "qwen3-thinking", "gemma", "gemma2", "gemma3", "llama", "llama2", "llama3", "mistral", "phi3", "chatml"]:
            # For these templates, use the formatted input as a single prompt
            payload = {
                "model": self._detected_model or self.model,
                "prompt": formatted_input,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p,
                "stream": True
            }
            # Use the basic /completions endpoint for raw prompt inputs
            api_endpoint = f"{self.api_base}/completions"
        else:
            # For standard OpenAI format (or when template is unknown/default), use messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            payload = {
                "model": self._detected_model or self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": settings.llm_top_p,
                "stream": True
            }
            # Use the standard /chat/completions endpoint for message-based inputs
            api_endpoint = f"{self.api_base}/chat/completions"

        if getattr(settings, 'llm_stop_tokens', None):
            payload["stop"] = settings.llm_stop_tokens

        print(f"üîç Streaming from LLM API...")
        print(f"   Template: {self.chat_template_name}")
        print(f"   Endpoint: {api_endpoint.split('/')[-1]}")
        if "messages" in payload:
            print(f"   Messages: {len(payload['messages'])} messages")
        else:
            print(f"   Prompt length: {len(payload['prompt'])} chars")

        async with self.client.stream(
            "POST",
            api_endpoint,
            json=payload
        ) as response:
            # If the server returns an error status, attempt to read the body
            try:
                response.raise_for_status()
            except Exception as e:
                # Guarded reading of the error body to preserve debug visibility
                try:
                    text = await response.aread()
                    logger.error(f"LLM server error {response.status_code}: {text}")
                except Exception:
                    logger.error(f"LLM server returned {response.status_code} without readable body")
                raise

            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data_str = line[6:]
                    if data_str == "[DONE]":
                        break
                    try:
                        data = json.loads(data_str)
                        if data.get("choices"):
                            delta = data["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield content
                    except json.JSONDecodeError:
                        continue


    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()


class ContextSizeExceededError(Exception):
    def __init__(self, message: str, n_ctx: Optional[int] = None, server_message: Optional[str] = None):
        super().__init__(message)
        self.n_ctx = n_ctx
        self.server_message = server_message


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\llm.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\main.py (Section: BACKEND_PYTHON) ---

"""Minimal, single-entrypoint for ECE_Core.

This file uses `src.app_factory.create_app_with_routers()` to construct the app; the factory
ensures routers are included and avoids initialization side effects at import time.
"""
from src.app_factory import create_app_with_routers
from src.config import settings
import logging

logging.basicConfig(level=getattr(logging, settings.ece_log_level), format='%(asctime)s - %(levelname)s - %(message)s')

app = create_app_with_routers()


if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host=settings.ece_host, port=settings.ece_port)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\main.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_client.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""
Minimal MCP client for ECE_Core to call external MCP servers.
This is used sparingly in ECE_Core when plugin manager isn't available, or when tools are offloaded.
"""
from __future__ import annotations

from typing import Any, Dict, Optional
import httpx
from src.config import settings


class MCPClient:
    def __init__(self, base_url: Optional[str] = None, api_key: Optional[str] = None, timeout: int = 10):
        if base_url:
            self.base_url = base_url.rstrip('/')
        elif getattr(settings, 'mcp_url', None):
            self.base_url = settings.mcp_url.rstrip('/')
        else:
            self.base_url = f"http://{settings.mcp_host}:{settings.mcp_port}"
        self.api_key = api_key or settings.mcp_api_key or settings.ece_api_key
        self._timeout = timeout

    def _headers(self) -> Dict[str, str]:
        h = {"Content-Type": "application/json"}
        if self.api_key:
            h["Authorization"] = f"Bearer {self.api_key}"
        return h

    async def get_tools(self) -> Any:
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            r = await client.get(f"{self.base_url}/mcp/tools", headers=self._headers())
            r.raise_for_status()
            return r.json()

    async def call_tool(self, name: str, **arguments) -> Any:
        payload = {"name": name, "arguments": arguments}
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            r = await client.post(f"{self.base_url}/mcp/call", json=payload, headers=self._headers())
            if r.status_code >= 400:
                return {"status": "error", "status_code": r.status_code, "error": r.text}
            return r.json()


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_client.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_runner.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""Small runner that uses `src.config.settings` to start uvicorn with configured host/port.

This avoids hardcoding in start scripts and allows values from config.yaml or env vars to drive MCP server startup.
"""
from __future__ import annotations

from src.config import settings
import uvicorn


def main():
    if not settings.mcp_enabled:
        print("MCP server is disabled in configuration. Set MCP_ENABLED to true in env or configs/config.yaml to start the MCP server.")
        # Print a hint showing the current derived settings so users can debug quickly
        try:
            print(f"Current settings: mcp_url={settings.mcp_url}, mcp_host={settings.mcp_host}, mcp_port={settings.mcp_port}")
        except Exception:
            pass
        return
    host = settings.mcp_host
    port = int(settings.mcp_port)
    # Print a friendly startup message containing config-derived values
    try:
        src = settings.mcp_url or f"{settings.mcp_host}:{settings.mcp_port}"
        print(f"Starting MCP server using settings derived from YAML/env: {src} (enabled={settings.mcp_enabled})")
    except Exception:
        print(f"Starting MCP server on {host}:{port}")
    uvicorn.run("src.mcp_server:app", host=host, port=port, log_level=settings.ece_log_level.lower())


if __name__ == "__main__":
    main()


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_runner.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_server.py (Section: BACKEND_PYTHON) ---

#!/usr/bin/env python3
"""
Minimal MCP server for ECE memory graph
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional
import asyncio

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from src.memory.neo4j_store import Neo4jStore
from src.config import settings
from fastapi import Request

logger = logging.getLogger(__name__)
app = FastAPI(title="ECE MCP Server")


class ToolSchema(BaseModel):
    name: str
    description: str
    inputSchema: Dict[str, Any]


class ToolCall(BaseModel):
    name: str
    arguments: Dict[str, Any]


ADD_MEMORY_TOOL = ToolSchema(
    name="add_memory",
    description="Add a memory node into ECE's Neo4j store",
    inputSchema={
        "type": "object",
        "properties": {
            "session_id": {"type": "string"},
            "content": {"type": "string"},
            "category": {"type": "string"},
            "tags": {"type": "array", "items": {"type": "string"}},
            "importance": {"type": "number"},
            "metadata": {"type": "object"},
            "entities": {"type": "array", "items": {"type": "object"}}
        },
        "required": ["session_id", "content", "category"]
    },
)

SEARCH_MEMORIES_TOOL = ToolSchema(
    name="search_memories",
    description="Search memories using the Neo4j store",
    inputSchema={
        "type": "object",
        "properties": {
            "query": {"type": "string"},
            "category": {"type": "string"},
            "limit": {"type": "number"}
        },
        "required": ["query"]
    },
)

GET_RECENT_SUMMARIES_TOOL = ToolSchema(
    name="get_summaries",
    description="Get recent session summaries",
    inputSchema={
        "type": "object",
        "properties": {
            "session_id": {"type": "string"},
            "limit": {"type": "number"}
        },
        "required": ["session_id"]
    },
)


_neo4j_store: Optional[Neo4jStore] = None

# Alias map for user-facing tools (Cline / other clients often use 'read_memory'/'write_memory')
TOOL_ALIASES = {
    "write_memory": ADD_MEMORY_TOOL.name,
    "read_memory": SEARCH_MEMORIES_TOOL.name,
    "get_memory_summaries": GET_RECENT_SUMMARIES_TOOL.name,
}


@app.on_event("startup")
async def _startup_event() -> None:
    global _neo4j_store
    _neo4j_store = Neo4jStore()
    try:
        await _neo4j_store.initialize()
    except Exception as e:
        logger.warning(f"Failed to initialize Neo4jStore: {e}")


@app.on_event("shutdown")
async def _shutdown_event() -> None:
    global _neo4j_store
    if _neo4j_store:
        await _neo4j_store.close()


@app.get("/mcp/tools")
async def list_tools(request: Request):
    # Enforce bearer token if required
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")
    # Expose canonical tools and also alias tools so clients using common names can discover them
    canonical = [ADD_MEMORY_TOOL.dict(), SEARCH_MEMORIES_TOOL.dict(), GET_RECENT_SUMMARIES_TOOL.dict()]
    aliases = []
    # Build alias schemas by copying canonical body but forcing name/description
    for alias_name, canonical_name in TOOL_ALIASES.items():
        for c in canonical:
            if c["name"] == canonical_name:
                schema = c.copy()
                schema["name"] = alias_name
                schema["description"] = f"Alias for {canonical_name}"
                aliases.append(schema)
                break
    return {"tools": canonical + aliases}


@app.post("/mcp/call")
async def call_tool(tool_call: ToolCall, request: Request):
    global _neo4j_store
    if not _neo4j_store:
        raise HTTPException(status_code=503, detail="Neo4j store not initialized")
    # Enforce bearer token if required
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")

    try:
        # Resolve aliases to canonical tool name
        name = TOOL_ALIASES.get(tool_call.name, tool_call.name)
        if name == ADD_MEMORY_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.add_memory(
                session_id=p.get("session_id"),
                content=p.get("content"),
                category=p.get("category"),
                tags=p.get("tags", []),
                importance=int(p.get("importance", 5)),
                metadata=p.get("metadata") or {},
                entities=p.get("entities") or [],
            )
            return {"tool": tool_call.name, "status": "success", "result": {"id": result}}

        elif name == SEARCH_MEMORIES_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.search_memories(p.get("query", ""), p.get("category"), int(p.get("limit", 10)))
            return {"tool": tool_call.name, "status": "success", "result": result}

        elif name == GET_RECENT_SUMMARIES_TOOL.name:
            p = tool_call.arguments
            result = await _neo4j_store.get_summaries(str(p.get("session_id")), int(p.get("limit", 5)))
            return {"tool": tool_call.name, "status": "success", "result": result}

        else:
            raise HTTPException(status_code=404, detail=f"Tool not found: {tool_call.name}")

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("MCP call failed")
        return {"tool": tool_call.name, "status": "error", "error": str(e)}


@app.get("/health")
async def health():
    return {"status": "ok", "service": "ECE MCP Server", "active": bool(_neo4j_store and _neo4j_store.neo4j_driver is not None)}


@app.get("/mcp/sse")
async def sse_status(request: Request):
    # Optional SSE endpoint for streaming / agent clients
    if settings.ece_require_auth:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        if not auth or not auth.lower().startswith("bearer "):
            raise HTTPException(status_code=401, detail="Missing authorization token")
        token = auth.split(None, 1)[1].strip()
        configured = settings.mcp_api_key or settings.ece_api_key
        if not configured or token != configured:
            raise HTTPException(status_code=403, detail="Forbidden: invalid API key")
    try:
        from sse_starlette.sse import EventSourceResponse
    except Exception:
        raise HTTPException(status_code=501, detail="SSE streaming not supported (missing sse_starlette dependency)")

    async def generator():
        # Send an initial status
        i = 0
        while True:
            if await request.is_disconnected():
                break
            yield {"event": "status", "data": f"ok-{i}"}
            i += 1
            await asyncio.sleep(3)

    return EventSourceResponse(generator())


if __name__ == "__main__":
    # When run directly, start the MCP server using the configured host & port
    import uvicorn
    if not settings.mcp_enabled:
        print("MCP server is disabled in configuration. Set MCP_ENABLED to true to start the MCP server.")
    else:
        print(f"Starting MCP server on {settings.mcp_host}:{settings.mcp_port} (derived from settings)")
        uvicorn.run("src.mcp_server:app", host=settings.mcp_host, port=int(settings.mcp_port), log_level=settings.ece_log_level.lower())


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\mcp_server.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\models.py (Section: BACKEND_PYTHON) ---

from enum import Enum
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class SourceType(str, Enum):
    GEMINI_CHAT = "GEMINI_CHAT"
    WEB_PAGE = "WEB_PAGE"
    USER_NOTE = "USER_NOTE"
    PDF_DOCUMENT = "PDF_DOCUMENT"

class PlaintextMemory(BaseModel):
    """
    Directive INJ-A1: The foundational atom of the GraphR1 memory system.
    Represents a raw, immutable ingestion event (The 'Page' in GAM).
    """
    uuid: str = Field(default_factory=lambda: str(uuid.uuid4()))
    source_type: SourceType
    source_identifier: str = Field(..., description="Filename, URL, or Session ID")
    ingest_timestamp_utc: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
    original_timestamp_utc: Optional[str] = None
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    version: str = "1.0"

    class Config:
        schema_extra = {
            "example": {
                "uuid": "550e8400-e29b-41d4-a716-446655440000",
                "source_type": "GEMINI_CHAT",
                "source_identifier": "session_12345",
                "ingest_timestamp_utc": "2025-12-06T12:00:00Z",
                "content": "User: Hello\nGemini: Hi there!",
                "metadata": {
                    "author": "Gemini 3",
                    "word_count": 50,
                    "summary": "Greeting exchange"
                }
            }
        }


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\models.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\prompts.py (Section: BACKEND_PYTHON) ---

"""Minimal and robust prompt utilities for ECE Core.

This file intentionally contains a single, minimal `build_system_prompt`
implementation and the small helper functions used in tests. The goal is to
remain tiny, explicit, and stable so runtime loading and tests are reliable.
"""
from datetime import datetime, timezone
import os
from typing import List, Dict, Optional


def build_system_prompt(
    tools_available: bool = False,
    tools_list: Optional[List[Dict]] = None,
    current_datetime: Optional[datetime] = None,
) -> str:
    """Return a concise system prompt emphasizing memory-first guidance.

    The prompt includes a short header, a 'memory-first' guidance, a concise
    instruction on tools, and an optional tools list when `tools_available` is
    True.
    """
    if current_datetime is None:
        current_datetime = datetime.now(timezone.utc)
    # If an explicit CODA_SYSTEM_PROMPT is provided via env (or YAML), prefer that as the system prompt.
    sys_prompt_env = os.getenv('CODA_SYSTEM_PROMPT')
    if sys_prompt_env:
        return sys_prompt_env
    current_date = current_datetime.strftime("%Y-%m-%d")
    current_time = current_datetime.strftime("%H:%M UTC")

    prompt = [f"**CURRENT DATE & TIME: {current_date} {current_time}**"]
    prompt.append("You are an AI assistant with access to the user's personal memory and context.")
    prompt.append("Working with Memory and Context:")
    prompt.append("- Check <retrieved_memory> and <memory> blocks FIRST for recall/summary queries; ONLY treat information enclosed within <memory>...</memory> as verified factual history; if the answer is present there, DO NOT use a tool.")
    prompt.append("- Use tools for real-time external data (web, filesystem, system state) or when memory lacks the needed information.")
    prompt.append("- When invoking a tool, put the tool call on its own line as: TOOL_CALL: tool_name(param1=value1)")
    prompt.append("- IMPORTANT: Do not output chain-of-thought or internal analysis to the user. Use internal channels for thoughts only and produce a single clear final response in natural conversational form. If diagnostics are required, place them in a 'thinking:' channel only when explicitly asked for diagnostics.")

    # SIMPLE HARNESS PROTOCOL
    prompt.append("\n[SIMPLE HARNESS PROTOCOL]")
    prompt.append("If you need to use a basic tool, you may output a single line in one of the following forms (do NOT use XML/JSON for these basic actions):")
    prompt.append("Action: search web query=\"...\"")
    prompt.append("Action: read file path=\"...\"")
    prompt.append("Action: execute cmd command=\"...\"")
    prompt.append("These lines are machine-processable and meant only for deterministic tool calls for small models. If you produce an 'Action:' line, do NOT include extra text on that line.")

    if tools_available and tools_list:
        prompt.append("\n**AVAILABLE TOOLS:**")
        for tool in tools_list:
            name = tool.get('name') if isinstance(tool, dict) else getattr(tool, 'name', 'UNKNOWN')
            desc = tool.get('description', '') if isinstance(tool, dict) else getattr(tool, 'description', '')
            params = ''
            if isinstance(tool, dict) and 'inputSchema' in tool and isinstance(tool['inputSchema'], dict):
                props = tool['inputSchema'].get('properties', {})
                params = ", ".join(props.keys())
            prompt.append(f"- {name}({params}): {desc}")

    prompt.append("Be concise, factual, and only ground answers in verified memory (<memory> tags) or explicit tool output.")
    return "\n".join(prompt)


def build_coda_persona_prompt() -> str:
    """Return a short persona prompt used for persona-specific conversations.

    Tests assert that the persona contains 'Coda C-001' and 'Kaizen'. Keep
    this minimal and stable.
    """
    return (
        "You are Coda C-001, a memory-augmented AI assistant.\n\n"
        "Core Philosophy: Kaizen (continuous improvement); Chutzpah; Shoshin.\n"
        "Communication: Concise, candid, and helpful."
    )


def build_summarization_prompt(text: str, max_tokens: int) -> str:
    return f"Summarize the following into approximately {max_tokens} tokens:\n\n{text}\n\nSummary:"


def build_entity_extraction_prompt(text: str) -> str:
    return (
        "Extract key entities as JSON using categories: PERSON, CONCEPT, PROJECT, CONDITION, SKILL.\n\n"
        f"Text:\n{text}\n\nEntities (JSON):"
    )


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\prompts.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\security.py (Section: BACKEND_PYTHON) ---

"""
Security middleware and utilities for ECE_Core.
Implements API key authentication and audit logging.
"""
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from src.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# API KEY AUTHENTICATION
# ============================================================================

security = HTTPBearer(auto_error=False)

async def verify_api_key(
    credentials: Optional[HTTPAuthorizationCredentials] = Security(security)
) -> bool:
    """
    Verify API key from Authorization header.
    Returns True if authentication is disabled or key is valid.
    Raises HTTPException if authentication is required but fails.
    """
    # If auth not required, allow all requests
    if not settings.ece_require_auth:
        logger.info("Auth check skipped (ece_require_auth=False)")
        return True
    
    # If auth required but no credentials provided
    if credentials is None:
        logger.warning("Auth failed: No credentials provided")
        raise HTTPException(
            status_code=401,
            detail="Authentication required. Provide API key in Authorization header."
        )
    
    # Verify API key
    if credentials.credentials != settings.ece_api_key:
        logger.warning(f"Auth failed: Invalid API key attempt. Received: {credentials.credentials[:4]}...")
        raise HTTPException(
            status_code=403,
            detail="Invalid API key"
        )
    
    return True

# ============================================================================
# AUDIT LOGGING
# ============================================================================

class AuditLogger:
    """Audit logger for security-sensitive operations."""
    
    def __init__(self):
        self.enabled = settings.audit_log_enabled
        self.log_path = Path(settings.audit_log_path)
        
        # Create log directory if needed
        if self.enabled:
            self.log_path.parent.mkdir(parents=True, exist_ok=True)
            # Initialize log file if it doesn't exist
            if not self.log_path.exists():
                self.log_path.touch()
    
    def log(self, event_type: str, details: dict):
        """Log a security event."""
        if not self.enabled:
            return
        
        try:
            timestamp = datetime.now().isoformat()
            log_entry = {
                "timestamp": timestamp,
                "event_type": event_type,
                **details
            }
            
            with open(self.log_path, 'a', encoding='utf-8') as f:
                f.write(f"{log_entry}\n")
            
            # Also log to application logger
            logger.info(f"AUDIT: {event_type} - {details}")
        except Exception as e:
            logger.error(f"Failed to write audit log: {e}")
    
    def log_tool_call(self, session_id: str, tool_name: str, arguments: dict, result: str):
        """Log a tool execution."""
        if settings.audit_log_tool_calls:
            self.log("tool_call", {
                "session_id": session_id,
                "tool_name": tool_name,
                "arguments": arguments,
                "result_preview": str(result)[:100]
            })
    
    def log_memory_access(self, session_id: str, operation: str, details: dict):
        """Log memory access operations."""
        if settings.audit_log_memory_access:
            self.log("memory_access", {
                "session_id": session_id,
                "operation": operation,
                **details
            })
    
    def log_auth_attempt(self, success: bool, details: dict):
        """Log authentication attempts."""
        self.log("auth_attempt", {
            "success": success,
            **details
        })

# Global audit logger instance
audit_logger = AuditLogger()


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\security.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\tools.py (Section: BACKEND_PYTHON) ---

"""Utility helpers for tools/ plugin management and tool listing.

This centralizes tool list formatting for use in the prompt builder and other
places in the app. Moving this out of main reduces duplicated code and keeps
the main module smaller.
"""
from typing import Any, Dict, List, Optional
import json
import time
import logging

logger = logging.getLogger(__name__)


def get_tools_list(plugin_manager: Optional[Any], mcp_client: Optional[Any]):
    """Return a normalized list of tool objects from plugin manager or mcp client.

    Each tool in the returned list will be a dict with keys 'name', 'description', and
    'inputSchema' at a minimum. If no tools found, returns [] and tools_available False.
    """
    tools = []
    try:
        if plugin_manager and getattr(plugin_manager, "enabled", False):
            tools = plugin_manager.list_tools() or []
        elif mcp_client:
            # mcp_client call is async in some paths; callers should handle that
            tools = []
            # We don't call the async MCP client here to keep this helper simple.
    except Exception:
        tools = []

    return tools


def format_tools_for_prompt(tools_list: List[Dict]) -> str:
    """Return a human-readable tools description suitable to append to the system prompt.

    E.g. "- fs_list(path): List files at path"
    """
    if not tools_list:
        return ""
    lines = []
    for tool in tools_list:
        params = ", ".join([p for p in tool.get('inputSchema', {}).get('properties', {}).keys()])
        lines.append(f"- {tool.get('name')}({params}): {tool.get('description','')}")
    out = "**AVAILABLE TOOLS:**\n" + "\n".join(lines) + "\n\nTo use a tool, respond with: TOOL_CALL: tool_name(param1=value1, param2=value2)"
    return out


class ToolExecutor:
    """Responsible for executing tool calls detected in an LLM response.

    This encapsulates validation, execution via plugins or MCP, audit logging,
    and re-generation after tool output.
    """
    def __init__(self, plugin_manager: Optional[Any], mcp_client: Optional[Any], tool_parser: Optional[Any], tool_validator: Optional[Any], llm_client: Optional[Any], audit_logger: Optional[Any], max_iterations: int = 3):
        self.plugin_manager = plugin_manager
        self.mcp_client = mcp_client
        self.tool_parser = tool_parser
        self.tool_validator = tool_validator
        self.llm = llm_client
        self.audit_logger = audit_logger
        self.max_iterations = max_iterations

    async def execute(self, parsed_response, full_context, request, system_prompt, context_mgr):
        """
        Execute tool calls detected in the LLM response.

        Behavior and output flow overview:
        - The LLM's initial text is passed via `initial_response` in the calling endpoint.
        - This executor validates tool calls, executes them via plugin manager or MCP, and then triggers a fresh LLM generation that includes tool results.
        - The returned `response` is the final LLM reply. The caller (chat endpoints) treats this as the 'response:' section, while the initial LLM output remains the 'thinking:' section.
        - Tool output is logged via `audit_logger` (if provided) and appended to the prompt so the LLM can reason with the results.
        """
        iteration = 0
        t_tools_total_ms = 0.0
        response = None
        while parsed_response and getattr(parsed_response, 'has_tool_calls', False) and iteration < self.max_iterations and self.tool_validator:
            iteration += 1
            # Choose first valid tool call
            tool_call = None
            validation_error = None
            for tc in parsed_response.tool_calls:
                if self.tool_validator:
                    is_valid, err = self.tool_validator.validate(tc)
                    if is_valid:
                        tool_call = tc
                        break
                    validation_error = err
                else:
                    tool_call = tc
                    break

            if not tool_call:
                # No valid tool calls; return a helpful response
                error_msg = validation_error or 'No valid tool calls found'
                logger.error(f"Tool call validation failed: {error_msg}")
                tool_ctx = f"\n\nTool call failed: {error_msg}\n\nPlease acknowledge and provide a helpful answer without tools."
                response = await self.llm.generate(prompt=full_context + tool_ctx, system_prompt=system_prompt)
                # Standardized fallback tag for deterministic detection in tests and UI
                response = f"[ToolExecutionFallback] {response}"
                return response, iteration, t_tools_total_ms

            # Execute tool call
            try:
                if self.audit_logger:
                    try:
                        self.audit_logger.log_tool_call(
                            session_id=request.session_id,
                            tool_name=tool_call.tool_name,
                            arguments=tool_call.parameters,
                            result='pending'
                        )
                    except Exception:
                        logger.warning("Failed to write audit log for tool call (pending)")
                t_tool_start = time.perf_counter()
                if self.plugin_manager and getattr(self.plugin_manager, 'enabled', False):
                    plugin_name = self.plugin_manager.lookup_plugin_for_tool(tool_call.tool_name)
                    if plugin_name:
                        tool_result = await self.plugin_manager.execute_tool(f"{plugin_name}:{tool_call.tool_name}", **tool_call.parameters)
                    else:
                        tool_result = {"error": f"Tool not found in plugins: {tool_call.tool_name}"}
                elif self.mcp_client:
                    tool_result = await self.mcp_client.call_tool(tool_call.tool_name, **tool_call.parameters)
                else:
                    tool_result = {"error": "Tools disabled"}
                t_tool_ms = (time.perf_counter() - t_tool_start) * 1000
                t_tools_total_ms += t_tool_ms
                if self.audit_logger:
                    try:
                        self.audit_logger.log_tool_call(
                            session_id=request.session_id,
                            tool_name=tool_call.tool_name,
                            arguments=tool_call.parameters,
                            result=str(tool_result)[:200]
                        )
                    except Exception:
                        logger.warning("Failed to write audit log for tool call (result)")

                # Treat explicit errors and empty/None results as failures and ask the LLM to provide feedback
                if tool_result is None or tool_result == {} or (isinstance(tool_result, (str, list)) and not tool_result) or (isinstance(tool_result, dict) and 'error' in tool_result):
                    # Report either the explicit error or a general 'no output' condition
                    if isinstance(tool_result, dict) and 'error' in tool_result:
                        err_text = tool_result.get('detail', tool_result.get('error'))
                    else:
                        err_text = 'Tool returned no output.'
                    tool_context = f"\n\nTool '{tool_call.tool_name}' failed or returned no output: {err_text}\n\nProvide helpful feedback to the user."
                else:
                    tool_context = f"\n\nTool '{tool_call.tool_name}' returned:\n{json.dumps(tool_result) if isinstance(tool_result, dict) else str(tool_result)}\n\nNow answer the user's question using this information."

                response = await self.llm.generate(prompt=full_context + tool_context, system_prompt=system_prompt)
                parsed_response = self.tool_parser.parse_response(response) if self.tool_parser else None
            except Exception as e:
                logger.error(f"Tool execution failed for {tool_call.tool_name}: {e}")
                response = await self.llm.generate(prompt=full_context + f"\n\nTool execution failed: {e}\n\nAcknowledge and suggest alternatives.", system_prompt=system_prompt)
                # Standardized fallback tag for deterministic detection in tests and UI
                response = f"[ToolExecutionFallback] {response}"
                return response, iteration, t_tools_total_ms

        return response, iteration, t_tools_total_ms



--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\tools.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\tool_call_models.py (Section: BACKEND_PYTHON) ---

"""
Tool Call Models and Validation

Pydantic models for validating and parsing tool calls from LLM responses.
Replaces brittle regex parsing with structured validation.
"""
from pydantic import BaseModel, Field, validator
from typing import Dict, Any, List, Optional, Literal
import re
import json
import logging

logger = logging.getLogger(__name__)


class ToolCallParam(BaseModel):
    """Single parameter for a tool call"""
    name: str
    value: Any
    
    class Config:
        extra = "forbid"


class ToolCall(BaseModel):
    """Validated tool call from LLM response"""
    tool_name: str = Field(..., description="Name of the tool to call")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Tool parameters")
    raw_match: Optional[str] = Field(None, description="Original matched string")
    
    @validator('tool_name')
    def validate_tool_name(cls, v):
        """Ensure tool name is valid identifier.

        Accepts either a simple snake_case name or a plugin-prefixed name like
        'utcp:tool_name'. We allow hyphens and dots in identifiers where needed.
        """
        if not v or not re.match(r'^[A-Za-z0-9_:\-\.]+$', v):
            raise ValueError(f"Invalid tool name format: {v}")
        return v
    
    class Config:
        extra = "forbid"


class LLMResponse(BaseModel):
    """Structured LLM response with optional tool calls"""
    response_text: str = Field(..., description="The LLM's text response")
    tool_calls: List[ToolCall] = Field(default_factory=list, description="Extracted tool calls")
    has_tool_calls: bool = Field(False, description="Whether response contains tool calls")
    
    class Config:
        extra = "allow"


class ToolCallParser:
    """
    Parser for extracting and validating tool calls from LLM responses.
    
    Supports multiple formats:
    1. TOOL_CALL: format (current regex-based)
    2. JSON format (for JSON mode models)
    3. Function call format (for function-calling models)
    """
    
    def __init__(self):
        # Regex pattern for TOOL_CALL: format
        # Accept plugin-prefixed names (e.g. utcp:filesystem_list) and allow dots, underscores, hyphens
        self.tool_call_pattern = re.compile(
            r'TOOL_CALL:\s*([A-Za-z0-9_:\-\.]+)\((.*?)\)',
            re.MULTILINE | re.DOTALL
        )
        
        # Alternative patterns for robustness
        self.json_tool_pattern = re.compile(
            r'\{[^}]*"tool":\s*"([^"]+)"[^}]*"params":\s*\{([^}]+)\}[^}]*\}',
            re.MULTILINE | re.DOTALL
        )
    
    def parse_response(self, response: str) -> LLMResponse:
        """
        Parse LLM response and extract tool calls.
        
        Args:
            response: Raw LLM response string
            
        Returns:
            LLMResponse with extracted tool calls
        """
        tool_calls = []
        
        # Try TOOL_CALL: format first (current format)
        matches = self.tool_call_pattern.findall(response)
        
        if matches:
            logger.debug(f"Found {len(matches)} TOOL_CALL format matches")
            for tool_name, params_str in matches:
                try:
                    params = self._parse_parameters(params_str)
                    tool_call = ToolCall(
                        tool_name=tool_name,
                        parameters=params,
                        raw_match=f"TOOL_CALL: {tool_name}({params_str})"
                    )
                    tool_calls.append(tool_call)
                    logger.debug(f"Parsed tool call: {tool_call.tool_name} with {len(params)} params")
                except Exception as e:
                    logger.error(f"Failed to parse tool call '{tool_name}': {e}")
                    # Don't add invalid tool calls
        
        # Try JSON format (for JSON mode)
        if not tool_calls and '{' in response:
            json_matches = self.json_tool_pattern.findall(response)
            if json_matches:
                logger.debug(f"Found {len(json_matches)} JSON format matches")
                for tool_name, params_str in json_matches:
                    try:
                        params = json.loads('{' + params_str + '}')
                        tool_call = ToolCall(
                            tool_name=tool_name,
                            parameters=params,
                            raw_match=f'{{"tool": "{tool_name}", "params": {{{params_str}}}}}'
                        )
                        tool_calls.append(tool_call)
                        logger.debug(f"Parsed JSON tool call: {tool_call.tool_name}")
                    except Exception as e:
                        logger.error(f"Failed to parse JSON tool call '{tool_name}': {e}")
        
        return LLMResponse(
            response_text=response,
            tool_calls=tool_calls,
            has_tool_calls=len(tool_calls) > 0
        )
    
    def _parse_parameters(self, params_str: str) -> Dict[str, Any]:
        """
        Parse parameter string into dictionary.
        
        Handles:
        - key=value format
        - Quoted strings
        - Nested structures
        - JSON values
        
        Args:
            params_str: Parameter string from tool call
            
        Returns:
            Dictionary of parsed parameters
        """
        params = {}
        
        if not params_str or not params_str.strip():
            return params
        
        # Split by comma, respecting nested structures
        param_parts = self._split_parameters(params_str)
        
        for part in param_parts:
            part = part.strip()
            if not part:
                continue
            
            if '=' not in part:
                logger.warning(f"Parameter without '=': {part}")
                continue
            
            key, value = part.split('=', 1)
            key = key.strip()
            value = value.strip()
            
            # Parse value
            parsed_value = self._parse_value(value)
            params[key] = parsed_value
        
        return params
    
    def _split_parameters(self, params_str: str) -> List[str]:
        """
        Split parameter string by comma, respecting nested structures.
        
        Args:
            params_str: Raw parameter string
            
        Returns:
            List of parameter strings
        """
        param_parts = []
        current = []
        depth = 0
        in_quotes = False
        quote_char = None
        
        for char in params_str + ',':
            if char in ['"', "'"]:
                if not in_quotes:
                    in_quotes = True
                    quote_char = char
                elif char == quote_char:
                    in_quotes = False
                    quote_char = None
            
            if not in_quotes:
                if char in '([{':
                    depth += 1
                elif char in ')]}':
                    depth -= 1
                elif char == ',' and depth == 0:
                    if current:
                        param_parts.append(''.join(current))
                        current = []
                    continue
            
            current.append(char)
        
        return param_parts
    
    def _parse_value(self, value: str) -> Any:
        """
        Parse a parameter value into appropriate Python type.
        
        Supports:
        - Strings (quoted)
        - Numbers (int, float)
        - Booleans
        - JSON objects/arrays
        - null/None
        
        Args:
            value: Raw value string
            
        Returns:
            Parsed value
        """
        value = value.strip()
        
        # Empty value
        if not value or value.lower() in ['null', 'none']:
            return None
        
        # Boolean
        if value.lower() == 'true':
            return True
        if value.lower() == 'false':
            return False
        
        # Quoted string
        if (value.startswith('"') and value.endswith('"')) or \
           (value.startswith("'") and value.endswith("'")):
            return value[1:-1]
        
        # Try JSON parse (for objects/arrays)
        if value.startswith(('{', '[')):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse JSON value: {value}")
                return value
        
        # Try number
        try:
            if '.' in value:
                return float(value)
            return int(value)
        except ValueError:
            pass
        
        # Default: return as string
        return value


class ToolCallValidator:
    """
    Validator for tool calls against available tools.
    
    Checks:
    - Tool exists
    - Required parameters present
    - Parameter types match schema
    """
    
    def __init__(self, available_tools: Dict[str, Any]):
        """
        Initialize validator with available tools.
        
        Args:
            available_tools: Dict of tool name -> tool schema
        """
        self.available_tools = available_tools
    
    def validate(self, tool_call: ToolCall) -> tuple[bool, Optional[str]]:
        """
        Validate a tool call.
        
        Args:
            tool_call: ToolCall to validate
            
        Returns:
            (is_valid, error_message)
        """
        # Normalization: allow 'plugin:tool' forms by checking both forms
        tool_name_to_check = tool_call.tool_name
        if ':' in tool_name_to_check:
            # If the tool_name contains a plugin prefix, prefer the latter part for validation
            _, possible_tool = tool_name_to_check.split(':', 1)
            if possible_tool in self.available_tools:
                tool_name_to_check = possible_tool

        # Check tool exists
        if tool_name_to_check not in self.available_tools:
            return False, f"Tool '{tool_call.tool_name}' not found. Available tools: {list(self.available_tools.keys())}"

        tool_schema = self.available_tools[tool_name_to_check]
        
        # Check required parameters
        required_params = tool_schema.get('inputSchema', {}).get('required', [])
        missing_params = set(required_params) - set(tool_call.parameters.keys())
        
        if missing_params:
            return False, f"Missing required parameters: {missing_params}"
        
        # All checks passed
        return True, None


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\tool_call_models.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapter.py (Section: BACKEND_PYTHON) ---

"""Abstract Vector DB adapter interface for ECE_Core.

This module exposes a small interface to integrate vector DBs such as
Pinecone, Milvus, Redis Vector, or FAISS as a local embed store.

Implementations should be lightweight and provide a test-backed
in-memory FAISS-like adapter for unit tests.
"""
from __future__ import annotations
from typing import Protocol, List, Dict, Any, Optional, Tuple
from src.config import settings
from src.vector_adapters.redis_vector_adapter import RedisVectorAdapter
from src.vector_adapters.fake_vector_adapter import FakeVectorAdapter

class VectorAdapter(Protocol):
    """Vector DB abstraction layer."""

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        """Index (or upsert) an embedding with metadata into the vector DB.

        embedding_id: unique ID for the vector entry
        node_id: Neo4j node id or external id
        chunk_index: index of the chunk within the content
        embedding: numeric embedding vector
        metadata: additional properties (raw text, timestamp)
        """

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        """Query the vector DB and return a list of hit dicts with keys: score, embedding_id, node_id, chunk_index, metadata"""

    async def delete(self, embedding_id: str) -> None:
        """Delete an embedding by id."""

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        """Get a vector entry by id, returning scoreless metadata and mapping"""

    async def health(self) -> bool:
        """Return health check boolean for adapter status."""

    async def initialize(self) -> None:
        """Optional initialization for the adapter (e.g. connect to Redis)."""


def create_vector_adapter(adapter_name: str | None = None) -> VectorAdapter:
    """Factory to create a vector adapter by name.
    Defaults to a Redis-backed adapter when `redis` is requested. Falls back
    to a memory-backed adapter (RedisVectorAdapter's in-memory mode) if Redis
    isn't available.
    """
    name = adapter_name or getattr(settings, "vector_adapter_name", "redis")
    if name == "redis":
        adapter = RedisVectorAdapter(redis_url=settings.redis_url)
        return adapter
    if name == "fake":
        return FakeVectorAdapter()
    # fallback to redis-based adapter which supports in-memory fallback
    adapter = RedisVectorAdapter(redis_url=settings.redis_url)
    return adapter


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapter.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\archivist.py (Section: BACKEND_PYTHON) ---

"""
Archivist Agent (Maintenance & Curation)
Handles Knowledge Base Freshness, Pruning, and Re-verification.
"""
import logging
import asyncio
from typing import List, Dict, Any, TYPE_CHECKING
from datetime import datetime, timedelta, timezone
if TYPE_CHECKING:
    # Avoid importing heavy deps during test collection (neo4j etc)
    from src.memory import TieredMemory
    from src.agents.verifier import VerifierAgent
else:
    TieredMemory = None  # type: ignore
    VerifierAgent = None  # type: ignore
from src.maintenance.weaver import MemoryWeaver
from src.config import Settings, settings as GLOBAL_SETTINGS

logger = logging.getLogger(__name__)

class ArchivistAgent:
    """
    Archivist Agent manages the health and freshness of the Knowledge Graph.
    It runs background tasks to prune stale nodes and trigger re-verification.
    """
    
    def __init__(self, memory: TieredMemory, verifier: VerifierAgent, settings: Settings | None = None):
        self.memory = memory
        self.verifier = verifier
        self.running = False
        self._task = None
        # Use the provided settings instance or fallback to the module-global settings
        self.settings = settings or GLOBAL_SETTINGS
        self.weaver = MemoryWeaver(self.settings)
        self._last_weave = None
        self._last_purge = None
        
    async def start(self):
        """Start the background maintenance loop."""
        self.running = True
        self._task = asyncio.create_task(self._maintenance_loop())
        logger.info("Archivist Agent started (Maintenance Loop)")

    async def stop(self):
        """Stop the background maintenance loop."""
        self.running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("Archivist Agent stopped")

    async def _maintenance_loop(self):
        """
        Main loop:
        1. Check for stale nodes (Freshness Protocol)
        2. Prune low-value/old nodes
        3. Sleep
        """
        while self.running:
            try:
                logger.info("Archivist: Starting maintenance cycle...")
                # Janitor: purge contaminated nodes if enabled and interval elapsed
                try:
                    if getattr(self.settings, 'archivist_auto_purge_enabled', False):
                        now = datetime.now(timezone.utc)
                        interval = getattr(self.settings, 'archivist_auto_purge_interval_seconds', 600)
                        if not self._last_purge or (now - self._last_purge).total_seconds() >= interval:
                            logger.info("Archivist: Running auto-purge (Janitor) cycle to clean contaminated nodes")
                            try:
                                await self.purge_contaminated_nodes(dry_run=getattr(self.settings, 'archivist_auto_purge_dry_run', True))
                            except Exception as purge_e:
                                logger.error(f"Archivist: Auto-purge failed: {purge_e}")
                            self._last_purge = now
                except Exception as purge_check_e:
                    # Don't let janitor failures stop maintenance loop
                    logger.error(f"Archivist: Janitor pre-check failed: {purge_check_e}")
                await self.check_freshness()

                # Run weaving on short cadence (every 60 minutes) if enabled
                if self.settings.weaver_enabled:
                    now = datetime.now(timezone.utc)
                    # Run every 60 minutes
                    if not self._last_weave or (now - self._last_weave).total_seconds() >= 3600:
                        logger.info("Archivist: Running MemoryWeaver weave_recent (dry-run) as scheduled heartbeat")
                        try:
                            # Use settings defaults; ensure dry-run by default
                            await self.run_weaving_cycle()
                            self._last_weave = now
                        except Exception as weave_e:
                            logger.error(f"Archivist: Weaver run failed: {weave_e}")
                # await self.prune_stale() # Disabled for now to prevent data loss during beta
                logger.info("Archivist: Maintenance cycle complete.")
            except Exception as e:
                logger.error(f"Archivist error: {e}")
            # Run every hour (3600s) - configurable
            await asyncio.sleep(3600)

    
    async def run_weaving_cycle(self, hours: int | None = None, threshold: float | None = None, max_commit: int | None = None, candidate_limit: int | None = None, batch_size: int | None = None, prefer_same_app: bool | None = None, dry_run: bool | None = None, csv_out: str | None = None):
        """
        Trigger a weaving cycle using the MemoryWeaver. Uses Settings defaults if parameters not supplied.
        """
        try:
            # If settings indicate weaving is disabled, skip it
            if not self.settings.weaver_enabled:
                logger.info("Archivist: Weaver disabled in settings; skipping")
                return
            # Resolve commit flag: avoid writes unless explicitly configured
            if dry_run is None:
                dry_run = self.settings.weaver_dry_run_default
            # Run the weave
            candidate_limit = candidate_limit if candidate_limit is not None else getattr(self.settings, 'weaver_candidate_limit', 200)
            batch_size = batch_size if batch_size is not None else getattr(self.settings, 'weaver_batch_size_default', getattr(self.settings, 'llm_embeddings_default_batch_size', 4))
            result = await self.weaver.weave_recent(hours=hours, threshold=threshold, max_commit=max_commit, candidate_limit=candidate_limit, prefer_same_app=prefer_same_app, dry_run=dry_run, csv_out=csv_out, batch_size=batch_size)
            logger.info(f"Archivist: Weaver run completed: {result}")
            return result
        except Exception as e:
            logger.error(f"Archivist: Weaver cycle error: {e}")
            return None

    async def check_freshness(self, limit: int = 10):
        """
        Scan for nodes that need re-verification.
        Criteria: High importance (>7) but old (>30 days) or missing verification.
        """
        # We need a custom Cypher query here.
        # Since we can't easily add methods to Neo4jStore at runtime without editing it,
        # we'll use execute_cypher if available, or add a method to Neo4jStore.
        # Neo4jStore has execute_cypher method.
        
        query = """
        MATCH (m:Memory)
        WHERE m.importance > 7 
          AND (m.last_verified_at IS NULL OR datetime(m.last_verified_at) < datetime($threshold))
        RETURN elementId(m) as id, m.content as content, m.metadata as metadata
        LIMIT $limit
        """
        
        threshold = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
        
        try:
            results = await self.memory.neo4j.execute_cypher(query, {"threshold": threshold, "limit": limit})
            
            for record in results:
                await self.reverify_node(record)
                
        except Exception as e:
            logger.error(f"Freshness check failed: {e}")

    async def reverify_node(self, record: Dict[str, Any]):
        """
        Trigger VerifierAgent to check a node.
        """
        node_id = record.get("id")
        content = record.get("content")
        
        logger.info(f"Archivist: Re-verifying node {node_id}...")
        
        # We need context to verify against. For now, we verify against the node itself 
        # (checking internal consistency) or we could search for related nodes.
        # Ideally, verifier searches for primary sources.
        
        # Search for related context to help verification
        context = await self.memory.search_memories(content[:100], None, limit=5)
        
        verification = await self.verifier.verify_claim(content, context)
        
        # Update node with verification result
        update_query = """
        MATCH (m:Memory) WHERE elementId(m) = $node_id
        SET m.last_verified_at = $now,
            m.freshness_score = $score,
            m.verification_note = $note
        """
        
        await self.memory.neo4j.execute_cypher(update_query, {
            "node_id": node_id,
            "now": datetime.now(timezone.utc).isoformat(),
            "score": verification.get("score", 0.0),
            "note": "Verified by VerifierAgent" if verification.get("verified") else "Verification failed"
        })
        
        logger.info(f"Archivist: Node {node_id} updated with score {verification.get('score')}")

    def _content_contains_marker(self, content: str, markers: list[str]) -> bool:
        """
        Simple helper to check if content or metadata contains any marker.
        Lower-casing helps match markers configured in the settings.
        """
        if not content:
            return False
        content_lower = content.lower()
        for m in markers:
            if m and m.lower() in content_lower:
                return True
        return False

    async def find_contaminated_nodes(self, markers: list[str]) -> list:
        """
        Use the provided markers to identify candidate nodes in Neo4j.
        This returns a list of node records (id, content, metadata, created_at, session_id, category).
        """
        if not markers:
            return []

        results = []
        try:
            q = """
            MATCH (m:Memory)
            WHERE (
                """ + ' OR '.join(["toLower(coalesce(m.content,'') ) CONTAINS $marker_%d" % i for i in range(len(markers))]) + """
            ) OR (
                """ + ' OR '.join(["toLower(coalesce(m.metadata,'') ) CONTAINS $marker_meta_%d" % i for i in range(len(markers))]) + """
            )
            RETURN elementId(m) as id, m.content as content, m.metadata as metadata, m.created_at as created_at, m.session_id as session_id, m.category as category, m.status as status
            """
            params = {}
            for i,m in enumerate(markers):
                params[f"marker_{i}"] = m.lower()
                params[f"marker_meta_{i}"] = m.lower()
            # Neo4j driver session loop
            drv = getattr(self.memory, 'neo4j', None)
            if not drv or not getattr(drv, 'neo4j_driver', None):
                logger.info("Archivist: Neo4j driver not configured; cannot find contaminated nodes")
                return []
            async with drv.neo4j_driver.session() as session:
                result = await session.run(q, params)
                rows = await result.data()
                for r in rows:
                    results.append(r)
        except Exception as e:
            logger.error(f"Archivist: Failed to find contaminated nodes: {e}")
        return results

    async def purge_contaminated_nodes(self, dry_run: bool = True, markers: list[str] | None = None) -> dict:
        """
        Detect nodes that match the configured contamination markers and optionally delete them.
        Safety: only delete nodes that are NOT committed (i.e., m.status != 'committed').
        Returns a dict with counts for found and deleted nodes.
        """
        if markers is None:
            markers = getattr(self.settings, 'archivist_auto_purge_markers', [])
        markers = [m for m in (markers or []) if m]
        if not markers:
            logger.info("Archivist: No markers configured for auto-purge; skipping")
            return {"found": 0, "deleted": 0}

        found = 0
        deleted = 0
        try:
            # Build a safe cypher that matches any content or metadata marker and excludes 'committed' nodes
            conds = []
            params = {}
            for i, m in enumerate(markers):
                params[f"marker_{i}"] = m.lower()
                params[f"marker_meta_{i}"] = m.lower()
                conds.append(f"toLower(coalesce(m.content,'')) CONTAINS $marker_{i}")
                conds.append(f"toLower(coalesce(m.metadata,'')) CONTAINS $marker_meta_{i}")

            if not conds:
                return {"found": 0, "deleted": 0}

            where_clause = '(' + ' OR '.join(conds) + ") AND (m.status IS NULL OR toLower(m.status) <> 'committed')"
            q = f"""
            MATCH (m:Memory)
            WHERE {where_clause}
            RETURN elementId(m) as id, m.content as content, m.metadata as metadata, m.created_at as created_at, m.session_id as session_id, m.category as category
            """
            drv = getattr(self.memory, 'neo4j', None)
            if not drv or not getattr(drv, 'neo4j_driver', None):
                logger.info("Archivist: Neo4j driver not configured; skipping purge")
                return {"found": 0, "deleted": 0}
            async with drv.neo4j_driver.session() as session:
                result = await session.run(q, params)
                rows = await result.data()
                found = len(rows)
                logger.info(f"Archivist-Janitor: Found {found} candidate contaminated nodes")
                if found and not dry_run:
                    for row in rows:
                        try:
                            await session.run('MATCH (m:Memory) WHERE elementId(m) = $id DETACH DELETE m', {'id': row.get('id')})
                            deleted += 1
                        except Exception as e:
                            logger.error(f"Archivist-Janitor: Failed to delete node {row.get('id')}: {e}")
                # if dry_run, list candidates to log
                if dry_run:
                    for row in rows:
                        logger.warning(f"Archivist-Janitor: Dry-run found candidate: id={row.get('id')} session_id={row.get('session_id')} category={row.get('category')} created_at={row.get('created_at')}")
        except Exception as e:
            logger.error(f"Archivist: purge_contaminated_nodes error: {e}")
        logger.info(f"Archivist-Janitor: Purge results: found={found} deleted={deleted} dry_run={dry_run}")
        return {"found": found, "deleted": deleted}

    async def prune_stale(self):
        """
        Prune nodes with low importance (<3) and old age (>90 days).
        """
        query = """
        MATCH (m:Memory)
        WHERE m.importance < 3 
          AND datetime(m.created_at) < datetime($threshold)
        DELETE m
        """
        threshold = (datetime.now(timezone.utc) - timedelta(days=90)).isoformat()
        
        try:
            await self.memory.neo4j.execute_cypher(query, {"threshold": threshold})
            logger.info("Archivist: Pruned stale nodes.")
        except Exception as e:
            logger.error(f"Pruning failed: {e}")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\archivist.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\planner.py (Section: BACKEND_PYTHON) ---

"""
Planner Agent

This module implements a PlannerAgent that uses the project's LLM client
to decompose a user query into a small plan of steps that use available tools.

The agent returns a JSON structure containing a `goal` and a list of `steps`.
Each step contains: `tool_name`, `args`, and `reasoning`.
"""
from __future__ import annotations

import json
import logging
from typing import List, Dict, Any, Optional

from pydantic import ValidationError

from src.llm import LLMClient
from src.schemas.plan_models import PlanResult, PlanStep
from src.config import settings

logger = logging.getLogger(__name__)


# Note: PlanStep and PlanResult are imported from src.schemas.plan_models


class PlannerAgent:
    def __init__(self, llm_client: Optional[LLMClient] = None):
        # Prefer an injected LLM client; otherwise create one from settings
        if llm_client is None:
            try:
                llm_client = LLMClient()
            except Exception as e:
                logger.warning(f"PlannerAgent: failed to create LLMClient: {e}")
                llm_client = None
        self.llm = llm_client

    async def create_plan(self, user_query: str, available_tools: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Create a plan given the user's query and the available tools.

        Returns a dict shaped like {"goal": str, "steps": [ {tool_name, args, reasoning} ]}
        """
        if not self.llm:
            raise RuntimeError("No LLM available for planning")

        tools_list = available_tools or []
        tools_str = "\n".join([f"- {t.get('name')}: {t.get('description','')}" for t in tools_list])

        system_prompt = (
            "You are a PLANNER. Break the user's request into atomic steps using ONLY the available tools. "
            "Output pure JSON with fields 'goal' and 'steps'. 'steps' is an array of objects with 'tool_name', 'args' and 'reasoning'. "
            "Use tool names exactly as provided. If a step needs no tool, place tool_name as 'none' and args as {}. "
            "Do not add any other exposition; return only a valid JSON object."
        )

        instruction = (
            f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
            "Output a JSON object with 'goal' and 'steps' (tool_name, args, reasoning)."
        )

        # Try up to N times to get a valid JSON plan from the model.
        # Re-prompt the LLM if parse/validation fails.
        max_retries = 3
        last_raw = None
        for attempt in range(max_retries):
            try:
                raw = await self.llm.generate(prompt=instruction, system_prompt=system_prompt)
            except Exception as e:
                logger.exception("PlannerAgent: LLM failed to generate plan")
                return {"goal": user_query, "steps": []}

            last_raw = raw
            # Resilient parse: find first '{' and last '}' and extract JSON
            try:
                if isinstance(raw, str) and '{' in raw and '}' in raw:
                    first = raw.find('{')
                    last = raw.rfind('}')
                    json_str = raw[first:last+1]
                    parsed = json.loads(json_str)
                else:
                    parsed = json.loads(raw) if isinstance(raw, str) else raw
            except Exception:
                parsed = None

            if parsed is None:
                # If parse failed, craft a repair-system prompt for the next attempt
                instruction = (
                    f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
                    "The previous assistant output was not valid JSON. Return only a valid JSON object with 'goal' and 'steps'. "
                )
                continue

            # Validate parsed structure using PlanResult pydantic model
            try:
                # Normalization: translate fields into expected shape when useful
                # Accept alternative keys like 'plan' or 'tool' by transforming
                normalized = {
                    'goal': parsed.get('goal') or user_query,
                    'steps': []
                }
                raw_steps = parsed.get('steps', parsed.get('plan', []))
                invalid_missing_tool = False
                for s in raw_steps:
                    has_tool_name = 'tool_name' in s or 'tool' in s
                    tn = s.get('tool_name') or s.get('tool') or 'none'
                    args = s.get('args') or s.get('parameters') or {}
                    reasoning = s.get('reasoning') or s.get('note') or ''
                    # If the step doesn't explicitly specify a tool (tool_name/tool), consider this invalid and request a repair
                    if not has_tool_name:
                        invalid_missing_tool = True
                    normalized['steps'].append({"tool_name": tn, "args": args, "reasoning": reasoning})

                if invalid_missing_tool:
                    raise ValueError("Plan step provided args but missing tool_name")

                plan_obj = PlanResult.parse_obj(normalized)
                # Validate steps tools against available tools if provided
                if tools_list:
                    allowed = {t.get('name') for t in tools_list if t.get('name')}
                    invalid_tools = [s.tool_name for s in plan_obj.steps if s.tool_name not in allowed and s.tool_name != 'none']
                    if invalid_tools:
                        raise ValueError(f"Plan contained tools not in available tools: {invalid_tools}")
                # Passed validation: return canonical dict
                return plan_obj.dict()
            except (ValidationError, Exception) as ve:
                logger.warning("PlannerAgent: validation failed on model output; retrying: %s", ve)
                # Re-prompt the LLM on next loop iteration
                instruction = (
                    f"Available tools:\n{tools_str}\n\nUser Request:\n{user_query}\n\n"
                    "Your previous output did not satisfy the required JSON schema. Please return a pure JSON document matching the schema."
                )
                continue

        # If we got here, we failed to produce a valid plan after retries; fall back to a minimal plan
        logger.warning("PlannerAgent: Failed to produce valid plan after %s attempts. Raw last output: %s", max_retries, last_raw)
        return {"goal": user_query, "steps": []}


__all__ = ["PlannerAgent", "PlanResult", "PlanStep"]


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\planner.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\verifier.py (Section: BACKEND_PYTHON) ---

"""
Verifier Agent (Empirical Distrust)
Implements System 2 verification loop for claims.
"""
from typing import List, Dict, Any, Optional
from src.llm import LLMClient
from src.memory import TieredMemory
import logging
import json

logger = logging.getLogger(__name__)

class VerifierAgent:
    """
    Verifier Agent implements 'Empirical Distrust'.
    It verifies claims by seeking primary source evidence and calculating provenance entropy.
    """
    
    def __init__(self, memory: TieredMemory, llm: LLMClient):
        self.memory = memory
        self.llm = llm
        
    async def verify_claim(self, claim: str, context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Verify a specific claim against provided context and primary sources.
        Returns verification result with score and evidence.
        """
        # 1. Identify key facts in claim
        facts = await self._extract_facts(claim)
        
        # 2. Check evidence for each fact
        verified_facts = []
        overall_score = 0.0
        
        for fact in facts:
            evidence = await self._find_evidence(fact, context)
            fact_score = self._calculate_provenance_score(evidence)
            verified_facts.append({
                "fact": fact,
                "evidence": evidence,
                "score": fact_score,
                "verified": fact_score > 0.7
            })
            overall_score += fact_score
            
        avg_score = overall_score / len(facts) if facts else 0.0
        
        return {
            "claim": claim,
            "verified": avg_score > 0.7,
            "score": avg_score,
            "details": verified_facts
        }

    async def _extract_facts(self, claim: str) -> List[str]:
        """Extract atomic facts from claim using LLM."""
        prompt = f"""Extract atomic, verifiable facts from this claim:
"{claim}"

Return as JSON list of strings."""
        
        try:
            response = await self.llm.generate(prompt, temperature=0.1)
            # Simple parsing attempt
            if "[" in response:
                start = response.find("[")
                end = response.rfind("]") + 1
                return json.loads(response[start:end])
            return [claim]
        except Exception:
            return [claim]

    async def _find_evidence(self, fact: str, context: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find supporting evidence in context."""
        evidence = []
        # Simple keyword matching for now, could be semantic
        fact_terms = set(fact.lower().split())
        
        for item in context:
            content = item.get("content", "").lower()
            # Check overlap
            if any(term in content for term in fact_terms if len(term) > 4):
                evidence.append(item)
                
        return evidence

    def _calculate_provenance_score(self, evidence: List[Dict[str, Any]]) -> float:
        """
        Calculate score based on source type.
        Primary sources (code, logs) > Secondary (docs) > Tertiary (chat).
        """
        if not evidence:
            return 0.0
            
        score_sum = 0.0
        for item in evidence:
            # Determine source type from metadata
            meta = item.get("metadata", {})
            source = meta.get("source", "unknown")
            category = item.get("category", "unknown")
            
            weight = 0.5 # Default
            
            if category == "code" or source.endswith(".py") or source.endswith(".log"):
                weight = 1.0 # Primary
            elif category == "doc" or source.endswith(".md"):
                weight = 0.8 # Secondary
            elif category == "chat":
                weight = 0.4 # Tertiary/Hearsay
                
            score_sum += weight
            
        # Normalize (diminishing returns)
        return min(1.0, score_sum)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\verifier.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\orchestrator.py (Section: BACKEND_PYTHON) ---

from typing import Any, Dict, List, Optional
import logging
import asyncio

logger = logging.getLogger(__name__)


class CrossTeamOrchestrator:
    """A minimal Cross-Team Orchestrator skeleton.

    Responsibilities:
    - Spawn N teams (cloned Planner + Reasoner instances)
    - Gather plans from teams in parallel
    - Aggregate plans using Greedy aggregation rules
    - Return a superior plan
    """

    def __init__(self, teams: int = 3, config: Optional[Dict[str, Any]] = None):
        self.teams = teams
        self.config = config or {}
        self.team_agents = []  # placeholder for agent instances

    async def spawn_teams(self):
        """Spawn team agents (placeholder)."""
        logger.debug("Spawning %d teams for Cross-Team Orchestration", self.teams)
        # In a real implementation, spawn planner/agent clones and wire them to local clients
        self.team_agents = [f"team_{i}" for i in range(self.teams)]
        return self.team_agents

    async def collect_plans(self, prompt: str) -> List[Dict[str, Any]]:
        """Collect plans from each team (placeholder).

        In a real implementation this would call each team's planner API and return a structured plan.
        """
        plans = []
        logger.debug("Collecting plans for prompt: %s", prompt)
        for i, t in enumerate(self.team_agents):
            # Replace with actual planner / agent call
            plans.append({"team": t, "plan": [{"step": 1, "tool_name": "none", "args": {}, "reasoning": "noop"}]})
        return plans

    def greedy_aggregate(self, plans: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Greedy aggregation: merges plan steps from each team into a 'SuperiorSolution' (placeholder).

        This is a very minimal placeholder that returns the first non-empty plan.
        """
        logger.debug("Aggregating plans: %s", plans)
        for p in plans:
            if p.get('plan') and len(p['plan']) > 0:
                return {"aggregated_plan": p['plan']}
        return {"aggregated_plan": []}

    async def run(self, prompt: str) -> Dict[str, Any]:
        """Run a single orchestration cycle.

        Returns an aggregated plan for the given prompt.
        """
        await self.spawn_teams()
        plans = await self.collect_plans(prompt)
        aggregate = self.greedy_aggregate(plans)
        return aggregate


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\agents\orchestrator\orchestrator.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\audit.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from src.bootstrap import get_components
from src.security import verify_api_key
from pathlib import Path
from src.config import settings

router = APIRouter()


@router.get('/audit/logs')
async def get_audit_logs(request_obj: Request, limit: int = 50, authenticated: bool = Depends(verify_api_key)):
    try:
        path = Path(settings.audit_log_path)
        if not path.exists():
            return {"logs": [], "message": "No audit log found"}
        with path.open('r', encoding='utf-8', errors='ignore') as f:
            lines = f.read().splitlines()
        tail = lines[-int(limit):] if limit and len(lines) > 0 else lines
        return {"logs": tail, "count": len(tail)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\audit.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\health.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, HTTPException
from pydantic import BaseModel
from src.bootstrap import get_components
from src.config import settings
from typing import Any, Dict
from pydantic import BaseModel

router = APIRouter()


@router.get("/health")
async def health_check(request: Request):
    components = get_components(request.app)
    memory = components.get("memory")
    llm = components.get("llm")
    plugin_manager = components.get("plugin_manager")

    health = {"status": "healthy", "components": {}, "version": getattr(settings, 'ece_version', 'dev')}

    # Redis
    try:
        health["components"]["redis"] = bool(getattr(memory, "redis", None))
    except Exception:
        health["components"]["redis"] = False

    # Neo4j
    try:
        neo4j_store = getattr(memory, "neo4j", None)
        health["components"]["neo4j"] = bool(neo4j_store and getattr(neo4j_store, "neo4j_driver", None))
    except Exception:
        health["components"]["neo4j"] = False
    # Attempt counter
    try:
        health["components"]["neo4j_reconnect_attempts"] = getattr(neo4j_store, "_neo4j_reconnect_attempts", 0)
        health["components"]["neo4j_reconnecting"] = getattr(neo4j_store, "_neo4j_reconnect_task", None) is not None
        # Expose auth error for Neo4j so admins can take action
        health["components"]["neo4j_auth_error"] = getattr(neo4j_store, "_neo4j_auth_error", False)
    except Exception:
        health["components"]["neo4j_reconnect_attempts"] = 0
        health["components"]["neo4j_reconnecting"] = False

    # LLM: check if API is reachable or local model is loaded
    try:
        llm_status = {"api": False, "local": False}
        if llm:
            # If a model detection has been attempted, we might have `_detected_model`
            try:
                detected = await llm.detect_model()
                llm_status["api"] = bool(detected)
            except Exception:
                llm_status["api"] = False
            # Check local model availability without initializing heavy loads
            local = getattr(llm, "_local_llm", None) is not None
            llm_status["local"] = local
        health["components"]["llm"] = llm_status
    except Exception:
        health["components"]["llm"] = {"api": False, "local": False}

    # Plugin manager
    try:
        health["components"]["plugins"] = bool(plugin_manager and getattr(plugin_manager, 'enabled', False))
    except Exception:
        health["components"]["plugins"] = False

    return health


class ReconnectRequest(BaseModel):
    force: bool = False


class Neo4jConfigUpdate(BaseModel):
    neo4j_uri: str | None = None
    neo4j_user: str | None = None
    neo4j_password: str | None = None


@router.post("/admin/neo4j/reconnect")
async def admin_neo4j_reconnect(request: Request, body: ReconnectRequest | None = None):
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    # If the DB is disabled by config, return a helpful message
    if not getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None):
        return {"status": "disabled", "message": "Neo4j not configured in memory component"}

    force = bool(body.force) if body else False
    result = await memory.trigger_reconnect(force=force)
    return {"status": "ok", "result": result}


def _mask(val: str | None, show: int = 2) -> str:
    if not val:
        return ""
    if len(val) <= show:
        return "*" * len(val)
    return val[:1] + "*" * (len(val) - show - 1) + val[-show:]


@router.get("/admin/neo4j/config")
async def admin_neo4j_config(request: Request) -> Dict[str, Any]:
    """Return the Neo4j config being used by the running app (masked password)."""
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    return {
        "neo4j_enabled": getattr(getattr(memory, 'neo4j', None), "neo4j_driver", None) is not None or getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None) is not None,
        "neo4j_uri": getattr(getattr(memory, 'neo4j', None), "neo4j_uri", None),
        "neo4j_user": getattr(getattr(memory, 'neo4j', None), "neo4j_user", None),
        "neo4j_password_masked": _mask(getattr(getattr(memory, 'neo4j', None), "neo4j_password", None)),
        "neo4j_auth_error": getattr(getattr(memory, 'neo4j', None), "_neo4j_auth_error", False),
        "neo4j_reconnect_attempts": getattr(getattr(memory, 'neo4j', None), "_neo4j_reconnect_attempts", 0),
        "neo4j_reconnecting": getattr(getattr(memory, 'neo4j', None), "_neo4j_reconnect_task", None) is not None,
    }


@router.post("/admin/neo4j/config")
async def admin_neo4j_config_update(request: Request, body: Neo4jConfigUpdate):
    """Update Neo4j connection settings for the running app and trigger reconnect.

    NOTE: This endpoint updates the in-memory values on the `memory` object. It does not persist changes to `.env`.
    Use this to quickly correct credentials and test reconnects without restarting the app.
    """
    components = get_components(request.app)
    memory = components.get("memory")
    if memory is None:
        raise HTTPException(status_code=404, detail="Memory component not found")

    # Update runtime values
    changed = {}
    if body.neo4j_uri:
        memory.neo4j.neo4j_uri = body.neo4j_uri
        changed['neo4j_uri'] = body.neo4j_uri
    if body.neo4j_user:
        memory.neo4j.neo4j_user = body.neo4j_user
        changed['neo4j_user'] = body.neo4j_user
    if body.neo4j_password:
        memory.neo4j.neo4j_password = body.neo4j_password
        changed['neo4j_password'] = '***'  # do not echo back password

    # If we changed credentials, force a reconnect
    result = await memory.trigger_reconnect(force=True)
    return {"status": "ok", "changed": changed, "reconnect_result": result}


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\health.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\memory.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel
from typing import Optional
from src.bootstrap import get_components

router = APIRouter()


@router.get("/context/{session_id}")
async def get_context(request_obj: Request, session_id: str):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    active = await memory.get_active_context(session_id)
    summaries = await memory.get_summaries(session_id)
    return {"session_id": session_id, "active_context": active, "active_tokens": memory.count_tokens(active) if active else 0, "summaries": summaries}


@router.get("/memories/{category}")
async def get_memories_by_category(request_obj: Request, category: str, limit: int = 10):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    memories = await memory.get_recent_by_category(category, limit)
    return {"category": category, "count": len(memories), "memories": memories}


@router.get("/memories/search")
async def search_memories(request_obj: Request, query: str | None = None, category: str = None, tags: str = None, limit: int = 10):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    tag_list = [t.strip() for t in tags.split(',')] if tags else None
    # If query provided, pass as a content search
    memories = await memory.search_memories(query_text=query, category=category, tags=tag_list, limit=limit)
    return {"query": {"query": query, "category": category, "tags": tag_list}, "count": len(memories), "memories": memories}


@router.get("/memories")
async def get_memories(request_obj: Request, limit: int = 10):
    """Compatibility endpoint: return recent memories (search with no filters)."""
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    memories = await memory.search_memories(limit=limit)
    return {"count": len(memories), "memories": memories}


class MemoryAddRequest(BaseModel):
    category: str
    content: str
    tags: list[str] | None = None
    importance: int = 5
    metadata: dict | None = None


@router.post("/memories")
async def add_memory(request_obj: Request, body: MemoryAddRequest):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    # Ensure Neo4j is available (avoid accepting writes that won't be persisted)
    if not getattr(memory, 'neo4j', None) or not getattr(memory.neo4j, 'neo4j_driver', None):
        raise HTTPException(status_code=503, detail="Neo4j unavailable; cannot add memory")
    await memory.add_memory(session_id="api", content=body.content, category=body.category, tags=body.tags, importance=body.importance, metadata=body.metadata)
    return {"status": "success", "message": "Memory added"}


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\memory.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\openai_adapter.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, Depends, HTTPException
from src.bootstrap import get_components
from src.security import verify_api_key
from pydantic import BaseModel
from typing import Any, Dict
import time
from src.api.chat import ChatRequest, chat, chat_stream

router = APIRouter()


@router.post('/v1/chat/completions')
async def openai_chat_completions(request_obj: Request, body: Dict[str, Any], authenticated: bool = Depends(verify_api_key)):
    # very small adapter that maps old OpenAI payload to our ChatRequest
    model = body.get('model')
    messages = body.get('messages', [])
    stream = body.get('stream', False)
    system_prompt = None
    session_id = body.get('session_id') or body.get('conversation_id') or 'openai-adapter-session'
    user_message = None
    conversation_buffer = []
    for m in messages:
        role = m.get('role')
        content = m.get('content')
        if role == 'system':
            system_prompt = (system_prompt or '') + (content or '')
        elif role == 'user':
            conversation_buffer.append(f"User: {content}")
            user_message = content
        elif role == 'assistant':
            conversation_buffer.append(f"Assistant: {content}")

    if user_message is None:
        raise HTTPException(status_code=400, detail='No user message supplied')

    full_message = '\n'.join([m for m in conversation_buffer]) + f"\nUser: {user_message}"
    req = ChatRequest(session_id=session_id, message=full_message, system_prompt=system_prompt)
    if stream:
        return await chat_stream(request_obj, req, authenticated)
    else:
        chat_response = await chat(request_obj, req, authenticated)
        return {
            'id': f"chatcmpl-{int(time.time()*1000)}",
            'object': 'chat.completion',
            'created': int(time.time()),
            'model': model or 'ece-core',
            'choices': [{
                'index': 0,
                'message': {'role': 'assistant', 'content': chat_response.response},
                'finish_reason': 'stop'
            }],
            'usage': {
                'prompt_tokens': chat_response.context_tokens,
                'completion_tokens': len(chat_response.response.split()),
                'total_tokens': chat_response.context_tokens + len(chat_response.response.split())
            }
        }


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\openai_adapter.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plan.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Request, HTTPException, Depends
from pydantic import BaseModel
from typing import Optional
import logging

from src.bootstrap import get_components
from src.security import verify_api_key

logger = logging.getLogger(__name__)

router = APIRouter()


class PlanRequest(BaseModel):
    session_id: str
    message: str


@router.post("/plan")
async def plan(request_obj: Request, payload: PlanRequest, authenticated: bool = Depends(verify_api_key)):
    comps = get_components(request_obj.app)
    planner = getattr(request_obj.app.state, 'planner', None)
    plugin_manager = comps.get('plugin_manager')
    if not planner:
        raise HTTPException(status_code=503, detail="Planner not initialized")
    tools = []
    try:
        if plugin_manager and getattr(plugin_manager, 'enabled', False):
            tools = plugin_manager.list_tools()
    except Exception:
        tools = []
    try:
        plan = await planner.create_plan(payload.message, tools)
        return {"plan": plan}
    except Exception as e:
        logger.exception("/plan failed")
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plan.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plugins.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from src.bootstrap import get_components

router = APIRouter()


@router.get('/plugins/tools')
async def plugins_tools(request_obj: Request):
    components = get_components(request_obj.app)
    plugin_manager = components.get('plugin_manager')
    if plugin_manager and getattr(plugin_manager, 'enabled', False):
        return {'tools': plugin_manager.list_tools()}
    raise HTTPException(status_code=404, detail='Plugins disabled or not available')


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\plugins.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\reason.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel
from typing import Any, List, Dict
from src.bootstrap import get_components

router = APIRouter()


class ReasonRequest(BaseModel):
    session_id: str
    question: str
    mode: str = "graph"


class ReasonResponse(BaseModel):
    answer: str
    reasoning_trace: List[Dict[str, Any]]
    iterations: int
    confidence: str


@router.post("/reason", response_model=ReasonResponse)
async def reason_with_graph(request_obj: Request, body: ReasonRequest):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    llm = components.get("llm")
    graph_reasoner = components.get("graph_reasoner")
    markov_reasoner = components.get("markov_reasoner")
    context_mgr = components.get("context_mgr")
    if not all([memory, llm, graph_reasoner, markov_reasoner]):
        raise HTTPException(status_code=503, detail="Not initialized")
    try:
        if body.mode == "graph":
            result = await graph_reasoner.reason(body.session_id, body.question)
        elif body.mode == "markov":
            initial_context = await context_mgr.build_context(body.session_id, body.question)
            answer = await markov_reasoner.reason(body.question, initial_context)
            result = {
                "answer": answer,
                "reasoning_trace": [{"type": "markovian", "result": "Used Markovian chunked reasoning"}],
                "iterations": markov_reasoner.max_chunks,
                "confidence": "medium"
            }
        else:
            raise HTTPException(status_code=400, detail=f"Invalid mode: {body.mode}")

        await context_mgr.update_context(body.session_id, body.question, result["answer"])
        return ReasonResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reasoning/trace/{session_id}")
async def get_reasoning_trace(request_obj: Request, session_id: str):
    components = get_components(request_obj.app)
    memory = components.get("memory")
    if not memory:
        raise HTTPException(status_code=503, detail="Not initialized")
    summaries = await memory.get_summaries(session_id, limit=5)
    return {"session_id": session_id, "traces": summaries}


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\api\reason.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair.py (Section: BACKEND_PYTHON) ---

"""Compatibility shim for src.maintenance.repair.

This module re-exports `run_repair` from `src.maintenance.repair_wrapper` for
backwards compatibility. Keep it minimal to avoid duplication and errors.
"""
from __future__ import annotations

from src.maintenance.repair_wrapper import run_repair

__all__ = ['run_repair']
"""Compatibility shim for src.maintenance.repair.

This module re-exports `run_repair` from `src.maintenance.repair_wrapper` for
backwards compatibility.
"""
from __future__ import annotations

from src.maintenance.repair_wrapper import run_repair

__all__ = ['run_repair']
"""src.maintenance.repair

Lightweight wrapper that exposes `run_repair` from the scripts-based implementation while
providing a stable `src`-side import path for packaging.

This does not copy the heavy repair logic; it simply delegates to the implementation in
`scripts/neo4j/repair/repair_missing_links_similarity_embeddings.py` (or the legacy
`scripts/repair_missing_links_similarity_embeddings.py`). This keeps maintenance code in
one place, while enabling packaged builds to import `run_repair` from `src`.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _find_run_repair():
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    last_exc = None
    for c in candidates:
        try:
            mod = importlib.import_module(c)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception as e:
            last_exc = e
            logger.debug('repair wrapper: import failed for %s: %s', c, e)
    raise ModuleNotFoundError('run_repair not importable; tried: %s' % ','.join(candidates)) from last_exc


# resolve eagerly at import time; if unavailable, callers will get a ModuleNotFoundError
try:
    _run_repair = _find_run_repair()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Proxy to the real `run_repair` implementation.

    Returns the coroutine produced by the underlying implementation (do not await here).
    """
    if not _run_repair:
        raise ModuleNotFoundError('run_repair not available; ensure scripts package exists and is importable')
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper for run_repair.

This module provides a stable import for MemoryWeaver and other src code to call
`run_repair` without relying on dynamic imports of the `scripts` package.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_candidate() -> Any:
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper: failed to import %s', cand, exc_info=True)
    raise ModuleNotFoundError('Could not import run_repair from candidates: %s' % (', '.join(candidates),))


_run_repair = None
try:
    _run_repair = _import_candidate()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Delegate to the underlying repair script's `run_repair`.

    Raises ModuleNotFoundError if the underlying script isn't available.
    """
    if not _run_repair:
        raise ModuleNotFoundError('run_repair not available; ensure scripts package is present')
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    """Attempt to import run_repair from known candidate module paths.

    Returns the callable if found, otherwise raises ModuleNotFoundError.
    """
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


# Try to import the repair function at import time so that callers can directly use it.
_run_repair = None
try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


_run_repair = None

try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    # Defer raising until a caller attempts to use run_repair
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper that exposes `run_repair` to be imported as a src module.

This wrapper imports the implementation that currently resides under
`scripts.neo4j.repair.repair_missing_links_similarity_embeddings` and
re-exports `run_repair` so that code under `src` can import it reliably.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _import_run_repair():
    candidates = [
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    ]
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair wrapper could not import %s', cand, exc_info=True)
    raise ModuleNotFoundError("Could not import run_repair from scripts.* candidates: %s" % candidates)


_run_repair = None

try:
    _run_repair = _import_run_repair()
except ModuleNotFoundError:
    # Defer raising until a caller attempts to use run_repair
    _run_repair = None


def run_repair(*args: Any, **kwargs: Any):
    """Call the underlying `run_repair` implementation.

    This wrapper provides a stable import path under `src.maintenance.repair` and
    will raise a clear error if the underlying script is not available.
    """
    if not _run_repair:
        raise ModuleNotFoundError("run_repair is not available; ensure repair scripts package is installed or the project root contains the scripts/ package")
    # Delegate call to the real run_repair (async function). We return its coroutine.
    return _run_repair(*args, **kwargs)


__all__ = ['run_repair']
"""
Repair wrapper module
- Purpose: Export `run_repair` callable for MemoryWeaver and other internal callers.
- Implementation: Import the run_repair function from the existing scripts module path (scripts.neo4j.repair.repair_missing_links_similarity_embeddings)
  and re-export it. This keeps the heavy repair logic in the `scripts` area while ensuring a stable `src` import path that is bundled with the app.
"""
from __future__ import annotations

import importlib
import sys
import os
import logging
from typing import Any, Optional

logger = logging.getLogger(__name__)

# Attempt to import the repair module from the canonical nested script path
_run_repair_callable = None

# Ensure repo root in sys.path (defensive)
_repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if _repo_root not in sys.path:
    sys.path.insert(0, _repo_root)

try:
    _mod = importlib.import_module('scripts.neo4j.repair.repair_missing_links_similarity_embeddings')
    if hasattr(_mod, 'run_repair'):
        _run_repair_callable = getattr(_mod, 'run_repair')
except Exception:
    # Try legacy flattened path
    try:
        _mod = importlib.import_module('scripts.repair_missing_links_similarity_embeddings')
        if hasattr(_mod, 'run_repair'):
            _run_repair_callable = getattr(_mod, 'run_repair')
    except Exception:
        logger.exception('Repair wrapper could not import the repair module from scripts.* paths')


def run_repair(*args, **kwargs) -> Optional[Any]:
    """Call the underlying run_repair implementation.

    If the underlying script is not available, this function will raise ModuleNotFoundError.

    Returns whatever the underlying `run_repair` returns.
    """
    if not _run_repair_callable:
        raise ModuleNotFoundError("Could not import the repair script implementation (scripts.neo4j.repair.repair_missing_links_similarity_embeddings)")
    return _run_repair_callable(*args, **kwargs)

__all__ = ['run_repair']


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair_wrapper.py (Section: BACKEND_PYTHON) ---

"""Safe wrapper for the repair script implementation.

This helper exposes `run_repair` while delegating to the scripts-based implementation.
It avoids editing or duplicating heavy logic and ensures a stable `src` import path.
"""
from __future__ import annotations

import importlib
import logging
from typing import Any

logger = logging.getLogger(__name__)


def _load_run_repair():
    candidates = (
        'scripts.neo4j.repair.repair_missing_links_similarity_embeddings',
        'scripts.repair_missing_links_similarity_embeddings',
    )
    for cand in candidates:
        try:
            mod = importlib.import_module(cand)
            if hasattr(mod, 'run_repair'):
                return getattr(mod, 'run_repair')
        except Exception:
            logger.debug('repair_wrapper: failed to import %s', cand, exc_info=True)
    raise ModuleNotFoundError('repair_wrapper: could not find run_repair on candidates: %s' % ','.join(candidates))


try:
    _run_repair_fn = _load_run_repair()
except ModuleNotFoundError:
    _run_repair_fn = None


def run_repair(*args: Any, **kwargs: Any):
    if not _run_repair_fn:
        raise ModuleNotFoundError('repair_wrapper: run_repair not available; ensure scripts package is present and importable')
    # Filter kwargs to only those accepted by the underlying implementation to maintain compatibility
    try:
        import inspect
        sig = inspect.signature(_run_repair_fn)
        accepted = set(sig.parameters.keys())
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}
    except Exception:
        # If introspection fails for any reason, fall back to passing all kwargs
        filtered_kwargs = kwargs
    return _run_repair_fn(*args, **filtered_kwargs)


__all__ = ['run_repair']


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\repair_wrapper.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\weaver.py (Section: BACKEND_PYTHON) ---

"""
Memory Weaver - a lightweight engine that runs the repair logic as a scheduled/programmable task.

This module uses `src.maintenance.repair_wrapper.run_repair` as the canonical import path.
The wrapper dynamically loads the script-based implementation from one of these candidates:
 - `scripts.neo4j.repair.repair_missing_links_similarity_embeddings`
 - `scripts.repair_missing_links_similarity_embeddings`

The wrapper protects the package import from changes to the script location and provides
an introspection-based API to filter parameters passed to the underlying function.

The MemoryWeaver is safe to call in dry-run mode and only performs writes if the
master switch `WEAVER_COMMIT_ENABLED` is set in settings. The wrapper also raises a
clear `ModuleNotFoundError` when the implementation is not available and avoids
hard-failing the import path during startup.
"""
import asyncio
import time
import logging
import uuid
from datetime import datetime, timezone
from typing import Optional

from src.config import Settings, settings as GLOBAL_SETTINGS
from src.memory.redis_cache import RedisCache

logger = logging.getLogger(__name__)


class MemoryWeaver:
    def __init__(self, settings: Optional[Settings] = None):
        # Default to the module-global settings if none passed; this allows centralized overrides
        self.settings = settings or GLOBAL_SETTINGS

    async def weave_recent(self, hours: int | None = None, threshold: float | None = None, max_commit: int | None = None, candidate_limit: int | None = None, prefer_same_app: bool | None = None, dry_run: bool | None = None, csv_out: Optional[str] = None, run_id: Optional[str] = None, batch_size: int | None = None):
        """
        Run a repair cycle for the recent time window (hours) and commit matches if not dry_run.
        Returns: dict with run_id, processed items and commit count.
        """
        # Import the central `src.maintenance.repair` wrapper which exposes run_repair.
        # This wrapper keeps the weaver import simple (no dynamic path handling here) and is included in the 'src' package.
        try:
            from src.maintenance.repair_wrapper import run_repair
        except Exception as e:
            logger.error("MemoryWeaver: failed to import run_repair from src.maintenance.repair_wrapper; error=%s", e)
            return {'run_id': run_id, 'status': 'import_failed', 'message': str(e)}

        if not run_id:
            run_id = str(uuid.uuid4())

        # Build args
        # Resolve defaults from settings unless explicitly provided
        if hours is None:
            hours = self.settings.weaver_time_window_hours
        if threshold is None:
            threshold = self.settings.weaver_threshold
        if max_commit is None:
            max_commit = self.settings.weaver_max_commit
        if prefer_same_app is None:
            prefer_same_app = self.settings.weaver_prefer_same_app
        if dry_run is None:
            dry_run = self.settings.weaver_dry_run_default

        # Master Switch: If weaver_commit_enabled is True, we want to actually commit (auto-apply)
        if self.settings.weaver_commit_enabled:
            # If operator has enabled commit, force write-mode and clear dry-run
            dry_run = False
            commit_mode = True
        else:
            commit_mode = False

        # Resolve candidate limit and batch size defaults using settings when not supplied
        if candidate_limit is None:
            candidate_limit = self.settings.weaver_candidate_limit
        if batch_size is None:
            batch_size = getattr(self.settings, 'weaver_batch_size_default', self.settings.llm_embeddings_default_batch_size)

        params = {
            'threshold': threshold,
            'limit': 1000,
            'candidate_limit': candidate_limit,
            'dry_run': dry_run,
            'csv_out': csv_out,
            'time_window_hours': hours,
            'prefer_same_app': prefer_same_app,
            'min_origin_length': 100,
            'exclude_phrases': ["Genesis memory", "ECE Core System Initialized"],
            'delta': self.settings.weaver_delta,
            'max_commit': max_commit,
            # Use commit_mode (explicit master switch) when set; otherwise infer from dry_run
            'commit': commit_mode or (not dry_run),
            'run_id': run_id,
            'exclude_tag': self.settings.weaver_exclude_tag,
            'batch_size': batch_size,
        }

        logger.info(f"MemoryWeaver: Starting weave run {run_id} (hours={hours}, threshold={threshold}, commit={not dry_run})")

        # Adaptive throttling: process the large 'limit' value in smaller batches and sleep between them.
        sleep_between = getattr(self.settings, 'weaver_sleep_between_batches', 1.0)

        # Resolve batch size using prioritized sources: function arg -> explicit env -> default config values
        resolved_batch = int(batch_size or getattr(self.settings, 'weaver_batch_size', None) or getattr(self.settings, 'weaver_batch_size_default', None) or getattr(self.settings, 'llm_embeddings_default_batch_size', 2))
        if resolved_batch <= 0:
            resolved_batch = 1

        total_limit = int(params.get('limit', 1000))
        offset = 0

        # If Redis is available, check for recent user activity and pause if within last N seconds
        # 5 minutes (300 seconds) is used as the active-user window
        redis_client = RedisCache()
        await redis_client.initialize()

        while offset < total_limit:
            # Check user activity to avoid collisions with interactive sessions
            try:
                last_active = None
                if redis_client.redis:
                    val = await redis_client.redis.get(f"session:{self.settings.anchor_session_id}:last_active_at")
                    if val:
                        try:
                            last_active = int(val)
                        except Exception:
                            # value might be an ISO timestamp; try conversion
                            try:
                                last_active = int(float(val))
                            except Exception:
                                last_active = None
                if last_active:
                    now = int(time.time())
                    if (now - last_active) < 300:
                        logger.info(f"MemoryWeaver: Paused weave run {run_id} because user activity detected {now - last_active}s ago")
                        await redis_client.close()
                        return {'run_id': run_id, 'status': 'paused_user_active', 'delay_seconds': now - last_active}
            except Exception as e:
                logger.debug("MemoryWeaver: Failed to check last_active, continuing. err=%s", e)

            # Prepare params for this batch
            batch_params = dict(params)
            batch_params['limit'] = resolved_batch
            batch_params['skip'] = offset
            logger.info(f"MemoryWeaver: Running batch {offset}:{offset + resolved_batch} (limit {resolved_batch})")
            try:
                await run_repair(threshold=batch_params['threshold'], limit=batch_params['limit'], candidate_limit=batch_params['candidate_limit'], dry_run=batch_params['dry_run'], csv_out=batch_params['csv_out'], time_window_hours=batch_params['time_window_hours'], prefer_same_app=batch_params['prefer_same_app'], min_origin_length=batch_params['min_origin_length'], exclude_phrases=batch_params['exclude_phrases'], delta=batch_params['delta'], max_commit=batch_params['max_commit'], commit=batch_params['commit'], run_id=batch_params['run_id'], exclude_tag=batch_params['exclude_tag'], batch_size=batch_params['batch_size'], skip=batch_params.get('skip', 0))
            except Exception as e:
                logger.error(f"MemoryWeaver: Batch failed for range {offset}:{offset + resolved_batch} with error: {e}")
                # If a batch fails, record and continue (the underlying run_repair already uses resilient embedding backoff)
            # Sleep between batches to avoid simultaneous bursts with user requests
            offset += resolved_batch
            if offset < total_limit:
                logger.debug(f"MemoryWeaver: Sleeping {sleep_between}s between batches to avoid resource contention")
                await asyncio.sleep(sleep_between)

        await redis_client.close()
        logger.info(f"MemoryWeaver: Completed weave run {run_id}")
        return {'run_id': run_id}


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\maintenance\weaver.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\manager.py (Section: BACKEND_PYTHON) ---

import logging
import tiktoken
from typing import Optional, List, Dict, Any
from src.config import settings
from src.content_utils import clean_content, is_json_like, is_html_like, has_technical_signal
import hashlib
from src.vector_adapter import create_vector_adapter
from src.memory.redis_cache import RedisCache
from src.memory.neo4j_store import Neo4jStore
from src.distiller import distill_moment

logger = logging.getLogger(__name__)

class TieredMemory:
    """
    Orchestrator for Tiered Memory (Redis + Neo4j).
    Replaces the monolithic src/memory.py.
    """

    def __init__(self, neo4j_uri: Optional[str] = None, redis_url: Optional[str] = None, neo4j_user: Optional[str] = None, neo4j_password: Optional[str] = None, llm_client=None):
        self.redis = RedisCache(redis_url)
        self.neo4j = Neo4jStore(neo4j_uri, neo4j_user, neo4j_password)
        self.llm_client = llm_client
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # Vector support
        self.vector_adapter = None
        self._vector_enabled = getattr(settings, "vector_enabled", False)
        if self._vector_enabled:
            try:
                self.vector_adapter = create_vector_adapter()
            except Exception as e:
                logger.warning(f"Failed to create vector adapter: {e}")

        # Backwards-compatible property accessors for legacy tests & code

    async def initialize(self):
        """Initialize all stores."""
        await self.redis.initialize()
        await self.neo4j.initialize()
        if self.vector_adapter and hasattr(self.vector_adapter, "initialize"):
            await self.vector_adapter.initialize()
        
        # Auto-init LLM for embeddings if needed
        if getattr(settings, "vector_auto_embed", False) and not self.llm_client:
            try:
                from src.llm import LLMClient
                self.llm_client = LLMClient()
            except Exception as e:
                logger.warning(f"Failed to init LLM client for auto-embed: {e}")

    @property
    def neo4j_driver(self):
        return getattr(self.neo4j, 'neo4j_driver', None)

    @neo4j_driver.setter
    def neo4j_driver(self, val):
        if self.neo4j:
            self.neo4j.neo4j_driver = val

    @property
    def neo4j_uri(self):
        return getattr(self.neo4j, 'neo4j_uri', None)

    @neo4j_uri.setter
    def neo4j_uri(self, val):
        if self.neo4j:
            self.neo4j.neo4j_uri = val

    @property
    def neo4j_user(self):
        return getattr(self.neo4j, 'neo4j_user', None)

    @neo4j_user.setter
    def neo4j_user(self, val):
        if self.neo4j:
            self.neo4j.neo4j_user = val

    @property
    def neo4j_password(self):
        return getattr(self.neo4j, 'neo4j_password', None)

    @neo4j_password.setter
    def neo4j_password(self, val):
        if self.neo4j:
            self.neo4j.neo4j_password = val

    @property
    def _neo4j_reconnect_attempts(self):
        return getattr(self.neo4j, '_neo4j_reconnect_attempts', 0)

    @property
    def _neo4j_reconnect_task(self):
        return getattr(self.neo4j, '_neo4j_reconnect_task', None)

    @property
    def _neo4j_auth_error(self):
        return getattr(self.neo4j, '_neo4j_auth_error', False)

    async def close(self):
        """Close all stores."""
        if self.redis:
            await self.redis.close()
        if self.neo4j:
            await self.neo4j.close()

    async def trigger_reconnect(self, force: bool = False) -> dict:
        """Proxy to Neo4j trigger reconnect to expose admin command."""
        if not self.neo4j:
            return {"started": False, "message": "Neo4j store not configured"}
        return await self.neo4j.trigger_reconnect(force=force)

    # Delegate Redis methods
    async def get_active_context(self, session_id: str) -> str:
        if self.redis:
            return await self.redis.get_active_context(session_id)
        return ""  # Return empty string when Redis is not available

    async def save_active_context(self, session_id: str, context: str):
        if self.redis:
            await self.redis.save_active_context(session_id, context)
        # Silently fail when Redis is not available

    async def touch_session(self, session_id: str):
        """Mark session as active by updating last_active timestamp in Redis without changing the active context."""
        try:
            if self.redis and self.redis.redis:
                import time
                await self.redis.redis.set(f"session:{session_id}:last_active_at", int(time.time()), ex=settings.redis_ttl)
        except Exception:
            # Not critical; ignore failures
            pass

    # Delegate Neo4j methods
    async def add_memory(self, session_id: Optional[str] = None, content: str = "", category: Optional[str] = None, tags: Optional[List[str]] = None, importance: int = 5, metadata: Optional[Dict[str, Any]] = None, llm_client=None):
        # 0. Preliminary cleaning & hygiene checks
        raw_content = content or ''
        # Skip JSON dump / HTML noisy content unless it contains technical signals
        if is_json_like(raw_content) and not has_technical_signal(raw_content):
            logger.warning('Skipping add_memory: json-like content without technical signal')
            return None
        if is_html_like(raw_content) and not has_technical_signal(raw_content):
            logger.warning('Skipping add_memory: html-like content without technical signal')
            return None

        # Compute cleaned content and detect technical signal
        tech_signal = has_technical_signal(raw_content)
        content_cleaned = clean_content(raw_content, remove_emojis=not tech_signal, remove_non_ascii=False, annotate_technical=tech_signal)
        tech_signal = has_technical_signal(raw_content)
        if not tech_signal and (not content_cleaned or len(content_cleaned) < 20):
            logger.warning('Skipping add_memory: content empty or too short after cleaning')
            return None

        # Compute a content hash for dedup (based on cleaned content to avoid duplicate noisy entries)
        content_hash = hashlib.sha256((content_cleaned or '').encode('utf-8')).hexdigest()

        # 1. Distill entities (Graph Wiring)
        entities = []
        try:
            # Use provided llm_client or self.llm_client
            client = llm_client or self.llm_client
            if client and content_cleaned:
                distilled = await distill_moment(content_cleaned, llm_client=client, metadata=metadata)
                if isinstance(distilled, dict):
                    entities = distilled.get("entities", [])
        except Exception as e:
            logger.warning(f"Failed to distill entities: {e}")

        # 2. Add to Neo4j (Graph + Document)
        # Pass cleaned content and additional properties to Neo4j
        # Tag technical content
        if tech_signal:
            tags = tags or []
            if 'technical' not in tags and '#technical' not in tags:
                tags.append('#technical')

        memory_id = await self.neo4j.add_memory(session_id, content, category, tags, importance, metadata, entities=entities, content_cleaned=content_cleaned, content_hash=content_hash, content_embedding_text=content_cleaned if not tech_signal else content_cleaned)
        
        # 3. Vector Indexing (Semantic Search)
        if self.vector_adapter and self._vector_enabled and memory_id and content_cleaned:
            try:
                client = llm_client or self.llm_client
                if client:
                    # Generate embedding
                    embeddings = await client.get_embeddings(content_cleaned)
                    if embeddings and len(embeddings) > 0:
                        # Handle list of lists or single list
                        embedding = embeddings[0] if isinstance(embeddings[0], list) else embeddings
                        # Index
                        await self.vector_adapter.index_chunk(
                            embedding_id=f"mem:{memory_id}",
                            node_id=memory_id,
                            chunk_index=0,
                            embedding=embedding,
                            metadata={
                                "content": content_cleaned,
                                "category": category,
                                "session_id": session_id,
                                "importance": importance,
                                "created_at": metadata.get("created_at") if metadata else None
                            }
                        )
            except Exception as e:
                logger.warning(f"Failed to index vector: {e}")

        # Return the created memory id
        return memory_id
    async def search_memories(self, query_text: Optional[str] = None, category: Optional[str] = None, tags: Optional[List[str]] = None, limit: int = 10) -> List[Dict[str, Any]]:
        if not query_text:
            # Fallback to recent if no query
            # Note: Neo4jStore needs a get_recent method, adding it to TODO or using direct cypher
            # For now, simple search
            return await self.neo4j.search_memories("", category, limit)
        return await self.neo4j.search_memories(query_text, category, limit)

    async def search_memories_neo4j(self, query_text: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search memories specifically in Neo4j (full-text)."""
        return await self.neo4j.search_memories(query_text, None, limit)

    async def get_recent_by_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Get recent memories by category."""
        # This requires a new method in Neo4jStore or a direct query here.
        # Adding direct query support via Neo4jStore.execute_cypher would be cleaner,
        # but for now let's add a helper to Neo4jStore or just use search with empty query if supported.
        # Actually, let's implement it properly by delegating to a new method we'll add to Neo4jStore,
        # or using search_memories with empty query if it supports sorting by time.
        # The current search_memories implementation in Neo4jStore sorts by nothing explicit if no query.
        # Let's add a dedicated method to Neo4jStore in a separate step, but for now we can try search.
        # Wait, the error is AttributeError on TieredMemory, so we MUST define it here.
        return await self.neo4j.get_recent_by_category(category, limit)

    async def get_summaries(self, session_id: str, limit: int = 5) -> List[str]:
        """Get recent summaries."""
        # Delegate to Neo4jStore
        return await self.neo4j.get_summaries(session_id, limit)

    async def save_summary(self, session_id: str, summary: str):
        """Save a conversation summary."""
        await self.neo4j.add_memory(
            session_id=session_id,
            content=summary,
            category="summary",
            tags=["summary"],
            importance=3,
            metadata={}
        )

    async def flush_to_neo4j(self, session_id: str, summary: str, original_tokens: int):
        """Flush summary to Neo4j."""
        await self.neo4j.add_memory(
            session_id=session_id,
            content=summary,
            category="summary",
            tags=["summary", "auto-flush"],
            importance=3,
            metadata={"original_token_count": original_tokens}
        )

    def count_tokens(self, text: str) -> int:
        if not text: return 0
        try: return len(self.tokenizer.encode(text, disallowed_special=()))
        except: return len(text) // 4


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\manager.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\neo4j_store.py (Section: BACKEND_PYTHON) ---

import json
import asyncio
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from neo4j import AsyncGraphDatabase
from neo4j.exceptions import AuthError
from src.config import settings

logger = logging.getLogger(__name__)

class Neo4jStore:
    """Handles Neo4j interactions for TieredMemory."""

    def __init__(self, neo4j_uri: Optional[str] = None, neo4j_user: Optional[str] = None, neo4j_password: Optional[str] = None):
        self.neo4j_uri = neo4j_uri or settings.neo4j_uri
        self.neo4j_user = neo4j_user or settings.neo4j_user
        self.neo4j_password = neo4j_password or settings.neo4j_password
        self.neo4j_driver = None
        self._neo4j_reconnect_task = None
        self._neo4j_reconnect_attempts = 0
        self._neo4j_auth_error = False

    async def initialize(self):
        """Connect to Neo4j."""
        if not getattr(settings, "neo4j_enabled", True):
            logger.info("Neo4j disabled by configuration")
            return

        try:
            self.neo4j_driver = AsyncGraphDatabase.driver(
                self.neo4j_uri,
                auth=(self.neo4j_user, self.neo4j_password)
            )
            async with self.neo4j_driver.session() as session:
                await session.run("RETURN 1")
                # Create schema indexes to prevent warnings and improve performance
                await session.run("CREATE INDEX memory_category IF NOT EXISTS FOR (n:Memory) ON (n.category)")
                await session.run("CREATE INDEX memory_created_at IF NOT EXISTS FOR (n:Memory) ON (n.created_at)")
                await session.run("CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name)")
                # Index for deduplication by content hash
                await session.run("CREATE INDEX memory_content_hash IF NOT EXISTS FOR (n:Memory) ON (n.content_hash)")
                # Indexes to support provenance/freshness-driven queries
                await session.run("CREATE INDEX memory_provenance_score IF NOT EXISTS FOR (n:Memory) ON (n.provenance_score)")
                await session.run("CREATE INDEX memory_freshness_score IF NOT EXISTS FOR (n:Memory) ON (n.freshness_score)")
            logger.info("Neo4j connected")
        except Exception as e:
            logger.warning(f"Neo4j unavailable: {e}")
            if isinstance(e, AuthError) or "unauthorized" in str(e).lower():
                self._neo4j_auth_error = True
            self.neo4j_driver = None
            if getattr(settings, 'neo4j_reconnect_enabled', False) and not self._neo4j_auth_error:
                self._neo4j_reconnect_task = asyncio.create_task(self._neo4j_reconnect_loop())

    async def close(self):
        """Close Neo4j connection."""
        if self.neo4j_driver:
            await self.neo4j_driver.close()
        if self._neo4j_reconnect_task:
            self._neo4j_reconnect_task.cancel()

    async def _neo4j_reconnect_loop(self):
        """Background retry loop."""
        delay = getattr(settings, 'neo4j_reconnect_initial_delay', 5)
        max_attempts = getattr(settings, 'neo4j_reconnect_max_attempts', 6)
        backoff = getattr(settings, 'neo4j_reconnect_backoff_factor', 2.0)
        attempt = 0
        
        while attempt < max_attempts and self.neo4j_driver is None:
            attempt += 1
            try:
                driver = AsyncGraphDatabase.driver(
                    self.neo4j_uri,
                    auth=(self.neo4j_user, self.neo4j_password)
                )
                async with driver.session() as session:
                    await session.run("RETURN 1")
                self.neo4j_driver = driver
                logger.info("Neo4j reconnected successfully")
                break
            except Exception as e:
                if isinstance(e, AuthError):
                    self._neo4j_auth_error = True
                    break
                await asyncio.sleep(delay)
                delay *= backoff

    async def trigger_reconnect(self, force: bool = False) -> dict:
        """Trigger a reconnect loop for Neo4j. If force is True, close any existing driver and start a new reconnect."""
        if force and self.neo4j_driver:
            try:
                await self.neo4j_driver.close()
            except Exception:
                pass
            self.neo4j_driver = None

        if self._neo4j_auth_error:
            return {"started": False, "message": "Neo4j auth error; credential fix required"}

        # If a reconnect task is already running, return status
        if self._neo4j_reconnect_task and not self._neo4j_reconnect_task.done():
            return {"started": False, "message": "Reconnect already in progress"}

        self._neo4j_reconnect_task = asyncio.create_task(self._neo4j_reconnect_loop())
        return {"started": True}

    async def execute_cypher(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Execute raw Cypher query."""
        if not self.neo4j_driver:
            return []
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(query, params or {})
                return await result.data()
        except Exception as e:
            logger.error(f"Cypher execution failed: {e}")
            return []

    async def add_memory(self, session_id: str, content: str, category: str, tags: List[str], importance: int, metadata: Dict[str, Any], entities: List[Dict[str, Any]] = None, content_cleaned: str = None, content_hash: str = None, content_embedding_text: str = None):
        """Add memory node and link entities."""
        if not self.neo4j_driver:
            return
        try:
            async with self.neo4j_driver.session() as session:
                # Compute / enforce app_id inside metadata if missing
                if metadata is None:
                    metadata = {}
                # Prefer provided app_id in metadata, otherwise compute deterministically
                app_id = None
                try:
                    if isinstance(metadata, dict) and metadata.get('app_id'):
                        app_id = str(metadata.get('app_id'))
                    elif isinstance(metadata, dict) and metadata.get('source') and metadata.get('chunk_index') is not None:
                        import uuid
                        ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')
                        app_id = str(uuid.uuid5(ns, f"{metadata.get('source')}:{metadata.get('chunk_index')}"))
                    else:
                        import uuid
                        ns = uuid.UUID('f8bd0f6e-0c4c-4654-9201-12c4f2b4b5ef')
                        app_id = str(uuid.uuid5(ns, (content or '')[:4096]))
                except Exception:
                    # Fallback to uuid4 if anything goes wrong
                    import uuid
                    app_id = str(uuid.uuid4())
                # Write app_id back into metadata JSON for consistency
                if isinstance(metadata, dict):
                    metadata['app_id'] = app_id
                else:
                    # if metadata is a string, attempt to parse and re-serialize with app_id
                    try:
                        md = json.loads(metadata) if isinstance(metadata, str) and metadata else {}
                        md['app_id'] = app_id
                        metadata = md
                    except Exception:
                        metadata = {'app_id': app_id}

                # Deduplication: if a content_hash exists, check if we've already stored it
                if content_hash:
                    dedup_q = "MATCH (m:Memory) WHERE m.content_hash = $content_hash RETURN elementId(m) as id LIMIT 1"
                    dedup_res = await session.run(dedup_q, {'content_hash': content_hash})
                    dedup_rec = await dedup_res.single()
                    if dedup_rec and dedup_rec.get('id'):
                        # Found existing memory; do not create duplicate
                        return dedup_rec.get('id')

                # Compute provenance_score and freshness_score defaults
                def _derive_provenance_score(meta: Dict[str, Any], category: str) -> float:
                    try:
                        if category:
                            c = category.lower()
                            if c == 'code':
                                return 1.0
                            if c in ('log', 'logs'):
                                return 0.95
                            if c in ('doc', 'docs', 'documentation'):
                                return 0.8
                            if c in ('chat', 'conversation', 'message'):
                                return 0.4
                    except Exception:
                        pass
                    # inspect metadata for source file info
                    try:
                        if isinstance(meta, dict):
                            src = meta.get('source', '')
                            if isinstance(src, str):
                                if src.endswith('.py') or src.endswith('.js'):
                                    return 1.0
                                if src.endswith('.log'):
                                    return 0.95
                            # Source type field
                            if meta.get('source_type') == 'doc':
                                return 0.8
                    except Exception:
                        pass
                    return float(getattr(settings, 'memory_default_provenance_score', 0.5))

                provenance_score = _derive_provenance_score(metadata or {}, category)
                freshness_score = float(getattr(settings, 'memory_default_freshness_score', 1.0))
                # default last_verified_at is None; Verifier will fill it
                last_verified_at = metadata.get('last_verified_at') if isinstance(metadata, dict) else None

                # Create Memory node with app_id property
                result = await session.run(
                    """
                    CREATE (m:Memory {
                        session_id: $session_id,
                        content: $content,
                        content_cleaned: $content_cleaned,
                        content_hash: $content_hash,
                        provenance_score: $provenance_score,
                        freshness_score: $freshness_score,
                        last_verified_at: $last_verified_at,
                        content_embedding_text: $content_embedding_text,
                        category: $category,
                        app_id: $app_id,
                        tags: $tags,
                        importance: $importance,
                        metadata: $metadata,
                        created_at: $created_at
                    })
                    RETURN elementId(m) as id
                    """,
                    {
                        "session_id": session_id or "unknown",
                        "content": content,
                        "content_cleaned": content_cleaned,
                        "content_hash": content_hash,
                        "content_embedding_text": content_embedding_text,
                        "category": category,
                        "app_id": app_id,
                        "tags": tags or [],
                        "importance": importance,
                        "metadata": json.dumps(metadata, default=str) if metadata else None,
                        "provenance_score": provenance_score,
                        "freshness_score": freshness_score,
                        "last_verified_at": last_verified_at,
                        "created_at": datetime.now(timezone.utc).isoformat()
                    }
                )
                record = await result.single()
                memory_id = record["id"] if record else None

                # Link Entities if provided
                if entities and memory_id:
                    await session.run(
                        """
                        MATCH (m:Memory) WHERE elementId(m) = $memory_id
                        UNWIND $entities as ent
                        MERGE (e:Entity {name: ent.text})
                        ON CREATE SET e.type = ent.type, e.metadata = ent.metadata
                        MERGE (m)-[:MENTIONS]->(e)
                        """,
                        {
                            "memory_id": memory_id,
                            "entities": [
                                {
                                    "text": e.get("text"),
                                    "type": e.get("type", "unknown"),
                                    "metadata": json.dumps(e.get("metadata", {}))
                                }
                                for e in entities if e.get("text")
                            ]
                        }
                    )
                
                return memory_id
        except Exception as e:
            logger.error(f"Failed to add memory: {e}")
            return None

    async def search_memories(self, query: str, category: Optional[str], limit: int) -> List[Dict[str, Any]]:
        """Search memories."""
        if not self.neo4j_driver:
            return []
        
        # Prefer fulltext index search for more reliable matches (memorySearch index)
        # Fallback to a CONTAINS search if the fulltext index isn't available or fails.
        cypher_fulltext = """
        CALL db.index.fulltext.queryNodes('memorySearch', $query) YIELD node, score
        WHERE ($category IS NULL OR node.category = $category)
        RETURN elementId(node) as id, node as m, score
        ORDER BY score DESC
        LIMIT $limit
        """
        
        try:
            async with self.neo4j_driver.session() as session:
                try:
                    result = await session.run(cypher_fulltext, {"query": query, "category": category, "limit": limit})
                    records = await result.data()
                except Exception:
                    # If fulltext index isn't available, fallback to an older contains query
                    cypher_contains = """
                    MATCH (m:Memory)
                    WHERE m.content CONTAINS $query
                    """ + ("AND m.category = $category" if category else "") + """
                    RETURN elementId(m) as id, m
                    LIMIT $limit
                    """
                    result = await session.run(cypher_contains, {"query": query, "category": category, "limit": limit})
                    records = await result.data()
                return [self._parse_memory_record(r) for r in records]
        except Exception as e:
            logger.error(f"Memory search failed: {e}")
            return []

    def _parse_memory_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Neo4j record into standard memory dict."""
        node = record.get("m") or record.get("node")
        eid = record.get("id")
        score = record.get("score")
        
        # Defensive parsing
        tags = node.get("tags")
        if isinstance(tags, str):
            try: tags = json.loads(tags)
            except: tags = [tags]
        
        meta = node.get("metadata")
        if isinstance(meta, str):
            try: meta = json.loads(meta)
            except: meta = {}
            
        return {
            "id": eid,
            "memory_id": eid,
            "content": node.get("content"),
            "tags": tags or [],
            "importance": node.get("importance", 5),
            "session_id": node.get("session_id"),
            "timestamp": node.get("created_at"),
            "category": node.get("category"),
            "metadata": meta or {},
            "score": score if score is not None else node.get("importance", 5) / 10.0
        }

    async def get_recent_by_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Get recent memories by category."""
        if not self.neo4j_driver:
            return []
        
        cypher = """
        MATCH (m:Memory)
        WHERE m.category = $category
        RETURN elementId(m) as id, m
        ORDER BY m.created_at DESC
        LIMIT $limit
        """
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(cypher, {"category": category, "limit": limit})
                records = await result.data()
                return [self._parse_memory_record(r) for r in records]
        except Exception as e:
            logger.error(f"Failed to get recent memories for category {category}: {e}")
            return []

    async def get_summaries(self, session_id: str, limit: int = 5) -> List[str]:
        """Get recent summaries for a session."""
        if not self.neo4j_driver:
            return []
            
        cypher = """
        MATCH (m:Memory)
        WHERE m.session_id = $session_id AND m.category = 'summary'
        RETURN m.content as content
        ORDER BY m.created_at DESC
        LIMIT $limit
        """
        try:
            async with self.neo4j_driver.session() as session:
                result = await session.run(cypher, {"session_id": session_id, "limit": limit})
                records = await result.data()
                return [r["content"] for r in records if r.get("content")]
        except Exception as e:
            logger.error(f"Failed to get summaries for session {session_id}: {e}")
            return []


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\neo4j_store.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\redis_cache.py (Section: BACKEND_PYTHON) ---

import redis.asyncio as redis
import logging
import time
from typing import Optional
from src.config import settings

logger = logging.getLogger(__name__)

class RedisCache:
    """Handles Redis interactions for TieredMemory."""
    
    def __init__(self, redis_url: Optional[str] = None):
        self.redis_url = redis_url or settings.redis_url
        self.redis = None

    async def initialize(self):
        """Connect to Redis."""
        try:
            maybe_client = redis.from_url(self.redis_url, decode_responses=True)
            if hasattr(maybe_client, "__await__"):
                self.redis = await maybe_client
            else:
                self.redis = maybe_client
            
            ping_ret = self.redis.ping()
            if hasattr(ping_ret, "__await__"):
                await ping_ret
            logger.info("Redis connected")
        except redis.ConnectionError as e:
            logger.warning(f"Redis unavailable: {e}")
            self.redis = None
        except Exception as e:
            logger.error(f"Redis connection failed: {e}")
            self.redis = None

    async def close(self):
        """Close Redis connection."""
        if self.redis:
            try:
                await self.redis.close()
            except Exception:
                pass

    async def get_active_context(self, session_id: str) -> str:
        """Get active context from Redis."""
        if not self.redis:
            return ""
        try:
            context = await self.redis.get(f"session:{session_id}:context")
            return context or ""
        except Exception as e:
            logger.error(f"Redis get failed for session {session_id}: {e}")
            return ""

    async def save_active_context(self, session_id: str, context: str):
        """Save active context to Redis with TTL."""
        if not self.redis:
            return
        try:
            await self.redis.set(f"session:{session_id}:context", context, ex=settings.redis_ttl)
            # Also set a last-active timestamp to help background tasks avoid interfering with active sessions
            try:
                await self.redis.set(f"session:{session_id}:last_active_at", int(time.time()), ex=settings.redis_ttl)
            except Exception:
                # Not critical; continue saving context even if last_active failed
                pass
        except Exception as e:
            logger.error(f"Redis set failed for session {session_id}: {e}")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\memory\redis_cache.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\plugins\browser_bridge\plugin.py (Section: BACKEND_PYTHON) ---

"""
Browser Bridge Plugin for ECE

This plugin provides API endpoints for the Chrome extension to communicate with the ECE system.
It allows for chat ingestion and context retrieval from the browser.
"""
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Dict, Any
import logging
from src.config import settings
from src.security import verify_api_key

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/v1/browser", tags=["browser_bridge"])

# Pydantic models for request/response
class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str
    timestamp: str = None


class IngestRequest(BaseModel):
    messages: List[ChatMessage]
    source_url: str = None
    session_id: str = None


class IngestResponse(BaseModel):
    success: bool
    processed_count: int
    message: str


class ContextRequest(BaseModel):
    draft_prompt: str
    max_results: int = 10


class ContextResponse(BaseModel):
    success: bool
    context: str
    retrieved_count: int


# Dependencies
async def get_components():
    """Get ECE components from app state."""
    from src.bootstrap import get_components as get_ece_components
    # This will be injected by the main app
    return get_ece_components


@router.post("/ingest", response_model=IngestResponse)
async def ingest_browser_chat(
    request: IngestRequest,
    # components: dict = Depends(get_components),
    auth: bool = Depends(verify_api_key)  # Only if auth is required
):
    """
    Ingest chat messages from the browser extension.
    Saves messages to Neo4j using the Archivist agent.
    """
    try:
        # Import components from the main app state
        from src.app_factory import app
        components = {
            "memory": getattr(app.state, "memory", None),
            "archivist_agent": getattr(app.state, "archivist_agent", None),
            "context_mgr": getattr(app.state, "context_mgr", None),
        }
        
        # Validate components
        if not components["memory"]:
            raise HTTPException(status_code=500, detail="Memory system not available")
        
        # Process each message and save to memory
        processed_count = 0
        for msg in request.messages:
            try:
                # Add to memory system
                memory_id = await components["memory"].add_memory(
                    session_id=request.session_id or "browser_session",
                    content=f"[Browser Chat] {msg.role.title()}: {msg.content}",
                    category="browser_chat",
                    tags=["browser", "chat", msg.role.lower()],
                    importance=5,
                    metadata={
                        "source": "browser_extension",
                        "url": request.source_url,
                        "role": msg.role,
                        "timestamp": msg.timestamp
                    }
                )
                
                if memory_id:
                    processed_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to process message: {e}")
                continue
        
        logger.info(f"Processed {processed_count}/{len(request.messages)} browser chat messages")
        
        return IngestResponse(
            success=True,
            processed_count=processed_count,
            message=f"Successfully processed {processed_count} out of {len(request.messages)} messages"
        )
        
    except Exception as e:
        logger.error(f"Error ingesting browser chat: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to ingest chat: {str(e)}")


@router.post("/context", response_model=ContextResponse)
async def get_context_for_prompt(
    request: ContextRequest,
    # components: dict = Depends(get_components),
    auth: bool = Depends(verify_api_key)  # Only if auth is required
):
    """
    Retrieve relevant context for a draft prompt from the browser.
    """
    try:
        # Import components from the main app state
        from src.app_factory import app
        components = {
            "memory": getattr(app.state, "memory", None),
            "context_mgr": getattr(app.state, "context_mgr", None),
        }
        
        # Validate components
        if not components["memory"]:
            raise HTTPException(status_code=500, detail="Memory system not available")
        
        # Search for relevant memories
        try:
            # Try to search with the draft prompt
            results = await components["memory"].search_memories(
                query_text=request.draft_prompt,
                limit=request.max_results
            )
        except Exception as search_error:
            logger.warning(f"Search failed: {search_error}")
            results = []
        
        # Format the context
        if results:
            context_parts = ["Relevant Memories:", "-" * 40]
            for i, result in enumerate(results, 1):
                content = result.get('content', '')[:200] + "..." if len(result.get('content', '')) > 200 else result.get('content', '')
                score = result.get('score', 0)
                context_parts.append(f"{i}. [{score:.2f}] {content}")
                context_parts.append("")
            
            formatted_context = "\n".join(context_parts)
        else:
            formatted_context = "No relevant memories found."
        
        logger.info(f"Retrieved {len(results)} context items for prompt: {request.draft_prompt[:50]}...")
        
        return ContextResponse(
            success=True,
            context=formatted_context,
            retrieved_count=len(results)
        )
        
    except Exception as e:
        logger.error(f"Error retrieving context: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve context: {str(e)}")


@router.get("/health")
async def browser_bridge_health():
    """Health check for the browser bridge."""
    return {
        "status": "healthy",
        "service": "Browser Bridge Plugin",
        "api_version": "1.0.0"
    }


# Additional utility endpoints
@router.get("/session/current")
async def get_current_session():
    """Get information about the current browser session."""
    return {
        "session_id": "browser_session",
        "connected": True,
        "features": ["chat_ingestion", "context_retrieval"]
    }

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\plugins\browser_bridge\plugin.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\archivist.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import logging
import time
import json
from datetime import datetime, timezone

from src.bootstrap import get_components
from src.security import verify_api_key
from src.models import PlaintextMemory, SourceType

logger = logging.getLogger(__name__)

router = APIRouter(tags=["archivist"])

class ExtensionIngestRequest(BaseModel):
    content: str
    type: str
    adapter: Optional[str] = None

class IngestResponse(BaseModel):
    status: str
    memory_ids: List[str]
    message: str

@router.post("/ingest", response_model=IngestResponse)
async def ingest_content(
    request_obj: Request, 
    payload: ExtensionIngestRequest, 
    authenticated: bool = Depends(verify_api_key)
):
    """
    Archivist Endpoint: Ingests raw content, distills it, and commits it to long-term memory.
    This bypasses the standard chat flow to ensure high-quality, verified memories are stored.
    """
    components = get_components(request_obj.app)
    memory = components.get("memory")
    
    if not memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")

    try:
        logger.info(f"Archivist ingesting content from adapter: {payload.adapter}")
        
        # Map source type
        source_type = SourceType.WEB_PAGE
        if "gemini" in payload.type.lower() or (payload.adapter and "gemini" in payload.adapter.lower()):
            source_type = SourceType.GEMINI_CHAT
        
        # Create PlaintextMemory (Directive INJ-A1)
        plaintext_memory = PlaintextMemory(
            source_type=source_type,
            source_identifier=f"browser_session_{datetime.now().strftime('%Y%m%d')}",
            content=payload.content,
            metadata={
                "adapter": payload.adapter,
                "raw_type": payload.type,
                "ingested_by": "Archivist"
            }
        )
        
        # Persist to Corpus (ark_corpus.jsonl)
        # Using standard open for simplicity and robustness
        with open("ark_corpus.jsonl", "a", encoding="utf-8") as f:
            f.write(plaintext_memory.json() + "\n")
            
        # Index via MemoryManager (Reflex Memory)
        session_id = f"archivist-{int(time.time())}"
        
        # Use memory.add_memory to trigger the full pipeline (Distillation -> Neo4j -> Vector)
        memory_id = await memory.add_memory(
            session_id=session_id,
            content=plaintext_memory.content,
            category="knowledge",
            tags=["#ingested", f"#{source_type.value.lower()}"],
            importance=3,
            metadata=plaintext_memory.metadata
        )

        return IngestResponse(
            status="success",
            memory_ids=[memory_id] if memory_id else [],
            message="Content successfully ingested, archived, and indexed."
        )

    except Exception as e:
        logger.exception("Archivist ingestion failed")
        raise HTTPException(status_code=500, detail=str(e))


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\archivist.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\coda_chat.py (Section: BACKEND_PYTHON) ---

from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Any, List, Dict, AsyncGenerator
import httpx
import json, time, asyncio, logging

from src.bootstrap import get_components
from src.security import verify_api_key
from src.prompts import build_system_prompt
from src.tools import ToolExecutor
from src.config import settings

logger = logging.getLogger(__name__)

router = APIRouter(tags=["coda_chat"])


class ChatRequest(BaseModel):
    session_id: str
    message: str
    system_prompt: Optional[str] = None


class ChatResponse(BaseModel):
    response: str
    session_id: str
    context_tokens: int


@router.post("/", response_model=ChatResponse)
async def chat(request_obj: Request, payload: ChatRequest, authenticated: bool = Depends(verify_api_key)):
    """
    Non-streaming chat endpoint.

    Output format:
    - "thinking: <initial LLM output>" : The first LLM generation (raw) which may contain introspective thoughts or tool-call markers.
    - "response: <final LLM output>" : The final LLM response returned after optionally executing tools and regenerating.

    Notes:
    - The 'thinking' portion is meant for diagnostics and is not persisted in memory; the 'response' section is stored via `context_mgr.update_context`.
    - Tool output is appended to the prompt during tool execution and is incorporated into the final response.
    """
    components = get_components(request_obj.app)
    memory = components.get("memory")
    llm = components.get("llm")
    context_mgr = components.get("context_mgr")
    chunker = components.get("chunker")
    plugin_manager = components.get("plugin_manager")
    mcp_client = components.get("mcp_client")
    tool_parser = components.get("tool_parser")
    tool_validator = components.get("tool_validator")
    audit_logger = components.get("audit_logger")

    if not all([memory, llm, context_mgr, chunker]):
        raise HTTPException(status_code=503, detail="Not initialized")

    try:
        # Mark session as active to let the MemoryWeaver avoid collisions
        try:
            await memory.touch_session(payload.session_id)
        except Exception:
            logger.debug(f"Failed to touch session {payload.session_id} on chat start")
        t0 = time.perf_counter()
        logger.debug(f"Chat request from session {payload.session_id}")
        current_dt = None
        try:
            from datetime import datetime, timezone
            current_dt = datetime.now(timezone.utc)
        except Exception:
            current_dt = None

        try:
            if plugin_manager and getattr(plugin_manager, "enabled", False):
                tools = plugin_manager.list_tools()
            elif mcp_client:
                tools = await mcp_client.get_tools()
            else:
                tools = []
        except Exception as e:
            logger.warning(f"Failed to fetch tools for prompt: {e}")
            tools = []

        system_prompt = payload.system_prompt or build_system_prompt(tools_available=bool(tools), tools_list=tools, current_datetime=current_dt)
        
        # Inject Browser Capabilities into System Prompt
        browser_capabilities = """
## BROWSER CAPABILITIES
You can execute JavaScript in the active tab. Use this to automate tasks, extract specific data, or navigate.
Syntax:
:::action {"type": "script", "code": "document.body.style.background = 'red';"} :::
"""
        system_prompt += "\n" + browser_capabilities

        t_chunk_start = time.perf_counter()
        processed_message = await chunker.process_large_input(payload.message, query_context="User is chatting with their memory-augmented AI")
        t_chunk_ms = (time.perf_counter() - t_chunk_start) * 1000

        t_ctx_start = time.perf_counter()
        full_context = await context_mgr.build_context(payload.session_id, processed_message)
        t_ctx_ms = (time.perf_counter() - t_ctx_start) * 1000

        t_llm_start = time.perf_counter()
        # Pre-flight check: ensure the prompt token count does not exceed the server's micro-batch (ubatch) size
        try:
            prompt_tokens = memory.count_tokens(full_context)
        except Exception:
            prompt_tokens = None
        ubatch = getattr(settings, 'llama_server_ubatch_size', getattr(settings, 'llm_ubatch_size', 2048))
        if prompt_tokens and prompt_tokens > ubatch:
            # Informative 400 rather than allowing the llama.cpp server to assert and return 500
            raise HTTPException(status_code=400, detail=f"Prompt tokens ({prompt_tokens}) exceed the current LLAMA_UBATCH={ubatch}. Reduce context size or increase LLAMA_SERVER_UBATCH_SIZE.")
        # Initial LLM output (thoughts / internal chain-of-thought)
        initial_response = await llm.generate(prompt=full_context, system_prompt=system_prompt, tools=tools)
        response = initial_response
        t_llm_ms = (time.perf_counter() - t_llm_start) * 1000

        parsed_response = tool_parser.parse_response(response) if tool_parser else None
        if parsed_response and parsed_response.has_tool_calls:
            logger.info(f"Detected {len(parsed_response.tool_calls)} tool call(s)")

        t_tools_total_ms = 0.0
        if tool_validator:
            max_tool_iterations = getattr(settings, 'tool_max_iterations', getattr(settings, 'mcp_max_tool_iterations', 3))
            logger.info(f"ToolExecutor will use max_tool_iterations = {max_tool_iterations}")
            tool_executor = ToolExecutor(plugin_manager, mcp_client, tool_parser, tool_validator, llm, audit_logger, max_tool_iterations)
            resp_after_tools, tool_iter, t_tools_total_ms = await tool_executor.execute(parsed_response, full_context, payload, system_prompt, context_mgr)
            if resp_after_tools:
                response = resp_after_tools

        # Compose structured output for client: thinking + response
        final_response = response
        # Ensure we include the initial LLM text as 'thinking' so the client can display it separately
        structured_response = f"thinking: {initial_response}\n\nresponse: {final_response}"
        t_update_start = time.perf_counter()
        await context_mgr.update_context(payload.session_id, payload.message, final_response)
        t_update_ms = (time.perf_counter() - t_update_start) * 1000
        t_total_ms = (time.perf_counter() - t0) * 1000
        logger.info(f"metrics: chat_total_ms={t_total_ms:.2f} chunk_ms={t_chunk_ms:.2f} context_ms={t_ctx_ms:.2f} llm_ms={t_llm_ms:.2f} tools_ms={t_tools_total_ms:.2f} update_ms={t_update_ms:.2f}")

        return ChatResponse(response=structured_response, session_id=payload.session_id, context_tokens=memory.count_tokens(full_context))
    except httpx.HTTPStatusError as http_err:
        # LLM server returned non-2xx; try to surface the server message if present
        try:
            # http_err.response may not be set in all cases
            body = http_err.response.text if http_err.response is not None else str(http_err)
        except Exception:
            body = str(http_err)
        logger.exception("LLM server returned an error for chat: %s", body)
        raise HTTPException(status_code=502, detail=f"LLM server error: {body}")
    except Exception as e:
        logger.exception("Chat error")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/stream")
async def chat_stream(request_obj: Request, payload: ChatRequest):
    # Manually verify API key to debug dependency issues
    # authenticated: bool = Depends(verify_api_key)
    from src.security import verify_api_key
    # await verify_api_key() # Skip for now to debug CORS/403
    
    components = get_components(request_obj.app)
    memory = components.get("memory")
    llm = components.get("llm")
    context_mgr = components.get("context_mgr")
    chunker = components.get("chunker")
    plugin_manager = components.get("plugin_manager")
    mcp_client = components.get("mcp_client")
    tool_parser = components.get("tool_parser")
    tool_validator = components.get("tool_validator")
    audit_logger = components.get("audit_logger")

    if not all([memory, llm, context_mgr, chunker]):
        raise HTTPException(status_code=503, detail="Not initialized")

    async def stream_generator() -> AsyncGenerator[str, None]:
        try:
            # Mark session as active to let the MemoryWeaver avoid collisions
            try:
                await memory.touch_session(payload.session_id)
            except Exception:
                logger.debug(f"Failed to touch session {payload.session_id} on chat stream start")
            t0 = time.perf_counter()
            from datetime import datetime, timezone
            current_dt = datetime.now(timezone.utc)

            # Log start of streaming if audit logger enabled
            if audit_logger:
                try:
                    audit_logger.log("chat_stream_start", {"session_id": payload.session_id})
                except Exception:
                    logger.debug("audit_logger failed to log chat_stream_start")

            try:
                if plugin_manager and getattr(plugin_manager, "enabled", False):
                    tools = plugin_manager.list_tools()
                elif mcp_client:
                    tools = await mcp_client.get_tools()
                else:
                    tools = []
            except Exception as e:
                logger.warning(f"Failed to fetch tools for streaming prompt: {e}")
                tools = []

            system_prompt = payload.system_prompt or build_system_prompt(tools_available=bool(tools), tools_list=tools, current_datetime=current_dt)

            t_chunk_start = time.perf_counter()
            processed_message = await chunker.process_large_input(payload.message, query_context="User is chatting with their memory-augmented AI")
            t_chunk_ms = (time.perf_counter() - t_chunk_start) * 1000
            t_ctx_start = time.perf_counter()
            full_context = await context_mgr.build_context(payload.session_id, processed_message)
            t_ctx_ms = (time.perf_counter() - t_ctx_start) * 1000

            # First, get the initial response (we need the full response to check for tool calls)
            full_response = ""
            tokens = 0
            t_stream_start = time.perf_counter()
            # Pre-flight ubatch/token check for streaming: prevents encoder assert in llama.cpp
            try:
                prompt_tokens = memory.count_tokens(full_context)
            except Exception:
                prompt_tokens = None
            ubatch = getattr(settings, 'llama_server_ubatch_size', getattr(settings, 'llm_ubatch_size', 2048))
            if prompt_tokens and prompt_tokens > ubatch:
                raise HTTPException(status_code=400, detail=f"Prompt tokens ({prompt_tokens}) exceed the current LLAMA_UBATCH={ubatch}. Reduce context size or increase LLAMA_SERVER_UBATCH_SIZE.")
            initial_full_response = ""

            # Indicate the assistant is 'thinking' and stream initial tokens
            # Emit a labeled 'thinking:' tag first
            yield f"data: {json.dumps({'chunk': 'thinking:'})}\n\n"
            # Collect the initial response to check for tool calls
            async for chunk in llm.stream_generate(prompt=full_context, system_prompt=system_prompt, tools=tools):
                initial_full_response += chunk
                tokens += len(chunk)
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                await asyncio.sleep(0.01)

            # Check if the response contains tool calls
            parsed_response = tool_parser.parse_response(initial_full_response) if tool_parser else None
            if parsed_response and parsed_response.has_tool_calls:
                logger.info(f"Detected {len(parsed_response.tool_calls)} tool call(s) in streaming response")

                # Execute tool calls if needed
                if tool_validator:
                    max_tool_iterations = getattr(settings, 'tool_max_iterations', getattr(settings, 'mcp_max_tool_iterations', 3))
                    logger.info(f"ToolExecutor will use max_tool_iterations = {max_tool_iterations} (streaming)")
                    # Safe execution of tools with robust exception handling
                    try:
                        tool_executor = ToolExecutor(plugin_manager, mcp_client, tool_parser, tool_validator, llm, audit_logger, max_tool_iterations)
                        final_response, tool_iter, t_tools_total_ms = await tool_executor.execute(parsed_response, full_context, payload, system_prompt, context_mgr)
                    except Exception as e:
                        logger.exception("ToolExecutor failed during streaming execution")
                        # Generate helpful fallback response acknowledging the tool failure
                        fallback_prompt = full_context + f"\n\n[Tool Execution Error] The tool execution failed with error: {e}. Please continue and try to answer the user's request without tool output."
                        try:
                            final_response = await llm.generate(prompt=fallback_prompt, system_prompt=system_prompt, tools=tools)
                            # Standardize fallback tag for determinism. We prefix the final text.
                            final_response = f"[ToolExecutionFallback] {final_response}"
                        except Exception as e2:
                            logger.exception("LLM generate failed for fallback response")
                            final_response = "[ToolExecutionFallback] [ERROR] Failed to obtain a response after tool execution error."

                    # Stream the final response after tool execution, prefixed by a 'response:' label
                    yield "data: " + json.dumps({'chunk': '\nresponse:'}) + "\n\n"
                    if final_response and final_response != initial_full_response:
                        # Yield a tool processing indicator
                        yield "data: " + json.dumps({'chunk': "\n[Processing tool results...]\n"}) + "\n\n"

                        # Stream the final response after tool processing
                        for i in range(0, len(final_response), 10):  # Stream in chunks of 10 characters
                            chunk = final_response[i:i+10]
                            if chunk:
                                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                                await asyncio.sleep(0.01)
            else:
                # No tool calls, proceed with updating context using initial response
                pass

            # Update context with the final response (either initial or after tool processing)
            final_response_for_context = final_response if 'final_response' in locals() and final_response else initial_full_response
            await context_mgr.update_context(payload.session_id, payload.message, final_response_for_context)
            yield "data: {\"done\": true}\n\n"
        except httpx.HTTPStatusError as http_err:
            try:
                body = http_err.response.text if http_err.response is not None else str(http_err)
            except Exception:
                body = str(http_err)
            logger.exception("Stream error (LLM server error): %s", body)
            yield f"data: {json.dumps({'error': 'LLM server error', 'detail': body})}\n\n"
        except Exception as e:
            logger.exception("Stream error")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # Ensure we log stream end and perform cleanup
            if audit_logger:
                try:
                    audit_logger.log("chat_stream_end", {"session_id": payload.session_id, "duration_ms": int((time.perf_counter() - t0) * 1000)})
                except Exception:
                    logger.debug("audit_logger failed to log chat_stream_end")

    return StreamingResponse(stream_generator(), media_type="text/event-stream")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\recipes\coda_chat.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\llm_response.py (Section: BACKEND_PYTHON) ---

from __future__ import annotations
from pydantic import BaseModel, Field
from typing import List, Optional
from src.tool_call_models import ToolCall


class LLMStructuredResponse(BaseModel):
    """Validated LLM response expected by downstream flows.

    - `answer`: Main assistant text
    - `sources`: Optional list of source IDs or URLs
    - `tool_calls`: Optional list of tool calls that should be executed
    - `confidence`: Optional string describing confidence
    """

    answer: str = Field(..., description="Main text answer from the LLM")
    sources: List[str] = Field(default_factory=list)
    tool_calls: List[ToolCall] = Field(default_factory=list)
    confidence: Optional[str] = Field(None, description="Optional model self-reported confidence")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\llm_response.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\plan_models.py (Section: BACKEND_PYTHON) ---

from __future__ import annotations

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field


class PlanStep(BaseModel):
    tool_name: str = Field(..., description="Name of the tool to invoke, or 'none' for no tool")
    args: Dict[str, Any] = Field(default_factory=dict, description="Arguments for the tool")
    reasoning: Optional[str] = Field(None, description="Optional human-friendly reasoning for step")


class PlanResult(BaseModel):
    goal: str = Field(..., description="High-level goal for the plan")
    steps: List[PlanStep] = Field(default_factory=list, description="Ordered steps to accomplish the goal")


__all__ = ["PlanResult", "PlanStep"]


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\schemas\plan_models.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\config_finder.py (Section: BACKEND_PYTHON) ---

from pathlib import Path


def find_config_path() -> str | None:
    """Find a config.yaml across known locations: repo_root/configs/config.yaml, repo_root/config.yaml, repo_root/ece-core/config.yaml.

    Returns the first path that exists or None.
    """
    repo_root = Path(__file__).resolve().parents[2]
    candidates = [repo_root / "configs" / "config.yaml", repo_root / "config.yaml", repo_root / "ece-core" / "config.yaml"]
    for p in candidates:
        if p.exists():
            return str(p)
    return None


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\config_finder.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\neo4j_embedded.py (Section: BACKEND_PYTHON) ---

"""
Embedded Neo4j server manager for ECE_Core.
Launches Neo4j as subprocess, just like Redis.
"""
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional


class EmbeddedNeo4j:
    """Manages embedded Neo4j server instance."""
    
    def __init__(self):
        self.process: Optional[subprocess.Popen] = None
        
        # Determine paths
        if getattr(sys, 'frozen', False):
            # Running as bundled exe - look for Neo4j relative to exe
            self.app_dir = Path(sys._MEIPASS)
            self.data_dir = Path.cwd()
            # Check for Neo4j in db/ directory relative to exe
            local_neo4j = self.data_dir / "db" / "neo4j-community-2025.10.1"
            if local_neo4j.exists():
                self.neo4j_home = local_neo4j
            else:
                self.neo4j_home = None
        else:
            # Running as script - look for Neo4j in db/ or External-Context-Engine-ECE
            self.app_dir = Path(__file__).parent.parent
            local_neo4j = self.app_dir / "db" / "neo4j-community-2025.10.1"
            external_neo4j = self.app_dir.parent / "External-Context-Engine-ECE" / "db" / "neo4j-community-2025.10.1"
            
            if local_neo4j.exists():
                self.neo4j_home = local_neo4j
            elif external_neo4j.exists():
                self.neo4j_home = external_neo4j
            else:
                self.neo4j_home = None
        
        # Neo4j configuration
        self.bolt_port = 7687
        self.http_port = 7474
        self.username = "neo4j"
        self.password = "password"  # Default, should be configurable
    
    def start(self) -> bool:
        """Start Neo4j server."""
        if not self.neo4j_home:
            print("Neo4j not found - graph features disabled")
            return False
        
        if not self.neo4j_home.exists():
            print(f"Neo4j not found at: {self.neo4j_home}")
            return False
        
        print("Starting embedded Neo4j server...")
        
        # Configure Neo4j for embedded use
        conf_file = self.neo4j_home / "conf" / "neo4j.conf"
        self._configure_neo4j(conf_file)
        
        try:
            # Use neo4j console (foreground mode) so we can control it
            if sys.platform == 'win32':
                neo4j_exe = self.neo4j_home / "bin" / "neo4j.bat"
                cmd = [str(neo4j_exe), "console"]
            else:
                neo4j_exe = self.neo4j_home / "bin" / "neo4j"
                cmd = [str(neo4j_exe), "console"]
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=str(self.neo4j_home),
                creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0,
                env={
                    **subprocess.os.environ,
                    "NEO4J_HOME": str(self.neo4j_home),
                    "NEO4J_CONF": str(self.neo4j_home / "conf")
                }
            )
            
            # Wait for Neo4j to be ready
            if self._wait_for_ready():
                print(f"Neo4j started (bolt://localhost:{self.bolt_port})")
                return True
            else:
                print("Neo4j failed to start within timeout")
                self.stop()
                return False
                
        except Exception as e:
            print(f"Could not start Neo4j: {e}")
            return False
    
    def _configure_neo4j(self, conf_file: Path):
        """Write minimal Neo4j configuration."""
        # Ensure conf directory exists
        conf_file.parent.mkdir(parents=True, exist_ok=True)
        
        config = f"""# ECE_Core Embedded Neo4j Configuration
# Generated automatically

# Server ports
server.bolt.enabled=true
server.bolt.listen_address=127.0.0.1:7687
server.http.enabled=true
server.http.listen_address=127.0.0.1:7474

# CRITICAL: Disable authentication completely for embedded use
dbms.security.auth_enabled=false
server.bolt.tls_level=DISABLED
server.https.enabled=false

# Memory settings (conservative for embedded use)
server.memory.heap.initial_size=256m
server.memory.heap.max_size=512m
server.memory.pagecache.size=256m

# Database location  
server.directories.data=data
server.directories.logs=logs
server.directories.import=import

# Disable anonymous usage reporting
dbms.usage_report.enabled=false

# Performance tweaks for local use
dbms.tx_log.rotation.retention_policy=false
dbms.checkpoint.interval.time=5m
"""
        conf_file.write_text(config)
    
    def _wait_for_ready(self, timeout: int = 30) -> bool:
        """Wait for Neo4j to be ready to accept connections."""
        import socket
        
        start = time.time()
        while time.time() - start < timeout:
            # Check if process died
            if self.process.poll() is not None:
                return False
            
            # Try to connect to bolt port
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex(('127.0.0.1', self.bolt_port))
                sock.close()
                
                if result == 0:
                    time.sleep(2)  # Extra time for full startup
                    return True
            except:
                pass
            
            time.sleep(1)
        
        return False
    
    def stop(self):
        """Stop Neo4j server."""
        if not self.process:
            return
        
        print("  Stopping Neo4j...")
        try:
            self.process.terminate()
            self.process.wait(timeout=10)
        except subprocess.TimeoutExpired:
            print("  Force killing Neo4j...")
            self.process.kill()
            self.process.wait()
        
        self.process = None
    
    def is_running(self) -> bool:
        """Check if Neo4j process is running."""
        return self.process is not None and self.process.poll() is None
    
    def get_bolt_uri(self) -> str:
        """Get Neo4j bolt connection URI."""
        return f"bolt://localhost:{self.bolt_port}"
    
    def get_http_uri(self) -> str:
        """Get Neo4j HTTP URI."""
        return f"http://localhost:{self.http_port}"


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\neo4j_embedded.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\setup_files.py (Section: BACKEND_PYTHON) ---

import os

files = {
    "requirements.txt": """# ECE_Core - Minimal Dependencies
fastapi==0.115.0
uvicorn==0.32.0
redis==5.2.0
httpx==0.28.1
openai==1.54.0
python-dotenv==1.1.1
pydantic==2.10.2
pydantic-settings==2.6.1
tiktoken==0.8.0
""",
    
    ".env.example": """# ECE_Core Configuration
REDIS_URL=redis://localhost:6379
REDIS_TTL=3600
NEO4J_URI=bolt://localhost:7687
NEO4J_HTTP=http://localhost:7474
LLM_API_BASE=http://localhost:8080/v1
LLM_MODEL=your-model-name
LLM_MAX_TOKENS=32000
ECE_HOST=127.0.0.1
ECE_PORT=8000
MAX_REDIS_TOKENS=8000
SUMMARIZE_THRESHOLD=6000
"""
}

for filename, content in files.items():
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"Created: {filename}")

print("\\nDone! Files created.")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\setup_files.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\toon_formatter.py (Section: BACKEND_PYTHON) ---

"""
TOON (Token-Oriented Object Notation) Formatter.
A lightweight, indentation-based format designed to save tokens in LLM prompts.
"""
from typing import Any, Dict, List

def format_as_toon(data: Any, indent: int = 0) -> str:
    """
    Format data as TOON (Token-Oriented Object Notation).
    
    Rules:
    - No quotes around keys
    - No braces or commas
    - Indentation defines hierarchy (2 spaces)
    - Lists are denoted by `-`
    - Strings are unquoted unless they contain special chars (simple heuristic)
    """
    spaces = "  " * indent
    
    if isinstance(data, dict):
        lines = []
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                lines.append(f"{spaces}{key}:")
                lines.append(format_as_toon(value, indent + 1))
            else:
                lines.append(f"{spaces}{key}: {value}")
        return "\n".join(lines)
    
    elif isinstance(data, list):
        lines = []
        for item in data:
            if isinstance(item, (dict, list)):
                # For complex items, start with dash then indent content
                # But TOON usually prefers:
                # - key: value
                #   other: value
                formatted_item = format_as_toon(item, indent + 1)
                # Strip first indent to align with dash
                lines.append(f"{spaces}- {formatted_item.lstrip()}")
            else:
                lines.append(f"{spaces}- {item}")
        return "\n".join(lines)
    
    else:
        return str(data)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\toon_formatter.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_filesystem.py (Section: BACKEND_PYTHON) ---

"""
UTCP Filesystem Tool Service for ECE_Core
Simple file operations accessible via UTCP protocol
"""
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import Optional, List
import os
from pathlib import Path
import json

app = FastAPI(title="UTCP Filesystem Service")

class FileReadRequest(BaseModel):
    path: str
    
class FileWriteRequest(BaseModel):
    path: str
    content: str
    
class DirectoryListRequest(BaseModel):
    path: str
    recursive: bool = False

@app.get("/")
async def root():
    return {"service": "UTCP Filesystem", "status": "running"}

@app.get("/utcp")
async def utcp_manual():
    """UTCP Manual - describes available tools"""
    return {
        "service": "filesystem",
        "version": "1.0.0",
        "tools": [
            {
                "name": "read_file",
                "description": "Read contents of a file",
                "parameters": {
                    "path": {"type": "string", "description": "File path to read"}
                },
                "endpoint": "/read_file"
            },
            {
                "name": "write_file",
                "description": "Write content to a file",
                "parameters": {
                    "path": {"type": "string", "description": "File path to write"},
                    "content": {"type": "string", "description": "Content to write"}
                },
                "endpoint": "/write_file"
            },
            {
                "name": "list_directory",
                "description": "List files in a directory",
                "parameters": {
                    "path": {"type": "string", "description": "Directory path"},
                    "recursive": {"type": "boolean", "description": "List recursively", "default": False}
                },
                "endpoint": "/list_directory"
            },
            {
                "name": "run_command",
                "description": "Execute a whitelisted CLI command (safe-mode) on the server",
                "parameters": {
                    "command": {"type": "string", "description": "Command to run"},
                    "cwd": {"type": "string", "description": "Working directory (optional)"},
                    "timeout": {"type": "integer", "description": "Timeout seconds", "default": 5}
                },
                "endpoint": "/run_command"
            }
        ]
    }

@app.post("/read_file")
@app.get("/read_file")
async def read_file(path: str = None, request: Request = None):
    """Read file contents"""
    try:
        # Accept JSON body or query params
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
        file_path = Path(path).resolve()
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {path}")
        
        if not file_path.is_file():
            raise HTTPException(status_code=400, detail=f"Not a file: {path}")
        
        content = file_path.read_text(encoding='utf-8')
        return {
            "success": True,
            "path": str(file_path),
            "content": content,
            "size": len(content)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/run_command")
async def run_command(request: Request, command: str = None, cwd: str = None, timeout: int = 5):
    """Execute a whitelisted CLI command in a safe manner.
    `command` should be a single command with optional arguments.
    `cwd` if provided must be under `UTCP_FILESYSTEM_ROOT` (if configured).
    Timeout in seconds applies to command execution.
    """
    from shlex import split as shlex_split
    import subprocess
    import platform
    try:
        # Accept both query params and JSON body payloads for compatibility with different clients
        try:
            payload = await request.json()
        except Exception:
            payload = None
        if payload:
            command = payload.get("command", command)
            cwd = payload.get("cwd", cwd)
            timeout = payload.get("timeout", timeout)
        # Security: Only allow simple commands from allowlist
        default_allowed = ["ls", "pwd", "cat", "dir", "echo", "type"]
        env_allow = os.environ.get("UTCP_RUN_COMMAND_ALLOWLIST")
        if env_allow:
            try:
                allowed_cmds = [s.strip() for s in env_allow.split(",") if s.strip()]
            except Exception:
                allowed_cmds = default_allowed
        else:
            allowed_cmds = default_allowed
        parts = shlex_split(command)
        if not parts:
            raise HTTPException(status_code=400, detail="Empty command")
        if parts[0] not in allowed_cmds:
            raise HTTPException(status_code=403, detail=f"Command not allowed: {parts[0]}")

        # Validate cwd under allowed root
        root_env = os.environ.get("UTCP_FILESYSTEM_ROOT")
        if cwd:
            wd = Path(cwd).resolve()
            if root_env:
                root = Path(root_env).resolve()
                try:
                    wd.relative_to(root)
                except Exception:
                    raise HTTPException(status_code=403, detail=f"CWD not allowed: {cwd}")
        else:
            wd = None

        # On Windows, some commands like 'dir', 'type', or 'pwd' are shell builtins.
        # Wrap them using `cmd.exe /c` so they execute correctly without shell=True.
        is_windows = platform.system().lower() == "windows"
        exec_parts = parts
        if is_windows and parts[0] in ("dir", "type", "pwd", "ls"):
            exec_parts = ["cmd", "/c"] + parts

        # Resource limiting: try to enforce CPU and memory limits on POSIX
        preexec_fn = None
        try:
            if os.name != 'nt':
                import resource
                mem_limit_mb = int(os.environ.get("UTCP_RUN_COMMAND_MEM_LIMIT_MB", "256"))
                mem_bytes = mem_limit_mb * 1024 * 1024
                def _preexec():
                    # CPU seconds limit slightly above timeout
                    try:
                        resource.setrlimit(resource.RLIMIT_CPU, (timeout + 1, timeout + 1))
                    except Exception:
                        pass
                    try:
                        resource.setrlimit(resource.RLIMIT_AS, (mem_bytes, mem_bytes))
                    except Exception:
                        pass
                preexec_fn = _preexec
        except Exception:
            preexec_fn = None

        # Run the command with optional preexec limits
        proc = subprocess.run(exec_parts, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=str(wd) if wd else None, timeout=timeout, preexec_fn=preexec_fn)
        # Audit logging: record the run in a log file under logs/utcp_run_command_audit.log
        try:
            # determine repo root by walking up until we find '.git' or 'package.json' or default to parents[3]
            candidate = Path(__file__).resolve().parent
            repo_root = candidate
            while repo_root and not (repo_root / 'package.json').exists():
                if repo_root.parent == repo_root:
                    break
                repo_root = repo_root.parent
            if not repo_root or not (repo_root / 'package.json').exists():
                repo_root = Path(__file__).resolve().parents[3]
            logs_dir = repo_root / 'logs'
            logs_dir.mkdir(parents=True, exist_ok=True)
            audit_file = logs_dir / 'utcp_run_command_audit.log'
            with open(audit_file, 'a', encoding='utf-8') as fh:
                audit_entry = {
                    'timestamp': __import__('datetime').datetime.utcnow().isoformat() + 'Z',
                    'command': command,
                    'cwd': str(wd) if wd else None,
                    'exit_code': proc.returncode,
                    'stdout': proc.stdout.decode('utf-8', errors='ignore')[:5000],
                    'stderr': proc.stderr.decode('utf-8', errors='ignore')[:2000]
                }
                fh.write(json.dumps(audit_entry) + '\n')
        except Exception:
            pass

        return {
            "success": True,
            "command": command,
            "exit_code": proc.returncode,
            "stdout": proc.stdout.decode("utf-8", errors="ignore"),
            "stderr": proc.stderr.decode("utf-8", errors="ignore"),
        }
    except subprocess.TimeoutExpired as e:
        raise HTTPException(status_code=504, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/write_file")
async def write_file(request: FileWriteRequest):
    """Write content to file"""
    try:
        file_path = Path(request.path).resolve()
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_path.write_text(request.content, encoding='utf-8')
        return {
            "success": True,
            "path": str(file_path),
            "size": len(request.content)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/list_directory")
@app.get("/list_directory")
async def list_directory(path: str = None, recursive: bool = False, request: Request = None):
    """List directory contents"""
    try:
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
                recursive = payload.get("recursive", recursive)
        dir_path = Path(path).resolve()
        if not dir_path.exists():
            raise HTTPException(status_code=404, detail=f"Directory not found: {path}")
        
        if not dir_path.is_dir():
            raise HTTPException(status_code=400, detail=f"Not a directory: {path}")
        
        files = []
        if recursive:
            for item in dir_path.rglob("*"):
                files.append({
                    "path": str(item),
                    "name": item.name,
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })
        else:
            for item in dir_path.iterdir():
                files.append({
                    "path": str(item),
                    "name": item.name,
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })
        
        return {
            "success": True,
            "path": str(dir_path),
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search_files")
@app.get("/search_files")
async def search_files(
    path: str,
    pattern: str = "*",
    content_query: str = None,
    max_results: int = 50,
    recursive: bool = True
    , request: Request = None
):
    """Search for files under `path` by name or content.

    - `pattern` is a glob-like filename pattern (default '*').
    - `content_query` if provided, will search inside files and return line-snippets.
    - `max_results` limits the number of files returned.
    - `recursive` toggles recursive search.
    """
    try:
        from os import environ
        root_env = environ.get("UTCP_FILESYSTEM_ROOT")
        # Ensure paths are resolved and not outside root (if configured)
        # Accept JSON body payloads if request is provided by FastAPI.
        # When called directly in unit tests, request will be None and path param will be used.
        if request is not None:
            try:
                payload = await request.json()
            except Exception:
                payload = None
            if payload:
                path = payload.get("path", path)
                pattern = payload.get("pattern", pattern)
                content_query = payload.get("content_query", content_query)
                max_results = payload.get("max_results", max_results)
                recursive = payload.get("recursive", recursive)
        base = Path(path).resolve()
        if root_env:
            root = Path(root_env).resolve()
            try:
                base.relative_to(root)
            except Exception:
                raise HTTPException(status_code=403, detail=f"Path not allowed: {path}")

        if not base.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {path}")
        if not base.is_dir():
            raise HTTPException(status_code=400, detail=f"Not a directory: {path}")

        results = []
        count = 0
        iter_fn = base.rglob if recursive else base.glob
        for p in iter_fn(pattern):
            if count >= max_results:
                break
            if p.is_dir():
                continue
            item = {"path": str(p), "name": p.name}
            matches = []
            if content_query:
                try:
                    with p.open('r', encoding='utf-8', errors='ignore') as fh:
                        for lineno, line in enumerate(fh, start=1):
                            if content_query in line:
                                snippet = line.strip()
                                matches.append({"line": lineno, "snippet": snippet[:300]})
                                if len(matches) >= 10:
                                    break
                except Exception:
                    # Could not read file; skip content check but include file
                    matches = []
            item["matches"] = matches
            # If content_query is present, include only files with matches
            if content_query and not matches:
                continue
            results.append(item)
            count += 1

        return {"success": True, "root": str(base), "count": count, "files": results}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8006)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_filesystem.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_websearch.py (Section: BACKEND_PYTHON) ---

"""
UTCP Web Search Service for ECE_Core
Simple DuckDuckGo search accessible via UTCP protocol
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict
import httpx
from bs4 import BeautifulSoup
import asyncio

app = FastAPI(title="UTCP Web Search Service")

class SearchRequest(BaseModel):
    query: str
    max_results: int = 5

class FetchRequest(BaseModel):
    url: str

@app.get("/")
async def root():
    return {"service": "UTCP Web Search", "status": "running"}

@app.get("/utcp")
async def utcp_manual():
    """UTCP Manual - describes available tools"""
    return {
        "service": "websearch",
        "version": "1.0.0",
        "tools": [
            {
                "name": "search_web",
                "description": "Search the web using DuckDuckGo",
                "parameters": {
                    "query": {"type": "string", "description": "Search query"},
                    "max_results": {"type": "integer", "description": "Max results to return", "default": 5}
                },
                "endpoint": "/search"
            },
            {
                "name": "fetch_url",
                "description": "Fetch and extract text content from a URL",
                "parameters": {
                    "url": {"type": "string", "description": "URL to fetch"}
                },
                "endpoint": "/fetch"
            }
        ]
    }

@app.post("/search")
@app.get("/search")
async def search_web(query: str = None, search_term: str = None, max_results: int = 5):
    """
    Search DuckDuckGo HTML (no API key required).
    Returns list of results with title, snippet, and URL.
    Accepts query or search_term parameter.
    """
    # Accept either 'query' or 'search_term' parameter name
    search_query = query or search_term
    if not search_query:
        raise HTTPException(status_code=400, detail="Missing search query (query or search_term parameter required)")
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            # DuckDuckGo HTML search
            response = await client.get(
                "https://html.duckduckgo.com/html/",
                params={"q": search_query},
                headers={"User-Agent": "Mozilla/5.0"}
            )
            response.raise_for_status()
            
            # Parse results
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            for result in soup.select('.result')[:max_results]:
                title_elem = result.select_one('.result__a')
                snippet_elem = result.select_one('.result__snippet')
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    results.append({
                        "title": title,
                        "url": url,
                        "snippet": snippet
                    })
            
            return {
                "success": True,
                "query": search_query,
                "results": results,
                "count": len(results)
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

@app.post("/fetch")
async def fetch_url(url: str):
    """
    Fetch a URL and extract readable text content.
    Returns cleaned text for LLM consumption.
    """
    try:
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(
                url,
                headers={"User-Agent": "Mozilla/5.0"}
            )
            response.raise_for_status()
            
            # Parse HTML and extract text
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # Get text
            text = soup.get_text(separator='\n', strip=True)
            
            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            # Truncate if too long (max 10k chars for LLM context)
            if len(text) > 10000:
                text = text[:10000] + "\n\n[Content truncated...]"
            
            return {
                "success": True,
                "url": url,
                "content": text,
                "length": len(text)
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Fetch failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8007)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\utils\utcp_websearch.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\fake_vector_adapter.py (Section: BACKEND_PYTHON) ---

"""A minimal fake vector adapter for unit tests.

This adapter stores embeddings in a local dictionary and performs cosine
similarity-based queries in Python. It's deterministic and suitable for
unit testing of vector-related behavior without requiring Redis or a
vector DB.
"""
from __future__ import annotations
from typing import List, Dict, Any, Optional
import math
import logging

logger = logging.getLogger(__name__)


def _cosine_similarity(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    mag_a = math.sqrt(sum(x * x for x in a))
    mag_b = math.sqrt(sum(x * x for x in b))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


class FakeVectorAdapter:
    def __init__(self):
        self._index: Dict[str, Dict[str, Any]] = {}

    async def initialize(self):
        # No-op for fake
        return

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        self._index[embedding_id] = {
            "embedding": embedding,
            "node_id": node_id,
            "chunk_index": chunk_index,
            "metadata": metadata or {}
        }

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        candidates = []
        for eid, data in self._index.items():
            score = _cosine_similarity(embedding, data["embedding"]) if data.get("embedding") else 0.0
            candidates.append({
                "score": float(score),
                "embedding_id": eid,
                "node_id": data["node_id"],
                "chunk_index": data["chunk_index"],
                "metadata": data.get("metadata", {})
            })
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates[:top_k]

    async def delete(self, embedding_id: str) -> None:
        self._index.pop(embedding_id, None)

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        return self._index.get(embedding_id)

    async def health(self) -> bool:
        return True


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\fake_vector_adapter.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\redis_vector_adapter.py (Section: BACKEND_PYTHON) ---

"""Redis VectorAdapter implementation with an in-memory fallback.

This adapter stores embeddings in Redis as simple JSON-serialized fields
and performs similarity queries in-process when the Redis server doesn't
have RediSearch vector index capabilities. This keeps tests deterministic
and avoids requiring Redis modules in CI.
"""
from __future__ import annotations
from typing import List, Dict, Any, Optional
import math
import json
import logging
import redis.asyncio as redis

logger = logging.getLogger(__name__)


def _cosine_similarity(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    mag_a = math.sqrt(sum(x * x for x in a))
    mag_b = math.sqrt(sum(x * x for x in b))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


class RedisVectorAdapter:
    """A lightweight Redis vector adapter.

    This adapter stores each vector entry under `vec:{embedding_id}`
    as a Redis hash with fields: embedding (JSON), node_id, chunk_index, metadata.
    During queries, it will enumerate the stored ids from a Redis set 'vec:index'
    and compute cosine similarity in Python if the server does not provide
    a vector-search capability.
    """

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.client: Optional[redis.Redis] = None
        # Local in-memory fallback index when Redis is not available or for fast tests
        self._in_memory: Dict[str, Dict[str, Any]] = {}
        # RediSearch availability
        self._redis_search_available: bool = False
        self._index_created: bool = False
        self._vector_dim: Optional[int] = None

    async def initialize(self):
        try:
            self.client = await redis.from_url(self.redis_url, decode_responses=True)
            await self.client.ping()
            logger.info("RedisVectorAdapter: connected to Redis")
            # Try to detect RediSearch FT API (client.ft exists) when using redis-py
            try:
                if hasattr(self.client, "ft"):
                    # Try to run a minimal info command for 'vec_index' to detect if there's a vector index
                    try:
                        await self.client.ft("vec_index").info()
                        self._redis_search_available = True
                        self._index_created = True
                    except Exception:
                        # Index does not exist; still mark search available because Redis supports FT API
                        self._redis_search_available = True
                        self._index_created = False
            except Exception:
                self._redis_search_available = False
        except Exception as e:
            logger.warning(f"RedisVectorAdapter: unable to connect redis: {e}. Using in-memory fallback.")
            self.client = None

    async def index_chunk(self, embedding_id: str, node_id: str, chunk_index: int, embedding: List[float], metadata: Optional[Dict[str, Any]] = None) -> None:
        entry = {
            "embedding": embedding,
            "node_id": node_id,
            "chunk_index": chunk_index,
            "metadata": metadata or {}
        }
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                # Store embedding as JSON by default
                await self.client.hset(key, mapping={
                    "embedding": json.dumps(embedding),
                    "node_id": node_id,
                    "chunk_index": chunk_index,
                    "metadata": json.dumps(metadata or {})
                })
                await self.client.sadd("vec:index", embedding_id)
                # Create RediSearch vector index if available and not yet created
                if self._redis_search_available and not self._index_created:
                    try:
                        # Determine vector dimension from embedding
                        dim = len(embedding)
                        self._vector_dim = dim
                        # Try to create an index with a simple HNSW vector field. Use dialect 2 compatibility.
                        # Try both mechanisms: explicit execute_command or ft().create depending on redis client
                        try:
                            await self.client.execute_command(
                                "FT.CREATE",
                                "vec_index",
                                "ON",
                                "HASH",
                                "PREFIX",
                                "1",
                                "vec:",
                                "SCHEMA",
                                "embedding",
                                "VECTOR",
                                "HNSW",
                                "6",
                                "TYPE",
                                "FLOAT32",
                                "DIM",
                                str(dim),
                                "DISTANCE_METRIC",
                                "COSINE",
                            )
                        except Exception:
                            # Try the high-level API if available
                            try:
                                if hasattr(self.client, "ft") and hasattr(self.client.ft("vec_index"), "create"):
                                    if hasattr(self.client.ft("vec_index"), "create"):
                                        # Some redis clients have 'create' on ft object; attempt to call
                                        await self.client.ft("vec_index").create(
                                            [
                                                ("embedding", {"TYPE": "VECTOR", "ALGORITHM": "HNSW", "TYPE_PARAMS": {"TYPE": "FLOAT32", "DIM": dim, "DISTANCE_METRIC": "COSINE"}})
                                            ]
                                        )
                            except Exception as e2:
                                logger.info(f"RedisVectorAdapter: failed to create RediSearch index via execute or high-level API: {e2}")
                        self._index_created = True
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: failed to create RediSearch index: {e}")
                return
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis write failed: {e}")

        # Fallback to in-memory storage
        self._in_memory[embedding_id] = entry

    async def query_vector(self, embedding: List[float], top_k: int = 10) -> List[Dict[str, Any]]:
        candidates = []
        # If redis is present, enumerate ids in the set and HGET each one
        if self.client:
            try:
                # If RediSearch is available and index is created, use FT.SEARCH
                if self._redis_search_available and self._index_created and hasattr(self.client, "ft"):
                    try:
                        # Use redispy's FT API if available; pack float32 bytes as $vec_param
                        import struct

                        if isinstance(embedding, list):
                            vec_bytes = struct.pack(f"{len(embedding)}f", *embedding)
                        else:
                            vec_bytes = embedding

                        # Compose a KNN vector search query
                        query = f"*=>[KNN {top_k} @embedding $vec_param AS score]"

                        # Try high-level ft().search first
                        if hasattr(self.client, "ft") and hasattr(self.client.ft("vec_index"), "search"):
                            res = await self.client.ft("vec_index").search(query, query_params={"vec_param": vec_bytes})
                            for doc in getattr(res, "docs", []):
                                fields = getattr(doc, "__dict__", {})
                                score = float(getattr(doc, "score", 0.0))
                                candidates.append({
                                    "score": float(score),
                                    "embedding_id": str(doc.id).replace("vec:", ""),
                                    "node_id": getattr(doc, "node_id", None) or fields.get("node_id"),
                                    "chunk_index": int(getattr(doc, "chunk_index", 0) or fields.get("chunk_index", 0)),
                                    "metadata": json.loads(getattr(doc, "metadata", "{}") or fields.get("metadata", "{}")),
                                })
                            candidates.sort(key=lambda x: x["score"], reverse=True)
                            return candidates[:top_k]
                        else:
                            # Fallback to execute_command FT.SEARCH with PARAMS
                            try:
                                # FT.SEARCH vec_index query PARAMS 2 vec_param <bytes> DIALECT 2
                                res = await self.client.execute_command("FT.SEARCH", "vec_index", query, "PARAMS", 2, "vec_param", vec_bytes, "DIALECT", 2)
                                # res is a list; parse accordingly: [total, docId1, {fields}, docId2,...]
                                if isinstance(res, list) and len(res) >= 1:
                                    it = iter(res[1:])
                                    while True:
                                        try:
                                            docId = next(it)
                                        except StopIteration:
                                            break
                                        fields = next(it)
                                        score = 1.0
                                        # fields is a dict mapping field to value
                                        docid_str = str(docId)
                                        candidates.append({
                                            "score": float(score),
                                            "embedding_id": docid_str.replace("vec:", ""),
                                            "node_id": fields.get(b"node_id" if isinstance(fields, dict) else "node_id"),
                                            "chunk_index": int(fields.get(b"chunk_index", 0) if isinstance(fields, dict) else fields.get("chunk_index", 0)),
                                            "metadata": json.loads(fields.get(b"metadata", b"{}").decode() if isinstance(fields, dict) and isinstance(fields.get(b"metadata"), bytes) else (fields.get("metadata") or "{}")),
                                        })
                                    candidates.sort(key=lambda x: x["score"], reverse=True)
                                    return candidates[:top_k]
                            except Exception as e:
                                logger.info(f"RedisVectorAdapter: execute FT.SEARCH failed: {e}")
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: RediSearch query failed, fallback: {e}")
                        # If we got results, return top_k
                        candidates.sort(key=lambda x: x["score"], reverse=True)
                        return candidates[:top_k]
                    except Exception as e:
                        logger.info(f"RedisVectorAdapter: RediSearch query failed, fallback: {e}")
                        # Fall back to scanning members
                ids = await self.client.smembers("vec:index")
                for eid in ids:
                    key = f"vec:{eid}"
                    data = await self.client.hgetall(key)
                    if not data:
                        continue
                    try:
                        emb = json.loads(data.get("embedding"))
                        node_id = data.get("node_id")
                        chunk_index = int(data.get("chunk_index", 0))
                        metadata = json.loads(data.get("metadata") or "{}")
                    except Exception:
                        continue
                    score = _cosine_similarity(embedding, emb)
                    candidates.append({
                        "score": float(score),
                        "embedding_id": eid,
                        "node_id": node_id,
                        "chunk_index": chunk_index,
                        "metadata": metadata,
                    })
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis read failed during query: {e}")
                # Fall through to in-memory fallback

        # In-memory candidate enumeration
        for eid, data in self._in_memory.items():
            try:
                score = _cosine_similarity(embedding, data["embedding"])
                candidates.append({
                    "score": float(score),
                    "embedding_id": eid,
                    "node_id": data["node_id"],
                    "chunk_index": int(data["chunk_index"]),
                    "metadata": data["metadata"],
                })
            except Exception:
                continue

        # Sort by score (descending) and return top_k
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates[:top_k]

    async def delete(self, embedding_id: str) -> None:
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                await self.client.delete(key)
                await self.client.srem("vec:index", embedding_id)
                return
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis delete failed: {e}")

        self._in_memory.pop(embedding_id, None)

    async def get(self, embedding_id: str) -> Optional[Dict[str, Any]]:
        if self.client:
            try:
                key = f"vec:{embedding_id}"
                data = await self.client.hgetall(key)
                if not data:
                    return None
                emb = json.loads(data.get("embedding"))
                return {
                    "embedding_id": embedding_id,
                    "embedding": emb,
                    "node_id": data.get("node_id"),
                    "chunk_index": int(data.get("chunk_index", 0)),
                    "metadata": json.loads(data.get("metadata") or "{}")
                }
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: Redis get failed: {e}")
                return None

        entry = self._in_memory.get(embedding_id)
        if not entry:
            return None
        return {
            "embedding_id": embedding_id,
            "embedding": entry["embedding"],
            "node_id": entry["node_id"],
            "chunk_index": int(entry["chunk_index"]),
            "metadata": entry["metadata"],
        }

    async def health(self) -> bool:
        if self.client:
            try:
                await self.client.ping()
                return True
            except Exception as e:
                logger.warning(f"RedisVectorAdapter: health ping failed: {e}")
                return False
        # In-memory fallback always healthy
        return True


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\src\vector_adapters\redis_vector_adapter.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\spec.md (Section: BACKEND_SPECS) ---

# ECE_Core - Technical Specification

## Mission

Build a **personal external memory system** as an assistive cognitive tool using:
- Redis + Neo4j tiered memory (pure graph architecture)
- Markovian reasoning (chunked thinking)
- Graph-R1 reasoning (iterative retrieval)
- Local-first LLM integration (llama.cpp)
- Plugin-based tool system (UTCP - Simple Tool Mode)

**Current**: Neo4j + Redis architecture (SQLite removed)
**Protocol**: Plugin System (migrated from MCP 2025-11-13)
**Tools**: Tools loaded via `PluginManager` from `plugins/` directory:
  - `web_search` - DuckDuckGo search with results
  - `filesystem_read` - File and directory operations
  - `shell_execute` - Shell command execution (with safety checks)
  - `mgrep` - Semantic code & natural language file search (semantic `grep`) - Implemented as a standalone plugin in `plugins/mgrep/`

## Architecture Overview

### Memory Architecture: Neo4j + Redis Tiered System

**Neo4j (port 7687)** - PRIMARY STORAGE
- All memories, summaries, relationships in graph format
- Node types: `(:Memory)`, `(:Event)`, `(:Person)`, `(:Idea)`, `(:Code)`, `(:ContextGist)`
- Relationship types: `[:RELATED_TO]`, `[:MENTIONS]`, `[:CAUSED_BY]`, `[:NEXT_GIST]`
- Full-text search capabilities with Cypher
- Temporal reasoning with creation timestamps

**Redis (port 6379)** - ACTIVE SESSION CACHE
- Hot cache for active conversations (24h TTL)
- Session state management
- Temporary context assembly
- Graceful fallback to Neo4j if unavailable

### Cognitive Architecture: Agent-Based System

**Verifier Agent** - Truth Verification
- **Role**: Fact-checking via Empirical Distrust
- **Method**: Provenance-aware scoring (primary sources > summaries)
- **Goal**: Reduce hallucinations, increase factual accuracy

**Distiller Agent** - Memory Compression & Context Rotation
- **Role**: Memory summarization and compression + Context Rotation Protocol
- **Method**: LLM-assisted distillation with salience scoring + context gist creation
- **Goal**: Maintain high-value context, enable infinite context, prune noise

**Archivist Agent** - Memory Maintenance & Context Management
- **Role**: Knowledge base maintenance, freshness checks + Context Coordination
- **Method**: Scheduled verification, stale node detection, context rotation oversight
- **Goal**: Keep memory graph current and trustworthy, manage context windows

**Memory Weaver** - Automated Relationship Repair
- **Role**: Automated graph relationship repair and optimization
- **Method**: Embedding-based similarity with audit trail (`auto_commit_run_id`)
- **Goal**: Maintain graph integrity with full traceability

### Reasoning Architecture: Graph-R1 + Markovian Reasoning

**Graph-R1 Reasoning Pattern**:
1. **Think** - High-level planning based on question
2. **Generate Query** - Create Cypher query for Neo4j
3. **Retrieve Subgraph** - Fetch relevant memories and relationships  
4. **Rethink** - Plan next iteration based on retrieved context
5. **Repeat** - Iterate until confident or max iterations reached

**Markovian Memory**: Chunked context management for infinite windows
- **Active Context**: Current working memory (in Redis)
- **Gist Memory**: Compressed historical context (in Neo4j as `:ContextGist`)
- **Rotation Protocol**: When active context approaches 55k tokens, compress oldest segments to gists

### Tool Architecture: UTCP Plugin System

**Current Implementation**: Plugin-based UTCP (Simple Tool Mode)
- Discovery via `plugins/` directory
- Safety layers with whitelist/blacklist
- Human confirmation flows for dangerous operations

**Available Tools**:
- `web_search` - DuckDuckGo with result limits
- `filesystem_read` - File operations with path restrictions
- `shell_execute` - Command execution with safety checks
- `mgrep` - Semantic code search with context

## Infinite Context Pipeline

### Phase 1: Hardware Foundation
- **64k Context Windows**: All LLM servers boot with 65,536 token capacity
- **GPU Optimization**: Full layer offload with Q8 quantized KV cache
- **Flash Attention**: Enabled when available for optimal long-context performance

### Phase 2: Context Rotation Protocol
- **Monitoring**: ContextManager monitors total context length
- **Trigger**: When context approaches 55k tokens (safety buffer for 64k window)
- **Compression**: Distiller compresses old segments into "Narrative Gists"
- **Storage**: Gists stored in Neo4j as `(:ContextGist)` nodes with `[:NEXT_GIST]` relationships
- **Rewriting**: New context = `[System Prompt] + [Historical Gists Summary] + [Recent Context] + [New Input]`

### Phase 3: Graph-R1 Enhancement
- **Historical Retrieval**: GraphReasoner includes `:ContextGist` nodes in retrieval
- **Continuity Maintenance**: Reasoning flow maintained across context rotations
- **Temporal Awareness**: Reasoning considers chronological relationships in gists

## API Specification

### Core Endpoints (Port 8000)

**Chat Interface**:
- `POST /chat/stream` - Streaming conversation with full memory context
- Request: `{"session_id": str, "message": str, "stream": bool}`
- Response: Streaming SSE with full context injection

**Memory Operations**:
- `POST /memory/add` - Add memory to Neo4j graph
- `POST /memory/search` - Semantic search with relationships  
- `GET /memory/summaries` - Session summary retrieval
- `POST /archivist/ingest` - Ingest content with distillation

**Health & Info**:
- `GET /health` - Server health check
- `GET /v1/models` - Available models
- `GET /health/memory` - Memory system status

**MCP Integration** (when enabled):
- `GET /mcp/tools` - Available memory tools
- `POST /mcp/call` - Execute memory tools

## Configuration

### Required Parameters (in `.env` or config.yaml)
- `NEO4J_URI` - Neo4j connection URI (default: bolt://localhost:7687)
- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)
- `LLM_MODEL_PATH` - Path to GGUF model file
- `ECE_HOST` - Host for ECE server (default: 127.0.0.1)
- `ECE_PORT` - Port for ECE server (default: 8000)

### Optional Parameters
- `ECE_REQUIRE_AUTH` - Enable API token authentication (default: false)
- `ECE_API_KEY` - Static API key when auth enabled
- `MCP_ENABLED` - Enable Model Context Protocol integration (default: true)
- `VERIFIER_AGENT_ENABLED` - Enable truth-checking agent (default: true)
- `ARCHIVIST_AGENT_ENABLED` - Enable memory maintenance agent (default: true)
- `DISTILLER_AGENT_ENABLED` - Enable summarization agent (default: true)

## Security

### Authentication
- Optional API token authentication (controlled by `ECE_REQUIRE_AUTH`)
- Session isolation with UUID-based session IDs
- Memory access limited to owner's session

### Authorization
- Path restrictions on filesystem operations
- Command whitelisting for shell execution
- Rate limiting on all endpoints
- Input validation on all parameters

### Data Protection
- All data stored locally by default
- End-to-end encryption for sensitive memories (optional)
- Audit logging for all memory operations
- Traceability for automated repairs and context rotations

## Performance Optimization

### Hardware Recommendations
- **Minimum**: 16GB RAM, CUDA-capable GPU (RTX series)
- **Recommended**: 32GB+ RAM, RTX 4090 or similar
- **Context Windows**: 64k requires ~8GB VRAM for KV cache with 7B-14B models

### Memory Management
- **Hot Cache**: Redis for active session context (24h TTL)
- **Cold Storage**: Neo4j for persistent memories with relationships
- **Context Rotation**: Automatic compression of old context when approaching limits
- **Caching Strategy**: L1 (Redis) for active context, L2 (Neo4j) for historical context

## Integration Points

### With Anchor CLI
- HTTP API communication on configured port (default: 8000)
- Streaming responses via Server-Sent Events
- Memory operations through dedicated endpoints

### With Browser Extension
- HTTP API communication for context injection and memory saving
- Streaming chat interface via Side Panel
- Page content reading and memory ingestion

### With LLM Servers
- OpenAI-compatible API for LLM communication
- Streaming response handling via SSE
- Context window management with rotation protocol

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\spec.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\plan.md (Section: BACKEND_SPECS) ---

# ECE_Core - Implementation Plan

## Vision & Philosophy

**Mission**: Build a personal external memory system that extends human cognitive capabilities while preserving sovereignty and privacy.

**Core Values**:
1. **Local-First**: Your data, your hardware, your control
2. **Cognitive Enhancement**: Augment human reasoning, don't replace human judgment
3. **Truth-Seeking**: Empirical verification over confident hallucination
4. **Sustainable Growth**: Infinite context without infinite complexity

## Strategic Objectives

### Primary Goal: Infinite Context Pipeline
Achieve truly **infinite context capability** through the harmonious integration of:
1. **Hardware Foundation**: 64k+ context windows on local hardware
2. **Smart Rotation Protocol**: Automatic context compression and archival
3. **Continuous Reasoning**: Seamless reasoning flow across context boundaries

### Secondary Goals:
- **Memory Sovereignty**: Complete data ownership and processing on user's hardware
- **Cognitive Enhancement**: Assist with executive function and memory management
- **Tool Integration**: Reliable, safe integration with local system tools
- **Privacy Preservation**: Zero telemetry, zero cloud dependency, 100% local

## Implementation Phases

### Phase 1-4: Foundation (COMPLETED)
- [x] Neo4j + Redis tiered memory architecture (SQLite completely removed)
- [x] Plugin-based tool system (UTCP integration, MCP archived)  
- [x] Cognitive agents (Verifier, Archivist, Distiller)
- [x] Traceability & rollback for automated repairs
- [x] Security hardening (API auth, audit logs)
- [x] MCP integration into main ECE server
- [x] PyInstaller packaging

### Phase 5: Infinite Context Pipeline (IN PROGRESS)
- [x] Phase 1: Hardware Foundation - 64k context windows (COMPLETED Dec 2025)
- [x] Phase 2: Context Rotation Protocol - Automatic context compression (COMPLETED Dec 2025) 
- [x] Phase 3: Graph-R1 Enhancement - Historical context retrieval (COMPLETED Dec 2025)
- [ ] Phase 4: Performance Optimization - Vector adapters + hot replicas (IN PROGRESS)

### Phase 6: Consolidation & Hardening (PLANNED)
- [ ] Complete documentation reset to `specs/` policy
- [ ] Comprehensive security audit
- [ ] Performance benchmarking and optimization
- [ ] User experience refinements

### Phase 7: Expansion (FUTURE)
- [ ] Vector adapter + C2C hot-replica for semantic retrieval
- [ ] Compressed summaries + passage recall (EC-T-133)
- [ ] SLM benchmarking and ALScore measurements
- [ ] Mobile and cross-platform deployment

## Technical Implementation Priorities

### Current Focus (Phase 5-6): Infinite Context & Optimization
1. **Context Management**:
   - Optimize context rotation algorithms for better compression fidelity
   - Enhance ContextGist creation with richer metadata
   - Improve historical context retrieval performance

2. **Memory Architecture**:
   - Vector adapter integration for hybrid search (graph + vector)
   - C2C (Context-to-Context) hot replica system for instant availability
   - Memory weaving optimization for relationship maintenance

3. **Performance**:
   - Implement ALScore for algorithmic latency measurement
   - Optimize graph queries for large-scale deployments
   - Profile and optimize memory usage patterns

### Future Priorities (Phase 7+)  
- **Scalability**: Horizontal partitioning for multi-user deployments
- **Multimodal**: Image and audio input capabilities
- **Federation**: Secure sharing across multiple Context-Engine instances
- **Edge Deployment**: Optimized for embedded and mobile devices

## Research Foundation

### Validated Approaches
- **Graph-R1 Reasoning**: Iterative graph traversal significantly improves memory retrieval accuracy
- **Empirical Distrust**: Provenance-aware verification reduces hallucinations by 60%+
- **Markovian Memory**: Chunked thinking with state preservation enables infinite context
- **Empirical Validation**: Real-world testing confirms cognitive enhancement benefits

### Emerging Research Areas
- **Quantum-Inspired Retrieval**: Exploring quantum-like superposition in memory search
- **Continuous Learning**: Methods to update knowledge graph while system operates
- **Distributed Consciousness**: Multi-node cognitive architecture patterns
- **Cognitive Load Measurement**: Quantifying productivity impact of memory augmentation

## Competitive Advantages

### vs Cloud AI Systems
- **Local Processing**: 100% data sovereignty, no privacy concerns
- **Infinite Context**: No artificial limits on conversation length or document processing
- **Personal Memory**: Long-term relationship with user's evolving knowledge graph
- **Tool Integration**: Native access to local files, systems, and applications

### vs Other Local Solutions  
- **Graph Architecture**: Superior relationship tracking and context retrieval vs simple vector stores
- **Cognitive Agents**: Automated maintenance vs manual memory management
- **Infinite Context**: Unique hardware+logic context rotation pipeline
- **Modular Design**: Easy extensibility vs monolithic architecture

## Success Metrics

### Technical Metrics
- **Context Window**: Achieved 64k effective capacity with infinite rotation capability
- **Memory Accuracy**: >95% retrieval accuracy for stored information
- **Response Latency**: <2s for context-rich queries
- **System Uptime**: >99% availability for local deployment

### User Experience Metrics
- **Session Length**: Users engaging in conversations >1 hour continuously
- **Memory Retention**: Users successfully retrieving information from weeks/months ago
- **Productivity Impact**: Measurable improvement in task completion and context management
- **Privacy Satisfaction**: 100% of data remaining local to user's device

## Risk Assessment

### Technical Risks
- **Memory Scalability**: Mitigated by context rotation and compression algorithms
- **Performance Degradation**: Managed through active maintenance and pruning
- **Hardware Requirements**: Addressed through optimization and varied deployment options

### Adoption Risks
- **Complexity**: Mitigated through excellent documentation and CLI automation
- **Privacy Concerns**: Addressed by 100% local processing default
- **Tool Reliability**: Managed through safety layers and human confirmation

### Competitive Risks
- **Cloud AI Services**: Differentiated through sovereignty and infinite context
- **Other Local Solutions**: Ahead with Graph-R1 and infinite context capabilities

## Timeline & Milestones

### Phase 5 Milestones (Infinite Context Pipeline) - COMPLETED
- [x] Hardware Foundation (64k windows) - Dec 2025
- [x] Context Rotation Protocol - Dec 2025
- [x] Graph-R1 Historical Context - Dec 2025
- [x] Continuity Maintenance - Dec 2025

### Phase 6 Milestones (Consolidation) - IN PROGRESS
- [ ] Documentation Reset - Dec 2025
- [ ] Security Audit - Jan 2026
- [ ] Performance Benchmarking - Jan 2026

### Phase 7 Milestones (Expansion) - PLANNED
- [ ] Vector Adapter Integration - Feb 2026
- [ ] Compressed Summary Architecture - Mar 2026
- [ ] SLM Benchmarking Framework - Apr 2026

## Ethical Framework

### Core Principles
1. **User Sovereignty**: All data belongs to and remains with the user
2. **Cognitive Liberty**: Enhancement without control or manipulation  
3. **Transparency**: Clear visibility into how the system processes information
4. **Autonomy**: Tools that enhance human decision-making, not replace it

### Implementation Guidelines
- Open source codebase with MIT license
- Local processing by default, no telemetry
- Clear audit trail for all automated operations
- Human confirmation for all autonomous changes

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\plan.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\tasks.md (Section: BACKEND_SPECS) ---

# ECE_Core - Implementation Tasks

## Active Work Queue (Phase 5-6: Infinite Context & Consolidation)

### [IN PROGRESS] Vector Adapter & C2C Implementation (EC-V-201)
- [ ] Implement VectorAdapter abstract interface (EC-V-201.01)
  - [ ] Define base class with required methods: `query_vector`, `insert_vector`, `delete_vector`
  - [ ] Implement Redis-based VectorAdapter with HNSW indexing
  - [ ] Implement FAISS VectorAdapter for local deployments
  - [ ] Performance benchmarking against graph-only retrieval
  - [ ] Integration testing with existing ContextManager

- [ ] C2C (Context-to-Context) Hot-Replica System (EC-V-201.02)
  - [ ] Define hot-replica synchronization protocol for vector indices
  - [ ] Implement real-time vector index updates during memory ingestion
  - [ ] Cross-validation between graph and vector retrieval methods  
  - [ ] Automatic failover from vector to graph when needed for reliability

### [IN PROGRESS] Compressed Summary Architecture (EC-CS-133)
- [ ] Implement compressed summary generation pipeline (EC-CS-133.01)
  - [ ] Design salience scoring algorithm for context gist compression
  - [ ] Implement passage recall mechanism from compressed representations  
  - [ ] Optimize compression ratios vs. information retention balance
  - [ ] Integration with ContextGist rotation system in Neo4j
  - [ ] Performance testing with 100k+ token context windows

### [PLANNED] Performance Optimization (EC-PERF-202)
- [ ] Implement ALScore (Augmentation Latency Score) measurement framework (EC-PERF-202.01)
  - [ ] Define latency benchmarks for different context window sizes
  - [ ] Create tool execution time measurements and analysis
  - [ ] Implement memory retrieval accuracy and completeness metrics
  - [ ] Establish optimization recommendations based on ALScore results

## Current Development (Phase 6: Consolidation)

### [IN PROGRESS] Security Audit & Hardening (EC-SEC-301)
- [ ] Complete comprehensive security audit of all HTTP endpoints
- [ ] Implement additional input validation and sanitization layers
- [ ] Perform penetration testing on authenticated endpoints
- [ ] Verify proper isolation between user sessions in multi-user scenarios

### [IN PROGRESS] Documentation Reset (EC-DOC-302)  
- [ ] Migrate all documentation to `specs/` policy structure
- [ ] Update README.md and CHANGELOG.md to match new architecture
- [ ] Consolidate all wiki-style docs into spec.md, plan.md, tasks.md format
- [ ] Archive obsolete documentation files appropriately

### [IN PROGRESS] Performance Benchmarking (EC-PM-303)
- [ ] Create standardized benchmark suite for infinite context operations
- [ ] Measure context rotation performance with 30k+ token inputs
- [ ] Compare memory retrieval speeds with and without historical gists
- [ ] Profile memory usage patterns during long-running sessions

## Upcoming Priorities (Phase 7: Expansion)

### [PLANNED] Small Model Benchmarking (EC-SLM-401)
- [ ] Establish SLM (Small Language Model) benchmarking framework
- [ ] Implement ALScore measurements for different model architectures
- [ ] Create optimization recommendations for various hardware configurations
- [ ] Evaluate gemma-2, phi-3, and mistral-nemo performance in infinite context

### [PLANNED] Memory Weaver Enhancements (EC-MW-402)  
- [ ] Automated relationship repair with improved similarity algorithms
- [ ] Enhanced audit trail with comprehensive rollback capabilities
- [ ] Performance optimization for large-scale graph repairs
- [ ] Integration with new vector adapter for hybrid repairs

### [PLANNED] Mobile Deployment Preparation (EC-MOB-403)
- [ ] Create ARM64 build pipeline for Raspberry Pi and mobile devices
- [ ] Optimize memory architecture for constrained resource environments
- [ ] Implement offline-only mode for air-gapped deployments
- [ ] Create native mobile applications for iOS and Android

## Tool Integration Tasks

### [IN PROGRESS] UTCP Plugin System Enhancement (EC-UTCP-250)
- [ ] Implement dynamic plugin loading and unloading
- [ ] Add safety sandboxing for external plugins
- [ ] Create plugin marketplace integration
- [ ] Implement plugin-specific rate limiting and resource quotas

### [PLANNED] Advanced Tool Integration (EC-ADV-251)
- [ ] OS-level tool integration (filesystem, clipboard, window management)
- [ ] IDE integration (VS Code, Vim, Emacs) for context injection
- [ ] Email client integration for inbox management
- [ ] Calendar integration for scheduling and time management

## Maintenance Tasks

### [ONGOING] System Health (EC-OPS-001)
- [ ] Monitor Neo4j performance under large graph conditions
- [ ] Track Redis memory usage and implement eviction policies
- [ ] Profile context rotation performance with historical data
- [ ] Maintain backup and recovery procedures for Neo4j/Redis

### [MONTHLY] Dependency Updates (EC-OPS-002)
- [ ] Update Python dependencies with security scanning
- [ ] Verify compatibility with latest llama.cpp releases
- [ ] Test with new GGUF model formats and quantizations
- [ ] Update HuggingFace model references and fallback URLs

### [QUARTERLY] Architecture Review (EC-ARCH-003)
- [ ] Performance analysis of graph vs vector retrieval
- [ ] Memory utilization optimization for long-running instances
- [ ] User experience improvements based on feedback
- [ ] Scalability assessment for enterprise deployment

## Completed Recently (Phase 5: Infinite Context Pipeline)

### [COMPLETED] Phase 1: Hardware Foundation (EC-HW-101)
- [x] Upgrade all LLM servers to 64k context windows (Dec 2025)
- [x] Implement Flash Attention support and optimization (Dec 2025)
- [x] Configure KV cache with Q8 quantization for memory efficiency (Dec 2025)

### [COMPLETED] Phase 2: Context Rotation Protocol (EC-CRP-102)
- [x] Implement ContextManager monitoring for 55k token threshold (Dec 2025)
- [x] Integrate Distiller for intelligent content compression (Dec 2025)
- [x] Create Neo4j storage for ContextGist nodes with chronological links (Dec 2025)
- [x] Implement context reconstruction with [System] + [Gists] + [Recent] + [New] (Dec 2025)

### [COMPLETED] Phase 3: Graph-R1 Enhancement (EC-GR1-103)  
- [x] Update GraphReasoner to include ContextGist node retrieval (Dec 2025)
- [x] Implement historical context integration in reasoning loop (Dec 2025)
- [x] Maintain reasoning continuity across context rotation boundaries (Dec 2025)

### [COMPLETED] System Integration & Testing (EC-INT-104)
- [x] End-to-end testing of infinite context pipeline (Dec 2025)
- [x] Performance benchmarking with 30k+ token inputs (Dec 2025)
- [x] Memory continuity verification across rotation boundaries (Dec 2025)

## Known Issues & Technical Debt

### Performance Issues
- [ ] Neo4j query optimization needed for large-scale graph traversal (EC-PERF-001)
- [ ] Redis memory usage monitoring and automated cleanup required (EC-PERF-002) 
- [ ] Context rotation timing optimization to minimize user experience disruption (EC-PERF-003)

### Reliability Issues
- [ ] Fallback mechanisms needed when Neo4j is temporarily unavailable (EC-REL-001)
- [ ] Retry logic for failed ContextGist creation during peak load periods (EC-REL-002)
- [ ] Graceful degradation when ContextGist retrieval fails (EC-REL-003)

### Usability Issues  
- [ ] Progress indicators needed during large context rotation operations (EC-USAB-001)
- [ ] User notifications for automatic context rotation events (EC-USAB-002)
- [ ] Configurable rotation thresholds based on model capabilities (EC-USAB-003)

## Research Tasks

### Active Research Projects
- [ ] Evaluation of different compression algorithms for ContextGist generation (EC-RES-001)
- [ ] Comparison of rotation strategies (oldest-first vs. least-relevant-first) (EC-RES-002)
- [ ] Investigation of hybrid retrieval effectiveness (graph + vector + keyword) (EC-RES-003)

### Planned Research Projects
- [ ] Long-term memory stability testing over 6+ month usage periods (EC-RES-004)
- [ ] Cognitive load measurement with infinite vs. finite context systems (EC-RES-005)
- [ ] User productivity impact assessment with comprehensive usage analytics (EC-RES-006)

---

**Current Status**: Active development on vector adapter integration and performance optimization
**Last Updated**: 2025-12-08
**Next Priority**: Complete vector adapter implementation for hybrid retrieval

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\backend\specs\tasks.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\README.md (Section: ANCHOR) ---

# Anchor CLI

> **The Body** - Terminal interface for the Context-Engine system

**Philosophy**: Your mind, augmented. Your data, sovereign. Your tools, open.

---

## Architecture: The Body

Anchor is the terminal interface that connects to the ECE_Core cognitive engine. It serves as the "body" of the Context-Engine system, providing direct interaction with the memory-enhanced AI.

### Interface Architecture
- **Type**: Pure Python CLI with streaming responses
- **Communication**: HTTP/SSE to `http://localhost:8000` (ECE_Core)
- **State**: Local session persistence with 24h TTL for conversation continuity
- **Safety**: T-101 protocols with human confirmation flows for dangerous operations

### Tool Integration
- **Simple Tool Mode**: Pattern-based execution for 4B-8B models (reliable execution)
- **Structured Tool Mode**: Full protocol for 14B+ models (advanced capabilities)
- **Plugin System**: Discoverable tools via `plugins/` directory
- **Safety Layers**: Whitelist/blacklist with confirmation for dangerous operations

---

## Quick Start

### Installation
```bash
cd anchor
pip install -e .
```

### Configuration
- **Primary Config**: `anchor/.env` (from `.env.example`)
- **ECE Connection**: `ECE_URL=http://localhost:8000`
- **Tool Settings**: `PLUGINS_ENABLED=true` to enable tools

### Run Interface
```bash
# Start Anchor CLI
python main.py

# Or use the packaged executable if available
anchor.exe
```

---

## Key Features

### ✅ Memory-Enhanced Conversations
- **Full Context Integration**: All conversations include full memory retrieval
- **Infinite Context Awareness**: Maintains flow across context rotations via ContextGist retrieval
- **Session Continuity**: Preserves conversation state across restarts

### ✅ Advanced Tool Execution
- **Smart Tool Detection**: Automatically uses Simple Tool Mode for smaller models
- **Structured Execution**: Full tool protocols for larger models
- **Safety Confirmation**: Explicit approval for dangerous operations
- **Error Recovery**: Graceful handling of tool execution failures

### ✅ Streaming Responses
- **Real-time Output**: Live token streaming to terminal
- **Preserved Formatting**: Syntax highlighting and code block rendering
- **Response Interrupt**: Press Ctrl+C to stop generation anytime

---

## Development

### Run Tests
```bash
python -m pytest tests/
```

### Package Distribution
```bash
pyinstaller anchor.spec
```

---

## Documentation

- `specs/spec.md` - Technical architecture and design
- `specs/plan.md` - Vision, roadmap, and strategic priorities  
- `specs/tasks.md` - Implementation backlog and current tasks

---

## Small Model Considerations

**Tool Usage**:
- ⚠️ Models < 14B parameters are unreliable for structured tool protocols
- ✅ Use "Simple Tool Mode" (pattern-based execution) for 4B-8B models
- ✅ Use 14B+ models (DeepSeek-R1, Qwen2.5-14B) for full tool support
- ✅ MCP Integration works with any model for memory operations

**Response Mode**:
- ✅ Streaming mode works reliably with all models
- ✅ Tool execution reliability varies with model size (4B < 8B < 14B+)

---

## Target Users

- **Developers**: Users needing persistent external memory for complex projects
- **Power Users**: Users wanting reliable, memory-enhanced AI interactions
- **Privacy-Conscious**: Users wanting 100% local tool execution

---

## Acknowledgments

Built as the terminal interface for cognitive enhancement systems.

**"Your commands, executed. Your data, local. Your tools, safe."**

---

## Need Help?

- **Usage Questions**: See `specs/spec.md`
- **Development Tasks**: See `specs/tasks.md`
- **Feature Requests**: See `specs/plan.md`

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\README.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\display.py (Section: ANCHOR_PYTHON) ---

"""
Display module for Anchor CLI - handles all output formatting and display-related functionality.
"""
import os
import sys


class AnchorDisplay:
    """Handles output formatting, streaming display, and all display-related methods."""
    
    def __init__(self, show_thinking: bool = False, stream_granularity: str = "auto", stream_sleep: float = 0.0):
        self.show_thinking = show_thinking
        self.stream_granularity = stream_granularity
        self.stream_sleep = stream_sleep

    def print_header(self, simple_handler=None, plugin_manager=None):
        """Print header once on startup."""
        print("\n" + "="*60)
        print("  Anchor - Personal Cognitive Command Center")
        print("  Memory-Enhanced Terminal AI with MCP Tools")
        print("="*60)
        print("  Type /help for commands, /exit to quit")

        # Check environment variables to show actual status
        plugins_enabled = os.getenv("PLUGINS_ENABLED", "").lower() in ("true", "1", "yes", "on")
        mcp_enabled = os.getenv("MCP_ENABLED", "").lower() in ("true", "1", "yes", "on")

        if simple_handler:
              print("  [TOOL] Simple Mode: ON (pattern-based tool execution)")
        if plugins_enabled and plugin_manager and hasattr(plugin_manager, 'enabled') and plugin_manager.enabled and len(plugin_manager.plugins) > 0:
              if self.show_thinking:
                 print(f"  Plugins: {', '.join(plugin_manager.plugins.keys())}")
        elif plugins_enabled or mcp_enabled:
              if self.show_thinking:
                 print("  Tools and Plugins: ENABLED (but may not be functional yet)")
        else:
              if self.show_thinking:
                 print("  Tools and Plugins: DISABLED (default - set PLUGINS_ENABLED=1 or MCP_ENABLED=1 to enable)")
        # Optionally display whether we show internal analysis (thinking) segments
        if self.show_thinking:
            print("  [MODE] Show internal thinking: ON (set ANCHOR_SHOW_THINKING=0 to hide)")
        else:
            print("  [MODE] Show internal thinking: OFF (set ANCHOR_SHOW_THINKING=1 to show)")
        # Streaming granularity display
        print(f"  [STREAM] Granularity: {self.stream_granularity} (sleep={self.stream_sleep})")
        print()

    def print_help(self):
        """Print help text."""
        print("""
Commands:
  /exit, /quit  - Exit Anchor
  /clear        - Clear terminal
  /help         - Show this help
  /session      - Show current session info
  /memories     - Show recent memories
  /tools        - List available tools
  /debug        - Toggle debug mode
  /simple       - Toggle simple tool mode (pattern-based execution)
  /stream-granularity <auto|char|word> [sleep_seconds] - Set streaming granularity for display and optional per-segment sleep

Input:
  ‚Ä¢ Enter          - Submit message
  ‚Ä¢ Alt+Enter      - Insert newline (for multiline input)
  ‚Ä¢ Ctrl+C         - Cancel/Exit
  ‚Ä¢ Paste          - Multiline paste is preserved

Tips:
  ‚Ä¢ Use arrow keys for command history (Unix/Linux/Mac)
  ‚Ä¢ Long responses are streamed in real-time
  ‚Ä¢ Terminal integration via MCP tools (filesystem, shell, web_search)
  ‚Ä¢ Tools execute automatically when needed
""")

    def show_session_info(self, session_id: str, ece_url: str, mcp_process=None, mcp_port: int = 8008):
        """Show current session information."""
        print(f"\nüìä Session Information:")
        print(f"   Session ID: {session_id}")
        print(f"   ECE_Core: {ece_url}")
        print(f"   MCP Server: Port {mcp_port} ({'Running' if mcp_process and mcp_process.poll() is None else 'Stopped'})")
        print()

    def print_error(self, message: str):
        """Print error message to console."""
        print(f"[ERROR] {message}")

    def print_warning(self, message: str):
        """Print warning message to console."""
        print(f"[WARN] {message}")

    def print_info(self, message: str):
        """Print info message to console."""
        print(f"[INFO] {message}")

    def print_success(self, message: str):
        """Print success message to console."""
        print(f"[SUCCESS] {message}")

    def print_tool_result(self, result, show_thinking: bool = False):
        """Print tool execution result."""
        if show_thinking:
            try:
                import json as _json
                print('\nüîß TOOL_CALL result:', _json.dumps(result, indent=2))
            except Exception:
                print('\nüîß TOOL_CALL result:', result)

    def print_action_result(self, result, show_thinking: bool = False):
        """Print action execution result."""
        if show_thinking:
            try:
                import json as _json
                print('\nüîß ACTION result:', _json.dumps(result, indent=2))
            except Exception:
                print('\nüîß ACTION result:', result)

    def print_simple_action_result(self, result, show_thinking: bool = False):
        """Print simple action execution result."""
        if show_thinking:
            print('\nüîß ACTION (simple mode) result:', result)

    def print_thinking_header(self):
        """Print thinking header if enabled."""
        if self.show_thinking:
            print("\n[THINKING]\n")

    def print_response_header(self):
        """Print response header if enabled."""
        if self.show_thinking:
            print("\n[RESPONSE]\n")

    def print_planner_proposed(self, plan_json):
        """Print planner proposed steps."""
        if self.show_thinking:
            print("\n[PLANNER] Proposed plan:")
        for i, s in enumerate(plan_json.get('steps', []), 1):
            print(f"  {i}. {s.get('tool_name')} - args: {s.get('args')} - reason: {s.get('reasoning')}")

    def print_tool_execution(self, plugin_call: str, params: dict, show_thinking: bool = False):
        """Print tool execution info."""
        if show_thinking:
            print(f"\nüîß Executing plan step: {plugin_call} with {params}")

    def print_tool_invocation(self, plugin_call: str, params: dict, show_thinking: bool = False):
        """Print tool invocation info."""
        if show_thinking:
            print(f"\nüîß Invoking tool: {plugin_call} with params: {params}\n")

    def print_tool_result_received(self):
        """Print tool result received message."""
        print(f"\n[TOOL] Result received. Generating response...\n")

    def print_tool_skipped(self, tool_name: str, show_thinking: bool = False):
        """Print tool skipped message."""
        if show_thinking:
            print(f"\nüîß Tool '{tool_name}' is 'none' - skipping execution\n")

    def print_action_skipped(self, action_name: str, show_thinking: bool = False):
        """Print action skipped message."""
        if show_thinking:
            print(f"\nüîß ACTION '{action_name}' skipped (none)\n")

    def print_tool_execution_error(self, error: str):
        """Print tool execution error."""
        print(f"‚ö†Ô∏è  Tool call failed: {error}")

    def print_action_execution_error(self, error: str):
        """Print action execution error."""
        print(f'\n‚ö†Ô∏è  ACTION execution failed (simple mode): {error}')

    def print_debug_mode_toggle(self, enabled: bool):
        """Print debug mode toggle message."""
        if enabled:
            print("\n[DEBUG] Debug mode: ON\n")
        else:
            print("\n[DEBUG] Debug mode: OFF\n")

    def print_simple_mode_toggle(self, enabled: bool):
        """Print simple mode toggle message."""
        if enabled:
            print("\nüöÄ Simple Mode: ON (pattern-based tool execution)\n")
        else:
            print("\nüöÄ Simple Mode: OFF (using full LLM tool calling)\n")

    def print_exit_message(self):
        """Print exit message."""
        print("\n[EXIT] Goodbye!\n")

    def print_reconnect_attempts_exceeded(self):
        """Print reconnection attempts exceeded message."""
        print("\n‚ùå Max reconnection attempts reached. Exiting.")

    def print_reconnecting_message(self):
        """Print reconnecting message."""
        print(f"\n[WAIT] Reconnecting to ECE_Core...")

    def print_retrying_connection(self, attempt: int, max_attempts: int):
        """Print retrying connection message."""
        print(f"[WAIT] Retrying connection (attempt {attempt + 2}/{max_attempts})...")

    def print_clear_terminal(self):
        """Clear terminal."""
        print("\033[2J\033[H")  # Clear terminal

    def print_connection_error(self, ece_url: str, error: str):
        """Print connection error."""
        print(f"[ERROR] Cannot connect to ECE_Core at {ece_url}: {error}")

    def print_health_check_error(self, error: str):
        """Print health check error."""
        print(f"[ERROR] Health check failed: {error}")

    def print_request_timeout(self):
        """Print request timeout message."""
        print("[TIMEOUT] Request timed out")

    def print_stream_cancelled(self):
        """Print stream cancelled message."""
        print("\n‚ö†Ô∏è  Response cancelled")

    def print_stream_error(self, error: str):
        """Print stream error."""
        print(f"[ERROR] {type(error).__name__}: {str(error)}")

    def print_stream_read_timeout(self, error: str):
        """Print stream read timeout message."""
        print(f"[ERROR] Stream read timeout: {error}")

    def print_connection_lost(self):
        """Print connection lost message."""
        print("[ERROR] Connection lost to ECE_Core. Attempting to reconnect...")

    def print_fallback_request_timeout(self):
        """Print fallback request timeout message."""
        print("[ERROR] Fallback request timed out")

    def print_connection_lost_during_fallback(self):
        """Print connection lost during fallback message."""
        print("[ERROR] Connection lost during fallback")

    def print_fallback_error(self, error: str):
        """Print fallback error."""
        print(f"[ERROR] {type(error).__name__}: {str(error)}")

    def print_old_version_hint(self):
        """Print old version hint message."""
        print("[HINT] ECE_Core may be running an older version without the updated settings. Restart ECE_Core (python launcher.py) to pick up the latest code.\n")

    def print_manual_tool_invocation(self, plugin_call: str, params: dict):
        """Print manual tool invocation."""
        print(f"\n[TOOL] Manual tool invocation: {plugin_call} {params}\n")

    def print_planner_execution_error(self):
        """Print planner execution error."""
        print("\n[WARN] Planner execution error - falling back to normal chat flow")

    def print_simple_mode_error(self, error: str):
        """Print simple mode error."""
        print(f"\n‚ö†Ô∏è  Could not enable simple mode: {error}\n")

    def print_simple_mode_cannot_handle(self):
        """Print simple mode cannot handle message."""
        print("\n[WARN] Could not execute tool: Simple mode couldn't handle, falling back to ECE_Core\n")

    def print_planner_error(self, error: str):
        """Print planner error."""
        print(f"\n[WARN] Could not execute tool: {error}\n")

    def print_unknown_command(self, cmd: str):
        """Print unknown command message."""
        print(f"\n[WARN] Unknown command: /{cmd}")
        print("   Type /help for available commands\n")



--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\display.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\input_handler.py (Section: ANCHOR_PYTHON) ---

"""
Input module for Anchor CLI - handles input processing, command parsing, and user interaction.
"""
import shlex
import re
from typing import Optional, Dict, Any, Tuple


class AnchorInput:
    """Handles input processing, command parsing, user interaction, and slash commands."""
    
    def __init__(self):
        # Pre-compile tool call regex for quick parsing of 'TOOL_CALL: name(...)' patterns
        # Allow plugin-prefixed tools like 'utcp:tool-name' including hyphens and dots
        self._tool_call_re = re.compile(r"TOOL_CALL:\s*([A-Za-z0-9_:\-\.]+)\s*\((.*)\)", re.DOTALL)
        # Pre-compile 'Action:' harness regex
        self._action_re = re.compile(r"Action:\s*(\w+)\s+(.*)", re.DOTALL)

    def parse_command(self, user_input: str) -> Tuple[str, str]:
        """Parse slash command and return command and arguments."""
        parts = user_input[1:].strip().split(None, 1)
        cmd = parts[0].lower()
        cmd_rest = parts[1] if len(parts) > 1 else ""
        return cmd, cmd_rest

    def is_command(self, user_input: str) -> bool:
        """Check if input is a command (starts with /)."""
        return user_input.startswith('/')

    def is_legacy_command(self, user_input: str) -> bool:
        """Check if input is a legacy command (without slash)."""
        return user_input.lower() in ["exit", "quit", "help", "clear"]

    def handle_command(self, cmd: str, cmd_rest: str, display, plugin_manager=None, simple_handler=None, 
                      stream_granularity: str = "auto", stream_sleep: float = 0.0) -> Optional[bool]:
        """Handle slash commands and return True if command was processed, False otherwise."""
        if cmd in ["exit", "quit"]:
            display.print_exit_message()
            return True
        elif cmd == "help":
            display.print_help()
            return True
        elif cmd == "clear":
            display.print_clear_terminal()
            return True
        elif cmd == "session":
            # This would need session_id and ece_url passed from main logic
            return False  # Defer to main logic
        elif cmd == "memories":
            # This is async, defer to main logic
            return False
        elif cmd == "tools":
            # This is async, defer to main logic
            return False
        elif cmd == "debug":
            # Toggle debug mode
            import logging
            current_level = logging.getLogger().level
            enabled = current_level != logging.DEBUG
            logging.getLogger().setLevel(logging.DEBUG if current_level == logging.WARNING else logging.WARNING)
            display.print_debug_mode_toggle(enabled)
            return True
        elif cmd == "stream-granularity":
            # Set streaming granularity at runtime
            if not cmd_rest:
                print(f"\n[Current streaming granularity] {stream_granularity} (sleep={stream_sleep})\n")
                return True
            parts = cmd_rest.split()
            gran = parts[0].strip().lower()
            if gran not in ("auto", "char", "word"):
                print(f"\n[WARN] Unknown granularity: {gran} (use auto|char|word)\n")
                return True
            # This would be handled by main logic
            return False
        elif cmd == "simple":
            # Toggle simple mode - this is complex and should be handled by main logic
            return False
        elif cmd == "call":
            # Manual plugin tool invocation
            if not plugin_manager or not hasattr(plugin_manager, 'enabled') or not plugin_manager.enabled:
                print("\n⚠️  Plugins disabled. Enable PLUGINS_ENABLED and restart Anchor to use plugin tools.\n")
                return True
            if not cmd_rest:
                print("\n⚠️  Usage: /call plugin:tool key=value [more] or /call tool(key=value ...)\n")
                return True
            # Parse and execute the tool call
            try:
                match = self._tool_call_re.search(cmd_rest)
                if match:
                    tool_name_raw = match.group(1).strip()
                    params_str = match.group(2).strip()
                else:
                    # Expect 'plugin:tool key=value key2=value2'
                    tokens = shlex.split(cmd_rest)
                    tool_name_raw = tokens[0]
                    params_str = ' '.join(tokens[1:]) if len(tokens) > 1 else ''
                
                parsed_tool_name, params = self._parse_tool_call(tool_name_raw, params_str)
                if ":" in parsed_tool_name:
                    plugin_call = parsed_tool_name
                else:
                    plugin_name = plugin_manager.lookup_plugin_for_tool(parsed_tool_name)
                    plugin_call = f"{plugin_name}:{parsed_tool_name}" if plugin_name else parsed_tool_name
                
                display.print_manual_tool_invocation(plugin_call, params)
                
                # This would be async and handled by main logic
                return False
            except Exception as e:
                display.print_planner_error(str(e))
                return True
        else:
            display.print_unknown_command(cmd)
            return True

    def handle_legacy_command(self, user_input: str, display) -> Optional[bool]:
        """Handle legacy commands (without slash) and return True if processed."""
        if user_input.lower() in ["exit", "quit"]:
            display.print_exit_message()
            return True
        elif user_input.lower() == "help":
            display.print_help()
            return True
        elif user_input.lower() == "clear":
            display.print_clear_terminal()
            return True
        return None

    def _parse_tool_call(self, raw_tool_name: str, params_str: str) -> Tuple[str, Dict[str, Any]]:
        """Parse a TOOL_CALL invocation string.

        - raw_tool_name: a string like 'filesystem_list_directory' or 'utcp:filesystem_list_directory'
        - params_str: the contents inside the parentheses, either JSON or a comma-separated key=value list
        Returns: (tool_name, params_dict)
        """
        params = {}
        s = params_str.strip()
        if not s:
            return raw_tool_name, params
        # Try JSON parsing first
        try:
            if s.startswith("{"):
                import json
                params = json.loads(s)
                return raw_tool_name, params
        except Exception:
            pass

        # Fallback: parse key=value pairs using shlex
        try:
            tokens = shlex.split(params_str)
        except Exception:
            tokens = [t.strip() for t in params_str.split(',') if t.strip()]

        for tok in tokens:
            if '=' in tok:
                k, v = tok.split('=', 1)
                k = k.strip()
                v = v.strip()
                if (v.startswith("'") and v.endswith("'")) or (v.startswith('"') and v.endswith('"')):
                    v = v[1:-1]
                # booleans
                if v.lower() in ("true", "false"):
                    v2 = v.lower() == 'true'
                else:
                    try:
                        if '.' in v:
                            v2 = float(v)
                        else:
                            v2 = int(v)
                    except Exception:
                        v2 = v
                params[k] = v2
        return raw_tool_name, params

    def is_tool_skippable(self, raw_tool_name: str) -> bool:
        """
        Helper to decide whether a tool name should be skipped (e.g., 'none', 'null', empty).
        Bundled here to ensure all checks are consistent across code paths.
        """
        if raw_tool_name is None:
            return True
        try:
            t = str(raw_tool_name).strip().lower()
            if t in ("none", "null", ""):
                return True
        except Exception:
            return False
        return False

    def should_invoke_planner(self, user_input: str, planner_enabled: bool = True, 
                             planner_length_threshold: int = 60, 
                             planner_keywords: list = None) -> bool:
        """
        Decide whether the Planner should be invoked for this input.
        Rules (conservative):
          - Input must be longer than 60 characters
          - Must contain at least one imperative keyword (find, search, scan, plan, analyze)
        """
        if not planner_enabled or not user_input or len(user_input.strip()) == 0:
            return False
        user_input_lower = user_input.lower()
        # Use configured keywords and threshold in the class defaults
        keywords = planner_keywords or ['find', 'search', 'scan', 'plan', 'analyze']
        threshold = planner_length_threshold
        return len(user_input) > threshold and any(k in user_input_lower for k in keywords)

    def extract_tool_calls(self, response: str):
        """Extract tool calls from a response string."""
        matches = list(self._tool_call_re.finditer(response))
        return matches

    def extract_actions(self, response: str):
        """Extract actions from a response string."""
        matches = list(self._action_re.finditer(response))
        return matches

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\input_handler.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\logic.py (Section: ANCHOR_PYTHON) ---

"""
Logic module for Anchor CLI - handles core business logic, API interactions, and decision making.
"""
import httpx
import asyncio
import sys
import json
import logging
import os
import subprocess
import time
from typing import Optional
from pathlib import Path

from prompt_toolkit import PromptSession
from prompt_toolkit.patch_stdout import patch_stdout
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.key_binding import KeyBindings

# Simple tool mode for small models
try:
    from simple_tool_mode import SimpleToolMode, SimpleToolHandler
except ImportError:
    # When installed via pip install -e ., the module may not be in Python path
    # so we directly execute it to get the classes
    import sys
    from pathlib import Path
    import importlib.util

    # Add the directory containing the script to Python path
    script_dir = Path(__file__).parent
    if str(script_dir) not in sys.path:
        sys.path.insert(0, str(script_dir))

    # Now try the import again
    from simple_tool_mode import SimpleToolMode, SimpleToolHandler

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)


class AnchorLogic:
    """Handles core business logic, API interactions, decision making, and tool execution."""
    
    def __init__(
        self,
        ece_url: Optional[str] = None,
        session_id: Optional[str] = None,
        timeout: Optional[int] = None,
        display=None,
        input_handler=None,
        create_prompt: bool = True
    ):
        self.display = display
        self.input_handler = input_handler
        
        # Only set PLUGINS_ENABLED to false if not already set to be more conservative
        # This ensures plugins are disabled by default to avoid redundancy with Qwen Code CLI
        if os.getenv("PLUGINS_ENABLED") is None:
            os.environ["PLUGINS_ENABLED"] = "false"
        # Default to a canonical IP address to avoid potential 'localhost' resolution issues
        default_ece = os.getenv("ECE_URL", "http://127.0.0.1:8000")
        self.ece_url = ece_url or default_ece
        # Normalize localhost to 127.0.0.1 to avoid environment-specific IPv6/IPv4 differences
        if 'localhost' in self.ece_url:
            self.ece_url = self.ece_url.replace('localhost', '127.0.0.1')
        self.session_id = session_id or os.getenv("SESSION_ID", "anchor-session")
        self.api_key = os.getenv("ECE_API_KEY")
        timeout_val = timeout or int(os.getenv("ECE_TIMEOUT", "120"))
        # default to a higher read timeout (long streaming responses need more time)
        self._timeout_seconds = float(timeout_val)
        # Set up headers with API key if configured
        headers = {}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        # Configure a robust httpx.Timeout: short connect; larger read/write timeouts
        client_timeout = httpx.Timeout(connect=5.0, read=self._timeout_seconds, write=self._timeout_seconds, pool=15.0)
        self.client = httpx.AsyncClient(timeout=client_timeout, headers=headers)
        self.running = True
        self.reconnect_attempts = 0
        self.max_reconnect_attempts = 3
        # Streaming retries (exponential backoff) - maximum number of attempts
        self.streaming_max_retries = int(os.getenv("ANCHOR_STREAM_MAX_RETRIES", "3"))

        # Set up key bindings for multiline support
        kb = KeyBindings()

        @kb.add('escape', 'enter')
        def _(event):
            """Alt+Enter to insert newline, Enter to submit"""
            event.current_buffer.insert_text('\n')

        if create_prompt:
            try:
                self.prompt_session = PromptSession(
                key_bindings=kb,
                enable_history_search=True,
                auto_suggest=AutoSuggestFromHistory(),
                multiline=False,  # Explicitly set to false, paste bracketing enabled by default
                enable_suspend=False  # Prevent Ctrl+Z issues
                )
            except Exception as e:
                # Handle terminal compatibility issues (e.g., when running in PowerShell/WSL)
                if "NoConsoleScreenBufferError" in str(type(e).__name__) or "xterm-256color" in str(e):
                    logger.warning(f"Console compatibility issue detected: {e}")
                    logger.warning("Falling back to basic input method")
                    # For Windows PowerShell compatibility, force stdout as the output
                    import sys
                    from prompt_toolkit.output import DummyOutput
                    # Create a minimal prompt session with a dummy output to avoid console issues
                    logger.warning("Using fallback terminal compatibility mode")
                    self.prompt_session = PromptSession(
                        output=DummyOutput(),  # Use dummy output to avoid console issues
                        key_bindings=kb,
                        enable_history_search=False,
                        auto_suggest=None,
                        multiline=False,
                        enable_suspend=False
                    )
                else:
                    # Re-raise if it's a different error
                    raise
        else:
            self.prompt_session = None

        # MCP server process (embedded tool server)
        self.mcp_process = None
        try:
            self.mcp_port = int(os.getenv('MCP_PORT', os.getenv('MCP_PORT', '8008')))
        except Exception:
            self.mcp_port = 8008

        # Simple tool mode (pattern-based tool execution for small models)
        self.simple_mode = SimpleToolMode()
        self.simple_handler = None  # Initialized after MCP server starts
        # Planner enabled by default; can be toggled with PLANNER_ENABLED
        self.planner_enabled = os.getenv("PLANNER_ENABLED", "true").lower() in ("true", "1", "yes", "on")
        # Planner configuration: threshold length and trigger keywords configurable via env
        try:
            self.planner_length_threshold = int(os.getenv("PLANNER_LENGTH_THRESHOLD", "50"))
        except Exception:
            self.planner_length_threshold = 60
        # default keywords for Planner invocation
        default_keywords = "find,search,scan,plan,analyze"
        raw_kw = os.getenv("PLANNER_KEYWORDS", default_keywords)
        # parse comma-separated list and normalize to lower-case stripped tokens
        self.planner_keywords = [k.strip().lower() for k in raw_kw.split(",") if k.strip()]
        # Streaming granularity: 'auto' (server-driven), 'char' or 'word'
        self.stream_granularity = os.getenv("ANCHOR_STREAM_GRANULARITY", "auto").lower()
        # Optional per-segment sleep in seconds to simulate token-by-token streaming
        try:
            self.stream_sleep = float(os.getenv("ANCHOR_STREAM_SLEEP", "0"))
        except Exception:
            self.stream_sleep = 0.0
        # Plugin manager (disabled by default - uses PLUGINS_ENABLED env var)
        # Import PluginManager dynamically so we can load from repo root when running from subdirs
        plugin_manager_cls = None
        try:
            from plugins.manager import PluginManager as plugin_manager_cls
        except Exception:
            # Try to add repo root to sys.path so imports resolve when launched from anchor/
            try:
                repo_root = Path(__file__).resolve().parent.parent
                sys.path.insert(0, str(repo_root))
                from plugins.manager import PluginManager as plugin_manager_cls
            except Exception:
                plugin_manager_cls = None

        # Keep plugins disabled by default to avoid redundancy with Qwen Code CLI
        if os.getenv("PLUGINS_ENABLED") is None:
            os.environ["PLUGINS_ENABLED"] = "false"
        # Only initialize plugin manager if enabled via env var
        plugins_enabled = os.getenv("PLUGINS_ENABLED", "").lower() in ("true", "1", "yes", "on")

        # If a real PluginManager class is available, instantiate it.
        if plugin_manager_cls and plugins_enabled:
            try:
                self.plugin_manager = plugin_manager_cls({})
            except Exception as e:
                logger.warning(f"PluginManager instantiation failed: {e}. Falling back to shim.")
                plugin_manager_cls = None
                self.plugin_manager = None
        else:
            self.plugin_manager = None

        # If plugins are enabled but import failed, create a minimal shim so the CLI can report 'enabled'
        if plugins_enabled and not self.plugin_manager:
            class _FallbackPluginManager:
                def __init__(self):
                    self.enabled = True
                    self.plugins = {}

                def discover(self):
                    return []

                def list_tools(self):
                    return []

                def lookup_plugin_for_tool(self, tool_name):
                    # No plugins available to resolve; return None
                    return None

                async def execute_tool(self, plugin_call, **kwargs):
                    # Raise a helpful error to indicate plugin manager isn't fully available
                    raise RuntimeError("PluginManager not available: no plugin manager was imported; please install/enable the PluginManager or run in simple mode.")

            # Instantiate shim to represent plugin manager is enabled but no functional manager is present
            self.plugin_manager = _FallbackPluginManager()
        
        # Default: don't show internal 'thinking' analysis segments unless enabled by env
        self.show_thinking = os.getenv("ANCHOR_SHOW_THINKING", "false").lower() in ("true", "1", "yes", "on")

    def start_plugins(self):
        """Start plugins via PluginManager (if enabled)."""
        # Check if plugins are explicitly enabled before attempting to start them
        plugins_enabled = os.getenv("PLUGINS_ENABLED", "").lower() in ("true", "1", "yes", "on")
        if not self.plugin_manager or not plugins_enabled:
            logger.debug("Plugin manager not available or disabled")
            return False
        discovered = self.plugin_manager.discover()
        if discovered:
            logger.info(f"Loaded plugins: {', '.join(discovered)}")
            return True
        logger.debug("No plugins discovered or plugin manager disabled")
        return False

    def start_mcp_server(self):
        """Start embedded MCP server for tools"""
        mcp_dir = Path(__file__).parent / "mcp"
        mcp_script = mcp_dir / "server.py"

        # MCP modules are archived and deprecated. They should not auto-start by default.
        # Use UTCP/plugins instead for local tooling.
        # Only start if explicitly enabled via the legacy env var - this is for backward compatibility.
        mcp_enabled = os.getenv("MCP_ENABLED", "").lower() in ("true", "1", "yes", "on")

        if not mcp_enabled:
            logger.debug("MCP server disabled by default. Set MCP_ENABLED=true to enable.")
            return False
        # Warn about deprecation
        logger.warning("MCP is deprecated and archived; prefer UTCP/plugin tools.")

        # If the mcp directory contains an ARCHIVED marker, don't attempt to start
        if (mcp_dir / "ARCHIVED.md").exists():
            logger.warning("MCP modules are archived and not available. Skipping MCP startup")
            return False

        if not mcp_script.exists():
            logger.warning(f"MCP server not found at: {mcp_script}")
            logger.warning("Tool calling will be disabled")
            return False

        try:
            logger.info("Starting embedded MCP server...")
            # Deeper check: try to detect a running MCP endpoint on localhost before trying to spawn.
            # This is a best-effort check‚Äîif it fails, we attempt to spawn the embedded server.
            try:
                import httpx
                host = os.getenv('MCP_HOST', '127.0.0.1')
                port = os.getenv('MCP_PORT', str(self.mcp_port))
                resp = httpx.get(f"http://{host}:{port}/mcp/tools", timeout=1.0)
                if resp.status_code == 200:
                    logger.info("Detected MCP server at localhost:%d (remote/embedded). Using it for compatibility.", self.mcp_port)
                    return True
            except Exception:
                # unable to confirm an existing MCP server; will attempt to spawn the embedded script (if available)
                pass
            self.mcp_process = subprocess.Popen(
                [sys.executable, str(mcp_script)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
            )
            time.sleep(2)  # Wait for server startup

            # Verify it's running
            if self.mcp_process.poll() is None:
                logger.info(f"‚úì MCP server started (port {self.mcp_port})")
                logger.info("  Tools: filesystem_read, shell_execute, web_search")

                # Initialize simple tool handler
                # Note: We'll create a minimal MCP client for simple mode
                try:
                    # Prefer plugin manager shim for simple mode
                    if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                        class PluginMCPShim:
                            def __init__(self, pm):
                                self.pm = pm

                            async def call_tool(self, tool_name, **kwargs):
                                plugin_name = self.pm.lookup_plugin_for_tool(tool_name)
                                if plugin_name:
                                    res = await self.pm.execute_tool(f"{plugin_name}:{tool_name}", **kwargs)
                                    return {"status": "success", "result": res}
                                return {"status": "error", "error": f"Tool not found: {tool_name}"}

                            async def get_tools(self):
                                return self.pm.list_tools()

                        simple_mcp = PluginMCPShim(self.plugin_manager)
                    else:
                        # Prefer plugin manager for execution. If requested, try to import MCP client.
                        # Users can run an archived MCP server manually if needed; anchor will try to connect to http://localhost:8008 before starting the embedded script.
                        try:
                            # Prefer a top-level 'mcp' package if installed, otherwise use our local anchor.mcp package
                            try:
                                from mcp.client import MCPClient
                            except Exception:
                                from anchor.mcp.client import MCPClient
                            mcp_env_url = os.getenv('MCP_URL') or f"http://{os.getenv('MCP_HOST', '127.0.0.1')}:{os.getenv('MCP_PORT', str(self.mcp_port))}"
                            simple_mcp = MCPClient(mcp_env_url, api_key=os.getenv('MCP_API_KEY') or os.getenv('ECE_API_KEY'))
                        except Exception:
                            # If MCP client isn't present, fall back to plugin manager shim or UTCP plugin implicitly via plugin manager
                            simple_mcp = PluginMCPShim(self.plugin_manager) if self.plugin_manager else None

                    # Create a minimal LLM client for formatting
                    class SimpleLLMClient:
                        def __init__(self, ece_url, headers):
                            self.ece_url = ece_url
                            self.headers = headers

                        async def generate(self, prompt, system_prompt=None):
                            """Minimal LLM generation for formatting"""
                            import httpx
                            async with httpx.AsyncClient(timeout=30) as client:
                                payload = {
                                    "session_id": "simple-mode",
                                    "message": prompt
                                }
                                response = await client.post(
                                    f"{self.ece_url}/chat",
                                    json=payload,
                                    headers=self.headers
                                )
                                if response.status_code == 200:
                                    data = response.json()
                                    return data.get("response", "")
                                return ""

                    simple_llm = SimpleLLMClient(self.ece_url, self.client.headers)
                    self.simple_handler = SimpleToolHandler(simple_mcp, simple_llm)
                    logger.info("  ‚úì Simple tool mode enabled (pattern-based execution)")
                except Exception as e:
                    logger.warning(f"  Simple tool mode initialization failed: {e}")

                return True
            else:
                # Get error output
                _, stderr = self.mcp_process.communicate(timeout=1)
                error_msg = stderr.decode('utf-8', errors='ignore') if stderr else "Unknown error"
                logger.warning(f"MCP server failed to start: {error_msg}")
                return False
        except Exception as e:
            logger.error(f"Could not start MCP server: {e}")
            return False

    def stop_mcp_server(self):
        """Stop MCP server on exit"""
        if self.mcp_process and self.mcp_process.poll() is None:
            logger.info("Stopping MCP server...")
            self.mcp_process.terminate()
            try:
                self.mcp_process.wait(timeout=5)
                logger.info("‚úì MCP server stopped")
            except subprocess.TimeoutExpired:
                self.mcp_process.kill()
                logger.warning("MCP server forcefully terminated")

    async def check_connection(self) -> bool:
        """Verify ECE_Core is running."""
        try:
            response = await self.client.get(f"{self.ece_url}/health")
            if response.status_code == 200:
                self.reconnect_attempts = 0  # Reset on success
                return True
            return False
        except httpx.ConnectError as e:
            logger.error(f"Cannot connect to ECE_Core at {self.ece_url}: {e}")
            self.display.print_connection_error(self.ece_url, str(e))
            return False
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            self.display.print_health_check_error(str(e))
            return False

    async def send_message_streaming(self, message: str):
        """Send message and stream response token-by-token. Falls back to regular endpoint if streaming not available."""
        # Use retry/backoff for streaming requests
        try:
            # Sanitize message to remove surrogate characters
            message = message.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')

            payload = {
                "session_id": self.session_id,
                "message": message
            }

            attempt = 0
            last_exc = None
            while attempt < self.streaming_max_retries:
                try:
                    # Try streaming first
                    async with self.client.stream(
                        "POST",
                        f"{self.ece_url}/chat/stream",
                        json=payload
                    ) as response:
                        if response.status_code == 404:
                            # Fallback to regular endpoint
                            async for chunk in self.send_message_fallback(message):
                                yield chunk
                            return

                        if response.status_code != 200:
                            yield f"Error: HTTP {response.status_code}"
                            return

                        # Track streaming label mode: None, 'thinking', 'response'
                        streaming_mode = None
                        # Buffer 'thinking' chunks that should be hidden from the user
                        hidden_thinking_buffer = ""
                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                data_str = line[6:]
                                try:
                                    data = json.loads(data_str)
                                    if data.get("chunk"):
                                        # Sanitize chunk to prevent encoding errors
                                        chunk = data["chunk"]
                                        # Handle structured labels inserted by ECE_Core
                                        chunk_stripped = chunk.strip()
                                        if chunk_stripped.lower() == 'thinking:' or chunk_stripped.endswith('thinking:'):
                                            # Switch to thinking mode: show a header
                                            streaming_mode = 'thinking'
                                            # Only show the analysis header if explicitly enabled by env
                                            if self.show_thinking:
                                                try:
                                                    chunk_to_print = "\n[THINKING]\n"
                                                    chunk_to_print = chunk_to_print.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                                                    yield chunk_to_print
                                                except Exception:
                                                    yield "\n[THINKING]\n"
                                            # Do not process the marker by _process_stream_chunk
                                            continue
                                        elif chunk_stripped.lower() == 'response:' or chunk_stripped.endswith('\nresponse:'):
                                            streaming_mode = 'response'
                                            try:
                                                chunk_to_print = "\n[RESPONSE]\n"
                                                chunk_to_print = chunk_to_print.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                                                # Show response label only if user specifically wants to see labels
                                                if self.show_thinking:
                                                    yield chunk_to_print
                                            except Exception:
                                                if self.show_thinking:
                                                    yield "\n[RESPONSE]\n"
                                            continue
                                        chunk = chunk.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                                        # If we're in an internal 'thinking' streaming mode and the client is configured
                                        # to hide internal analysis, skip yielding chunks until the assistant
                                        # switches back into 'response' mode.
                                        if streaming_mode == 'thinking' and not self.show_thinking:
                                            # Buffer internal 'thinking' content until we know whether a 'response'
                                            # marker will be emitted (e.g., in tool execution flows).
                                            hidden_thinking_buffer += chunk
                                            continue
                                        # For other cases (response mode or show_thinking True), yield segments according to granularity
                                        if self.stream_granularity == 'char' and len(chunk) > 1:
                                            for ch in chunk:
                                                yield ch
                                                if self.stream_sleep:
                                                    await asyncio.sleep(self.stream_sleep)
                                        elif self.stream_granularity == 'word' and len(chunk) > 1:
                                            for w in chunk.split():
                                                yield w + ' '
                                                if self.stream_sleep:
                                                    await asyncio.sleep(self.stream_sleep)
                                        else:
                                            yield chunk
                                        # Process chunk for TOOL_CALL streaming detection
                                        try:
                                            await self._process_stream_chunk(chunk)
                                        except Exception as e:
                                            logger.debug(f"Stream tool call processing error: {e}")
                                    elif data.get("done"):
                                        break
                                    elif data.get("error"):
                                        yield f"\n[ERROR] {data['error']}\n"
                                        break
                                except json.JSONDecodeError:
                                    continue
                        # If we reached here, the stream finished normally. If we were hiding a chain-of-thought
                        # (hidden_thinking_buffer) and the stream ended without a 'response:' marker, flush the
                        # hidden buffer so the user sees the final output (Anchor hid it while LLM was 'thinking').
                        if hidden_thinking_buffer and streaming_mode == 'thinking' and not self.show_thinking:
                            try:
                                # Yield buffered content as segments according to granularity
                                if self.stream_granularity == 'char' and len(hidden_thinking_buffer) > 1:
                                    for ch in hidden_thinking_buffer:
                                        yield ch
                                        if self.stream_sleep:
                                            await asyncio.sleep(self.stream_sleep)
                                elif self.stream_granularity == 'word' and len(hidden_thinking_buffer) > 1:
                                    for w in hidden_thinking_buffer.split():
                                        yield w + ' '
                                        if self.stream_sleep:
                                            await asyncio.sleep(self.stream_sleep)
                                else:
                                    yield hidden_thinking_buffer
                            except Exception:
                                # Fallback plain yield
                                yield hidden_thinking_buffer
                        # End of normal stream
                        return
                except (httpx.ReadTimeout, httpx.ReadError, httpx.ConnectError, asyncio.TimeoutError) as exc:
                    last_exc = exc
                    attempt += 1
                    if attempt >= self.streaming_max_retries:
                        logger.warning("Streaming failed after %d attempts: %s", attempt, exc)
                        # Surface error to user explicitly in addition to logs
                        print(f"[ERROR] Streaming failed after {attempt} attempts: {exc}")
                        break
                    backoff = 0.5 * (2 ** (attempt - 1))
                    logger.info("Streaming attempt %d failed, retrying after %.2fs: %s", attempt, backoff, exc)
                    await asyncio.sleep(backoff)
                    continue
            # If we got here without returning, that implies streaming failed in all attempts
            # and `last_exc` contains the last exception
            if last_exc is not None:
                # Fallback to non-streaming endpoint
                async for chunk in self.send_message_fallback(message):
                    yield chunk
                return
        except asyncio.CancelledError:
            # Gracefully handle cancellation (e.g., from pasting multiline text)
            logger.debug("Stream cancelled")
            yield "\n[WARN] Response cancelled\n"
        except asyncio.TimeoutError:
            logger.error("Request timed out")
            self.display.print_request_timeout()
            yield "\n[TIMEOUT] Request timed out\n"
        except httpx.ReadTimeout as e:
            logger.warning("Stream ReadTimeout: %s", e)
            # fallback to non-streaming
            self.display.print_stream_read_timeout(str(e))
            async for chunk in self.send_message_fallback(message):
                yield chunk
            return
        except httpx.ConnectError:
            logger.error("Connection lost to ECE_Core")
            self.display.print_connection_lost()
            yield "\n[ERROR] Connection lost to ECE_Core. Attempting to reconnect...\n"
            self.reconnect_attempts += 1
        except Exception as e:
            logger.error(f"Stream error: {e}")
            self.display.print_stream_error(e)
            err_text = str(e)
            if 'mcp_max_tool_iterations' in err_text or 'tool_max_iterations' in err_text:
                yield f"\n[ERROR] {err_text}\n[HINT] ECE_Core may be running an older version without the updated settings. Restart ECE_Core (python launcher.py) to pick up the latest code.\n"
            else:
                yield f"\n[ERROR] {type(e).__name__}: {str(e)}\n"

    async def send_message_fallback(self, message: str):
        """Fallback to regular /chat endpoint."""
        try:
            payload = {
                "session_id": self.session_id,
                "message": message
            }
            response = await self.client.post(
                f"{self.ece_url}/chat",
                json=payload
            )

            if response.status_code == 200:
                data = response.json()
                result_text = data.get("response", "No response from ECE_Core")
                # If the anchor client isn't configured to show 'thinking', strip it and return only the 'response' portion
                if not self.show_thinking and isinstance(result_text, str) and "response:" in result_text:
                    try:
                        parts = result_text.split('\nresponse:', 1)
                        final = parts[-1].strip() if len(parts) > 1 else result_text
                        yield final
                    except Exception:
                        yield result_text
                else:
                    yield result_text
            else:
                error_text = response.text or ""
                # Always surface the error to the CLI to remove silent failures
                print(f"[ERROR] ECE_Core returned HTTP {response.status_code}: {error_text}")
                if 'mcp_max_tool_iterations' in error_text or 'tool_max_iterations' in error_text:
                    yield f"Error: HTTP {response.status_code} - {error_text[:200]}\n[HINT] ECE_Core may be running an older version without the updated settings. Restart ECE_Core (python launcher.py) to pick up the latest code.\n"
                else:
                    yield f"Error: HTTP {response.status_code} - {error_text[:200]}"
        except asyncio.TimeoutError:
            logger.error("Fallback request timed out")
            self.display.print_fallback_request_timeout()
            yield "[TIMEOUT] Request timed out"
        except httpx.ConnectError:
            logger.error("Connection lost during fallback")
            self.display.print_connection_lost_during_fallback()
            yield "[ERROR] Connection lost to ECE_Core"
        except Exception as e:
            logger.error(f"Fallback error: {e}")
            self.display.print_fallback_error(e)
            yield f"[ERROR] {type(e).__name__}: {str(e)}"

    async def _process_stream_chunk(self, chunk: str):
        """Process a streaming chunk and fire a plugin tool call if a TOOL_CALL pattern is found.

        This function accumulates chunks in an instance buffer until a complete TOOL_CALL
        pattern is found (balanced parentheses). When found, it executes the tool and logs output.
        """
        if not hasattr(self, '_stream_buffer'):
            self._stream_buffer = ''
        # Append chunk and keep buffer at reasonable size
        self._stream_buffer += chunk
        if len(self._stream_buffer) > 8192:
            # Trim older content
            self._stream_buffer = self._stream_buffer[-8192:]
        # Search for tools; find the first complete TOOL_CALL occurrence
        m = self.input_handler._tool_call_re.search(self._stream_buffer)
        if not m:
            # No TOOL_CALL found; check for 'Action: ' lines
            a = self.input_handler._action_re.search(self._stream_buffer)
            if not a:
                return
            # Prepare and execute the action
            action_name = a.group(1).strip()
            params_str = a.group(2).strip()
            parsed_tool_name, params = self.input_handler._parse_tool_call(action_name, params_str)
            # Skip 'none' actions explicitly
            if self.input_handler.is_tool_skippable(parsed_tool_name):
                if self.show_thinking:
                    self.display.print_action_skipped(parsed_tool_name, self.show_thinking)
                # Remove handled action from buffer and return
                start = a.start()
                end = a.end()
                self._stream_buffer = self._stream_buffer[:start] + self._stream_buffer[end:]
                return
            plugin_call = None
            if ":" in parsed_tool_name:
                plugin_call = parsed_tool_name
            else:
                plugin_name = self.plugin_manager.lookup_plugin_for_tool(parsed_tool_name) if self.plugin_manager else None
                plugin_call = f"{plugin_name}:{parsed_tool_name}" if plugin_name else parsed_tool_name
            try:
                if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                    res = await self.plugin_manager.execute_tool(plugin_call, **params)
                    self.display.print_action_result(res, self.show_thinking)
                else:
                    # Fallback to simple handler mapping
                    if self.simple_handler:
                        # Attempt to map intent by combining name and params
                        # If this fails, print warning
                        try:
                            # Build a short query for simple mode heuristics
                            fake_query = f"{action_name} {params_str}"
                            res = await self.simple_handler.handle_query(fake_query, self.session_id)
                            self.display.print_simple_action_result(res, self.show_thinking)
                        except Exception as e:
                            self.display.print_action_execution_error(str(e))
            finally:
                # Remove the handled action from the buffer to avoid duplicate handling
                start = a.start()
                end = a.end()
                self._stream_buffer = self._stream_buffer[:start] + self._stream_buffer[end:]
            return
        # We need to ensure the parentheses are balanced; quick check
        start = m.start()
        end = m.end()
        # If regex matched, assume it's complete enough; extract tool and params
        tool_name_raw = m.group(1).strip()
        params_str = m.group(2).strip()
        parsed_tool_name, params = self.input_handler._parse_tool_call(tool_name_raw, params_str)
        # Skip 'none' tool calls (explicit no-op)
        if self.input_handler.is_tool_skippable(parsed_tool_name):
            if self.show_thinking:
                self.display.print_tool_skipped(parsed_tool_name, self.show_thinking)
            # Remove the handled tool call from the buffer to avoid duplicate handling
            self._stream_buffer = self._stream_buffer[:start] + self._stream_buffer[end:]
            return
        if ":" in parsed_tool_name:
            plugin_call = parsed_tool_name
        else:
            plugin_name = self.plugin_manager.lookup_plugin_for_tool(parsed_tool_name) if self.plugin_manager else None
            plugin_call = f"{plugin_name}:{parsed_tool_name}" if plugin_name else parsed_tool_name
        # Execute the tool and print the result only if plugin manager is enabled
        try:
            if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                res = await self.plugin_manager.execute_tool(plugin_call, **params)
                self.display.print_tool_result(res, self.show_thinking)
        finally:
            # Remove the handled tool call from the buffer to avoid duplicate handling
            self._stream_buffer = self._stream_buffer[:start] + self._stream_buffer[end:]

    async def show_recent_memories(self):
        """Show recent memories from ECE_Core."""
        try:
            # Try the `search` endpoint (supported by ECE_Core) for recent memories.
            response = await self.client.get(f"{self.ece_url}/memories/search?limit=5")
            # Backcompat: if server uses /memories with a 405, try the older endpoint
            if response.status_code == 405:
                response = await self.client.get(f"{self.ece_url}/memories?limit=5")
            if response.status_code == 200:
                memories = response.json().get('memories', [])
                print(f"\nüí≠ Recent Memories ({len(memories)}):")
                for i, mem in enumerate(memories, 1):
                    print(f"   {i}. {mem.get('content', 'N/A')[:60]}...")
                print()
            else:
                print("\n‚ö†Ô∏è  Could not retrieve memories")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error retrieving memories: {e}")

    async def show_tools(self):
        """Show available MCP tools."""
        # Check if tools are actually enabled before attempting to retrieve them
        plugins_enabled = os.getenv("PLUGINS_ENABLED", "").lower() in ("true", "1", "yes", "on")
        mcp_enabled = os.getenv("MCP_ENABLED", "").lower() in ("true", "1", "yes", "on")

        if not plugins_enabled and not mcp_enabled:
            print("\n‚ö†Ô∏è  Tools are currently disabled by default.")
            print("   To enable tools, set PLUGINS_ENABLED=1 or MCP_ENABLED=1 in your environment.")
            print("   Only enable tools after they have been properly configured and tested.")
            print()
            return

        # Prefer plugin manager tools, otherwise the MCP server
        try:
            if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                tools = self.plugin_manager.list_tools()
                if self.show_thinking:
                    print(f"\nüîß Available Plugin Tools ({len(tools)}):")
                for tool in tools:
                    print(f"   ‚Ä¢ {tool.get('name')}: {tool.get('description')}")
                print()
                return
            # Fallback to local MCP server (if present and enabled)
            if mcp_enabled:
                host = os.getenv('MCP_HOST', '127.0.0.1')
                port = os.getenv('MCP_PORT', str(self.mcp_port))
                response = await self.client.get(f"http://{host}:{port}/mcp/tools")
                if response.status_code == 200:
                    tools_data = response.json()
                    tools = tools_data.get('tools', [])
                    if self.show_thinking:
                        print(f"\nüîß Available Tools ({len(tools)}):")
                    for tool in tools:
                        print(f"   ‚Ä¢ {tool['name']}: {tool['description']}")
                    print()
                else:
                    print("\n‚ö†Ô∏è  Could not retrieve tools")
            else:
                print("\n‚ö†Ô∏è  MCP server not enabled. Set MCP_ENABLED=1 to enable.")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error retrieving tools: {e}")

    async def run(self):
        """Main CLI loop."""
        # Start plugin manager or embedded MCP server only if explicitly enabled
        # By default, tools are disabled to prevent issues with improperly configured tools
        plugins_enabled = os.getenv("PLUGINS_ENABLED", "").lower() in ("true", "1", "yes", "on")
        mcp_enabled = os.getenv("MCP_ENABLED", "").lower() in ("true", "1", "yes", "on")

        if plugins_enabled:
            self.start_plugins()
        if mcp_enabled:
            self.start_mcp_server()  # backward compatible: will no-op if archived

        # Check connection with retry
        for attempt in range(self.max_reconnect_attempts):
            connected = await self.check_connection()
            if connected:
                break
            if attempt < self.max_reconnect_attempts - 1:
                self.display.print_retrying_connection(attempt, self.max_reconnect_attempts)
                await asyncio.sleep(2)

        if not connected:
            print("[ERROR] ECE_Core not running. Start it first:")
            print("   cd ECE_Core && python launcher.py")
            self.stop_mcp_server()  # Clean up MCP server
            await self.client.aclose()
            return

        self.display.print_header(self.simple_handler, self.plugin_manager)

        try:
            while self.running:
                try:
                    # Check if we need to reconnect
                    if self.reconnect_attempts > 0 and self.reconnect_attempts < self.max_reconnect_attempts:
                        self.display.print_reconnecting_message()
                        await asyncio.sleep(2)
                        connected = await self.check_connection()
                        if not connected:
                            self.reconnect_attempts += 1
                            continue
                    elif self.reconnect_attempts >= self.max_reconnect_attempts:
                        self.display.print_reconnect_attempts_exceeded()
                        break

                    # Get user input (supports multiline paste)
                    with patch_stdout():
                        user_input = await self.prompt_session.prompt_async(
                            "You: ",
                            enable_open_in_editor=False,
                            multiline=False  # Single line by default, Alt+Enter for newlines
                        )
                    user_input = user_input.strip()

                    if not user_input:
                        continue

                    # Handle commands (slash commands)
                    if self.input_handler.is_command(user_input):
                        cmd, cmd_rest = self.input_handler.parse_command(user_input)
                        
                        # Handle commands with the input handler
                        handled = self.input_handler.handle_command(
                            cmd, cmd_rest, self.display, 
                            self.plugin_manager, self.simple_handler,
                            self.stream_granularity, self.stream_sleep
                        )
                        
                        if handled:
                            continue
                        elif cmd == "session":
                            self.display.show_session_info(self.session_id, self.ece_url, self.mcp_process, self.mcp_port)
                            continue
                        elif cmd == "memories":
                            await self.show_recent_memories()
                            continue
                        elif cmd == "tools":
                            await self.show_tools()
                            continue
                        elif cmd == "stream-granularity":
                            # Set streaming granularity at runtime
                            if not cmd_rest:
                                print(f"\n[Current streaming granularity] {self.stream_granularity} (sleep={self.stream_sleep})\n")
                                continue
                            parts = cmd_rest.split()
                            gran = parts[0].strip().lower()
                            if gran not in ("auto", "char", "word"):
                                print(f"\n[WARN] Unknown granularity: {gran} (use auto|char|word)\n")
                                continue
                            self.stream_granularity = gran
                            if len(parts) > 1:
                                try:
                                    self.stream_sleep = float(parts[1])
                                except Exception:
                                    print("\n[WARN] Invalid sleep value; keeping existing\n")
                            print(f"\n[OK] Streaming granularity set: {self.stream_granularity} (sleep={self.stream_sleep})\n")
                            continue
                        elif cmd == "simple":
                            # Toggle simple mode
                            if self.simple_handler:
                                # Disable by setting to None
                                old_handler = self.simple_handler
                                self.simple_handler = None
                                self.display.print_simple_mode_toggle(False)
                            else:
                                # Try to re-enable
                                try:
                                    # Prefer plugin manager shim for simple mode
                                    if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                                        # Create an MCP-like shim using plugin_manager
                                        class PluginMCPShim:
                                            def __init__(self, pm):
                                                self.pm = pm

                                            async def call_tool(self, tool_name, **kwargs):
                                                plugin_name = self.pm.lookup_plugin_for_tool(tool_name)
                                                if plugin_name:
                                                    res = await self.pm.execute_tool(f"{plugin_name}:{tool_name}", **kwargs)
                                                    return {"status": "success", "result": res}
                                                return {"status": "error", "error": f"Tool not found: {tool_name}"}

                                            async def get_tools(self):
                                                return self.pm.list_tools()

                                        simple_mcp = PluginMCPShim(self.plugin_manager)
                                    else:
                                        try:
                                            from mcp.client import MCPClient
                                        except Exception:
                                            from anchor.mcp.client import MCPClient
                                        simple_mcp = MCPClient(f"http://{os.getenv('MCP_HOST', '127.0.0.1')}:{os.getenv('MCP_PORT', str(self.mcp_port))}", api_key=os.getenv('MCP_API_KEY') or os.getenv('ECE_API_KEY'))

                                    class SimpleLLMClient:
                                        def __init__(self, ece_url, headers):
                                            self.ece_url = ece_url
                                            self.headers = headers

                                        async def generate(self, prompt, system_prompt=None):
                                            import httpx
                                            async with httpx.AsyncClient(timeout=30) as client:
                                                payload = {"session_id": "simple-mode", "message": prompt}
                                                response = await client.post(f"{self.ece_url}/chat", json=payload, headers=self.headers)
                                                if response.status_code == 200:
                                                    return response.json().get("response", "")
                                                return ""

                                    simple_llm = SimpleLLMClient(self.ece_url, self.client.headers)
                                    self.simple_handler = SimpleToolHandler(simple_mcp, simple_llm)
                                    self.display.print_simple_mode_toggle(True)
                                except Exception as e:
                                    self.display.print_simple_mode_error(str(e))
                            continue
                        elif cmd == "call":
                            # Manual plugin tool invocation: /call plugin:tool arg1=val1 arg2=v... [truncated]
                            if not self.plugin_manager or not hasattr(self.plugin_manager, 'enabled') or not self.plugin_manager.enabled:
                                print("\n‚ö†Ô∏è  Plugins disabled. Enable PLUGINS_ENABLED and restart Anchor to use plugin tools.\n")
                                continue
                            if not cmd_rest:
                                print("\n‚ö†Ô∏è  Usage: /call plugin:tool key=value [more] or /call tool(key=value ...)\n")
                                continue
                            # If cmd_rest contains parentheses, use the same parsing we use for LLM outputs
                            try:
                                match = self.input_handler._tool_call_re.search(cmd_rest)
                                if match:
                                    tool_name_raw = match.group(1).strip()
                                    params_str = match.group(2).strip()
                                else:
                                    # Expect 'plugin:tool key=value key2=value2'
                                    tokens = shlex.split(cmd_rest)
                                    tool_name_raw = tokens[0]
                                    params_str = ' '.join(tokens[1:]) if len(tokens) > 1 else ''
                                parsed_tool_name, params = self.input_handler._parse_tool_call(tool_name_raw, params_str)
                                if ":" in parsed_tool_name:
                                    plugin_call = parsed_tool_name
                                else:
                                    plugin_name = self.plugin_manager.lookup_plugin_for_tool(parsed_tool_name)
                                    plugin_call = f"{plugin_name}:{parsed_tool_name}" if plugin_name else parsed_tool_name
                                self.display.print_manual_tool_invocation(plugin_call, params)
                                res = await self.plugin_manager.execute_tool(plugin_call, **params)
                                try:
                                    import json as _json
                                    print(_json.dumps(res, indent=2))
                                except Exception:
                                    print(res)
                            except Exception as e:
                                self.display.print_planner_error(str(e))
                            continue
                        else:
                            self.display.print_unknown_command(cmd)
                            continue

                    # Legacy command support (without slash)
                    legacy_handled = self.input_handler.handle_legacy_command(user_input, self.display)
                    if legacy_handled is True:
                        continue
                    elif legacy_handled is False:
                        # Command not recognized as legacy command, continue processing
                        pass

                    # Priority 0: SIMPLE TOOL MODE - Pattern-based tool execution
                    # Intercept simple tool queries and execute directly (bypass LLM complexity)
                    if self.simple_handler and self.simple_handler.can_handle_directly(user_input):
                        logger.info("üéØ Using simple tool mode (pattern-based)")
                        sys.stdout.write("Assistant: ")
                        sys.stdout.flush()

                        try:
                            response = await self.simple_handler.handle_query(user_input, self.session_id)
                            if response:
                                # Simple mode handled it successfully
                                print(response)
                                print()
                                continue
                            else:
                                # Pattern matched but execution failed, fall through to normal mode
                                logger.debug("Simple mode couldn't handle, falling back to ECE_Core")
                        except Exception as e:
                            logger.error(f"Simple mode error: {e}")
                            # Fall through to normal mode

                    # Priority 1: Planner - invoke planner for long requests or explicit planning queries
                    # Planner trigger: only for longer/more explicit planning-like queries
                    # Only invoke planner for complex/long requests containing keywords (lazy planning)
                    # Relax the solver: only invoke planner for longer/more actionable requests
                    # Use a helper to determine whether to trigger the Planner.
                    # We are conservative: require a longer message and an imperative verb.
                    is_complex = self.input_handler.should_invoke_planner(
                        user_input, 
                        self.planner_enabled, 
                        self.planner_length_threshold, 
                        self.planner_keywords
                    )

                    if self.planner_enabled and is_complex and not user_input.startswith('/'):
                        try:
                            resp = await self.client.post(f"{self.ece_url}/plan", json={"session_id": self.session_id, "message": user_input})
                            if resp.status_code == 200:
                                plan_json = resp.json().get('plan')
                            else:
                                # Inform the user if planner endpoint returned an error
                                err_text = resp.text or ''
                                print(f"[ERROR] Planner endpoint returned HTTP {resp.status_code}: {err_text[:200]}")
                            if plan_json and plan_json.get('steps'):
                                self.display.print_planner_proposed(plan_json)
                                confirm = input("Execute this plan? [y/N]: ").strip().lower()
                                if confirm in ("y", "yes"):
                                    # Execute steps sequentially
                                    plan_had_tools = False
                                    try:
                                        for s in plan_json.get('steps', []):
                                            tool_name = s.get('tool_name') or ''
                                            params = s.get('args') or {}
                                            # If the plan step indicates no tool is needed, skip execution and record nothing
                                            if self.input_handler.is_tool_skippable(tool_name):
                                                if self.show_thinking:
                                                    print(f"[Plan] Step {s.get('step_id') or 'n/a'}: {s.get('reasoning')} (No tool needed)")
                                                # skip executing this step
                                                continue
                                            plugin_call = None
                                            if ":" in tool_name:
                                                plugin_call = tool_name
                                            else:
                                                plugin_name = self.plugin_manager.lookup_plugin_for_tool(tool_name) if self.plugin_manager else None
                                                plugin_call = f"{plugin_name}:{tool_name}" if plugin_name else tool_name
                                            self.display.print_tool_execution(plugin_call, params, self.show_thinking)
                                            try:
                                                # mark that we used a tool so we can feed the results later
                                                if self.plugin_manager and getattr(self.plugin_manager, 'enabled', False):
                                                    res = await self.plugin_manager.execute_tool(plugin_call, **params)
                                                    print(f"  ‚Üí Result: {res}")
                                                    plan_had_tools = True
                                                elif self.simple_handler:
                                                    fake_query = f"{tool_name} {params}"
                                                    res = await self.simple_handler.handle_query(fake_query, self.session_id)
                                                    print(f"  ‚Üí Result: {res}")
                                                    plan_had_tools = True
                                                else:
                                                    print("  ‚Üí No tool executor available (plugin_manager disabled)")
                                            except Exception as e:
                                                print(f"  ‚Üí Execution error: {e}")
                                    except Exception as e:
                                        logger.exception("Error while executing plan steps; falling back to chat: %s", e)
                                        self.display.print_planner_execution_error()
                        except Exception as e:
                            logger.debug(f"Planner invocation failed: {e}")
                            print(f"[ERROR] Planner invocation failed: {e}")

                    # Priority 2: Send to ECE_Core with streaming
                    sys.stdout.write("Assistant: ")
                    sys.stdout.flush()

                    full_response = ""
                    try:
                        async for chunk in self.send_message_streaming(user_input):
                            # Ensure chunk can be encoded to terminal encoding
                            try:
                                sys.stdout.write(chunk)
                                sys.stdout.flush()
                                full_response += chunk
                            except UnicodeEncodeError:
                                # Fallback: replace problematic characters
                                safe_chunk = chunk.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding)
                                sys.stdout.write(safe_chunk)
                                sys.stdout.flush()
                                full_response += safe_chunk
                    except asyncio.CancelledError:
                        # Handle cancellation gracefully
                        self.display.print_stream_cancelled()

                    print("\n")

                    # If the assistant produced a TOOL_CALL, try executing it via the plugin manager.
                    if self.plugin_manager and hasattr(self.plugin_manager, 'enabled') and self.plugin_manager.enabled:
                        try:
                            matches = self.input_handler.extract_tool_calls(full_response)
                            if matches:
                                match = matches[0]  # Get first match
                                tool_name_raw = match.group(1).strip()
                                params_str = match.group(2).strip()
                                parsed_tool_name, params = self.input_handler._parse_tool_call(tool_name_raw, params_str)
                                # If the tool name doesn't include a plugin prefix, resolve the owning plugin
                                if ":" in parsed_tool_name:
                                    plugin_call = parsed_tool_name
                                else:
                                    plugin_name = self.plugin_manager.lookup_plugin_for_tool(parsed_tool_name)
                                    plugin_call = f"{plugin_name}:{parsed_tool_name}" if plugin_name else parsed_tool_name

                                # If the plan/assistant explicitly returned 'none', skip execution
                                if self.input_handler.is_tool_skippable(parsed_tool_name):
                                    self.display.print_tool_skipped(parsed_tool_name, self.show_thinking)
                                else:
                                    self.display.print_tool_invocation(plugin_call, params, self.show_thinking)
                                    try:
                                        res = await self.plugin_manager.execute_tool(plugin_call, **params)
                                        # --- NEW: Feed result back to LLM ---
                                        self.display.print_tool_result_received()

                                        tool_followup_prompt = (
                                            f"The user asked: \"{user_input}\"\n\n"
                                            f"The tool '{plugin_call}' was executed and returned this result:\n"
                                            f"{json.dumps(res, indent=2) if isinstance(res, dict) else str(res)}\n\n"
                                            "Please use this information to answer the user's original question naturally."
                                        )

                                        sys.stdout.write("Assistant: ")
                                        sys.stdout.flush()
                                        # Stream the follow-up response to the user
                                        async for chunk in self.send_message_streaming(tool_followup_prompt):
                                            try:
                                                sys.stdout.write(chunk)
                                                sys.stdout.flush()
                                                full_response += chunk
                                            except UnicodeEncodeError:
                                                safe_chunk = chunk.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding)
                                                sys.stdout.write(safe_chunk)
                                                sys.stdout.flush()
                                                full_response += safe_chunk
                                    # ------------------------------------

                                    except Exception as e:
                                        self.display.print_tool_execution_error(str(e))
                        except Exception as e:
                            logger.debug(f"Tool call parsing/execution error: {e}")

                except KeyboardInterrupt:
                    self.display.print_exit_message()
                    break
                except EOFError:
                    self.display.print_exit_message()
                    break

        finally:
            # Clean shutdown
            self.stop_mcp_server()
            await self.client.aclose()

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\logic.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\main.py (Section: ANCHOR_PYTHON) ---

"""
Anchor - Lightweight Terminal CLI for ECE_Core
Personal cognitive command center (Copilot CLI style)
"""
import asyncio
import logging
import os
from typing import Optional

from anchor.display import AnchorDisplay
from anchor.input_handler import AnchorInput
from anchor.logic import AnchorLogic


class AnchorCLI:
    """Lightweight Copilot CLI-style interface for ECE_Core."""

    def __init__(
        self,
        ece_url: Optional[str] = None,
        session_id: Optional[str] = None,
        timeout: Optional[int] = None,
        create_prompt: bool = True
    ):
        # Initialize the display and input handler components
        self.display = AnchorDisplay(
            show_thinking=os.getenv("ANCHOR_SHOW_THINKING", "false").lower() in ("true", "1", "yes", "on"),
            stream_granularity=os.getenv("ANCHOR_STREAM_GRANULARITY", "auto").lower(),
            stream_sleep=float(os.getenv("ANCHOR_STREAM_SLEEP", "0"))
        )
        self.input_handler = AnchorInput()
        
        # Initialize the main logic component with dependencies
        self.logic = AnchorLogic(
            ece_url=ece_url,
            session_id=session_id,
            timeout=timeout,
            display=self.display,
            input_handler=self.input_handler,
            create_prompt=create_prompt
        )
        
        # Expose key properties for backward compatibility
        self.ece_url = self.logic.ece_url
        self.session_id = self.logic.session_id
        self.running = self.logic.running
        self.plugin_manager = self.logic.plugin_manager
        self.simple_handler = self.logic.simple_handler
        self.show_thinking = self.logic.show_thinking
        self.stream_granularity = self.logic.stream_granularity
        self.stream_sleep = self.logic.stream_sleep
        self.planner_enabled = self.logic.planner_enabled
        self.planner_length_threshold = self.logic.planner_length_threshold
        self.planner_keywords = self.logic.planner_keywords
        self.mcp_process = self.logic.mcp_process
        self.mcp_port = self.logic.mcp_port
        self.client = self.logic.client
        self.reconnect_attempts = self.logic.reconnect_attempts
        self.max_reconnect_attempts = self.logic.max_reconnect_attempts
        self.streaming_max_retries = self.logic.streaming_max_retries

    def start_plugins(self):
        """Start plugins via PluginManager (if enabled)."""
        return self.logic.start_plugins()

    def start_mcp_server(self):
        """Start embedded MCP server for tools"""
        return self.logic.start_mcp_server()

    def stop_mcp_server(self):
        """Stop MCP server on exit"""
        return self.logic.stop_mcp_server()

    async def check_connection(self) -> bool:
        """Verify ECE_Core is running."""
        return await self.logic.check_connection()

    async def send_message_streaming(self, message: str):
        """Send message and stream response token-by-token. Falls back to regular endpoint if streaming not available."""
        async for chunk in self.logic.send_message_streaming(message):
            yield chunk

    async def send_message_fallback(self, message: str):
        """Fallback to regular /chat endpoint."""
        async for chunk in self.logic.send_message_fallback(message):
            yield chunk

    def print_header(self):
        """Print header once on startup."""
        self.display.print_header(self.simple_handler, self.plugin_manager)

    async def run(self):
        """Main CLI loop."""
        await self.logic.run()

    def print_help(self):
        """Print help text."""
        self.display.print_help()

    def show_session_info(self):
        """Show current session information."""
        self.display.show_session_info(self.session_id, self.ece_url, self.mcp_process, self.mcp_port)

    async def show_recent_memories(self):
        """Show recent memories from ECE_Core."""
        await self.logic.show_recent_memories()

    async def show_tools(self):
        """Show available MCP tools."""
        await self.logic.show_tools()

    def _parse_tool_call(self, raw_tool_name: str, params_str: str):
        """Parse a TOOL_CALL invocation string."""
        return self.input_handler._parse_tool_call(raw_tool_name, params_str)

    def is_tool_skippable(self, raw_tool_name: str) -> bool:
        """Helper to decide whether a tool name should be skipped."""
        return self.input_handler.is_tool_skippable(raw_tool_name)

    def should_invoke_planner(self, user_input: str) -> bool:
        """Decide whether the Planner should be invoked for this input."""
        return self.input_handler.should_invoke_planner(
            user_input, 
            self.planner_enabled, 
            self.planner_length_threshold, 
            self.planner_keywords
        )


async def main():
    """Entry point."""
    cli = AnchorCLI()
    await cli.run()


def main_sync():
    """Synchronous entry point for console script."""
    asyncio.run(main())


if __name__ == "__main__":
    asyncio.run(main())

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\main.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\read_all.py (Section: ANCHOR_PYTHON) ---

import argparse
import os
import chardet
from typing import Iterable


def find_project_root(start_path: str | None = None) -> str:
    """
    Locate a project root by walking up from start_path to find files that indicate a repo root.
    Common indicators include: .git, pyproject.toml, package.json, README.md
    """
    if start_path is None:
        start_path = os.path.abspath(__file__)

    path = os.path.abspath(start_path)
    if os.path.isfile(path):
        path = os.path.dirname(path)

    root_indicators = (".git", "pyproject.toml", "package.json", "README.md")
    while True:
        if any(os.path.exists(os.path.join(path, ind)) for ind in root_indicators):
            return path
        parent = os.path.dirname(path)
        if parent == path:
            # reached filesystem root, fall back to cwd
            return os.getcwd()
        path = parent


def _is_binary_filename(filename: str) -> bool:
    binary_exts = (
        ".png",
        ".jpg",
        ".jpeg",
        ".gif",
        ".bmp",
        ".ico",
        ".zip",
        ".tar",
        ".gz",
        ".tgz",
        ".flac",
        ".mp3",
        ".mp4",
        ".pdf",
        ".exe",
        ".dll",
        ".class",
        ".so",
        ".o",
        ".pyc",
        ".whl",
        ".jar",
    )
    return filename.lower().endswith(binary_exts)


def create_full_corpus_recursive(
    root_dir_to_scan: str | None = None,
    output_file: str | None = None,
    include_extensions: Iterable[str] | None = None,
    exclude_dirs: Iterable[str] | None = None,
    exclude_files: Iterable[str] | None = None,
    dry_run: bool = False,
):
    """
    Aggregates all readable text content from a directory and its subdirectories
    into a single text corpus, correctly preserving special characters and emojis
    by auto-detecting file encodings and using robust pathing.
    """
    # Determine project root and defaults.
    project_root = find_project_root(os.path.abspath(__file__))
    root_dir_to_scan = root_dir_to_scan or project_root
    output_file = output_file or os.path.join(project_root, "combined_text.txt")

    print(f"Project Root Detected: {project_root}")
    print(f"Scanning Target Directory: {root_dir_to_scan}")

    default_include_extensions = (
        ".json",
        ".md",
        ".poml",
        ".yaml",
        ".txt",
        ".py",
        ".js",
        ".html",
        ".css",
        ".sh",
        ".ps1",
    )
    include_extensions = include_extensions or default_include_extensions
    default_exclude_dirs = {
        ".venv",
        "venv",
        ".git",
        ".vscode",
        "__pycache__",
        "node_modules",
        ".obsidian",
        "dist",
        "build",
        "target",
        "out",
        "public",
        "tmp",
        "temp",
        "htmlcov",
        ".pytest_cache",
        "logs",
        "archive",
        "backups",
        "db",
    }
    exclude_dirs = set(exclude_dirs or default_exclude_dirs)

    default_exclude_files = {
        "package-lock.json",
        "Pipfile.lock",
        "poetry.lock",
        "pyproject.toml",
        "package.json",
        "yarn.lock",
        ".env",
        ".env.example",
    }
    exclude_files = set(exclude_files or default_exclude_files)

    files_to_process = []
    for dirpath, dirnames, filenames in os.walk(root_dir_to_scan, topdown=True):
        dirnames[:] = [d for d in dirnames if d not in exclude_dirs]

        for f in filenames:
            # Skip binary file types and explicit excluded files
            if f in exclude_files:
                continue
            if _is_binary_filename(f):
                continue

            # Only include common source/documentation extensions by default
            if not any(f.endswith(ext) for ext in include_extensions):
                continue

            files_to_process.append(os.path.join(dirpath, f))

    files_to_process.sort()

    if not files_to_process:
        print(f"No processable files found in '{root_dir_to_scan}'.")
        return

    print(f"Found {len(files_to_process)} files to process.")

    if dry_run:
        print(f"Dry run enabled â€” would process {len(files_to_process)} files. First 10 files: \n{files_to_process[:10]}")
        return

    with open(output_file, "w", encoding="utf-8") as outfile:
        for file_path in files_to_process:
            if os.path.abspath(file_path) == os.path.abspath(output_file):
                continue

            print(f"Processing '{file_path}'...")
            try:
                with open(file_path, "rb") as raw_file:
                    raw_data = raw_file.read()
                    if not raw_data:
                        continue
                    # Use chardet to guess the encoding, but default to utf-8
                    encoding = chardet.detect(raw_data)["encoding"] or "utf-8"

                # Decode using the detected encoding, replacing any errors
                decoded_content = raw_data.decode(encoding, errors="replace")

                outfile.write(f"--- START OF FILE: {file_path} ---\n\n")
                outfile.write(decoded_content + "\n\n")
                outfile.write(f"--- END OF FILE: {file_path} ---\n\n")

            except Exception as e:
                print(f"An unexpected error occurred with file '{file_path}': {e}")

    print(f"\nCorpus aggregation complete. All content saved to '{output_file}'.")


def _parse_cli() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Aggregate textual source files into a single combined file.")
    p.add_argument("--root", "-r", help="Path to scan (project root by default)")
    p.add_argument(
        "--out",
        "-o",
        default=None,
        help="Output file path (defaults to combined_text.txt in project root)",
    )
    p.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be processed without writing the combined file",
    )
    p.add_argument(
        "--include-extensions",
        default=None,
        help="Comma-separated list of extensions to include (default is code & docs)",
    )
    p.add_argument(
        "--exclude-dirs",
        default=None,
        help="Comma-separated list of directories to exclude (relative names)",
    )
    p.add_argument(
        "--exclude-files",
        default=None,
        help="Comma-separated list of specific filenames to ignore (e.g. pyproject.toml)",
    )
    return p.parse_args()


if __name__ == "__main__":
    args = _parse_cli()
    include_extensions = None
    if args.include_extensions:
        include_extensions = [ext if ext.startswith(".") else "." + ext for ext in args.include_extensions.split(",")]
    exclude_dirs = None
    if args.exclude_dirs:
        exclude_dirs = [entry.strip() for entry in args.exclude_dirs.split(",") if entry.strip()]
    exclude_files = None
    if args.exclude_files:
        exclude_files = [entry.strip() for entry in args.exclude_files.split(",") if entry.strip()]

    create_full_corpus_recursive(
        root_dir_to_scan=args.root,
        output_file=args.out,
        include_extensions=include_extensions,
        exclude_dirs=exclude_dirs,
        exclude_files=exclude_files,
        dry_run=args.dry_run,
    )


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\read_all.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\simple_tool_mode.py (Section: ANCHOR_PYTHON) ---

"""
Simple Tool Mode for Small Models

Problem: Small models (<14B) struggle with complex tool calling formats and prompts.
Solution: Pattern-based direct execution - bypass LLM for obvious tool calls.

Design:
1. User says "list files" â†’ We IMMEDIATELY call filesystem_read(".")
2. User says "weather in Paris" â†’ We IMMEDIATELY call web_search("weather Paris")
3. NO complex prompting, NO asking model to format tool calls
4. Model only sees the RESULT and formats it nicely

This is like having a smart shell that understands natural language.
"""
import re
import os
import logging
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass
from tool_safety import get_safety_manager
import json

logger = logging.getLogger(__name__)


@dataclass
class SimpleToolIntent:
    """Simplified tool intent (no complex formatting needed)"""
    intent_type: str
    target: Any  # Path/query/command or dict for complex intents
    confidence: float
    raw_query: str


class SimpleToolMode:
    """
    Pattern-based tool execution for small models.
    
    Instead of asking the model to output TOOL_CALL: format(params),
    we detect patterns and execute directly, then give results to model.
    """
    
    def __init__(self):
        # HIGH-CONFIDENCE patterns (auto-execute without asking model)
        self.patterns = {
            "list_files": [
                (r'^(list|show|display|ls|dir)\s*(files?|directory|folder|contents?)?\s*(in\s+)?(.*)$', 4),
                (r'^what\s*(files?|is)\s*(are\s*)?(in|here|there)\s*(.*)$', 4),
                (r'^show\s+me\s+(the\s+)?files?\s*(in\s+)?(.*)$', 3),
                # More flexible patterns that allow extra words
                (r'.*\b(list|show|display)\s+(?:the\s+)?files?\s+(?:and\s+folders?\s+)?(?:in\s+(?:the\s+)?)?(.*)$', 2),
                (r'.*\b(list|show|display)\s+(?:all\s+)?(?:the\s+)?(?:files?\s+and\s+)?folders?\s+(?:in\s+(?:the\s+)?)?(.*)$', 2),
            ],
            "current_directory": [
                # Queries about current directory
                (r'.*what\s+(?:directory|folder|dir)\s+(?:are\s+we\s+in|am\s+i\s+in).*', 0),
                (r'.*(?:where|which)\s+(?:directory|folder|dir).*', 0),
                (r'.*current\s+(?:directory|folder|dir|path|location).*', 0),
                (r'.*pwd.*', 0),  # Unix command
            ],
            "read_file": [
                (r'^(read|cat|show|display|open)\s+(file\s+)?["\']?([^"\']+)["\']?$', 3),
                (r'^what\s*(is|\'s)\s*in\s+["\']?([^"\']+)["\']?\??$', 2),
            ],
            "search_web": [
                (r'^(search|google|find|lookup)\s+(for\s+)?(.+)$', 3),
                (r'^what\s*(is|\'s|are)\s+(.+)\??$', 2),
                (r'^(weather|temperature)\s+(in|for)\s+(.+)$', 3),
                (r'^(how|why|when|where)\s+(.+)\??$', 2),
                # More flexible patterns for conversational queries
                (r'.*\b(find\s+out|search\s+for|look\s+up|tell\s+me|show\s+me)\s+(?:about\s+)?(?:the\s+)?(.+)', 2),
                (r'.*\b(?:what\s+(?:is|\'s|are)|how\s+(?:do|does|is)|where\s+(?:is|are|can))\s+(.+)', 1),
                (r'.*\b(weather|temperature|forecast)\s+(?:in|for|at)\s+(.+)', 2),
            ],
            "run_command": [
                (r'^run\s+(command\s+)?["\']?(.+)["\']?$', 2),
                (r'^execute\s+["\']?(.+)["\']?$', 1),
            ],
            "write_file": [
                (r'^(create|make|write|save)\s+(file\s+)?["\']?([^"\']+)["\']?\s*(with|containing|that\s+has)\s+(.+)$', 0),
                (r'^(append)\s+(to\s+)?["\']?([^"\']+)["\']?\s*(with|:)?\s+(.+)$', 0),
            ],
            "code_search": [
                (r'^(search\s+code|find)\s+(for\s+)?(.+?)\s+(in|under)\s+(.+)$', 0),
                (r'^search\s+code\s+for\s+(.+)$', 1),
                (r'^(search\s+code|find)\s+(?:for\s+)?(.+?)\s+(?:in|under)\s+(.+?)\s+with\s+glob\s+(.+?)\s+and\s+context\s+(\d+)$', 0),
            ],
            "code_grep": [
                (r'^grep\s+(?:for\s+)?(.+?)\s+(?:in|under)\s+(.+?)(.*)$', 0),
            ],
        }
    
    def detect_intent(self, query: str) -> Optional[SimpleToolIntent]:
        """
        Detect tool intent from natural language query.
        
        Args:
            query: User's query
            
        Returns:
            SimpleToolIntent or None if no pattern matches
        """
        query_clean = query.strip()
        
        priority = [
            "write_file",
            "code_grep",
            "code_search",
            "list_files",
            "current_directory",
            "read_file",
            "run_command",
            "search_web",
        ]
        for intent_type in priority:
            pattern_list = self.patterns.get(intent_type, [])
            for pattern, target_group in pattern_list:
                match = re.match(pattern, query_clean, re.IGNORECASE)
                if match:
                    # Special handling for complex intents
                    if intent_type in ["write_file", "code_search", "code_grep"]:
                        groups = match.groups()
                        target = self._extract_complex_target(intent_type, groups)
                    else:
                        target = match.group(target_group).strip() if target_group <= len(match.groups()) else ""
                    
                    # Post-process target based on intent
                    target = self._clean_target(intent_type, target)
                    groups = match.groups()
                    logger.info(
                        {
                            "event": "simple_intent_match",
                            "intent": intent_type,
                            "target": target,
                            "raw_query": query,
                            "groups": len(groups),
                        }
                    )
                    
                    return SimpleToolIntent(
                        intent_type=intent_type,
                        target=target,
                        confidence=0.9,
                        raw_query=query
                    )
        
        return None

    def _extract_complex_target(self, intent_type: str, groups: tuple) -> Any:
        if intent_type == "write_file":
            # Patterns produce groups with filename and content
            if len(groups) >= 5:
                # create|write â€¦ file <name> â€¦ with <content>
                filename = groups[2]
                content = groups[4]
                return {"path": filename.strip().strip('"\''), "content": content.strip(), "append": False}
            if len(groups) >= 5 and groups[0] == "append":
                filename = groups[2]
                content = groups[4]
                return {"path": filename.strip().strip('"\''), "content": content.strip(), "append": True}
        elif intent_type == "code_search":
            # grep/search code/find for <query> in <root>
            if len(groups) == 5:
                return {"query": groups[2].strip(), "root": groups[4].strip(), "regex": False}
            if len(groups) == 4:
                return {"query": groups[1].strip(), "root": groups[2].strip(), "glob": groups[3].strip(), "context": 2, "regex": False}
            if len(groups) == 5:
                return {"query": groups[1].strip(), "root": groups[2].strip(), "glob": groups[3].strip(), "context": int(groups[4]), "regex": False}
            if len(groups) >= 2:
                return {"query": groups[1].strip(), "root": ".", "regex": False}
        elif intent_type == "code_grep":
            q = {"query": groups[0].strip(), "root": groups[1].strip(), "regex": False}
            tail = groups[2] if len(groups) >= 3 else ""
            if tail:
                m_glob = re.search(r'with\s+glob\s+([^\s]+)', tail, re.IGNORECASE)
                if m_glob:
                    q["glob"] = m_glob.group(1).strip()
                m_ctx = re.search(r'context\s+(\d+)', tail, re.IGNORECASE)
                if m_ctx:
                    try:
                        q["context"] = int(m_ctx.group(1))
                    except:
                        q["context"] = 2
                m_ex = re.search(r'exclude\s+([^\s]+)', tail, re.IGNORECASE)
                if m_ex:
                    q["exclude_globs"] = [s.strip() for s in m_ex.group(1).split(',')]
            return q
        return {}
    
    def _clean_target(self, intent_type: str, target: Any) -> Any:
        """Clean and normalize target based on intent type"""
        if isinstance(target, dict):
            return target
        target = target.strip()

        if intent_type == "list_files":
            # Default to current directory if empty
            if not target or target in ["here", "there", "this"]:
                return "."

            # Remove common filler words
            target = re.sub(r'^(the|in|this|that|all|any)\s+', '', target, flags=re.IGNORECASE)

            # Extract path from "current directory" or similar phrases
            if "current" in target.lower() or "currenty" in target.lower():
                return "."

            # Extract actual path from "files and folders in <path>"
            path_match = re.search(r'(?:in|from)\s+(.+)$', target, re.IGNORECASE)
            if path_match:
                path = path_match.group(1).strip()
                # Clean up the path
                path = re.sub(r'^(the|this|that)\s+', '', path, flags=re.IGNORECASE)
                if path.lower() in ["current directory", "here", "there", "."]:
                    return "."

                # Validate path to prevent malformed patterns
                if '..->' in path or '->' in path.replace('.', ''):
                    return "."
                return path or "."

            # If it looks like garbage from regex capture, default to current dir
            if any(word in target.lower() for word in ["files", "folders", "directory", "folder", "file"]):
                return "."

            # Validate path to prevent malformed patterns
            if '..->' in target or '->' in target.replace('.', ''):
                return "."
            return target or "."
        
        elif intent_type == "read_file":
            # Remove quotes
            target = target.strip('"\'')
            # Validate path to prevent malformed patterns
            if '..->' in target or '->' in target.replace('.', ''):
                return "."
            return target
        
        elif intent_type == "search_web":
            # Remove question words at start
            target = re.sub(r'^(what|how|why|when|where)\s+(is|are|was|were|\'s|about|the)\s+', '', target, flags=re.IGNORECASE)
            
            # Extract core query from conversational patterns
            # "could you find out what the weather in Bernalillo will be tomorrow" â†’ "weather in Bernalillo tomorrow"
            target = re.sub(r'\b(will|would|could|should|can|may|might)\s+be\b', '', target, flags=re.IGNORECASE)
            target = re.sub(r'\b(is|are|was|were)\s+going\s+to\s+be\b', '', target, flags=re.IGNORECASE)
            target = re.sub(r'^(the|a|an)\s+', '', target, flags=re.IGNORECASE)
            
            # Remove trailing question mark
            target = target.rstrip('?')
            return target.strip()
        
        elif intent_type == "run_command":
            # Remove quotes
            target = target.strip('"\'')
            return target
        elif intent_type == "write_file":
            return target
        elif intent_type == "code_search":
            # If target is a dict (complex params), validate the root
            if isinstance(target, dict):
                root = target.get("root", ".")
                if '..->' in root or '->' in root.replace('.', ''):
                    target["root"] = "."
            elif '..->' in target or '->' in target.replace('.', ''):
                target = "."
            return target

        # Validate target to prevent malformed patterns in any remaining cases
        if isinstance(target, str) and ('..->' in target or '->' in target.replace('.', '')):
            return "."
        # If target is a dict, validate any path-like fields within it
        elif isinstance(target, dict):
            for key, value in target.items():
                if isinstance(value, str) and key in ['root', 'path'] and ('..->' in value or '->' in value.replace('.', '')):
                    target[key] = "."
        return target
    
    def map_to_tool_call(self, intent: SimpleToolIntent) -> Tuple[str, Dict[str, Any]]:
        """
        Map simple intent to actual MCP tool call.
        
        Args:
            intent: SimpleToolIntent
            
        Returns:
            (tool_name, parameters) tuple
        """
        if intent.intent_type == "list_files":
            return ("filesystem_read", {"path": intent.target})
        
        elif intent.intent_type == "current_directory":
            # Use shell command to get current directory
            return ("shell_execute", {"command": "pwd" if os.name != 'nt' else "cd"})
        
        elif intent.intent_type == "read_file":
            return ("filesystem_read", {"path": intent.target})
        
        elif intent.intent_type == "search_web":
            return ("web_search", {"query": intent.target, "max_results": 5})
        
        elif intent.intent_type == "run_command":
            return ("shell_execute", {"command": intent.target})
        elif intent.intent_type == "write_file":
            params = intent.target if isinstance(intent.target, dict) else {"path": str(intent.target), "content": ""}
            return ("filesystem_write", params)
        elif intent.intent_type == "code_search":
            params = intent.target if isinstance(intent.target, dict) else {"query": str(intent.target), "root": "."}
            # Validate root path to prevent malformed patterns
            if "root" in params and isinstance(params["root"], str):
                root = params["root"]
                if '..->' in root or '->' in root.replace('.', ''):
                    params["root"] = "."
            if "max_results" not in params:
                params["max_results"] = 20
            return ("code_search", params)
        elif intent.intent_type == "code_grep":
            params = intent.target if isinstance(intent.target, dict) else {"query": str(intent.target), "root": "."}
            # Validate root path to prevent malformed patterns
            if "root" in params and isinstance(params["root"], str):
                root = params["root"]
                if '..->' in root or '->' in root.replace('.', ''):
                    params["root"] = "."
            if "max_results" not in params:
                params["max_results"] = 50
            if "context" not in params:
                params["context"] = 2
            return ("code_grep", params)
        
        raise ValueError(f"Unknown intent type: {intent.intent_type}")


class SimpleToolHandler:
    """
    Handler for simple tool mode execution.
    
    This is the magic: instead of asking the model to call tools,
    we detect patterns and execute tools FIRST, then give results
    to the model for natural language formatting.
    
    Usage:
        handler = SimpleToolHandler(mcp_client, llm_client)
        
        # Check if we should intercept
        if handler.can_handle_directly(user_query):
            response = await handler.handle_query(user_query)
            # Done! No complex prompting needed.
        else:
            # Fall back to normal LLM flow
    """
    
    def __init__(self, mcp_client, llm_client):
        self.mcp_client = mcp_client
        self.llm_client = llm_client
        self.mode = SimpleToolMode()
        self._fs_cache = []  # list[(key, value)] LRU
        self._web_cache = []
        self._fs_cache_size = 64
        self._web_cache_size = 64
        self._approval_cache = set()
    
    def can_handle_directly(self, query: str) -> bool:
        """Check if we can handle this query with direct tool execution"""
        intent = self.mode.detect_intent(query) or self._heuristic_intent(query)
        return intent is not None
    
    async def handle_query(self, query: str, session_id: str = None) -> str:
        """
        Handle query with direct tool execution.
        
        Flow:
        1. Detect intent
        2. Execute tool IMMEDIATELY (no LLM prompt)
        3. Give result to LLM with SIMPLE formatting prompt
        4. Return formatted response
        
        Args:
            query: User's natural language query
            session_id: Optional session ID for context
            
        Returns:
            Formatted response
        """
        # Step 1: Detect intent
        intent = self.mode.detect_intent(query) or self._heuristic_intent(query)
        
        if not intent:
            return None  # Fall back to normal flow
        
        logger.info(f"ðŸš€ Simple mode: {intent.intent_type} â†’ {intent.target}")
        
        # Step 2: Execute tool IMMEDIATELY
        try:
            tool_name, params = self.mode.map_to_tool_call(intent)
            
            logger.info(f"ðŸ”§ Direct execution: {tool_name}({params})")
            # LRU caches for common tools
            result = None
            if tool_name == "filesystem_read":
                key = f"fs::{params.get('path','')}"
                cached = self._lru_get(self._fs_cache, key)
                if cached is not None:
                    result = {"tool": tool_name, "status": "success", "result": cached}
            elif tool_name == "web_search":
                key = f"web::{params.get('query','')}::{params.get('max_results',5)}"
                cached = self._lru_get(self._web_cache, key)
                if cached is not None:
                    result = {"tool": tool_name, "status": "success", "result": cached}
            
            if result is None:
                safety = get_safety_manager()
                auto = safety.should_auto_execute(tool_name, params)
                cache_key = f"{tool_name}:{json.dumps(params, sort_keys=True)}"
                if not auto and cache_key not in self._approval_cache:
                    prompt = safety.get_confirmation_prompt(tool_name, params)
                    resp = input(prompt).strip().lower()
                    if resp not in ["y", "yes"]:
                        return "Tool execution denied"
                    self._approval_cache.add(cache_key)
                result = await self.mcp_client.call_tool(tool_name, **params)
                # Cache successful results
                if isinstance(result, dict) and result.get("status") == "success":
                    payload = result.get("result", result)
                    if tool_name == "filesystem_read":
                        self._lru_put(self._fs_cache, key, payload, self._fs_cache_size)
                    elif tool_name == "web_search":
                        self._lru_put(self._web_cache, key, payload, self._web_cache_size)
            
            # Step 3: Give result to LLM with MINIMAL prompt
            formatted = await self._format_with_llm(intent, result, query)
            
            logger.info(
                {
                    "event": "simple_intent_result",
                    "intent": intent.intent_type,
                    "tool": tool_name,
                    "params": params,
                    "result_keys": list(result.keys()) if isinstance(result, dict) else None,
                }
            )
            return formatted
            
        except Exception as e:
            logger.error(f"Simple mode execution failed: {e}")
            # Return error formatted nicely
            return await self._format_error(intent, str(e))

    def _heuristic_intent(self, query: str) -> Optional[SimpleToolIntent]:
        q = query.strip()
        lower = q.lower()
        # Obvious file reads from extension
        for ext in [".py", ".md", ".txt", ".json", ".yaml", ".yml", ".toml", ".ini", ".cfg"]:
            if ext in lower and ("read" in lower or "show" in lower or "open" in lower or lower.endswith(ext)):
                return SimpleToolIntent("read_file", q, 0.7, query)
        # Code search
        if any(k in lower for k in ["grep", "search code", "find in code", "where is"]):
            return SimpleToolIntent("code_search", {"query": q, "root": ".", "regex": False}, 0.6, query)
        # Run command heuristics
        shell_starters = ["git ", "npm ", "node ", "python ", "pip ", "ls ", "dir ", "echo ", "type "]
        if any(lower.startswith(s) for s in shell_starters):
            return SimpleToolIntent("run_command", q, 0.6, query)
        # Write file hints
        if any(k in lower for k in ["create file", "write file", "save file", "append to"]):
            # attempt to extract: filename after keyword
            m = re.search(r'(?:create|write|save)\s+file\s+["\']?([^"\']+)["\']?', lower)
            if m:
                return SimpleToolIntent("write_file", {"path": m.group(1), "content": "", "append": False}, 0.5, query)
        # Web search fallback
        if lower.endswith("?") or any(k in lower for k in ["what is", "how do", "weather", "news"]):
            return SimpleToolIntent("search_web", q, 0.5, query)
        return None

    def _lru_get(self, cache, key):
        for i, (k, v) in enumerate(cache):
            if k == key:
                cache.insert(0, cache.pop(i))
                return v
        return None

    def _lru_put(self, cache, key, value, max_size):
        cache.insert(0, (key, value))
        i = 1
        while i < len(cache):
            if cache[i][0] == key:
                cache.pop(i)
            else:
                i += 1
        while len(cache) > max_size:
            cache.pop()
    
    async def _format_with_llm(self, intent: SimpleToolIntent, result: Any, original_query: str) -> str:
        """
        Use LLM to format the tool result nicely.
        
        This is a MUCH simpler task than "decide what tool to call and format it correctly".
        Even a 4B model can format results well.
        """
        # Build simple formatting prompt
        if intent.intent_type in ["list_files", "read_file"]:
            if isinstance(result, dict) and "entries" in result:
                # Directory listing
                files = result["entries"]
                files_list = "\n".join([f"  {f}" for f in files[:30]])
                
                prompt = f"""The user asked: "{original_query}"

I checked and found these files:
{files_list}

Please format this as a natural, helpful response. Be concise."""
            
            elif isinstance(result, dict) and "content" in result:
                lines = result["content"].splitlines()
                preview_lines = "\n".join(lines[:30])
                prompt = f"""The user asked: "{original_query}"

Here are the first lines:
{preview_lines}

Please format this as a natural, helpful response. If it's code, mention what it is."""
            
            else:
                prompt = f"""The user asked: "{original_query}"

Result: {result}

Please format this as a natural response."""
        
        elif intent.intent_type == "search_web":
            if isinstance(result, dict) and "results" in result:
                results = result["results"][:3]
                results_text = "\n\n".join([
                    f"{i+1}. {r.get('title', 'Result')}\n   {r.get('snippet', 'No description')}"
                    for i, r in enumerate(results)
                ])
                
                prompt = f"""The user asked: "{original_query}"

I found these search results:
{results_text}

Please summarize this information naturally and helpfully. Be concise."""
            else:
                prompt = f"""The user asked: "{original_query}"

Search result: {result}

Please format this as a natural response."""
        
        elif intent.intent_type == "run_command":
            if isinstance(result, dict) and "output" in result:
                output = result["output"][:500]
                
                prompt = f"""The user asked to run: "{intent.target}"

Command output:
{output}

Please format this as a natural response, explaining what the command did."""
            else:
                prompt = f"""The user asked: "{original_query}"

Command result: {result}

Please format this as a natural response."""
        
        else:
            prompt = f"""The user asked: "{original_query}"

Result: {result}

Please format this as a natural, helpful response."""
        
        # Generate with simple prompt
        try:
            formatted = await self.llm_client.generate(
                prompt=prompt,
                system_prompt="You are a helpful assistant. Format the provided information in a clear, natural way. Be concise and direct."
            )
            return formatted.strip()
        except Exception as e:
            logger.error(f"LLM formatting failed: {e}")
            # Fallback to direct formatting
            return self._format_directly(intent, result)
    
    def _format_directly(self, intent: SimpleToolIntent, result: Any) -> str:
        """Fallback: format result without LLM"""
        if intent.intent_type in ["list_files", "read_file"]:
            # Check for MCP server response wrapper
            if isinstance(result, dict):
                if "result" in result and isinstance(result["result"], dict):
                    # Unwrap MCP response
                    result = result["result"]
                
                if "entries" in result:
                    # Directory listing
                    files = result["entries"]
                    formatted = "\n".join([f"  â€¢ {f}" for f in files[:20]])
                    return f"Files in {intent.target}:\n{formatted}"
                elif "content" in result:
                    lines = result["content"].splitlines()
                    preview = "\n".join(lines[:20])
                    return f"Content preview of {intent.target}:\n\n{preview}"
        
        elif intent.intent_type == "search_web":
            # Check for MCP wrapper
            if isinstance(result, dict):
                if "result" in result and isinstance(result["result"], dict):
                    result = result["result"]
                
                if "results" in result:
                    results = result["results"][:3]
                    formatted = "\n\n".join([
                        f"**{r.get('title', 'Result')}**\n{r.get('snippet', 'No description')}"
                        for r in results
                    ])
                    return f"Search results for '{intent.target}':\n\n{formatted}"
                else:
                    # DuckDuckGo stub response
                    return f"Web search for '{intent.target}' completed.\n{result.get('message', str(result))}"
        
        elif intent.intent_type == "run_command":
            if isinstance(result, dict):
                if "result" in result and isinstance(result["result"], dict):
                    result = result["result"]
                
                if "output" in result:
                    output = result["output"][:500]
                    return f"Command output:\n{output}"
        
        elif intent.intent_type == "current_directory":
            # Current directory query
            if isinstance(result, dict):
                if "result" in result and isinstance(result["result"], dict):
                    result = result["result"]
                
                if "output" in result:
                    return f"Current directory: {result['output'].strip()}"
        
        # Default: Just stringify
        return str(result)
    
    async def _format_error(self, intent: SimpleToolIntent, error: str) -> str:
        """Format error message"""
        return f"I tried to {intent.intent_type.replace('_', ' ')} but encountered an error: {error}"


# Test suite
if __name__ == "__main__":
    import asyncio
    
    # Test pattern matching
    mode = SimpleToolMode()
    
    test_queries = [
        "list files",
        "list files in current directory",
        "show me the files here",
        "what files are in .",
        "ls -la",
        "dir",
        "read file test.txt",
        "show me config.yaml",
        "what's in README.md?",
        "search for python async tutorial",
        "what is the weather in Paris?",
        "how do I use async in Python?",
        "run command ls -la",
        "execute pwd",
    ]
    
    print("=" * 60)
    print("Simple Tool Mode - Pattern Test")
    print("=" * 60)
    
    for query in test_queries:
        intent = mode.detect_intent(query)
        
        print(f"\nQuery: {query}")
        if intent:
            print(f"  âœ“ Intent: {intent.intent_type}")
            print(f"  âœ“ Target: '{intent.target}'")
            tool_name, params = mode.map_to_tool_call(intent)
            print(f"  âœ“ Tool: {tool_name}({params})")
        else:
            print(f"  âœ— No pattern matched")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\simple_tool_mode.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tool_execution.py (Section: ANCHOR_PYTHON) ---

"""
Tool Execution Manager (Copilot CLI Pattern)
Implements the GitHub Copilot CLI tool approval flow:
1. Model emits tool_use block
2. CLI shows proposed tool call
3. User approves/denies
4. Tool executes (if approved)
5. tool_result returned to model

Based on: github/copilot-cli architecture
"""
import logging
import json
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

from tool_safety import get_safety_manager, ToolSafety

logger = logging.getLogger(__name__)


class ToolExecutionStatus(Enum):
    """Status of tool execution request"""
    PENDING = "pending"      # Awaiting user approval
    APPROVED = "approved"    # User approved, ready to execute
    DENIED = "denied"        # User denied
    EXECUTED = "executed"    # Successfully executed
    FAILED = "failed"        # Execution failed
    ORPHANED = "orphaned"    # No tool_result paired with tool_use


@dataclass
class ToolCall:
    """Represents a tool call request from the model"""
    tool_name: str
    parameters: Dict[str, Any]
    call_id: str  # Unique ID to pair tool_use with tool_result
    status: ToolExecutionStatus = ToolExecutionStatus.PENDING
    result: Optional[Any] = None
    error: Optional[str] = None
    
    def to_tool_use_block(self) -> Dict[str, Any]:
        """Convert to tool_use message block for model"""
        return {
            "type": "tool_use",
            "id": self.call_id,
            "name": self.tool_name,
            "input": self.parameters
        }
    
    def to_tool_result_block(self) -> Dict[str, Any]:
        """Convert to tool_result message block for model"""
        if self.status == ToolExecutionStatus.DENIED:
            return {
                "type": "tool_result",
                "tool_use_id": self.call_id,
                "is_error": True,
                "content": "Tool execution denied by user"
            }
        elif self.status == ToolExecutionStatus.FAILED:
            return {
                "type": "tool_result",
                "tool_use_id": self.call_id,
                "is_error": True,
                "content": self.error or "Tool execution failed"
            }
        else:
            return {
                "type": "tool_result",
                "tool_use_id": self.call_id,
                "content": self.result
            }


class ToolExecutionManager:
    """
    Manages tool execution with Copilot CLI pattern:
    - Parses tool_use blocks from model output
    - Presents tool calls for user approval
    - Executes approved tools
    - Returns tool_result blocks to model
    """
    
    def __init__(self):
        self.safety_manager = get_safety_manager()
        self.pending_calls: List[ToolCall] = []
        self.parallel_execution_enabled = True  # Can be disabled via flag
        
    def parse_tool_use_from_response(self, model_output: str) -> List[ToolCall]:
        """
        Parse tool_use blocks from model response
        
        Expected format (examples):
        1. Simple: TOOL_CALL: tool_name(arg1="value1", arg2="value2")
        2. JSON: {"tool_use": {"name": "tool_name", "input": {...}}}
        3. Anthropic-style: <tool_use>...</tool_use>
        
        Returns list of ToolCall objects
        """
        tool_calls = []
        
        # Pattern 1: TOOL_CALL: syntax (simple)
        if "TOOL_CALL:" in model_output:
            # Parse simple tool call format
            import re
            pattern = r'TOOL_CALL:\s*(\w+)\((.*?)\)'
            matches = re.findall(pattern, model_output, re.DOTALL)
            
            for i, (tool_name, args_str) in enumerate(matches):
                try:
                    # Parse arguments
                    params = self._parse_tool_arguments(args_str)
                    call_id = f"call_{tool_name}_{i}"
                    
                    tool_calls.append(ToolCall(
                        tool_name=tool_name,
                        parameters=params,
                        call_id=call_id
                    ))
                except Exception as e:
                    logger.warning(f"Failed to parse tool call: {e}")
        
        # Pattern 2: JSON blocks
        try:
            # Look for JSON tool_use blocks
            json_match = re.search(r'\{.*"tool_use".*\}', model_output, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(0))
                if "tool_use" in data:
                    tool_use = data["tool_use"]
                    tool_calls.append(ToolCall(
                        tool_name=tool_use["name"],
                        parameters=tool_use.get("input", {}),
                        call_id=tool_use.get("id", f"call_{len(tool_calls)}")
                    ))
        except Exception as e:
            logger.debug(f"No JSON tool_use found: {e}")
        
        return tool_calls
    
    def _parse_tool_arguments(self, args_str: str) -> Dict[str, Any]:
        """Parse tool arguments from string format"""
        params = {}
        
        # Simple key=value parsing
        import re
        arg_pattern = r'(\w+)=(["\'])(.*?)\2'
        matches = re.findall(arg_pattern, args_str)
        
        for key, _, value in matches:
            params[key] = value
        
        return params
    
    def present_tool_call_for_approval(self, tool_call: ToolCall) -> str:
        """
        Generate approval prompt for tool call
        Mimics Copilot CLI's tool preview UI
        """
        safety = self.safety_manager.categorize_tool(tool_call.tool_name)
        
        # Build approval prompt
        prompt = "\n" + "â”€" * 60 + "\n"
        prompt += f"ðŸ”§ Tool Call Request\n"
        prompt += "â”€" * 60 + "\n"
        
        # Tool info
        prompt += f"Tool: {tool_call.tool_name}\n"
        prompt += f"Safety: {safety.value.upper()}\n"
        prompt += f"ID: {tool_call.call_id}\n"
        
        # Parameters
        prompt += "\nParameters:\n"
        for key, value in tool_call.parameters.items():
            # Truncate long values
            value_str = str(value)
            if len(value_str) > 100:
                value_str = value_str[:100] + "..."
            prompt += f"  â€¢ {key}: {value_str}\n"
        
        # Safety warnings
        if safety == ToolSafety.DANGEROUS:
            prompt += "\nâš ï¸�  WARNING: This tool performs dangerous operations\n"
            
            # Special handling for shell commands
            if tool_call.tool_name == "shell_execute":
                command = tool_call.parameters.get("command", "")
                is_safe, warning = self.safety_manager.sanitize_shell_command(command)
                
                if not is_safe:
                    prompt += f"ðŸš¨ DANGER: {warning}\n"
                elif warning:
                    prompt += f"âš ï¸�  {warning}\n"
        
        prompt += "\n" + "â”€" * 60 + "\n"
        prompt += "Approve this tool call? [y/n/a=allow all]: "
        
        return prompt
    
    def approve_tool_call(self, tool_call: ToolCall, approved: bool):
        """Mark tool call as approved or denied"""
        if approved:
            tool_call.status = ToolExecutionStatus.APPROVED
            logger.info(f"Tool call approved: {tool_call.tool_name}")
        else:
            tool_call.status = ToolExecutionStatus.DENIED
            logger.info(f"Tool call denied: {tool_call.tool_name}")
        
        # Log for audit trail
        self.safety_manager.log_tool_execution(
            tool_call.tool_name,
            tool_call.parameters,
            approved
        )
    
    async def execute_tool_call(self, tool_call: ToolCall, mcp_client=None) -> ToolCall:
        """
        Execute approved tool call
        
        Args:
            tool_call: The ToolCall to execute
            mcp_client: Optional MCP client for calling MCP tools
        
        Returns:
            Updated ToolCall with result or error
        """
        if tool_call.status != ToolExecutionStatus.APPROVED:
            tool_call.error = "Tool call not approved"
            tool_call.status = ToolExecutionStatus.FAILED
            return tool_call
        
        try:
            logger.info(f"Executing tool: {tool_call.tool_name}")
            
            # Check if tool is allowed
            if not self.safety_manager.is_allowed(tool_call.tool_name):
                tool_call.error = f"Tool '{tool_call.tool_name}' is blocked"
                tool_call.status = ToolExecutionStatus.FAILED
                return tool_call
            
            # Execute via MCP client if available
            if mcp_client:
                result = await self._execute_via_mcp(tool_call, mcp_client)
            else:
                # Fallback: try local execution
                result = await self._execute_local_tool(tool_call)
            
            tool_call.result = result
            tool_call.status = ToolExecutionStatus.EXECUTED
            logger.info(f"Tool executed successfully: {tool_call.tool_name}")
            
        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            tool_call.error = str(e)
            tool_call.status = ToolExecutionStatus.FAILED
        
        return tool_call
    
    async def _execute_via_mcp(self, tool_call: ToolCall, mcp_client) -> Any:
        """Execute tool via MCP server"""
        # Call MCP server endpoint
        response = await mcp_client.call_tool(
            tool_call.tool_name,
            tool_call.parameters
        )
        return response
    
    async def _execute_local_tool(self, tool_call: ToolCall) -> Any:
        """Execute tool locally (fallback)"""
        # This would implement local tool execution
        # For now, raise error
        raise NotImplementedError(f"Local execution not implemented for {tool_call.tool_name}")
    
    def get_orphaned_tool_calls(self) -> List[ToolCall]:
        """Get tool calls that have no paired tool_result"""
        return [tc for tc in self.pending_calls if tc.status == ToolExecutionStatus.PENDING]
    
    def cleanup_orphaned_calls(self):
        """Mark orphaned tool calls as such (on session abort)"""
        for tc in self.get_orphaned_tool_calls():
            tc.status = ToolExecutionStatus.ORPHANED
            logger.warning(f"Orphaned tool call cleaned up: {tc.call_id}")


# Global instance
_execution_manager: Optional[ToolExecutionManager] = None


def get_execution_manager() -> ToolExecutionManager:
    """Get global ToolExecutionManager instance"""
    global _execution_manager
    if _execution_manager is None:
        _execution_manager = ToolExecutionManager()
    return _execution_manager


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tool_execution.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tool_safety.py (Section: ANCHOR_PYTHON) ---

"""
Tool Safety Manager (T-101)
Implements tool sandboxing and safety checks for Anchor

Features:
- Categorizes tools as SAFE or DANGEROUS
- Requires confirmation for dangerous tools
- Input sanitization for shell commands
- Configurable via .env
"""
import re
import os
import logging
from typing import List, Dict, Any, Optional
from enum import Enum

logger = logging.getLogger(__name__)


class ToolSafety(Enum):
    """Tool safety categories"""
    SAFE = "safe"           # Can auto-execute
    DANGEROUS = "dangerous"  # Requires confirmation
    BLOCKED = "blocked"      # Never execute


class ToolSafetyManager:
    """Manages tool execution safety and sandboxing"""
    
    def __init__(self):
        # Default safe tools (read-only operations)
        self.safe_tools = set(os.getenv(
            "SAFE_TOOLS",
            "filesystem_read,filesystem_list_directory,web_search,websearch_search_web,websearch_fetch_url"
        ).split(","))
        
        # Default dangerous tools (write/execute operations)
        self.dangerous_tools = set(os.getenv(
            "DANGEROUS_TOOLS", 
            "shell_execute,filesystem_write,filesystem_write_file"
        ).split(","))
        
        # Blocked tools (never allowed)
        self.blocked_tools = set(os.getenv(
            "BLOCKED_TOOLS",
            ""  # Empty by default
        ).split(",")) - {""}  # Remove empty string
        
        # Configuration
        self.auto_execute_enabled = os.getenv("AUTO_TOOL_EXECUTION", "true").lower() == "true"
        self.confirmation_required = os.getenv("TOOL_CONFIRMATION_REQUIRED", "false").lower() == "true"
        
        # Shell command sanitization patterns
        self.dangerous_shell_patterns = [
            r"rm\s+-rf",           # Dangerous delete
            r">(>?)\s*/dev/",      # Redirect to device
            r";\s*rm\s+",          # Chained delete
            r"\|\s*sudo",          # Piped sudo
            r"mkfs",               # Format filesystem
            r"dd\s+if=",           # Disk operations
            r"wget.*\|\s*sh",      # Download and execute
            r"wget.*\|\s*bash",    # Download and execute
            r"curl.*\|\s*sh",      # Download and execute
            r"curl.*\|\s*bash",    # Download and execute
            r"nc\s+.*-e",          # Netcat backdoor
            r"eval\s*\(",          # Code evaluation
        ]

        # Strict Whitelist (CRITIQUE-DRIVEN)
        self.strict_whitelist_enabled = os.getenv("SHELL_STRICT_WHITELIST_ENABLED", "true").lower() == "true"
        self.shell_whitelist = set(os.getenv(
            "SHELL_ALLOWED_COMMANDS",
            "ls,dir,echo,cat,grep,find,pwd,cd,mkdir,git,npm,uv,python,node"
        ).split(","))
        
        logger.info(f"Tool Safety Manager initialized:")
        logger.info(f"  Safe tools: {len(self.safe_tools)}")
        logger.info(f"  Dangerous tools: {len(self.dangerous_tools)}")
        logger.info(f"  Blocked tools: {len(self.blocked_tools)}")
        logger.info(f"  Auto-execute: {self.auto_execute_enabled}")
        logger.info(f"  Confirmation required: {self.confirmation_required}")
    
    def categorize_tool(self, tool_name: str) -> ToolSafety:
        """Categorize a tool by safety level"""
        if tool_name in self.blocked_tools:
            return ToolSafety.BLOCKED
        elif tool_name in self.dangerous_tools:
            return ToolSafety.DANGEROUS
        elif tool_name in self.safe_tools:
            return ToolSafety.SAFE
        else:
            # Unknown tools are treated as dangerous by default
            logger.warning(f"Unknown tool '{tool_name}' - treating as DANGEROUS")
            return ToolSafety.DANGEROUS
    
    def requires_confirmation(self, tool_name: str) -> bool:
        """Check if tool requires user confirmation before execution"""
        if self.confirmation_required:
            # All tools require confirmation if globally enabled
            return True
        
        category = self.categorize_tool(tool_name)
        
        if category == ToolSafety.BLOCKED:
            return True  # Will be blocked anyway
        elif category == ToolSafety.DANGEROUS:
            return True  # Always confirm dangerous tools
        else:
            return False  # Safe tools can auto-execute
    
    def is_allowed(self, tool_name: str) -> bool:
        """Check if tool is allowed to execute at all"""
        return self.categorize_tool(tool_name) != ToolSafety.BLOCKED
    
    def sanitize_shell_command(self, command: str) -> tuple[bool, Optional[str]]:
        """
        Sanitize shell command and check for dangerous patterns
        
        Returns:
            (is_safe, warning_message)
        """
        # Check strict whitelist first if enabled
        if self.strict_whitelist_enabled:
            # Extract base command (first word)
            base_cmd = command.strip().split()[0] if command.strip() else ""
            if base_cmd not in self.shell_whitelist:
                return False, f"Command '{base_cmd}' not in strict whitelist (SHELL_ALLOWED_COMMANDS)"

        # Check for dangerous patterns
        for pattern in self.dangerous_shell_patterns:
            if re.search(pattern, command, re.IGNORECASE):
                return False, f"Dangerous pattern detected: {pattern}"
        
        # Check for suspicious characters
        if "$((" in command or "`" in command or "$(" in command:
            return False, "Command substitution detected - potentially dangerous"
        
        # Warn about file redirects
        if ">>" in command or ">" in command:
            return True, "Warning: File redirection detected (write operation)"
        
        return True, None
    
    def should_auto_execute(self, tool_name: str, parameters: Dict[str, Any]) -> bool:
        """
        Determine if tool should auto-execute or require confirmation
        
        Args:
            tool_name: Name of the tool
            parameters: Tool parameters (for additional validation)
        
        Returns:
            True if should auto-execute, False if needs confirmation
        """
        if not self.auto_execute_enabled:
            return False
        
        if not self.is_allowed(tool_name):
            return False
        
        if self.requires_confirmation(tool_name):
            return False
        
        # Additional checks for specific tools
        if tool_name == "shell_execute":
            command = parameters.get("command", "")
            is_safe, _ = self.sanitize_shell_command(command)
            return is_safe
        
        return True
    
    def get_confirmation_prompt(self, tool_name: str, parameters: Dict[str, Any]) -> str:
        """Generate confirmation prompt for tool execution"""
        category = self.categorize_tool(tool_name)
        
        prompt = f"\nâš ï¸�  Tool Execution Request\n"
        prompt += f"Tool: {tool_name}\n"
        prompt += f"Safety: {category.value.upper()}\n"
        prompt += f"Parameters:\n"
        
        for key, value in parameters.items():
            # Truncate long values
            value_str = str(value)
            if len(value_str) > 100:
                value_str = value_str[:100] + "..."
            prompt += f"  {key}: {value_str}\n"
        
        if tool_name == "filesystem_write":
            content = parameters.get("content", "")
            preview = str(content)[:200]
            prompt += "\nPreview:\n"
            prompt += preview + ("..." if len(str(content)) > 200 else "") + "\n"
            prompt += f"Length: {len(str(content))} chars\n"

        # Add specific warnings
        if tool_name == "shell_execute":
            command = parameters.get("command", "")
            is_safe, warning = self.sanitize_shell_command(command)
            if not is_safe:
                prompt += f"\nðŸš¨ DANGER: {warning}\n"
            elif warning:
                prompt += f"\nâš ï¸�  {warning}\n"
        
        prompt += "\nExecute this tool? (y/n): "
        return prompt
    
    def log_tool_execution(self, tool_name: str, parameters: Dict[str, Any], allowed: bool):
        """Log tool execution attempt for audit trail"""
        logger.info(f"Tool execution: {tool_name}")
        logger.info(f"  Allowed: {allowed}")
        logger.info(f"  Category: {self.categorize_tool(tool_name).value}")
        logger.debug(f"  Parameters: {parameters}")


# Global instance
_safety_manager: Optional[ToolSafetyManager] = None


def get_safety_manager() -> ToolSafetyManager:
    """Get global ToolSafetyManager instance"""
    global _safety_manager
    if _safety_manager is None:
        _safety_manager = ToolSafetyManager()
    return _safety_manager


# Convenience functions
def is_tool_safe(tool_name: str) -> bool:
    """Check if tool is categorized as safe"""
    return get_safety_manager().categorize_tool(tool_name) == ToolSafety.SAFE


def requires_confirmation(tool_name: str) -> bool:
    """Check if tool requires confirmation"""
    return get_safety_manager().requires_confirmation(tool_name)


def sanitize_command(command: str) -> tuple[bool, Optional[str]]:
    """Sanitize shell command"""
    return get_safety_manager().sanitize_shell_command(command)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tool_safety.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\autoreload.py (Section: ANCHOR_PYTHON) ---

"""IPython extension to reload modules before executing user code.

``autoreload`` reloads modules automatically before entering the execution of
code typed at the IPython prompt.

This makes for example the following workflow possible:

.. sourcecode:: ipython

   In [1]: %load_ext autoreload

   In [2]: %autoreload 2

   In [3]: from foo import some_function

   In [4]: some_function()
   Out[4]: 42

   In [5]: # open foo.py in an editor and change some_function to return 43

   In [6]: some_function()
   Out[6]: 43

The module was reloaded without reloading it explicitly, and the object
imported with ``from foo import ...`` was also updated.

Usage
=====

The following magic commands are provided:

``%autoreload``, ``%autoreload now``

    Reload all modules (except those excluded by ``%aimport``)
    automatically now.

``%autoreload 0``, ``%autoreload off``

    Disable automatic reloading.

``%autoreload 1``, ``%autoreload explicit``

    Reload all modules imported with ``%aimport`` every time before
    executing the Python code typed.

``%autoreload 2``, ``%autoreload all``

    Reload all modules (except those excluded by ``%aimport``) every
    time before executing the Python code typed.

``%autoreload 3``, ``%autoreload complete``

    Same as 2/all, but also adds any new objects in the module. See
    unit test at IPython/extensions/tests/test_autoreload.py::test_autoload_newly_added_objects

  Adding ``--print`` or ``-p`` to the ``%autoreload`` line will print autoreload activity to
  standard out. ``--log`` or ``-l`` will do it to the log at INFO level; both can be used
  simultaneously.

``%aimport``

    List modules which are to be automatically imported or not to be imported.

``%aimport foo``

    Import module 'foo' and mark it to be autoreloaded for ``%autoreload 1``

``%aimport foo, bar``

    Import modules 'foo', 'bar' and mark them to be autoreloaded for ``%autoreload 1``

``%aimport -foo``

    Mark module 'foo' to not be autoreloaded.

Import Conflict Resolution
==========================

In ``%autoreload 3`` mode, the extension tracks ``from X import Y`` style imports
and intelligently resolves conflicts when the same name is imported multiple ways.

Import tracking occurs after successful code execution, ensuring that only valid
imports are tracked. This approach handles edge cases such as:

- Importing a name that doesn't initially exist in a module, then adding that name
  to the module and importing it again
- Conflicts between aliased imports (``from X import Y as Z``) and direct imports
  (``from X import Z``)

When conflicts occur:

- If you first do ``from X import Y as Z`` then later ``from X import Z``,
  the extension will switch to reloading ``Z`` instead of ``Y`` under the name ``Z``.

- Similarly, if you first do ``from X import Z`` then later ``from X import Y as Z``,
  the extension will switch to reloading ``Y`` as ``Z`` instead of the original ``Z``.

- The most recent successful import always takes precedence in conflict resolution.

Caveats
=======

Reloading Python modules in a reliable way is in general difficult,
and unexpected things may occur. ``%autoreload`` tries to work around
common pitfalls by replacing function code objects and parts of
classes previously in the module with new versions. This makes the
following things to work:

- Functions and classes imported via 'from xxx import foo' are upgraded
  to new versions when 'xxx' is reloaded.

- Methods and properties of classes are upgraded on reload, so that
  calling 'c.foo()' on an object 'c' created before the reload causes
  the new code for 'foo' to be executed.

Some of the known remaining caveats are:

- Replacing code objects does not always succeed: changing a @property
  in a class to an ordinary method or a method to a member variable
  can cause problems (but in old objects only).

- Functions that are removed (eg. via monkey-patching) from a module
  before it is reloaded are not upgraded.

- C extension modules cannot be reloaded, and so cannot be autoreloaded.

- While comparing Enum and Flag, the 'is' Identity Operator is used (even in the case '==' has been used (Similar to the 'None' keyword)).

- Reloading a module, or importing the same module by a different name, creates new Enums. These may look the same, but are not.
"""

from IPython.core import magic_arguments
from IPython.core.magic import Magics, magics_class, line_magic
from IPython.extensions.deduperreload.deduperreload import DeduperReloader

__skip_doctest__ = True

# -----------------------------------------------------------------------------
#  Copyright (C) 2000 Thomas Heller
#  Copyright (C) 2008 Pauli Virtanen <pav@iki.fi>
#  Copyright (C) 2012  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
# -----------------------------------------------------------------------------
#
# This IPython module is written by Pauli Virtanen, based on the autoreload
# code by Thomas Heller.

# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------

import ast
import os
import sys
import traceback
import types
import weakref
import gc
import logging
from importlib import import_module, reload
from importlib.util import source_from_cache

# ------------------------------------------------------------------------------
# Autoreload functionality
# ------------------------------------------------------------------------------


class ModuleReloader:
    enabled = False
    """Whether this reloader is enabled"""

    check_all = True
    """Autoreload all modules, not just those listed in 'modules'"""

    autoload_obj = False
    """Autoreload all modules AND autoload all new objects"""

    def __init__(self, shell=None):
        # Modules that failed to reload: {module: mtime-on-failed-reload, ...}
        self.failed = {}
        # Modules specially marked as autoreloadable.
        self.modules = {}
        # Modules specially marked as not autoreloadable.
        self.skip_modules = {}
        # (module-name, name) -> weakref, for replacing old code objects
        self.old_objects = {}
        # Module modification timestamps
        self.modules_mtimes = {}
        self.shell = shell

        # Reporting callable for verbosity
        self._report = lambda msg: None  # by default, be quiet.

        # Deduper reloader
        self.deduper_reloader = DeduperReloader()

        # Persistent import tracker for from-imports
        self.import_from_tracker = ImportFromTracker({}, {})

        # Cache module modification times
        self.check(check_all=True, do_reload=False)

        # To hide autoreload errors
        self.hide_errors = False

    def mark_module_skipped(self, module_name):
        """Skip reloading the named module in the future"""
        try:
            del self.modules[module_name]
        except KeyError:
            pass
        self.skip_modules[module_name] = True

    def mark_module_reloadable(self, module_name):
        """Reload the named module in the future (if it is imported)"""
        try:
            del self.skip_modules[module_name]
        except KeyError:
            pass
        self.modules[module_name] = True

    def clear_import_tracker(self):
        """Clear the persistent import tracker state"""
        self.import_from_tracker = ImportFromTracker({}, {})

    def aimport_module(self, module_name):
        """Import a module, and mark it reloadable

        Returns
        -------
        top_module : module
            The imported module if it is top-level, or the top-level
        top_name : module
            Name of top_module

        """
        self.mark_module_reloadable(module_name)

        import_module(module_name)
        top_name = module_name.split(".")[0]
        top_module = sys.modules[top_name]
        return top_module, top_name

    def filename_and_mtime(self, module):
        if not hasattr(module, "__file__") or module.__file__ is None:
            return None, None

        if getattr(module, "__name__", None) in [None, "__mp_main__", "__main__"]:
            # we cannot reload(__main__) or reload(__mp_main__)
            return None, None

        filename = module.__file__
        path, ext = os.path.splitext(filename)

        if ext.lower() == ".py":
            py_filename = filename
        else:
            try:
                py_filename = source_from_cache(filename)
            except ValueError:
                return None, None

        try:
            pymtime = os.stat(py_filename).st_mtime
        except OSError:
            return None, None

        return py_filename, pymtime

    def check(self, check_all=False, do_reload=True, execution_info=None):
        """Check whether some modules need to be reloaded."""

        if not self.enabled and not check_all:
            return

        if check_all or self.check_all:
            modules = list(sys.modules.keys())
        else:
            modules = list(self.modules.keys())

        # Use the persistent import_from_tracker
        import_from_tracker = (
            self.import_from_tracker if self.import_from_tracker.imports_froms else None
        )
        for modname in modules:
            m = sys.modules.get(modname, None)

            if modname in self.skip_modules:
                continue

            py_filename, pymtime = self.filename_and_mtime(m)
            if py_filename is None:
                continue

            try:
                if pymtime <= self.modules_mtimes[modname]:
                    continue
            except KeyError:
                self.modules_mtimes[modname] = pymtime
                continue
            else:
                if self.failed.get(py_filename, None) == pymtime:
                    continue

            self.modules_mtimes[modname] = pymtime

            # If we've reached this point, we should try to reload the module
            if do_reload:
                self._report(f"Reloading '{modname}'.")
                try:
                    if self.autoload_obj:
                        superreload(
                            m,
                            reload,
                            self.old_objects,
                            self.shell,
                            import_from_tracker=import_from_tracker,
                        )
                    # if not using autoload, check if deduperreload is viable for this module
                    elif self.deduper_reloader.maybe_reload_module(m):
                        pass
                    else:
                        superreload(m, reload, self.old_objects)
                    if py_filename in self.failed:
                        del self.failed[py_filename]
                except:
                    if not self.hide_errors:
                        print(
                            "[autoreload of {} failed: {}]".format(
                                modname, traceback.format_exc(10)
                            ),
                            file=sys.stderr,
                        )
                    self.failed[py_filename] = pymtime
        self.deduper_reloader.update_sources()


# ------------------------------------------------------------------------------
# superreload
# ------------------------------------------------------------------------------


func_attrs = [
    "__code__",
    "__defaults__",
    "__doc__",
    "__closure__",
    "__globals__",
    "__dict__",
]


def update_function(old, new):
    """Upgrade the code object of a function"""
    for name in func_attrs:
        try:
            setattr(old, name, getattr(new, name))
        except (AttributeError, TypeError):
            pass


def update_instances(old, new):
    """Use garbage collector to find all instances that refer to the old
    class definition and update their __class__ to point to the new class
    definition"""

    refs = gc.get_referrers(old)

    for ref in refs:
        if type(ref) is old:
            object.__setattr__(ref, "__class__", new)


def update_class(old, new):
    """Replace stuff in the __dict__ of a class, and upgrade
    method code objects, and add new methods, if any"""
    for key in list(old.__dict__.keys()):
        old_obj = getattr(old, key)
        try:
            new_obj = getattr(new, key)
            # explicitly checking that comparison returns True to handle
            # cases where `==` doesn't return a boolean.
            if (old_obj == new_obj) is True:
                continue
        except AttributeError:
            # obsolete attribute: remove it
            try:
                delattr(old, key)
            except (AttributeError, TypeError):
                pass
            continue
        except ValueError:
            # can't compare nested structures containing
            # numpy arrays using `==`
            pass

        if update_generic(old_obj, new_obj):
            continue

        try:
            setattr(old, key, getattr(new, key))
        except (AttributeError, TypeError):
            pass  # skip non-writable attributes

    for key in list(new.__dict__.keys()):
        if key not in list(old.__dict__.keys()):
            try:
                setattr(old, key, getattr(new, key))
            except (AttributeError, TypeError):
                pass  # skip non-writable attributes

    # update all instances of class
    update_instances(old, new)


def update_property(old, new):
    """Replace get/set/del functions of a property"""
    update_generic(old.fdel, new.fdel)
    update_generic(old.fget, new.fget)
    update_generic(old.fset, new.fset)


def isinstance2(a, b, typ):
    return isinstance(a, typ) and isinstance(b, typ)


UPDATE_RULES = [
    (lambda a, b: isinstance2(a, b, type), update_class),
    (lambda a, b: isinstance2(a, b, types.FunctionType), update_function),
    (lambda a, b: isinstance2(a, b, property), update_property),
]
UPDATE_RULES.extend(
    [
        (
            lambda a, b: isinstance2(a, b, types.MethodType),
            lambda a, b: update_function(a.__func__, b.__func__),
        ),
    ]
)


def update_generic(a, b):
    for type_check, update in UPDATE_RULES:
        if type_check(a, b):
            update(a, b)
            return True
    return False


class StrongRef:
    def __init__(self, obj):
        self.obj = obj

    def __call__(self):
        return self.obj


mod_attrs = [
    "__name__",
    "__doc__",
    "__package__",
    "__loader__",
    "__spec__",
    "__file__",
    "__cached__",
    "__builtins__",
]


class ImportFromTracker:
    def __init__(self, imports_froms: dict, symbol_map: dict):
        self.imports_froms = imports_froms
        # symbol_map maps original_name -> list of resolved_names
        self.symbol_map = {}
        if symbol_map:
            for module_name, mappings in symbol_map.items():
                self.symbol_map[module_name] = {}
                for original_name, resolved_names in mappings.items():
                    if isinstance(resolved_names, list):
                        self.symbol_map[module_name][original_name] = resolved_names[:]
                    else:
                        self.symbol_map[module_name][original_name] = [resolved_names]
        else:
            self.symbol_map = symbol_map or {}

    def add_import(
        self, module_name: str, original_name: str, resolved_name: str
    ) -> None:
        """Add an import, handling conflicts with existing imports.

        This method is called after successful code execution, so we know the import is valid.
        """
        if module_name not in self.imports_froms:
            self.imports_froms[module_name] = []
        if module_name not in self.symbol_map:
            self.symbol_map[module_name] = {}

        # Check if there's already a different mapping for the same resolved_name from a different original_name
        # We need to remove any conflicting mappings
        for orig_name, res_names in list(self.symbol_map[module_name].items()):
            if resolved_name in res_names and orig_name != original_name:
                # Remove the conflicting resolved_name from the other original_name's list
                res_names.remove(resolved_name)
                if (
                    not res_names
                ):  # If the list is now empty, remove the original_name entirely
                    if orig_name in self.imports_froms[module_name]:
                        self.imports_froms[module_name].remove(orig_name)
                    del self.symbol_map[module_name][orig_name]

        # Add the new mapping
        if original_name not in self.imports_froms[module_name]:
            self.imports_froms[module_name].append(original_name)

        if original_name not in self.symbol_map[module_name]:
            self.symbol_map[module_name][original_name] = []

        # Add the resolved_name if it's not already in the list
        if resolved_name not in self.symbol_map[module_name][original_name]:
            self.symbol_map[module_name][original_name].append(resolved_name)


def append_obj(module, d, name, obj, autoload=False):
    in_module = hasattr(obj, "__module__") and obj.__module__ == module.__name__
    if autoload:
        # check needed for module global built-ins
        if not in_module and name in mod_attrs:
            return False
    else:
        if not in_module:
            return False

    key = (module.__name__, name)
    try:
        d.setdefault(key, []).append(weakref.ref(obj))
    except TypeError:
        pass
    return True


def superreload(
    module, reload=reload, old_objects=None, shell=None, import_from_tracker=None
):
    """Enhanced version of the builtin reload function.

    superreload remembers objects previously in the module, and

    - upgrades the class dictionary of every old class in the module
    - upgrades the code object of every old function and method
    - clears the module's namespace before reloading

    """
    if old_objects is None:
        old_objects = {}

    # collect old objects in the module
    for name, obj in list(module.__dict__.items()):
        if not append_obj(module, old_objects, name, obj):
            continue
        key = (module.__name__, name)
        try:
            old_objects.setdefault(key, []).append(weakref.ref(obj))
        except TypeError:
            pass

    # reload module
    try:
        # clear namespace first from old cruft
        old_dict = module.__dict__.copy()
        old_name = module.__name__
        module.__dict__.clear()
        module.__dict__["__name__"] = old_name
        module.__dict__["__loader__"] = old_dict["__loader__"]
    except (TypeError, AttributeError, KeyError):
        pass

    try:
        module = reload(module)
    except:
        # restore module dictionary on failed reload
        module.__dict__.update(old_dict)
        raise

    for name, new_obj in list(module.__dict__.items()):
        key = (module.__name__, name)
        if key not in old_objects:
            # here 'shell' acts both as a flag and as an output var
            imports_froms = (
                import_from_tracker.imports_froms if import_from_tracker else None
            )
            symbol_map = import_from_tracker.symbol_map if import_from_tracker else None
            if (
                shell is None
                or name == "Enum"
                or not append_obj(module, old_objects, name, new_obj, True)
                or (
                    imports_froms
                    and module.__name__ in imports_froms
                    and "*" not in imports_froms[module.__name__]
                    and name not in imports_froms[module.__name__]
                )
            ):
                continue

            # Handle symbol mapping - now supporting multiple resolved names per original name
            if symbol_map and name in symbol_map.get(module.__name__, {}):
                resolved_names = symbol_map.get(module.__name__, {})[name]
                for resolved_name in resolved_names:
                    shell.user_ns[resolved_name] = new_obj
            else:
                shell.user_ns[name] = new_obj

        new_refs = []
        for old_ref in old_objects[key]:
            old_obj = old_ref()
            if old_obj is None:
                continue
            new_refs.append(old_ref)
            update_generic(old_obj, new_obj)

        if new_refs:
            old_objects[key] = new_refs
        else:
            del old_objects[key]

    return module


# ------------------------------------------------------------------------------
# IPython connectivity
# ------------------------------------------------------------------------------


@magics_class
class AutoreloadMagics(Magics):
    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)
        self._reloader = ModuleReloader(self.shell)
        self._reloader.check_all = False
        self._reloader.autoload_obj = False
        self.loaded_modules = set(sys.modules)

    @line_magic
    @magic_arguments.magic_arguments()
    @magic_arguments.argument(
        "mode",
        type=str,
        default="now",
        nargs="?",
        help="""blank or 'now' - Reload all modules (except those excluded by %%aimport)
             automatically now.

             '0' or 'off' - Disable automatic reloading.

             '1' or 'explicit' - Reload only modules imported with %%aimport every
             time before executing the Python code typed.

             '2' or 'all' - Reload all modules (except those excluded by %%aimport)
             every time before executing the Python code typed.

             '3' or 'complete' - Same as 2/all, but also adds any new
             objects in the module.
             
             By default, a newer autoreload algorithm that diffs the module's source code
             with the previous version and only reloads changed parts is applied for modes
             2 and below. To use the original algorithm, add the `-` suffix to the mode,
             e.g. '%autoreload 2-', or pass in --full.
             """,
    )
    @magic_arguments.argument(
        "-p",
        "--print",
        action="store_true",
        default=False,
        help="Show autoreload activity using `print` statements",
    )
    @magic_arguments.argument(
        "-l",
        "--log",
        action="store_true",
        default=False,
        help="Show autoreload activity using the logger",
    )
    @magic_arguments.argument(
        "--hide-errors",
        action="store_true",
        default=False,
        help="Hide autoreload errors",
    )
    @magic_arguments.argument(
        "--full",
        action="store_true",
        default=False,
        help="Don't ever use new diffing algorithm",
    )
    def autoreload(self, line=""):
        r"""%autoreload => Reload modules automatically

        %autoreload or %autoreload now
        Reload all modules (except those excluded by %aimport) automatically
        now.

        %autoreload 0 or %autoreload off
        Disable automatic reloading.

        %autoreload 1 or %autoreload explicit
        Reload only modules imported with %aimport every time before executing
        the Python code typed.

        %autoreload 2 or %autoreload all
        Reload all modules (except those excluded by %aimport) every time
        before executing the Python code typed.

        %autoreload 3 or %autoreload complete
        Same as 2/all, but also but also adds any new objects in the module. See
        unit test at IPython/extensions/tests/test_autoreload.py::test_autoload_newly_added_objects

        The optional arguments --print and --log control display of autoreload activity. The default
        is to act silently; --print (or -p) will print out the names of modules that are being
        reloaded, and --log (or -l) outputs them to the log at INFO level.

        The optional argument --hide-errors hides any errors that can happen when trying to
        reload code.

        Reloading Python modules in a reliable way is in general
        difficult, and unexpected things may occur. %autoreload tries to
        work around common pitfalls by replacing function code objects and
        parts of classes previously in the module with new versions. This
        makes the following things to work:

        - Functions and classes imported via 'from xxx import foo' are upgraded
          to new versions when 'xxx' is reloaded.

        - Methods and properties of classes are upgraded on reload, so that
          calling 'c.foo()' on an object 'c' created before the reload causes
          the new code for 'foo' to be executed.

        Some of the known remaining caveats are:

        - Replacing code objects does not always succeed: changing a @property
          in a class to an ordinary method or a method to a member variable
          can cause problems (but in old objects only).

        - Functions that are removed (eg. via monkey-patching) from a module
          before it is reloaded are not upgraded.

        - C extension modules cannot be reloaded, and so cannot be
          autoreloaded.

        """
        args = magic_arguments.parse_argstring(self.autoreload, line)
        mode = args.mode.lower()

        enable_deduperreload = not args.full
        if mode.endswith("-"):
            enable_deduperreload = False
            mode = mode[:-1]
        self._reloader.deduper_reloader.enabled = enable_deduperreload

        p = print

        logger = logging.getLogger("autoreload")

        l = logger.info

        def pl(msg):
            p(msg)
            l(msg)

        if args.print is False and args.log is False:
            self._reloader._report = lambda msg: None
        elif args.print is True:
            if args.log is True:
                self._reloader._report = pl
            else:
                self._reloader._report = p
        elif args.log is True:
            self._reloader._report = l

        self._reloader.hide_errors = args.hide_errors

        if mode == "" or mode == "now":
            self._reloader.check(True)
        elif mode == "0" or mode == "off":
            self._reloader.enabled = False
        elif mode == "1" or mode == "explicit":
            self._reloader.enabled = True
            self._reloader.check_all = False
            self._reloader.autoload_obj = False
        elif mode == "2" or mode == "all":
            self._reloader.enabled = True
            self._reloader.check_all = True
            self._reloader.autoload_obj = False
        elif mode == "3" or mode == "complete":
            self._reloader.enabled = True
            self._reloader.check_all = True
            self._reloader.autoload_obj = True
        else:
            raise ValueError(f'Unrecognized autoreload mode "{mode}".')

    @line_magic
    def aimport(self, parameter_s="", stream=None):
        """%aimport => Import modules for automatic reloading.

        %aimport
        List modules to automatically import and not to import.

        %aimport foo
        Import module 'foo' and mark it to be autoreloaded for %autoreload explicit

        %aimport foo, bar
        Import modules 'foo', 'bar' and mark them to be autoreloaded for %autoreload explicit

        %aimport -foo, bar
        Mark module 'foo' to not be autoreloaded for %autoreload explicit, all, or complete, and 'bar'
        to be autoreloaded for mode explicit.
        """
        modname = parameter_s
        if not modname:
            to_reload = sorted(self._reloader.modules.keys())
            to_skip = sorted(self._reloader.skip_modules.keys())
            if stream is None:
                stream = sys.stdout
            if self._reloader.check_all:
                stream.write("Modules to reload:\nall-except-skipped\n")
            else:
                stream.write("Modules to reload:\n%s\n" % " ".join(to_reload))
            stream.write("\nModules to skip:\n%s\n" % " ".join(to_skip))
        else:
            for _module in [_.strip() for _ in modname.split(",")]:
                if _module.startswith("-"):
                    _module = _module[1:].strip()
                    self._reloader.mark_module_skipped(_module)
                else:
                    top_module, top_name = self._reloader.aimport_module(_module)

                    # Inject module to user namespace
                    self.shell.push({top_name: top_module})

    def pre_run_cell(self, info):
        # Store the execution info for later use in post_execute_hook
        self._last_execution_info = info

        if self._reloader.enabled:
            try:
                self._reloader.check()
            except:
                pass

    def post_execute_hook(self):
        """Cache the modification times of any modules imported in this execution and track imports"""

        # Track imports from the recently executed code if autoreload 3 is enabled
        if self._reloader.enabled and self._reloader.autoload_obj:
            # Use the stored execution info
            if (
                hasattr(self, "_last_execution_info")
                and self._last_execution_info
                and self._last_execution_info.transformed_cell
            ):
                self._track_imports_from_code(
                    self._last_execution_info.transformed_cell
                )

        newly_loaded_modules = set(sys.modules) - self.loaded_modules
        for modname in newly_loaded_modules:
            _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])
            if pymtime is not None:
                self._reloader.modules_mtimes[modname] = pymtime

        self.loaded_modules.update(newly_loaded_modules)

    def _track_imports_from_code(self, code: str) -> None:
        """Track import statements from executed code"""
        try:
            tree = ast.parse(code)

            for node in ast.walk(tree):
                # Handle "from X import Y" style imports
                if isinstance(node, ast.ImportFrom):
                    mod = node.module

                    # Skip relative imports that don't have a module name
                    if mod is None:
                        continue

                    for name in node.names:
                        # name.name is going to be actual name that we want to import from module
                        # name.asname is Z in the case of from X import Y as Z
                        # we should update Z in the shell in this situation, so track it too.
                        original_name = name.name
                        resolved_name = name.asname if name.asname else name.name

                        # Since the code executed successfully, we know this import is valid
                        self._reloader.import_from_tracker.add_import(
                            mod, original_name, resolved_name
                        )
        except (SyntaxError, ValueError):
            # If there's a syntax error, skip import tracking
            # (though this shouldn't happen since the code already executed successfully)
            pass


def load_ipython_extension(ip):
    """Load the extension in IPython."""
    auto_reload = AutoreloadMagics(ip)
    ip.register_magics(auto_reload)
    ip.events.register("pre_run_cell", auto_reload.pre_run_cell)
    ip.events.register("post_execute", auto_reload.post_execute_hook)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\autoreload.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\storemagic.py (Section: ANCHOR_PYTHON) ---

# -*- coding: utf-8 -*-
"""
%store magic for lightweight persistence.

Stores variables, aliases and macros in IPython's database.

To automatically restore stored variables at startup, add this to your
:file:`ipython_config.py` file::

  c.StoreMagics.autorestore = True
"""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

import inspect, os, sys, textwrap

from IPython.core.error import UsageError
from IPython.core.magic import Magics, magics_class, line_magic
from IPython.testing.skipdoctest import skip_doctest
from traitlets import Bool


def restore_aliases(ip, alias=None):
    staliases = ip.db.get('stored_aliases', {})
    if alias is None:
        for k,v in staliases.items():
            # print("restore alias",k,v)  # dbg
            #self.alias_table[k] = v
            ip.alias_manager.define_alias(k,v)
    else:
        ip.alias_manager.define_alias(alias, staliases[alias])


def refresh_variables(ip):
    db = ip.db
    for key in db.keys('autorestore/*'):
        # strip autorestore
        justkey = os.path.basename(key)
        try:
            obj = db[key]
        except KeyError:
            print("Unable to restore variable '%s', ignoring (use %%store -d to forget!)" % justkey)
            print("The error was:", sys.exc_info()[0])
        else:
            # print("restored",justkey,"=",obj)  # dbg
            ip.user_ns[justkey] = obj


def restore_dhist(ip):
    ip.user_ns['_dh'] = ip.db.get('dhist',[])


def restore_data(ip):
    refresh_variables(ip)
    restore_aliases(ip)
    restore_dhist(ip)


@magics_class
class StoreMagics(Magics):
    """Lightweight persistence for python variables.

    Provides the %store magic."""

    autorestore = Bool(False, help=
        """If True, any %store-d variables will be automatically restored
        when IPython starts.
        """
    ).tag(config=True)

    def __init__(self, shell):
        super(StoreMagics, self).__init__(shell=shell)
        self.shell.configurables.append(self)
        if self.autorestore:
            restore_data(self.shell)

    @skip_doctest
    @line_magic
    def store(self, parameter_s=''):
        """Lightweight persistence for python variables.

        Example::

          In [1]: l = ['hello',10,'world']
          In [2]: %store l
          Stored 'l' (list)
          In [3]: exit

          (IPython session is closed and started again...)

          ville@badger:~$ ipython
          In [1]: l
          NameError: name 'l' is not defined
          In [2]: %store -r
          In [3]: l
          Out[3]: ['hello', 10, 'world']

        Usage:

        * ``%store``          - Show list of all variables and their current
                                values
        * ``%store spam bar`` - Store the *current* value of the variables spam
                                and bar to disk
        * ``%store -d spam``  - Remove the variable and its value from storage
        * ``%store -z``       - Remove all variables from storage
        * ``%store -r``       - Refresh all variables, aliases and directory history
                                from store (overwrite current vals)
        * ``%store -r spam bar`` - Refresh specified variables and aliases from store
                                   (delete current val)
        * ``%store foo >a.txt``  - Store value of foo to new file a.txt
        * ``%store foo >>a.txt`` - Append value of foo to file a.txt

        It should be noted that if you change the value of a variable, you
        need to %store it again if you want to persist the new value.

        Note also that the variables will need to be pickleable; most basic
        python types can be safely %store'd.

        Also aliases can be %store'd across sessions.
        To remove an alias from the storage, use the %unalias magic.
        """

        opts,argsl = self.parse_options(parameter_s,'drz',mode='string')
        args = argsl.split()
        ip = self.shell
        db = ip.db
        # delete
        if 'd' in opts:
            try:
                todel = args[0]
            except IndexError as e:
                raise UsageError('You must provide the variable to forget') from e
            else:
                try:
                    del db['autorestore/' + todel]
                except BaseException as e:
                    raise UsageError("Can't delete variable '%s'" % todel) from e
        # reset
        elif 'z' in opts:
            for k in db.keys('autorestore/*'):
                del db[k]

        elif 'r' in opts:
            if args:
                for arg in args:
                    try:
                        obj = db["autorestore/" + arg]
                    except KeyError:
                        try:
                            restore_aliases(ip, alias=arg)
                        except KeyError:
                            print("no stored variable or alias %s" % arg)
                    else:
                        ip.user_ns[arg] = obj
            else:
                restore_data(ip)

        # run without arguments -> list variables & values
        elif not args:
            vars = db.keys('autorestore/*')
            vars.sort()
            if vars:
                size = max(map(len, vars))
            else:
                size = 0

            print('Stored variables and their in-db values:')
            fmt = '%-'+str(size)+'s -> %s'
            get = db.get
            for var in vars:
                justkey = os.path.basename(var)
                # print 30 first characters from every var
                print(fmt % (justkey, repr(get(var, '<unavailable>'))[:50]))

        # default action - store the variable
        else:
            # %store foo >file.txt or >>file.txt
            if len(args) > 1 and args[1].startswith(">"):
                fnam = os.path.expanduser(args[1].lstrip(">").lstrip())
                if args[1].startswith(">>"):
                    fil = open(fnam, "a", encoding="utf-8")
                else:
                    fil = open(fnam, "w", encoding="utf-8")
                with fil:
                    obj = ip.ev(args[0])
                    print("Writing '%s' (%s) to file '%s'." % (args[0],
                        obj.__class__.__name__, fnam))

                    if not isinstance (obj, str):
                        from pprint import pprint
                        pprint(obj, fil)
                    else:
                        fil.write(obj)
                        if not obj.endswith('\n'):
                            fil.write('\n')

                return

            # %store foo
            for arg in args:
                try:
                    obj = ip.user_ns[arg]
                except KeyError:
                    # it might be an alias
                    name = arg
                    try:
                        cmd = ip.alias_manager.retrieve_alias(name)
                    except ValueError as e:
                        raise UsageError("Unknown variable '%s'" % name) from e

                    staliases = db.get('stored_aliases',{})
                    staliases[name] = cmd
                    db['stored_aliases'] = staliases
                    print("Alias stored: %s (%s)" % (name, cmd))
                    return

                else:
                    modname = getattr(inspect.getmodule(obj), '__name__', '')
                    if modname == '__main__':
                        print(textwrap.dedent("""\
                        Warning:%s is %s
                        Proper storage of interactively declared classes (or instances
                        of those classes) is not possible! Only instances
                        of classes in real modules on file system can be %%store'd.
                        """ % (arg, obj) ))
                        return
                    #pickled = pickle.dumps(obj)
                    db[ 'autorestore/' + arg ] = obj
                    print("Stored '%s' (%s)" % (arg, obj.__class__.__name__))


def load_ipython_extension(ip):
    """Load the extension in IPython."""
    ip.register_magics(StoreMagics)



--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\storemagic.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\deduperreload\deduperreload.py (Section: ANCHOR_PYTHON) ---

from __future__ import annotations
import ast
import builtins
import contextlib
import itertools
import os
import pickle
import platform
import sys
import textwrap
from types import ModuleType
from typing import TYPE_CHECKING, Any, Generator, Iterable, NamedTuple, cast

from IPython.extensions.deduperreload.deduperreload_patching import (
    DeduperReloaderPatchingMixin,
)

if TYPE_CHECKING:
    TDefinitionAst = (
        ast.FunctionDef
        | ast.AsyncFunctionDef
        | ast.Import
        | ast.ImportFrom
        | ast.Assign
        | ast.AnnAssign
    )


def get_module_file_name(module: ModuleType | str) -> str | None:
    """Returns the module's file path, or the empty string if it's inaccessible"""
    if (mod := sys.modules.get(module) if isinstance(module, str) else module) is None:
        return ""
    return getattr(mod, "__file__", "") or ""


def compare_ast(node1: ast.AST | list[ast.AST], node2: ast.AST | list[ast.AST]) -> bool:
    """Checks if node1 and node2 have identical AST structure/values, apart from some attributes"""
    if type(node1) is not type(node2):
        return False

    if isinstance(node1, ast.AST):
        for k, v in node1.__dict__.items():
            if k in (
                "lineno",
                "end_lineno",
                "col_offset",
                "end_col_offset",
                "ctx",
                "parent",
            ):
                continue
            if not hasattr(node2, k) or not compare_ast(v, getattr(node2, k)):
                return False
        return True

    elif isinstance(node1, list) and isinstance(  # type:ignore [redundant-expr]
        node2, list
    ):
        return len(node1) == len(node2) and all(
            compare_ast(n1, n2) for n1, n2 in zip(node1, node2)
        )
    else:
        return node1 == node2


class DependencyNode(NamedTuple):
    """
    Each node represents a function.
    qualified_name: string which represents the namespace/name of the function
    abstract_syntax_tree: subtree of the overall module which corresponds to this function

    qualified_name is of the structure: (namespace1, namespace2, ..., name)

    For example, foo() in the following would be represented as (A, B, foo):

    class A:
        class B:
            def foo():
                pass
    """

    qualified_name: tuple[str, ...]
    abstract_syntax_tree: ast.AST


class GatherResult(NamedTuple):
    import_defs: list[tuple[tuple[str, ...], ast.Import | ast.ImportFrom]] = []
    assign_defs: list[tuple[tuple[str, ...], ast.Assign | ast.AnnAssign]] = []
    function_defs: list[
        tuple[tuple[str, ...], ast.FunctionDef | ast.AsyncFunctionDef]
    ] = []
    classes: dict[str, ast.ClassDef] = {}
    unfixable: list[ast.AST] = []

    @classmethod
    def create(cls) -> GatherResult:
        return cls([], [], [], {}, [])

    def all_defs(self) -> Iterable[tuple[tuple[str, ...], TDefinitionAst]]:
        return itertools.chain(self.import_defs, self.assign_defs, self.function_defs)

    def inplace_merge(self, other: GatherResult) -> None:
        self.import_defs.extend(other.import_defs)
        self.assign_defs.extend(other.assign_defs)
        self.function_defs.extend(other.function_defs)
        self.classes.update(other.classes)
        self.unfixable.extend(other.unfixable)


class ConstexprDetector(ast.NodeVisitor):
    def __init__(self) -> None:
        self.is_constexpr = True
        self._allow_builtins_exceptions = True

    @contextlib.contextmanager
    def disallow_builtins_exceptions(self) -> Generator[None, None, None]:
        prev_allow = self._allow_builtins_exceptions
        self._allow_builtins_exceptions = False
        try:
            yield
        finally:
            self._allow_builtins_exceptions = prev_allow

    def visit_Attribute(self, node: ast.Attribute) -> None:
        with self.disallow_builtins_exceptions():
            self.visit(node.value)

    def visit_Name(self, node: ast.Name) -> None:
        if self._allow_builtins_exceptions and hasattr(builtins, node.id):
            return
        self.is_constexpr = False

    def visit(self, node: ast.AST) -> None:
        if not self.is_constexpr:
            # can short-circuit if we've already detected that it's not a constexpr
            return
        super().visit(node)

    def __call__(self, node: ast.AST) -> bool:
        self.is_constexpr = True
        self.visit(node)
        return self.is_constexpr


class AutoreloadTree:
    """
    Recursive data structure to keep track of reloadable functions/methods. Each object corresponds to a specific scope level.
    children: classes inside given scope, maps class name to autoreload tree for that class's scope
    funcs_to_autoreload: list of function names that can be autoreloaded in given scope.
    new_nested_classes: Classes getting added in new autoreload cycle
    """

    def __init__(self) -> None:
        self.children: dict[str, AutoreloadTree] = {}
        self.defs_to_reload: list[tuple[tuple[str, ...], ast.AST]] = []
        self.defs_to_delete: set[str] = set()
        self.new_nested_classes: dict[str, ast.AST] = {}

    def traverse_prefixes(self, prefixes: list[str]) -> AutoreloadTree:
        """
        Return ref to the AutoreloadTree at the namespace specified by prefixes
        """
        cur = self
        for prefix in prefixes:
            if prefix not in cur.children:
                cur.children[prefix] = AutoreloadTree()
            cur = cur.children[prefix]
        return cur


class DeduperReloader(DeduperReloaderPatchingMixin):
    """
    This version of autoreload detects when we can leverage targeted recompilation of a subset of a module and patching
    existing function/method objects to reflect these changes.

    Detects what functions/methods can be reloaded by recursively comparing the old/new AST of module-level classes,
    module-level classes' methods, recursing through nested classes' methods. If other changes are made, original
    autoreload algorithm is called directly.
    """

    def __init__(self) -> None:
        self._to_autoreload: AutoreloadTree = AutoreloadTree()
        self.source_by_modname: dict[str, str] = {}
        self.dependency_graph: dict[tuple[str, ...], list[DependencyNode]] = {}
        self._enabled = True

    @property
    def enabled(self) -> bool:
        return self._enabled and platform.python_implementation() == "CPython"

    @enabled.setter
    def enabled(self, value: bool) -> None:
        self._enabled = value

    def update_sources(self) -> None:
        """
        Update dictionary source_by_modname with current modules' source codes.
        """
        if not self.enabled:
            return
        for new_modname in sys.modules.keys() - self.source_by_modname.keys():
            new_module = sys.modules[new_modname]
            if (
                (fname := get_module_file_name(new_module)) is None
                or "site-packages" in fname
                or "dist-packages" in fname
                or not os.access(fname, os.R_OK)
            ):
                self.source_by_modname[new_modname] = ""
                continue
            with open(fname, "r") as f:
                try:
                    self.source_by_modname[new_modname] = f.read()
                except Exception:
                    self.source_by_modname[new_modname] = ""

    constexpr_detector = ConstexprDetector()

    @staticmethod
    def is_enum_subclass(node: ast.Module | ast.ClassDef) -> bool:
        if isinstance(node, ast.Module):
            return False
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id == "Enum":
                return True
            elif (
                isinstance(base, ast.Attribute)
                and base.attr == "Enum"
                and isinstance(base.value, ast.Name)
                and base.value.id == "enum"
            ):
                return True
        return False

    @classmethod
    def is_constexpr_assign(
        cls, node: ast.AST, parent_node: ast.Module | ast.ClassDef
    ) -> bool:
        if not isinstance(node, (ast.Assign, ast.AnnAssign)) or node.value is None:
            return False
        if cls.is_enum_subclass(parent_node):
            return False
        for target in node.targets if isinstance(node, ast.Assign) else [node.target]:
            if not isinstance(target, ast.Name):
                return False
        return cls.constexpr_detector(node.value)

    @classmethod
    def _gather_children(
        cls, body: list[ast.stmt], parent_node: ast.Module | ast.ClassDef
    ) -> GatherResult:
        """
        Given list of ast elements, return:
        1. dict mapping function names to their ASTs.
        2. dict mapping class names to their ASTs.
        3. list of any other ASTs.
        """
        result = GatherResult.create()
        for ast_node in body:
            ast_elt: ast.expr | ast.stmt = ast_node
            while isinstance(ast_elt, ast.Expr):
                ast_elt = ast_elt.value
            if isinstance(ast_elt, (ast.FunctionDef, ast.AsyncFunctionDef)):
                result.function_defs.append(((ast_elt.name,), ast_elt))
            elif isinstance(ast_elt, (ast.Import, ast.ImportFrom)):
                result.import_defs.append(
                    (tuple(name.asname or name.name for name in ast_elt.names), ast_elt)
                )
            elif isinstance(ast_elt, ast.ClassDef):
                result.classes[ast_elt.name] = ast_elt
            elif isinstance(ast_elt, ast.If):
                result.unfixable.append(ast_elt.test)
                result.inplace_merge(cls._gather_children(ast_elt.body, parent_node))
                result.inplace_merge(cls._gather_children(ast_elt.orelse, parent_node))
            elif isinstance(ast_elt, (ast.AsyncWith, ast.With)):
                result.unfixable.extend(ast_elt.items)
                result.inplace_merge(cls._gather_children(ast_elt.body, parent_node))
            elif isinstance(ast_elt, ast.Try):
                result.inplace_merge(cls._gather_children(ast_elt.body, parent_node))
                result.inplace_merge(cls._gather_children(ast_elt.orelse, parent_node))
                result.inplace_merge(
                    cls._gather_children(ast_elt.finalbody, parent_node)
                )
                for handler in ast_elt.handlers:
                    if handler.type is not None:
                        result.unfixable.append(handler.type)
                    result.inplace_merge(
                        cls._gather_children(handler.body, parent_node)
                    )
            elif not isinstance(ast_elt, (ast.Constant, ast.Pass)):
                if cls.is_constexpr_assign(ast_elt, parent_node):
                    assert isinstance(ast_elt, (ast.Assign, ast.AnnAssign))
                    targets = (
                        ast_elt.targets
                        if isinstance(ast_elt, ast.Assign)
                        else [ast_elt.target]
                    )
                    result.assign_defs.append(
                        (
                            tuple(cast(ast.Name, target).id for target in targets),
                            ast_elt,
                        )
                    )
                else:
                    result.unfixable.append(ast_elt)
        return result

    def detect_autoreload(
        self,
        old_node: ast.Module | ast.ClassDef,
        new_node: ast.Module | ast.ClassDef,
        prefixes: list[str] | None = None,
    ) -> bool:
        """
        Returns
        -------
        `True` if we can run our targeted autoreload algorithm safely.
        `False` if we should instead use IPython's original autoreload implementation.
        """
        if not self.enabled:
            return False
        prefixes = prefixes or []

        old_result = self._gather_children(old_node.body, old_node)
        new_result = self._gather_children(new_node.body, new_node)
        old_defs_by_name: dict[str, ast.AST] = {
            name: ast_def for names, ast_def in old_result.all_defs() for name in names
        }
        new_defs_by_name: dict[str, ast.AST] = {
            name: ast_def for names, ast_def in new_result.all_defs() for name in names
        }

        if not compare_ast(old_result.unfixable, new_result.unfixable):
            return False

        cur = self._to_autoreload.traverse_prefixes(prefixes)
        for names, new_ast_def in new_result.all_defs():
            names_to_reload = []
            for name in names:
                if new_defs_by_name[name] is not new_ast_def:
                    continue
                if name not in old_defs_by_name or not compare_ast(
                    new_ast_def, old_defs_by_name[name]
                ):
                    names_to_reload.append(name)
            if names_to_reload:
                cur.defs_to_reload.append((tuple(names), new_ast_def))
        cur.defs_to_delete |= set(old_defs_by_name.keys()) - set(
            new_defs_by_name.keys()
        )
        for name, new_ast_def_class in new_result.classes.items():
            if name not in old_result.classes:
                cur.new_nested_classes[name] = new_ast_def_class
            elif not compare_ast(
                new_ast_def_class, old_result.classes[name]
            ) and not self.detect_autoreload(
                old_result.classes[name], new_ast_def_class, prefixes + [name]
            ):
                return False
        return True

    def _check_dependents(self) -> bool:
        """
        If a decorator function is modified, we should similarly reload the functions which are decorated by this
        decorator. Iterate through the Dependency Graph to find such cases in the given AutoreloadTree.
        """
        for node in self._check_dependents_inner():
            self._add_node_to_autoreload_tree(node)
        return True

    def _add_node_to_autoreload_tree(self, node: DependencyNode) -> None:
        """
        Given a node of the dependency graph, add decorator dependencies to the autoreload tree.
        """
        if len(node.qualified_name) == 0:
            return
        cur = self._to_autoreload.traverse_prefixes(list(node.qualified_name[:-1]))
        if node.abstract_syntax_tree is not None:
            cur.defs_to_reload.append(
                ((node.qualified_name[-1],), node.abstract_syntax_tree)
            )

    def _check_dependents_inner(
        self, prefixes: list[str] | None = None
    ) -> list[DependencyNode]:
        prefixes = prefixes or []
        cur = self._to_autoreload.traverse_prefixes(prefixes)
        ans = []
        for (func_name, *_), _ in cur.defs_to_reload:
            node = tuple(prefixes + [func_name])
            ans.extend(self._gen_dependents(node))
        for class_name in cur.new_nested_classes:
            ans.extend(self._check_dependents_inner(prefixes + [class_name]))
        return ans

    def _gen_dependents(self, qualname: tuple[str, ...]) -> list[DependencyNode]:
        ans = []
        if qualname not in self.dependency_graph:
            return []
        for elt in self.dependency_graph[qualname]:
            ans.extend(self._gen_dependents(elt.qualified_name))
            ans.append(elt)
        return ans

    def _patch_namespace_inner(
        self, ns: ModuleType | type, prefixes: list[str] | None = None
    ) -> bool:
        """
        This function patches module functions and methods. Specifically, only objects with their name in
        self.to_autoreload will be considered for patching. If an object has been marked to be autoreloaded,
        new_source_code gets executed in the old version's global environment. Then, replace the old function's
        attributes with the new function's attributes.
        """
        prefixes = prefixes or []
        cur = self._to_autoreload.traverse_prefixes(prefixes)
        namespace_to_check = ns
        for prefix in prefixes:
            namespace_to_check = namespace_to_check.__dict__[prefix]
        seen_names: set[str] = set()
        for names, new_ast_def in cur.defs_to_reload:
            if len(names) == 1 and names[0] in seen_names:
                continue
            seen_names.update(names)
            local_env: dict[str, Any] = {}
            if (
                isinstance(new_ast_def, (ast.FunctionDef, ast.AsyncFunctionDef))
                and (name := names[0]) in namespace_to_check.__dict__
            ):
                assert len(names) == 1
                to_patch_to = namespace_to_check.__dict__[name]
                if isinstance(to_patch_to, (staticmethod, classmethod)):
                    to_patch_to = to_patch_to.__func__
                # exec new source code using old function's (obj) globals environment.
                func_code = textwrap.dedent(ast.unparse(new_ast_def))
                if is_method := (len(prefixes) > 0):
                    func_code = "class __autoreload_class__:\n" + textwrap.indent(
                        func_code, "    "
                    )
                global_env = ns.__dict__
                if not isinstance(global_env, dict):
                    global_env = dict(global_env)
                # Compile with correct filename to preserve in traceback
                filename = (
                    getattr(to_patch_to, "__code__", None)
                    and to_patch_to.__code__.co_filename
                    or "<string>"
                )
                func_asts = [ast.parse(func_code)]
                if len(cast(ast.FunctionDef, func_asts[0].body[0]).decorator_list) > 0:
                    without_decorator_list = pickle.loads(pickle.dumps(func_asts[0]))
                    cast(
                        ast.FunctionDef, without_decorator_list.body[0]
                    ).decorator_list = []
                    func_asts.insert(0, without_decorator_list)
                for func_ast in func_asts:
                    compiled_code = compile(
                        func_ast, filename, mode="exec", dont_inherit=True
                    )
                    exec(compiled_code, global_env, local_env)  # type: ignore[arg-type]
                    # local_env contains the function exec'd from  new version of function
                    if is_method:
                        to_patch_from = getattr(local_env["__autoreload_class__"], name)
                    else:
                        to_patch_from = local_env[name]
                    if isinstance(to_patch_from, (staticmethod, classmethod)):
                        to_patch_from = to_patch_from.__func__
                    if isinstance(to_patch_to, property) and isinstance(
                        to_patch_from, property
                    ):
                        for attr in ("fget", "fset", "fdel"):
                            if (
                                getattr(to_patch_to, attr) is None
                                or getattr(to_patch_from, attr) is None
                            ):
                                self.try_patch_attr(to_patch_to, to_patch_from, attr)
                            else:
                                self.patch_function(
                                    getattr(to_patch_to, attr),
                                    getattr(to_patch_from, attr),
                                    is_method,
                                )
                    elif not isinstance(to_patch_to, property) and not isinstance(
                        to_patch_from, property
                    ):
                        self.patch_function(to_patch_to, to_patch_from, is_method)
                    else:
                        raise ValueError(
                            "adding or removing property decorations not supported"
                        )
            else:
                exec(
                    ast.unparse(new_ast_def),
                    ns.__dict__ | namespace_to_check.__dict__,
                    local_env,
                )
                for name in names:
                    setattr(namespace_to_check, name, local_env[name])
        cur.defs_to_reload.clear()
        for name in cur.defs_to_delete:
            try:
                delattr(namespace_to_check, name)
            except (AttributeError, TypeError, ValueError):
                # give up on deleting the attribute, let the stale one dangle
                pass
        cur.defs_to_delete.clear()
        for class_name, class_ast_node in cur.new_nested_classes.items():
            local_env_class: dict[str, Any] = {}
            exec(
                ast.unparse(class_ast_node),
                ns.__dict__ | namespace_to_check.__dict__,
                local_env_class,
            )
            setattr(namespace_to_check, class_name, local_env_class[class_name])
        cur.new_nested_classes.clear()
        for class_name in cur.children.keys():
            if not self._patch_namespace(ns, prefixes + [class_name]):
                return False
        cur.children.clear()
        return True

    def _patch_namespace(
        self, ns: ModuleType | type, prefixes: list[str] | None = None
    ) -> bool:
        """
        Wrapper for patching all elements in a namespace as specified by the to_autoreload member variable.
        Returns `true` if patching was successful, and `false` if unsuccessful.
        """
        try:
            return self._patch_namespace_inner(ns, prefixes=prefixes)
        except Exception:
            return False

    def maybe_reload_module(self, module: ModuleType) -> bool:
        """
        Uses Deduperreload to try to update a module.
        Returns `true` on success and `false` on failure.
        """
        if not self.enabled:
            return False
        if not (modname := getattr(module, "__name__", None)):
            return False
        if (fname := get_module_file_name(module)) is None:
            return False
        with open(fname, "r") as f:
            new_source_code = f.read()
        patched_flag = False
        if old_source_code := self.source_by_modname.get(modname):
            # get old/new module ast
            try:
                old_module_ast = ast.parse(old_source_code)
                new_module_ast = ast.parse(new_source_code)
            except Exception:
                return False
            # detect if we are able to use our autoreload algorithm
            ctx = contextlib.suppress()
            with ctx:
                self._build_dependency_graph(new_module_ast)
                if (
                    self.detect_autoreload(old_module_ast, new_module_ast)
                    and self._check_dependents()
                    and self._patch_namespace(module)
                ):
                    patched_flag = True

        self.source_by_modname[modname] = new_source_code
        self._to_autoreload = AutoreloadTree()
        return patched_flag

    def _separate_name(
        self,
        decorator: ast.Attribute | ast.Name | ast.Call | ast.expr,
        accept_calls: bool,
    ) -> list[str] | None:
        """
        Generates a qualified name for a given decorator by finding its relative namespace.
        """
        if isinstance(decorator, ast.Name):
            return [decorator.id]
        elif isinstance(decorator, ast.Call):
            if accept_calls:
                return self._separate_name(decorator.func, False)
            else:
                return None
        if not isinstance(decorator, ast.Attribute):
            return None
        if pref := self._separate_name(decorator.value, False):
            return pref + [decorator.attr]
        else:
            return None

    def _gather_dependents(
        self, body: list[ast.stmt], body_prefixes: list[str] | None = None
    ) -> bool:
        body_prefixes = body_prefixes or []
        for ast_node in body:
            ast_elt: ast.expr | ast.stmt = ast_node
            if isinstance(ast_elt, ast.ClassDef):
                self._gather_dependents(ast_elt.body, body_prefixes + [ast_elt.name])
                continue
            if not isinstance(ast_elt, (ast.FunctionDef, ast.AsyncFunctionDef)):
                continue
            qualified_name = tuple(body_prefixes + [ast_elt.name])
            cur_dependency_node = DependencyNode(qualified_name, ast_elt)
            for decorator in ast_elt.decorator_list:
                decorator_path = self._separate_name(decorator, True)
                if not decorator_path:
                    continue
                decorator_path_tuple = tuple(decorator_path)
                self.dependency_graph.setdefault(decorator_path_tuple, []).append(
                    cur_dependency_node
                )
        return True

    def _build_dependency_graph(self, new_ast: ast.Module | ast.ClassDef) -> bool:
        """
        Wrapper function for generating dependency graph given some AST.
        Returns `true` on success. Returns `false` on failure.
        Currently, only returns `true` as we do not block on failure to build this graph.
        """
        return self._gather_dependents(new_ast.body)


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\deduperreload\deduperreload.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\deduperreload\deduperreload_patching.py (Section: ANCHOR_PYTHON) ---

from __future__ import annotations
import ctypes
import sys
from typing import Any

NOT_FOUND: object = object()
_MAX_FIELD_SEARCH_OFFSET = 50

if sys.maxsize > 2**32:
    WORD_TYPE: type[ctypes.c_int32] | type[ctypes.c_int64] = ctypes.c_int64
    WORD_N_BYTES = 8
else:
    WORD_TYPE = ctypes.c_int32
    WORD_N_BYTES = 4


class DeduperReloaderPatchingMixin:
    @staticmethod
    def infer_field_offset(
        obj: object,
        field: str,
    ) -> int:
        field_value = getattr(obj, field, NOT_FOUND)
        if field_value is NOT_FOUND:
            return -1
        obj_addr = ctypes.c_void_p.from_buffer(ctypes.py_object(obj)).value
        field_addr = ctypes.c_void_p.from_buffer(ctypes.py_object(field_value)).value
        if obj_addr is None or field_addr is None:
            return -1
        ret = -1
        for offset in range(1, _MAX_FIELD_SEARCH_OFFSET):
            if (
                ctypes.cast(
                    obj_addr + WORD_N_BYTES * offset, ctypes.POINTER(WORD_TYPE)
                ).contents.value
                == field_addr
            ):
                ret = offset
                break
        return ret

    @classmethod
    def try_write_readonly_attr(
        cls,
        obj: object,
        field: str,
        new_value: object,
        offset: int | None = None,
    ) -> None:
        prev_value = getattr(obj, field, NOT_FOUND)
        if prev_value is NOT_FOUND:
            return
        if offset is None:
            offset = cls.infer_field_offset(obj, field)
        if offset == -1:
            return
        obj_addr = ctypes.c_void_p.from_buffer(ctypes.py_object(obj)).value
        new_value_addr = ctypes.c_void_p.from_buffer(ctypes.py_object(new_value)).value
        if obj_addr is None or new_value_addr is None:
            return
        if prev_value is not None:
            ctypes.pythonapi.Py_DecRef(ctypes.py_object(prev_value))
        if new_value is not None:
            ctypes.pythonapi.Py_IncRef(ctypes.py_object(new_value))
        ctypes.cast(
            obj_addr + WORD_N_BYTES * offset, ctypes.POINTER(WORD_TYPE)
        ).contents.value = new_value_addr

    @classmethod
    def try_patch_readonly_attr(
        cls,
        old: object,
        new: object,
        field: str,
        new_is_value: bool = False,
        offset: int = -1,
    ) -> None:

        old_value = getattr(old, field, NOT_FOUND)
        new_value = new if new_is_value else getattr(new, field, NOT_FOUND)
        if old_value is NOT_FOUND or new_value is NOT_FOUND:
            return
        elif old_value is new_value:
            return
        elif old_value is not None and offset < 0:
            offset = cls.infer_field_offset(old, field)
        elif offset < 0:
            assert not new_is_value
            assert new_value is not None
            offset = cls.infer_field_offset(new, field)
        cls.try_write_readonly_attr(old, field, new_value, offset=offset)

    @classmethod
    def try_patch_attr(
        cls,
        old: object,
        new: object,
        field: str,
        new_is_value: bool = False,
        offset: int = -1,
    ) -> None:
        try:
            setattr(old, field, new if new_is_value else getattr(new, field))
        except (AttributeError, TypeError, ValueError):
            cls.try_patch_readonly_attr(old, new, field, new_is_value, offset)

    @classmethod
    def patch_function(
        cls, to_patch_to: Any, to_patch_from: Any, is_method: bool
    ) -> None:
        new_freevars = []
        new_closure = []
        for freevar, closure_val in zip(
            to_patch_from.__code__.co_freevars or [], to_patch_from.__closure__ or []
        ):
            new_freevars.append(freevar)
            if (
                callable(closure_val.cell_contents)
                and freevar in to_patch_to.__code__.co_freevars
            ):
                new_closure.append(
                    to_patch_to.__closure__[
                        to_patch_to.__code__.co_freevars.index(freevar)
                    ]
                )
            else:
                new_closure.append(closure_val)
        code_with_new_freevars = to_patch_from.__code__.replace(
            co_freevars=tuple(new_freevars)
        )
        # lambdas may complain if there is more than one freevar
        cls.try_patch_attr(
            to_patch_to, code_with_new_freevars, "__code__", new_is_value=True
        )
        offset = -1
        if to_patch_to.__closure__ is None and to_patch_from.__closure__ is not None:
            offset = cls.infer_field_offset(to_patch_from, "__closure__")
        cls.try_patch_readonly_attr(
            to_patch_to,
            tuple(new_closure) or None,
            "__closure__",
            new_is_value=True,
            offset=offset,
        )
        for attr in ("__defaults__", "__kwdefaults__", "__doc__", "__dict__"):
            cls.try_patch_attr(to_patch_to, to_patch_from, attr)
        if is_method:
            cls.try_patch_readonly_attr(to_patch_to, to_patch_from, "__self__")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\dist\anchor\_internal\IPython\extensions\deduperreload\deduperreload_patching.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\client.py (Section: ANCHOR_PYTHON) ---

#!/usr/bin/env python3
"""
Minimal MCP client used by Anchor CLI to call ECE's MCP endpoints.
This is intentionally small and provides only get_tools() and call_tool().
"""
from __future__ import annotations

import httpx
from typing import Any, Dict, Optional


class MCPClient:
    def __init__(self, base_url: str, api_key: Optional[str] = None, timeout: int = 10):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self._timeout = timeout

    def _build_headers(self) -> Dict[str, str]:
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    async def get_tools(self) -> Any:
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            resp = await client.get(f"{self.base_url}/mcp/tools", headers=self._build_headers())
            resp.raise_for_status()
            return resp.json()

    async def call_tool(self, name: str, **arguments) -> Any:
        payload = {"name": name, "arguments": arguments}
        async with httpx.AsyncClient(timeout=self._timeout) as client:
            resp = await client.post(f"{self.base_url}/mcp/call", json=payload, headers=self._build_headers())
            # Do not raise here; return detailed error
            if resp.status_code >= 400:
                return {"status": "error", "status_code": resp.status_code, "error": resp.text}
            return resp.json()
"""
THIS FILE HAS BEEN DELETED (2025-11-28)

The MCP client implementation was removed from the active runtime and preserved in
`archive/removed_tool_protocols/mcp-utcp/anchor/mcp/` for audit history.
"""

raise ImportError("MCP client DELETED on 2025-11-28; see archive/removed_tool_protocols/mcp-utcp/anchor/mcp/")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\client.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\security.py (Section: ANCHOR_PYTHON) ---

"""
THIS FILE HAS BEEN DELETED (2025-11-28)

The MCP security utilities were removed from the active runtime and preserved in
`archive/removed_tool_protocols/mcp-utcp/anchor/mcp/` for audit history.
"""

raise ImportError("MCP security DELETED on 2025-11-28; see archive/removed_tool_protocols/mcp-utcp/anchor/mcp/")


# In test contexts we provide basic filesystem and shell security helpers so
# archived MCP tools work. In production, this module is intentionally archived
# and will raise ImportError (to avoid using archived functionality accidentally).
if os.environ.get("ANCHOR_ALLOW_ARCHIVED_MCP_TESTS", "false").lower() not in ("1", "true", "yes"):
    import os
    from pathlib import Path
    import shlex
    import subprocess
    from typing import Tuple, Any, Dict

    # To keep the repo secure in production, the MCP security utilities are archived
    # and by default cause an ImportError. For local tests, set the env var
    # ANCHOR_ALLOW_ARCHIVED_MCP_TESTS=1 to get a minimal, non-secure implementation
    # that provides the expected interfaces for the tests.
    if os.environ.get("ANCHOR_ALLOW_ARCHIVED_MCP_TESTS", "false").lower() not in ("1", "true", "yes"):
        raise ImportError("MCP security utilities archived; see archive/removed_tool_protocols/mcp-utcp/anchor/mcp/")


    class FileSystemSecurity:
        def validate_path(self, path: str) -> Tuple[bool, str]:
            p = Path(path)
            if not p.exists():
                return False, f"Path does not exist: {path}"
            return True, ""

        def read_safe(self, path: str) -> Dict[str, Any]:
            p = Path(path)
            if not p.exists():
                return {"error": f"Path not found: {path}"}
            if p.is_dir():
                entries = [str(ch) for ch in p.iterdir()]
                return {"path": str(p), "is_dir": True, "entries": entries}
            try:
                content = p.read_text(encoding="utf-8", errors="ignore")
                return {"path": str(p), "is_dir": False, "content": content}
            except Exception as e:
                return {"error": str(e)}

        def write_safe(self, path: str, content: str, append: bool = False) -> Dict[str, Any]:
            try:
                p = Path(path)
                p.parent.mkdir(parents=True, exist_ok=True)
                mode = "a" if append else "w"
                p.write_text(content, encoding="utf-8")
                return {"path": str(p), "ok": True}
            except Exception as e:
                return {"error": str(e)}


    class ShellSecurity:
        def execute_safe(self, command: str, timeout: int = 30) -> Dict[str, Any]:
            try:
                args = shlex.split(command)
                res = subprocess.run(args, capture_output=True, text=True, timeout=timeout)
                return {"stdout": res.stdout, "stderr": res.stderr, "returncode": res.returncode}
            except Exception as e:
                return {"error": str(e)}


    # Instances used by the archived server module and tests
    filesystem_security = FileSystemSecurity()
    shell_security = ShellSecurity()



class FilesystemSecurity:
    @staticmethod
    def validate_path(root: str) -> Tuple[bool, str]:
        p = Path(root)
        if not p.exists() or not p.is_dir():
            return False, f"Root not a directory: {root}"
        return True, ""

    @staticmethod
    def read_safe(path: str) -> dict:
        try:
            p = Path(path)
            if p.is_dir():
                return {"error": "Path is a directory", "path": str(p)}
            content = p.read_text(encoding="utf-8", errors="ignore")
            return {"status": "success", "content": content}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    @staticmethod
    def write_safe(path: str, content: str, append: bool = False) -> dict:
        try:
            p = Path(path)
            mode = "a" if append else "w"
            p.parent.mkdir(parents=True, exist_ok=True)
            p.write_text(content, encoding="utf-8")
            return {"status": "success", "path": str(p)}
        except Exception as e:
            return {"status": "error", "error": str(e)}


class ShellSecurity:
    @staticmethod
    def execute_safe(command: str, timeout: int = 30) -> dict:
        try:
            # NOTE: This is a simplified execution wrapper for tests
            completed = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout)
            return {"status": "success", "output": completed.stdout, "returncode": completed.returncode}
        except Exception as e:
            return {"status": "error", "error": str(e)}


filesystem_security = FilesystemSecurity()
shell_security = ShellSecurity()


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\security.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\server.py (Section: ANCHOR_PYTHON) ---

#!/usr/bin/env python3
"""
THIS FILE HAS BEEN DELETED (2025-11-28)

The MCP server implementation was removed from the active runtime and preserved in
`archive/removed_tool_protocols/mcp-utcp/anchor/mcp/` for audit history.
"""

raise ImportError("MCP server DELETED on 2025-11-28; see archive/removed_tool_protocols/mcp-utcp/anchor/mcp/")
import asyncio
import json
import subprocess
import os
from pathlib import Path
from typing import Any, List
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx

# Import security modules
try:
    from mcp.security import shell_security, filesystem_security
except ImportError:
    # Fallback for running server.py directly
    from security import shell_security, filesystem_security

app = FastAPI(title="Anchor MCP Server")

# ============================================================================
# SCHEMAS
# ============================================================================

class ToolSchema(BaseModel):
    name: str
    description: str
    inputSchema: dict

class ToolCall(BaseModel):
    name: str
    arguments: dict

# ============================================================================
# MCP TOOLS - FILESYSTEM
# ============================================================================

FILESYSTEM_TOOL = ToolSchema(
    name="filesystem_read",
    description="Read contents of a file or list directory contents",
    inputSchema={
        "type": "object",
        "properties": {
            "path": {
                "type": "string",
                "description": "Absolute path to file or directory"
            }
        },
        "required": ["path"]
    }
)

def filesystem_read(path: str) -> dict:
    """Read file or list directory with security checks"""
    return filesystem_security.read_safe(path)

# Safe write tool
FILESYSTEM_WRITE_TOOL = ToolSchema(
    name="filesystem_write",
    description="Write or append text content to a file (safe extensions only)",
    inputSchema={
        "type": "object",
        "properties": {
            "path": {"type": "string", "description": "Absolute path to file"},
            "content": {"type": "string", "description": "Text content to write"},
            "append": {"type": "boolean", "description": "Append instead of overwrite (default: false)"}
        },
        "required": ["path", "content"]
    }
)

def filesystem_write(path: str, content: str, append: bool = False) -> dict:
    return filesystem_security.write_safe(path, content, append)

# ============================================================================
# MCP TOOLS - SHELL COMMANDS
# ============================================================================

SHELL_TOOL = ToolSchema(
    name="shell_execute",
    description="Execute a shell command and return output",
    inputSchema={
        "type": "object",
        "properties": {
            "command": {
                "type": "string",
                "description": "Shell command to execute (e.g., 'ls -la', 'python --version')"
            },
            "timeout": {
                "type": "number",
                "description": "Timeout in seconds (default: 30)"
            }
        },
        "required": ["command"]
    }
)

def shell_execute(command: str, timeout: int = 30) -> dict:
    """Execute shell command with security checks"""
    return shell_security.execute_safe(command, timeout)

# ============================================================================
# MCP TOOLS - WEB SEARCH
# ============================================================================

WEB_SEARCH_TOOL = ToolSchema(
    name="web_search",
    description="Search the web using DuckDuckGo",
    inputSchema={
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "Search query"
            },
            "max_results": {
                "type": "number",
                "description": "Maximum number of results (default: 5)"
            }
        },
        "required": ["query"]
    }
)

async def web_search(query: str, max_results: int = 5) -> dict:
    """Search the web"""
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            # Using DuckDuckGo HTML search (no API key needed)
            response = await client.get(
                "https://html.duckduckgo.com/",
                params={"q": query},
                headers={"User-Agent": "Mozilla/5.0"}
            )
            
            # Basic HTML parsing (would use BeautifulSoup in production)
            if response.status_code == 200:
                return {
                    "query": query,
                    "status": "success",
                    "message": f"Search completed for: {query}",
                    "results_requested": max_results,
                    "note": "To get actual results, integrate with DuckDuckGo API or use search library"
                }
            else:
                return {"error": f"Search failed with status {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# MCP TOOLS - CODE SEARCH
# ============================================================================

CODE_SEARCH_TOOL = ToolSchema(
    name="code_search",
    description="Search code files under a root for a query (substring or regex)",
    inputSchema={
        "type": "object",
        "properties": {
            "root": {"type": "string", "description": "Root directory to search"},
            "query": {"type": "string", "description": "Search string or regex"},
            "regex": {"type": "boolean", "description": "Treat query as regex (default: false)"},
            "max_results": {"type": "number", "description": "Max results (default: 50)"}
        },
        "required": ["root", "query"]
    }
)

def _is_code_file(path: Path) -> bool:
    exts = {".py", ".ts", ".tsx", ".js", ".jsx", ".json", ".yml", ".yaml", ".md", ".toml", ".ini", ".cfg", ".go", ".rs", ".java", ".c", ".h", ".cpp", ".cs", ".txt"}
    return path.suffix.lower() in exts

def code_search(root: str, query: str, regex: bool = False, max_results: int = 50, glob: str = None, context: int = 2) -> dict:
    ok, err = filesystem_security.validate_path(root)
    if not ok:
        return {"error": err, "root": root, "blocked": True}
    try:
        r = Path(root)
        if not r.exists() or not r.is_dir():
            return {"error": f"Root not a directory: {root}"}
        results: List[dict] = []
        import re as _re
        pattern = None
        if regex:
            try:
                pattern = _re.compile(query)
            except Exception as e:
                return {"error": f"Invalid regex: {e}"}
        max_file_size = 500000
        import fnmatch as _fn
        for dirpath, dirnames, filenames in os.walk(r):
            for name in filenames:
                p = Path(dirpath) / name
                if not _is_code_file(p):
                    continue
                if glob and not _fn.fnmatch(name, glob):
                    continue
                try:
                    size = p.stat().st_size
                    if size > max_file_size:
                        continue
                    lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
                    matches = []
                    for i, line in enumerate(lines):
                        hit = False
                        if pattern:
                            if pattern.search(line):
                                hit = True
                        else:
                            if query.lower() in line.lower():
                                hit = True
                        if hit:
                            start = max(i - context, 0)
                            end = min(i + context + 1, len(lines))
                            snippet = "\n".join(f"{j+1}: {lines[j]}" for j in range(start, end))
                            matches.append({"line": i + 1, "snippet": snippet})
                            if len(matches) >= 3:
                                break
                    if matches:
                        results.append({"path": str(p), "size": size, "matches": matches})
                        if len(results) >= max_results:
                            break
                except Exception:
                    continue
            if len(results) >= max_results:
                break
        return {"root": str(r), "query": query, "count": len(results), "results": results}
    except Exception as e:
        return {"error": str(e)}

def code_grep(root: str, query: str, regex: bool = False, max_results: int = 50, glob: str = None, exclude_globs: List[str] = None, context: int = 2) -> dict:
    ok, err = filesystem_security.validate_path(root)
    if not ok:
        return {"error": err, "root": root, "blocked": True}
    try:
        r = Path(root)
        if not r.exists() or not r.is_dir():
            return {"error": f"Root not a directory: {root}"}
        import re as _re
        import fnmatch as _fn
        pattern = None
        if regex:
            try:
                pattern = _re.compile(query)
            except Exception as e:
                return {"error": f"Invalid regex: {e}"}
        total_matches = 0
        results: List[dict] = []
        max_file_size = 500000
        for dirpath, dirnames, filenames in os.walk(r):
            for name in filenames:
                p = Path(dirpath) / name
                if not _is_code_file(p):
                    continue
                if glob and not _fn.fnmatch(name, glob):
                    continue
                if exclude_globs:
                    excluded = False
                    for eg in exclude_globs:
                        if _fn.fnmatch(name, eg):
                            excluded = True
                            break
                    if excluded:
                        continue
                try:
                    size = p.stat().st_size
                    if size > max_file_size:
                        continue
                    lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
                    matches = []
                    for i, line in enumerate(lines):
                        hit = False
                        if pattern:
                            if pattern.search(line):
                                hit = True
                        else:
                            if query.lower() in line.lower():
                                hit = True
                        if hit:
                            total_matches += 1
                            start = max(i - context, 0)
                            end = min(i + context + 1, len(lines))
                            snippet = "\n".join(f"{j+1}: {lines[j]}" for j in range(start, end))
                            matches.append({"line": i + 1, "snippet": snippet})
                            if len(matches) >= 5:
                                break
                    if matches:
                        results.append({"path": str(p), "size": size, "match_count": len(matches), "matches": matches})
                        if len(results) >= max_results:
                            break
                except Exception:
                    continue
            if len(results) >= max_results:
                break
        return {"root": str(r), "query": query, "files": len(results), "total_matches": total_matches, "results": results}
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# MCP ENDPOINTS
# ============================================================================

@app.get("/mcp/tools")
async def list_tools():
    """List available tools with their schemas"""
    return {
        "tools": [
            FILESYSTEM_TOOL.dict(),
            FILESYSTEM_WRITE_TOOL.dict(),
            SHELL_TOOL.dict(),
            WEB_SEARCH_TOOL.dict(),
            CODE_SEARCH_TOOL.dict(),
            {
                "name": "code_grep",
                "description": "Grep-like search with match counts and optional exclusions",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "root": {"type": "string"},
                        "query": {"type": "string"},
                        "regex": {"type": "boolean"},
                        "max_results": {"type": "number"},
                        "glob": {"type": "string"},
                        "exclude_globs": {"type": "array", "items": {"type": "string"}},
                        "context": {"type": "number"}
                    },
                    "required": ["root", "query"]
                }
            }
        ]
    }

@app.post("/mcp/call")
async def call_tool(tool_call: ToolCall):
    """Execute a tool call"""
    try:
        if tool_call.name == "filesystem_read":
            result = filesystem_read(**tool_call.arguments)
        elif tool_call.name == "filesystem_write":
            result = filesystem_write(**tool_call.arguments)
        elif tool_call.name == "shell_execute":
            result = shell_execute(**tool_call.arguments)
        elif tool_call.name == "web_search":
            result = await web_search(**tool_call.arguments)
        elif tool_call.name == "code_search":
            result = code_search(**tool_call.arguments)
        elif tool_call.name == "code_grep":
            result = code_grep(**tool_call.arguments)
        else:
            raise HTTPException(status_code=404, detail=f"Tool not found: {tool_call.name}")
        
        return {
            "tool": tool_call.name,
            "status": "success",
            "result": result
        }
    except Exception as e:
        return {
            "tool": tool_call.name,
            "status": "error",
            "error": str(e)
        }

@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "ok",
        "service": "Anchor MCP Server",
        "tools": 6
    }

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    print("MCP server functionality archived. See archive/removed_tool_protocols/mcp-utcp/anchor/mcp/")


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\mcp\server.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tools\code_tools.py (Section: ANCHOR_PYTHON) ---

from pathlib import Path
from typing import List, Dict, Any, Optional
import os
import fnmatch as _fn
import re as _re


def _is_code_file(path: Path) -> bool:
    exts = {".py", ".ts", ".tsx", ".js", ".jsx", ".json", ".yml", ".yaml", ".md", ".toml", ".ini", ".cfg", ".go", ".rs", ".java", ".c", ".h", ".cpp", ".cs", ".txt"}
    return path.suffix.lower() in exts


def code_search(root: str, query: str, regex: bool = False, max_results: int = 50, glob: Optional[str] = None, context: int = 2) -> Dict[str, Any]:
    # Sanitize and validate the root path to prevent malformed paths
    if '..->' in root or '->' in root.replace('.', ''):
        return {"error": f"Invalid path format: {root}"}

    root = root.strip()
    if root.startswith('../') or '../' in root:
        return {"error": f"Directory traversal not allowed: {root}"}

    r = Path(root)
    if not r.exists() or not r.is_dir():
        return {"error": f"Root not a directory: {root}"}
    results: List[dict] = []
    pattern = None
    if regex:
        try:
            pattern = _re.compile(query)
        except Exception as e:
            return {"error": f"Invalid regex: {e}"}
    max_file_size = 500000
    for dirpath, dirnames, filenames in os.walk(r):
        for name in filenames:
            p = Path(dirpath) / name
            if not _is_code_file(p):
                continue
            if glob and not _fn.fnmatch(name, glob):
                continue
            try:
                size = p.stat().st_size
                if size > max_file_size:
                    continue
                lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
                matches = []
                for i, line in enumerate(lines):
                    hit = False
                    if pattern:
                        if pattern.search(line):
                            hit = True
                    else:
                        if query.lower() in line.lower():
                            hit = True
                    if hit:
                        start = max(i - context, 0)
                        end = min(i + context + 1, len(lines))
                        snippet = "\n".join(f"{j+1}: {lines[j]}" for j in range(start, end))
                        matches.append({"line": i + 1, "snippet": snippet})
                        if len(matches) >= 3:
                            break
                if matches:
                    results.append({"path": str(p), "size": size, "matches": matches})
                    if len(results) >= max_results:
                        break
            except Exception:
                continue
        if len(results) >= max_results:
            break
    return {"root": str(r), "query": query, "count": len(results), "results": results}


def code_grep(root: str, query: str, regex: bool = False, max_results: int = 50, glob: Optional[str] = None, exclude_globs: Optional[List[str]] = None, context: int = 2) -> Dict[str, Any]:
    # Sanitize and validate the root path to prevent malformed paths
    if '..->' in root or '->' in root.replace('.', ''):
        return {"error": f"Invalid path format: {root}"}

    root = root.strip()
    if root.startswith('../') or '../' in root:
        return {"error": f"Directory traversal not allowed: {root}"}

    r = Path(root)
    if not r.exists() or not r.is_dir():
        return {"error": f"Root not a directory: {root}"}
    pattern = None
    if regex:
        try:
            pattern = _re.compile(query)
        except Exception as e:
            return {"error": f"Invalid regex: {e}"}
    total_matches = 0
    results: List[dict] = []
    max_file_size = 500000
    for dirpath, dirnames, filenames in os.walk(r):
        for name in filenames:
            p = Path(dirpath) / name
            if not _is_code_file(p):
                continue
            if glob and not _fn.fnmatch(name, glob):
                continue
            if exclude_globs:
                excluded = False
                for eg in exclude_globs:
                    if _fn.fnmatch(name, eg):
                        excluded = True
                        break
                if excluded:
                    continue
            try:
                size = p.stat().st_size
                if size > max_file_size:
                    continue
                lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
                matches = []
                for i, line in enumerate(lines):
                    hit = False
                    if pattern:
                        if pattern.search(line):
                            hit = True
                    else:
                        if query.lower() in line.lower():
                            hit = True
                    if hit:
                        total_matches += 1
                        start = max(i - context, 0)
                        end = min(i + context + 1, len(lines))
                        snippet = "\n".join(f"{j+1}: {lines[j]}" for j in range(start, end))
                        matches.append({"line": i + 1, "snippet": snippet})
                        if len(matches) >= 5:
                            break
                if matches:
                    results.append({"path": str(p), "size": size, "match_count": len(matches), "matches": matches})
                    if len(results) >= max_results:
                        break
            except Exception:
                continue
        if len(results) >= max_results:
            break
    return {"root": str(r), "query": query, "files": len(results), "total_matches": total_matches, "results": results}


--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\tools\code_tools.py ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\spec.md (Section: ANCHOR_SPECS) ---

# Anchor CLI - Technical Specification

## Mission

Build **"The Body"** - a lightweight terminal interface that connects to the ECE_Core backend for memory-enhanced conversations and tool execution.

**Philosophy**: Minimal surface area, maximum cognitive amplification.

## Architecture Overview

### Terminal Interface
- **Type**: Pure Python CLI with streaming responses
- **Communication**: HTTP/SSE to `localhost:8000`
- **State**: Session management via local storage
- **Security**: T-101 Safety Protocols (Human-in-the-Loop confirmation flows)

### Integration Points
- **ECE API**: Connects to ECE_Core at `http://localhost:8000` for memory-enhanced conversations
- **Tool Execution**: Pattern-based tool mode for reliable execution (Simple Tool Mode)
- **Memory Access**: Direct API access to Neo4j/Redis memory graph for reading/writing

### Deployment
- **Package**: Can be packaged as standalone executable via PyInstaller
- **Dependencies**: Minimal (only core Python + requests + asyncio)

## Component Architecture

### Core Components
1. **Input Handler**: Processes user input and command parsing
2. **Logic Handler**: Orchestrates API calls and memory operations
3. **Display Handler**: Formats and streams responses to terminal
4. **Tool Executor**: Executes tools via pattern-based Simple Tool Mode

### API Interface
- **Chat Endpoint**: `POST /chat/stream` for memory-enhanced conversations
- **Memory Endpoints**: 
  - `POST /memory/add` - Add to memory graph
  - `POST /memory/search` - Search with graph relationships
  - `GET /memory/summaries` - Get session summaries
- **Health Check**: `GET /health` for ECE_Core availability
- **MCP Endpoints**: `/mcp/tools` and `/mcp/call` when MCP enabled

### Tool Execution Architecture

#### Simple Tool Mode (Pattern-Based Execution for ≤8B models)
For smaller models that struggle with structured tool protocols:
1. **Pattern Recognition**: Detect tool calls in model responses using regex patterns
2. **Parameter Extraction**: Parse arguments from text patterns
3. **Execution**: Execute local tools with safety checks
4. **Result Injection**: Return results to model in structured format

**Available Tools** (via pattern matching):
- `web_search(query)` - DuckDuckGo search results
- `read_file(path)` - File content reading with safety checks
- `list_dir(path)` - Directory listing with recursion limits
- `shell_exec(command)` - Shell command execution with confirmation

#### Structured Tool Mode (Full Protocol for ≥14B models)
For larger models that support structured tool calling:
1. **JSON Schema**: Proper function definitions with type hints
2. **Structured Calls**: Native JSON tool call format
3. **Error Handling**: Structured error responses
4. **Result Formatting**: Proper JSON result format

## Safety & Sovereignty Architecture

### T-101 Safety Protocols
All potentially dangerous operations require explicit human confirmation:

**SAFE Operations** (No Confirmation Required):
- File reading operations (`read_file`, `list_dir`)
- Web search operations (`web_search`)
- Memory read operations (`search_memories`, `get_summaries`)

**DANGEROUS Operations** (Explicit Confirmation Required):
- File writing/deletion (`write_file`, `delete_file`)
- Shell execution (`shell_exec`)
- Memory modification (`add_memory`, `update_memory`)
- Network calls to external services

### Confirmation Flow
1. **Detection**: Tool executor identifies dangerous operation
2. **Prompt**: Ask user for explicit confirmation
3. **Execution**: Only execute if user confirms
4. **Logging**: Record all confirmed actions for audit trail

### Memory Sovereignty
- **Local Storage**: Session history stored locally in anchor directory
- **Selective Sharing**: User chooses what context to share with LLM
- **Clear Boundaries**: Clear distinction between local memory and model processing

## Performance Characteristics

### Responsiveness
- **Streaming**: Real-time response streaming when ECE supports Server-Sent Events
- **Latency**: Minimal overhead beyond ECE_Core processing time
- **Offline Handling**: Graceful error messages when ECE_Core unavailable

### Memory Management
- **Local History**: Maintain conversation history in local storage
- **Context Limits**: Respect ECE_Core session limits
- **Cleanup**: Automatic cleanup of old session data

## Security Model

### Authentication
- **Optional API Keys**: When ECE_Core requires authentication
- **Local Communication**: Communication only via localhost (hardened against external access)
- **Path Validation**: Strict validation of file paths in tool operations

### Authorization
- **Read-Only Default**: Tools operate in read-only mode unless explicitly authorized
- **Scope Limitation**: Tools restricted to user-specified directories
- **Confirmation Flows**: User ratifies all write/modify operations

### Privacy Protection
- **Local Processing**: No external communication except to ECE_Core
- **Selective Context**: User controls what information is sent to ECE
- **History Management**: Local history does not persist sensitive information

## Integration with Infinite Context Pipeline

### Context Rotation Awareness
- **Large Input Handling**: Properly handles large inputs that may trigger context rotation in ECE
- **Response Continuation**: Maintains conversation state across ECE context rotations
- **Memory Continuity**: Preserves local session state even when ECE creates ContextGists

### Tool Execution with Memory
- **Memory Tools**: Can execute MCP tools that manipulate memory graph
- **Context Injection**: Properly formats memory retrieval results for model consumption
- **Historical Access**: Can query historical ContextGist memories through ECE API

## Deployment Architecture

### Standalone Executable
- **PyInstaller**: Single executable with all dependencies bundled
- **Version Bundling**: Specific versions of Python and dependencies included
- **Minimal Footprint**: ~50MB executable size target

### Configuration
- **Environment Variables**: Configurable ECE_Core URL via `ECE_URL`
- **Session Management**: Local configuration persisted across runs
- **Tool Settings**: Enable/disable specific tool categories

## Extensibility Points

### Plugin Architecture
- **Tool Plugins**: Extend available tools via new plugin modules
- **Display Plugins**: Customize response formatting
- **Input Plugins**: Extend command parsing capabilities
- **Storage Plugins**: Alternative session storage backends

### API Compatibility
- **Version Tolerance**: Backward compatibility with older ECE_Core versions
- **Feature Negotiation**: Query available features from ECE_Core API
- **Protocol Adaptation**: Adjust behavior based on ECE_Core capabilities

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\spec.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\plan.md (Section: ANCHOR_SPECS) ---

# Anchor CLI - Implementation Plan

## Vision Statement

Create a minimalist but powerful CLI interface that serves as the "Body" for the ECE_Core "Brain" - providing safe, efficient access to infinite memory-augmented conversations and reliable tool execution.

## Strategic Objectives

### Primary Goal: Cognitive Extension
Build a terminal interface that extends human cognitive capabilities by:
- Providing seamless access to infinite context memory
- Offering reliable tool execution with safety-first principles
- Maintaining human sovereignty over AI operations

### Secondary Goals
- **Privacy Preservation**: Zero telemetry, local processing only
- **Reliability**: Deterministic tool execution regardless of model capabilities
- **Simplicity**: Minimal interface, maximum functionality
- **Safety**: T-101 protocols ensuring human-in-the-loop control

## Implementation Phases

### Phase 1-4: Foundation (COMPLETED)
- [x] Basic CLI interface with streaming responses
- [x] ECE_Core API integration for memory-enhanced conversations
- [x] Pattern-based tool execution (Simple Tool Mode) for small models
- [x] T-101 safety protocols with human confirmation flows

### Phase 5: Infinite Context Integration (COMPLETED)
- [x] Context rotation awareness from ECE_Core
- [x] Handling of large inputs that trigger context rotations
- [x] Continuity maintenance across conversation boundaries
- [x] Memory sovereignty preservation during extended interactions

### Phase 6: Optimization & Enhancement (IN PROGRESS)
- [ ] Performance optimization for large context windows
- [ ] Enhanced tool safety with granular permission controls
- [ ] Improved user experience with progress indicators
- [ ] Session persistence and history management

### Phase 7: Expansion (PLANNED)
- [ ] Cross-platform standalone executables
- [ ] Advanced tool integration (IDE, email, calendar)
- [ ] Rich terminal UI with panels and visualizations
- [ ] Mobile terminal applications

## Technical Implementation Priorities

### Current Focus (Phase 6): Optimization
1. **Performance Enhancement**:
   - Optimize streaming response handling for large context windows
   - Implement local context caching for faster response times
   - Profile memory usage during extended sessions

2. **User Experience Improvements**:
   - Rich terminal interface with syntax highlighting
   - Progress indicators for long-running operations
   - Session history navigation and search
   - Configurable safety settings and confirmation flows

3. **Tool Execution Reliability**:
   - Enhanced pattern matching for Simple Tool Mode
   - Better error handling and recovery for tool operations
   - Granular safety controls per tool type

### Future Enhancements (Phase 7+)
- **Rich Terminal Interface**: TUI with panels, syntax highlighting, and visualizations
- **Advanced Tool Modes**: IDE integration, OS-level tools, automation workflows
- **Cross-Platform Executables**: Standalone binaries for Windows, macOS, Linux
- **Mobile Terminals**: Apps for iOS and Android with full functionality

## Research Foundation

### Validated Approaches
- **Simple Tool Mode**: Pattern-based tool execution works well with â‰¤8B models
- **T-101 Safety**: Human confirmation protocols effectively maintain sovereignty
- **Streaming Responses**: Real-time output provides better UX than batch responses
- **Local First**: Privacy and sovereignty requirements validated through user feedback

### Emerging Research Areas
- **Terminal UX for Cognitive Enhancement**: Optimal interface design for memory-augmented work
- **Tool Execution Patterns**: Reliability vs. flexibility trade-offs in tool systems
- **Cognitive Load**: Impact of terminal vs. GUI interfaces on productive thinking
- **Memory Integration**: Optimal presentation of memory retrieval results

## Competitive Advantages

### vs Web-based Interfaces
- **Privacy**: 100% local processing with no web transmission
- **Customization**: Full control over interface and workflow
- **Reliability**: No internet connection required
- **Performance**: No web latency for local operations

### vs Other CLI Tools  
- **Memory Integration**: Unique integration with infinite context memory system
- **Safety Focus**: T-101 protocols ensuring human sovereignty
- **Tool Reliability**: Pattern-based execution works regardless of model sophistication
- **Local-First**: Zero telemetry, complete data sovereignty

## Success Metrics

### Technical Metrics
- **Response Latency**: <500ms for streaming responses from ECE_Core
- **Tool Reliability**: >95% successful execution of safe operations
- **Session Stability**: >99% uptime for extended conversation sessions
- **Memory Accuracy**: 100% fidelity in context preservation

### User Experience Metrics
- **Safety Perception**: 100% of users report feeling in control
- **Productivity Impact**: Measurable improvement in cognitive task completion
- **Privacy Satisfaction**: 100% of users report comfort with data handling
- **Reliability Rating**: >4.0/5.0 for tool execution success rate

## Risk Assessment

### Technical Risks
- **Performance**: Ensuring responsiveness with large context windows (mitigated by local caching)
- **Compatibility**: Supporting various ECE_Core API versions (mitigated by version negotiation)
- **Security**: Tool execution vulnerabilities (mitigated by T-101 protocols)

### Adoption Risks
- **Complexity**: CLI interface may be intimidating (mitigated by excellent documentation)
- **Learning Curve**: Users may prefer web interfaces (mitigated by superior functionality)
- **Reliability**: Tool execution failures may frustrate users (mitigated by safety protocols)

## Timeline & Milestones

### Phase 5 Milestones (Infinite Context Integration) - COMPLETED
- [x] Context rotation awareness - Dec 2025
- [x] Large input handling - Dec 2025
- [x] Memory continuity - Dec 2025

### Phase 6 Milestones (Optimization) - IN PROGRESS
- [ ] Streaming performance optimization - Jan 2026
- [ ] Safety protocol enhancements - Jan 2026
- [ ] Session management improvements - Feb 2026

### Phase 7 Milestones (Expansion) - PLANNED
- [ ] Standalone executables - Mar 2026
- [ ] Rich terminal interfaces - Apr 2026
- [ ] Mobile applications - Q3 2026

## Ethical Framework

### Core Principles
1. **Human Sovereignty**: Humans maintain ultimate control over AI actions
2. **Cognitive Liberty**: Enhancement without coercion or manipulation
3. **Data Sovereignty**: All user data remains local and under user control
4. **Transparency**: Clear visibility into system operations and tool execution

### Implementation Guidelines
- All tool executions require explicit user confirmation when dangerous
- No data telemetry or external data transmission
- Clear audit trails for all automated operations
- User control over context sharing with AI systems

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\plan.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\tasks.md (Section: ANCHOR_SPECS) ---

# Anchor CLI - Current Tasks

## Active Development (Phase 6: Optimization)

### [IN PROGRESS] Performance Optimization (ANC-PERF-101)
- [ ] Optimize streaming response handling for 64k+ context windows (ANC-PERF-101.01)
  - [ ] Implement local context caching for faster response times
  - [ ] Profile memory usage during extended sessions with large contexts
  - [ ] Optimize terminal rendering for large response streams
  - [ ] Test performance with 30k+ token inputs

- [ ] Large Input Handling (ANC-PERF-101.02)
  - [ ] Display progress indicators during large input processing
  - [ ] Optimize input parsing for multi-MB documents
  - [ ] Implement chunked input for very large pastes
  - [ ] Test with ContextGist integration during rotation

### [IN PROGRESS] User Experience Enhancements (ANC-UX-102)
- [ ] Rich Terminal Interface Implementation (ANC-UX-102.01)
  - [ ] Implement syntax highlighting for code blocks in responses
  - [ ] Add visual indicators for tool execution states
  - [ ] Create conversation history navigation
  - [ ] Implement configurable color schemes

- [ ] Session Management (ANC-UX-102.02)
  - [ ] Enhanced session history with search functionality
  - [ ] Named session persistence across restarts
  - [ ] Session export/import capabilities
  - [ ] Context window visualization

### [PLANNED] Tool Execution Reliability (ANC-TOOL-103)
- [ ] Enhanced Pattern Matching for ST Mode (ANC-TOOL-103.01)
  - [ ] More robust tool call detection patterns
  - [ ] Better argument parsing for malformed tool calls
  - [ ] Fallback execution strategies for uncertain tool calls
  - [ ] Tool execution error recovery mechanisms

- [ ] Granular Safety Controls (ANC-TOOL-103.02)
  - [ ] Per-tool permission settings
  - [ ] Time-limited tool authorizations
  - [ ] Batch operation confirmations
  - [ ] Tool execution audit logging

## Upcoming Priorities (Phase 7: Expansion)

### [PLANNED] Cross-Platform Distribution (ANC-DIST-201)
- [ ] PyInstaller Build Pipeline (ANC-DIST-201.01)
  - [ ] Create standalone executable specifications
  - [ ] Implement versioned release builds
  - [ ] Cross-platform testing (Windows, macOS, Linux)
  - [ ] Size optimization for distributable binaries

- [ ] Mobile Terminal Applications (ANC-MOB-202)
  - [ ] iOS SSH client configuration
  - [ ] Android terminal app integration
  - [ ] Mobile-optimized UI adaptations
  - [ ] Touch-first interaction patterns

### [PLANNED] Advanced Tool Integration (ANC-ADV-203)
- [ ] IDE Integration (ANC-ADV-203.01)
  - [ ] VS Code extension for context injection
  - [ ] Vim plugin for code-to-conversation workflows
  - [ ] Emacs integration for literate programming
  - [ ] Direct integration with popular editors

- [ ] OS-Level Tools (ANC-ADV-203.02)
  - [ ] Clipboard integration
  - [ ] File system monitoring for automatic context updates
  - [ ] Window manager integration
  - [ ] Email client integration

## Maintenance Tasks

### [ONGOING] Compatibility & Testing (ANC-TEST-001)
- [ ] ECE_Core API compatibility testing with new releases
- [ ] Cross-platform terminal compatibility verification
- [ ] Large context window compatibility with various terminals
- [ ] Performance regression testing

### [MONTHLY] Dependency Updates (ANC-DEP-002)
- [ ] Update Python dependencies with security scanning
- [ ] Verify compatibility with new ECE_Core API versions
- [ ] Test with updated terminal emulators and shells
- [ ] Update SSL/TLS certificates and security protocols

### [QUARTERLY] Security Review (ANC-SEC-003)
- [ ] Tool execution security assessment
- [ ] API communication validation
- [ ] Path traversal vulnerability verification
- [ ] Privilege escalation prevention verification

## Recently Completed (Phase 5: Infinite Context Integration)

### [COMPLETED] Context Rotation Awareness (ANC-CRA-01)
- [x] Detect when ECE_Core performs context rotation (Dec 2025)
- [x] Maintain local session state across context rotations (Dec 2025)
- [x] Handle large input responses that trigger rotations (Dec 2025)
- [x] Preserve conversation continuity with historical context (Dec 2025)

### [COMPLETED] Memory Sovereignty (ANC-MEM-02)
- [x] Local session storage implementation (Nov 2025)
- [x] Selective context sharing controls (Nov 2025) 
- [x] Privacy-preservation in tool execution (Nov 2025)
- [x] Clear data boundaries between local and remote (Nov 2025)

### [COMPLETED] T-101 Safety Protocol (ANC-T101-03)
- [x] Dangerous operation confirmation flows (Oct 2025)
- [x] SAFE vs DANGEROUS operation classification (Oct 2025)
- [x] Audit logging for confirmed operations (Oct 2025)
- [x] Human-in-the-loop safety verification (Oct 2025)

## Known Issues & Technical Debt

### Performance Issues
- [ ] Slow response rendering in some terminal emulators (ANC-PERF-001)
- [ ] High memory usage during long conversations with large contexts (ANC-PERF-002)
- [ ] Delayed input processing during large context uploads (ANC-PERF-003)

### Usability Issues
- [ ] Poor visual feedback during tool execution (ANC-USAB-001)
- [ ] Confusing session management for new users (ANC-USAB-002)
- [ ] Difficult discovery of advanced features (ANC-USAB-003)

### Compatibility Issues
- [ ] Terminal rendering inconsistencies across platforms (ANC-COMP-001)
- [ ] SSH connection stability with long-running sessions (ANC-COMP-002)
- [ ] Large paste handling in some terminal emulators (ANC-COMP-003)

---

**Current Focus**: Performance optimization for infinite context workflows
**Next Priority**: Rich terminal interface enhancements for better UX
**Last Updated**: 2025-12-08

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\anchor\specs\tasks.md ---

--- START OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\extension\README.md (Section: EXTENSION) ---

# Browser Extension (Bridge)

> **Coda Bridge** - Browser integration for Context-Engine

**Philosophy**: Your mind, augmented. Your data, sovereign. Your tools, open.

## Overview
The Browser Extension is the "Bridge" between your browser and the Context-Engine system. This Chrome Extension (Manifest V3) connects the browser to the Coda Core for context-aware browsing and memory-enhanced interactions.

## Capabilities
- **Voice**: Streaming chat interface via Side Panel.
- **Sight**: Context injection (reading active page content).
- **Memory**: **[Save to Memory]** button to ingest the current page/chat into the permanent knowledge graph.
- **Hands**: JavaScript execution on active pages (User-ratified).

## Architecture
- **Type**: Chrome Extension (Manifest V3)
- **Communication**: HTTP/SSE to `localhost:8000`
- **State**: Local Storage (Persistence)
- **Integration**: Connects to ECE_Core API for context-aware browsing

## Key Features
- ✅ Persistent chat history in side panel
- ✅ Real-time context injection from active tabs
- ✅ One-click memory saving with "Save to Memory" button
- ✅ Secure JavaScript execution with user confirmation

## Documentation
- `specs/spec.md` - Technical architecture
- `specs/plan.md` - Vision and implementation roadmap  
- `specs/tasks.md` - Current work items

## Integration
Connects to the ECE_Core backend via API endpoints for context-aware page reading and memory ingestion.

--- END OF FILE: C:\Users\rsbiiw\Projects\Context-Engine\extension\README.md ---

