[
  {
    "role": "system",
    "type": "document",
    "source": "index.js",
    "timestamp": 1767473486191,
    "content": "const express = require('express');\nconst cors = require('cors');\nconst bodyParser = require('body-parser');\nconst { CozoDb } = require('cozo-node');\nconst chokidar = require('chokidar');\nconst fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\nconst { createReadStream } = require('fs');\nconst { join } = require('path');\n\n// Initialize CozoDB with RocksDB backend\nconst db = new CozoDb('rocksdb', './context.db');\n\n// Set up Express app\nconst app = express();\nconst PORT = 3000;\n\n// Serve static files from tools directory\napp.use(express.static(join(__dirname, '..', '..', 'tools')));\n\n// Middleware\napp.use(cors());\napp.use(bodyParser.json({ limit: '50mb' }));\napp.use(bodyParser.urlencoded({ extended: true }));\n\n// Initialize database schema\nasync function initializeDb() {\n  try {\n    // Check if the memory relation already exists\n    const checkQuery = '::relations';\n    const relations = await db.run(checkQuery);\n\n    // Only create the memory table if it doesn't already exist\n    if (!relations.rows.some(row => row[0] === 'memory')) {\n        const schemaQuery = ':create memory {id: String, timestamp: Int, content: String, source: String, type: String}';\n        await db.run(schemaQuery);\n        console.log('Database schema initialized');\n    } else {\n        console.log('Database schema already exists');\n    }\n\n    // Try to create FTS index (optional, may not be supported in all builds)\n    try {\n      const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;\n      await db.run(ftsQuery);\n      console.log('FTS index created');\n    } catch (e) {\n      console.log('FTS creation failed (optional feature):', e.message);\n    }\n  } catch (error) {\n    console.error('Error initializing database:', error);\n    throw error;\n  }\n}\n\n// POST /v1/ingest endpoint\napp.post('/v1/ingest', async (req, res) => {\n  try {\n    const { content, filename, source, type = 'text' } = req.body;\n    \n    if (!content) {\n      return res.status(400).json({ error: 'Content is required' });\n    }\n    \n    const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;\n    const timestamp = Date.now();\n    \n    // Insert into CozoDB\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const params = {\n      data: [[\n        id,\n        timestamp,\n        content,\n        source || filename || 'unknown',\n        type\n      ]]\n    };\n    \n    const result = await db.run(query, params);\n    \n    res.json({ \n      status: 'success', \n      id: id,\n      message: 'Content ingested successfully'\n    });\n  } catch (error) {\n    console.error('Ingest error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/query endpoint\napp.post('/v1/query', async (req, res) => {\n  try {\n    const { query, params = {} } = req.body;\n\n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n\n    const result = await db.run(query, params);\n\n    res.json(result);\n  } catch (error) {\n    console.error('Query error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/memory/search endpoint (for context.html)\napp.post('/v1/memory/search', async (req, res) => {\n  try {\n    const { query, max_chars = 5000 } = req.body;\n\n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n\n    // Simple search implementation - retrieve all memory entries\n    // We'll filter on the server side since CozoDB's text search functions may vary\n    const searchQuery = `?[*] := *memory{id, timestamp, content, source, type}`;\n    const params = {};\n\n    const result = await db.run(searchQuery, params);\n\n    if (result.ok) {\n      let context = '';\n      let charCount = 0;\n\n      if (result.rows) {\n        // Filter rows that contain the query term (case insensitive)\n        const filteredRows = result.rows.filter(row => {\n          const [id, timestamp, content, source, type] = row;\n          return content.toLowerCase().includes(query.toLowerCase()) ||\n                 source.toLowerCase().includes(query.toLowerCase());\n        });\n\n        // Sort by relevance (rows with query in content first, then in source)\n        filteredRows.sort((a, b) => {\n          const [a_id, a_timestamp, a_content, a_source, a_type] = a;\n          const [b_id, b_timestamp, b_content, b_source, b_type] = b;\n\n          const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());\n          const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());\n\n          // Prioritize content matches over source matches\n          if (aContentMatch && !bContentMatch) return -1;\n          if (!aContentMatch && bContentMatch) return 1;\n          return 0;\n        });\n\n        for (const row of filteredRows) {\n          const [id, timestamp, content, source, type] = row;\n          const entryText = `### Source: ${source}\\n${content}\\n\\n`;\n          if (charCount + entryText.length > max_chars) {\n            // Add partial content if we're near the limit\n            const remainingChars = max_chars - charCount;\n            context += entryText.substring(0, remainingChars);\n            break;\n          }\n          context += entryText;\n          charCount += entryText.length;\n        }\n      }\n\n      res.json({ context: context || 'No results found.' });\n    } else {\n      res.status(500).json({ error: 'Search failed' });\n    }\n  } catch (error) {\n    console.error('Memory search error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// POST /v1/system/spawn_shell endpoint (for index.html)\napp.post('/v1/system/spawn_shell', async (req, res) => {\n  try {\n    // For now, just return success - spawning a shell is complex and platform-dependent\n    // In a real implementation, this would spawn a PowerShell terminal\n    res.json({ success: true, message: \"Shell spawned successfully\" });\n  } catch (error) {\n    console.error('Spawn shell error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// GET /health endpoint\napp.get('/health', (req, res) => {\n  res.json({ \n    status: 'Sovereign',\n    timestamp: new Date().toISOString(),\n    uptime: process.uptime()\n  });\n});\n\n// Set up file watcher for context directory\nfunction setupFileWatcher() {\n  const contextDir = path.join(__dirname, '..', 'context');\n  \n  // Ensure context directory exists\n  if (!fs.existsSync(contextDir)) {\n    fs.mkdirSync(contextDir, { recursive: true });\n  }\n  \n  const watcher = chokidar.watch(contextDir, {\n    ignored: /(^|[\\/\\\\])\\../, // ignore dotfiles\n    persistent: true,\n    ignoreInitial: true, // Don't trigger events for existing files\n    awaitWriteFinish: {\n      stabilityThreshold: 2000,\n      pollInterval: 100\n    }\n  });\n\n  watcher\n    .on('add', filePath => handleFileChange(filePath))\n    .on('change', filePath => handleFileChange(filePath))\n    .on('error', error => console.error('Watcher error:', error));\n    \n  console.log('File watcher initialized for context directory');\n}\n\nasync function handleFileChange(filePath) {\n  console.log(`File changed: ${filePath}`);\n  \n  try {\n    const content = fs.readFileSync(filePath, 'utf8');\n    const relPath = path.relative(\n      path.join(__dirname, '..', 'context'), \n      filePath\n    );\n    \n    // Ingest the file content\n    const query = `:insert memory {id, timestamp, content, source, type} <- $data`;\n    const id = `file_${Date.now()}_${path.basename(filePath)}`;\n    const params = {\n      data: [[\n        id,\n        Date.now(),\n        content,\n        relPath,\n        path.extname(filePath) || 'unknown'\n      ]]\n    };\n    \n    await db.run(query, params);\n    console.log(`File ingested: ${relPath}`);\n  } catch (error) {\n    console.error(`Error processing file ${filePath}:`, error);\n  }\n}\n\n// Initialize and start server\nasync function startServer() {\n  try {\n    await initializeDb();\n    setupFileWatcher();\n    \n    app.listen(PORT, () => {\n      console.log(`Sovereign Context Engine listening on port ${PORT}`);\n      console.log(`Health check: http://localhost:${PORT}/health`);\n    });\n  } catch (error) {\n    console.error('Failed to start server:', error);\n    process.exit(1);\n  }\n}\n\n// Handle graceful shutdown\nprocess.on('SIGINT', async () => {\n  console.log('Shutting down gracefully...');\n  try {\n    await db.close();\n  } catch (e) {\n    console.error('Error closing database:', e);\n  }\n  process.exit(0);\n});\n\n// Start the server\nstartServer();\n\nmodule.exports = { db, app };"
  },
  {
    "role": "system",
    "type": "document",
    "source": "migrate_history.js",
    "timestamp": 1767457063827,
    "content": "const fs = require('fs');\nconst path = require('path');\nconst glob = require('glob');\nconst yaml = require('js-yaml');\n\n// Migration script to consolidate legacy session files\nasync function migrateHistory() {\n  console.log('Starting legacy session migration...');\n\n  // Find all session files\n  const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');\n  const pattern = path.join(sessionsDir, 'sessions_part_*.json');\n\n  // Use glob to find all matching files\n  const sessionFiles = glob.sync(pattern);\n\n  if (sessionFiles.length === 0) {\n    console.log('No session files found in the expected location.');\n    // Try alternative path\n    const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');\n    const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');\n    const altSessionFiles = glob.sync(altPattern);\n\n    if (altSessionFiles.length === 0) {\n      console.log('No session files found in alternative location either.');\n      return;\n    }\n\n    console.log(`Found ${altSessionFiles.length} session files in alternative location.`);\n    processSessionFiles(altSessionFiles);\n    return;\n  }\n\n  console.log(`Found ${sessionFiles.length} session files`);\n  processSessionFiles(sessionFiles);\n}\n\nfunction processSessionFiles(sessionFiles) {\n  // Sort files numerically (part_1, part_2, ..., part_10, etc.)\n  sessionFiles.sort((a, b) => {\n    const matchA = a.match(/part_(\\d+)/);\n    const matchB = b.match(/part_(\\d+)/);\n\n    if (matchA && matchB) {\n      return parseInt(matchA[1]) - parseInt(matchB[1]);\n    }\n    return a.localeCompare(b);\n  });\n\n  let allSessions = [];\n\n  for (const file of sessionFiles) {\n    console.log(`Processing: ${path.basename(file)}`);\n    try {\n      const content = fs.readFileSync(file, 'utf8');\n\n      // Try to extract valid JSON from potentially corrupted files\n      let data = extractValidJson(content);\n\n      if (!data) {\n        console.error(`Could not extract valid JSON from ${file}`);\n        continue;\n      }\n\n      // Handle both list and object formats\n      if (Array.isArray(data)) {\n        allSessions = allSessions.concat(data);\n      } else if (typeof data === 'object') {\n        allSessions.push(data);\n      } else {\n        console.log(`Unexpected data format in ${file}, skipping...`);\n      }\n    } catch (error) {\n      console.error(`Error reading ${file}:`, error.message);\n    }\n  }\n\n  console.log(`Merged ${allSessions.length} total sessions`);\n\n  // Save to YAML file\n  const outputDir = path.join(__dirname, '..', '..', 'context');\n  const outputFile = path.join(outputDir, 'full_history.yaml');\n\n  // Custom YAML representer for multiline strings\n  yaml.representer = {\n    ...yaml.representer,\n    string: (data) => {\n      if (data.includes('\\n')) {\n        return new yaml.types.Str(data, { style: '|' });\n      }\n      return data;\n    }\n  };\n\n  try {\n    const yamlContent = yaml.dump(allSessions, {\n      lineWidth: -1,\n      noRefs: true,\n      skipInvalid: true\n    });\n\n    fs.writeFileSync(outputFile, yamlContent, 'utf8');\n    console.log(`YAML file created: ${outputFile}`);\n\n    // Also save as JSON for compatibility\n    const jsonOutputFile = path.join(outputDir, 'full_history.json');\n    fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');\n    console.log(`JSON file created: ${jsonOutputFile}`);\n\n  } catch (error) {\n    console.error('Error saving YAML file:', error.message);\n    return;\n  }\n\n  console.log('Migration completed successfully!');\n}\n\n// Function to extract valid JSON from potentially corrupted files\nfunction extractValidJson(content) {\n  try {\n    // First, try to parse as regular JSON\n    return JSON.parse(content);\n  } catch (e) {\n    // If that fails, clean the content and try again\n    try {\n      // Remove null bytes and other control characters that often corrupt JSON\n      let cleanContent = content.replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]/g, '');\n\n      // Try to parse the cleaned content\n      return JSON.parse(cleanContent);\n    } catch (e2) {\n      // If still failing, try to extract JSON array from the content\n      try {\n        // Find the main JSON array by looking for opening [ and closing ]\n        const startIdx = cleanContent.indexOf('[');\n        const endIdx = cleanContent.lastIndexOf(']');\n\n        if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {\n          const arrayContent = cleanContent.substring(startIdx, endIdx + 1);\n\n          // Try to parse the extracted array\n          return JSON.parse(arrayContent);\n        }\n      } catch (e3) {\n        // If all attempts fail, return null\n        return null;\n      }\n    }\n  }\n\n  return null;\n}\n\n// Run migration if this file is executed directly\nif (require.main === module) {\n  migrateHistory().catch(console.error);\n}\n\nmodule.exports = { migrateHistory };"
  },
  {
    "role": "system",
    "type": "document",
    "source": "read_all.js",
    "timestamp": 1767457667701,
    "content": "const fs = require('fs');\nconst path = require('path');\nconst yaml = require('js-yaml');\n\n/**\n * Aggregates all readable text content from a directory and its subdirectories\n * into:\n * 1. A single text corpus (combined_text.txt) for human reading.\n * 2. A structured JSON memory file (combined_memory.json) for Sovereign DB ingestion.\n * 3. A structured YAML memory file (combined_memory.yaml) for easier processing and migration.\n */\nfunction createFullCorpusRecursive() {\n  // Set the root directory to scan as the directory containing this script\n  const rootDirToScan = path.dirname(__filename);\n\n  const outputTextFile = path.join(rootDirToScan, 'combined_text.txt');\n  const outputJsonFile = path.join(rootDirToScan, 'combined_memory.json');\n  const outputYamlFile = path.join(rootDirToScan, 'combined_memory.yaml');\n\n  console.log(`Scanning Target Directory: ${rootDirToScan}`);\n\n  const textExtensions = new Set([\n    '.json', '.md', '.poml', '.yaml', '.yml', '.txt', \n    '.py', '.js', '.ts', '.css', '.sh', '.ps1', '.html', '.bat'\n  ]);\n\n  const excludeDirs = new Set([\n    '.venv', '.git', '.vscode', '__pycache__', \n    'node_modules', '.obsidian', 'random', 'archive', \n    'build', 'dist', 'logs'\n  ]);\n\n  // Files to exclude from the corpus itself to avoid recursion\n  const excludeFiles = new Set([\n    path.basename(outputTextFile),\n    path.basename(outputJsonFile),\n    path.basename(outputYamlFile),\n    'package-lock.json',\n    'yarn.lock'\n  ]);\n\n  const filesToProcess = [];\n\n  function walkDirectory(currentPath) {\n    const items = fs.readdirSync(currentPath);\n\n    for (const item of items) {\n      const itemPath = path.join(currentPath, item);\n      const stat = fs.statSync(itemPath);\n\n      if (stat.isDirectory()) {\n        if (!excludeDirs.has(item)) {\n          walkDirectory(itemPath);\n        }\n      } else if (stat.isFile()) {\n        const ext = path.extname(item).toLowerCase();\n        if (textExtensions.has(ext) && !excludeFiles.has(item)) {\n          filesToProcess.push(itemPath);\n        }\n      }\n    }\n  }\n\n  walkDirectory(rootDirToScan);\n  filesToProcess.sort();\n\n  if (filesToProcess.length === 0) {\n    console.log(`No processable files found in '${rootDirToScan}'.`);\n    return;\n  }\n\n  console.log(`Found ${filesToProcess.length} files to process.`);\n\n  const memoryRecords = [];\n\n  // 1. Generate Text Corpus\n  const textStream = fs.createWriteStream(outputTextFile, { encoding: 'utf-8' });\n\n  for (const filePath of filesToProcess) {\n    console.log(`Processing '${filePath}'...`);\n    try {\n      // Get file metadata\n      const fileStats = fs.statSync(filePath);\n      const modTime = fileStats.mtimeMs; // milliseconds timestamp\n      const relPath = path.relative(rootDirToScan, filePath);\n\n      // Read file content\n      const rawContent = fs.readFileSync(filePath);\n      // For simplicity in JS, we'll assume UTF-8, but could implement encoding detection\n      const decodedContent = rawContent.toString('utf-8');\n\n      // Write to Text File\n      textStream.write(`--- START OF FILE: ${relPath} ---\\n`);\n      textStream.write(decodedContent + \"\\n\");\n      textStream.write(`--- END OF FILE: ${relPath} ---\\n\\n`);\n\n      // Add to Memory Records\n      memoryRecords.push({\n        role: 'system',\n        type: 'document',\n        source: relPath,\n        timestamp: Math.floor(modTime), // Convert to integer\n        content: decodedContent\n      });\n\n    } catch (e) {\n      console.log(`An unexpected error occurred with file '${filePath}': ${e.message}`);\n    }\n  }\n\n  textStream.end();\n\n  // 2. Generate JSON Memory File\n  console.log(`Generating Structured Memory: ${outputJsonFile}`);\n  fs.writeFileSync(outputJsonFile, JSON.stringify(memoryRecords, null, 2), 'utf-8');\n\n  // 3. Generate YAML Memory File\n  console.log(`Generating YAML Memory: ${outputYamlFile}`);\n\n  // Custom YAML representer for multiline strings\n  const schema = yaml.DEFAULT_SCHEMA.extend([\n    new yaml.Type('!long-string', {\n      kind: 'scalar',\n      predicate: (data) => typeof data === 'string' && data.includes('\\n'),\n      represent: (data) => ({ value: data, style: '|' })\n    })\n  ]);\n\n  // Use default representer with multiline string style\n  const yamlContent = yaml.dump(memoryRecords, {\n    lineWidth: -1, // Don't wrap lines\n    noRefs: true,\n    quotingType: '\"', // Use double quotes when needed\n    forceQuotes: false\n  });\n\n  fs.writeFileSync(outputYamlFile, yamlContent, 'utf-8');\n\n  console.log('\\nCorpus aggregation complete.');\n  console.log(`1. Text Corpus: '${outputTextFile}'`);\n  console.log(`2. JSON Memory: '${outputJsonFile}' (Drop this into Coda Console)`);\n  console.log(`3. YAML Memory: '${outputYamlFile}' (Alternative format for easier processing)`);\n}\n\n// Run the function if this script is executed directly\nif (require.main === module) {\n  createFullCorpusRecursive();\n}\n\nmodule.exports = { createFullCorpusRecursive };"
  }
]