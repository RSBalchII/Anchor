- role: system
  type: document
  source: index.js
  timestamp: 1767473486191
  content: |-
    const express = require('express');
    const cors = require('cors');
    const bodyParser = require('body-parser');
    const { CozoDb } = require('cozo-node');
    const chokidar = require('chokidar');
    const fs = require('fs');
    const path = require('path');
    const yaml = require('js-yaml');
    const { createReadStream } = require('fs');
    const { join } = require('path');

    // Initialize CozoDB with RocksDB backend
    const db = new CozoDb('rocksdb', './context.db');

    // Set up Express app
    const app = express();
    const PORT = 3000;

    // Serve static files from tools directory
    app.use(express.static(join(__dirname, '..', '..', 'tools')));

    // Middleware
    app.use(cors());
    app.use(bodyParser.json({ limit: '50mb' }));
    app.use(bodyParser.urlencoded({ extended: true }));

    // Initialize database schema
    async function initializeDb() {
      try {
        // Check if the memory relation already exists
        const checkQuery = '::relations';
        const relations = await db.run(checkQuery);

        // Only create the memory table if it doesn't already exist
        if (!relations.rows.some(row => row[0] === 'memory')) {
            const schemaQuery = ':create memory {id: String, timestamp: Int, content: String, source: String, type: String}';
            await db.run(schemaQuery);
            console.log('Database schema initialized');
        } else {
            console.log('Database schema already exists');
        }

        // Try to create FTS index (optional, may not be supported in all builds)
        try {
          const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`;
          await db.run(ftsQuery);
          console.log('FTS index created');
        } catch (e) {
          console.log('FTS creation failed (optional feature):', e.message);
        }
      } catch (error) {
        console.error('Error initializing database:', error);
        throw error;
      }
    }

    // POST /v1/ingest endpoint
    app.post('/v1/ingest', async (req, res) => {
      try {
        const { content, filename, source, type = 'text' } = req.body;
        
        if (!content) {
          return res.status(400).json({ error: 'Content is required' });
        }
        
        const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
        const timestamp = Date.now();
        
        // Insert into CozoDB
        const query = `:insert memory {id, timestamp, content, source, type} <- $data`;
        const params = {
          data: [[
            id,
            timestamp,
            content,
            source || filename || 'unknown',
            type
          ]]
        };
        
        const result = await db.run(query, params);
        
        res.json({ 
          status: 'success', 
          id: id,
          message: 'Content ingested successfully'
        });
      } catch (error) {
        console.error('Ingest error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/query endpoint
    app.post('/v1/query', async (req, res) => {
      try {
        const { query, params = {} } = req.body;

        if (!query) {
          return res.status(400).json({ error: 'Query is required' });
        }

        const result = await db.run(query, params);

        res.json(result);
      } catch (error) {
        console.error('Query error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/memory/search endpoint (for context.html)
    app.post('/v1/memory/search', async (req, res) => {
      try {
        const { query, max_chars = 5000 } = req.body;

        if (!query) {
          return res.status(400).json({ error: 'Query is required' });
        }

        // Simple search implementation - retrieve all memory entries
        // We'll filter on the server side since CozoDB's text search functions may vary
        const searchQuery = `?[*] := *memory{id, timestamp, content, source, type}`;
        const params = {};

        const result = await db.run(searchQuery, params);

        if (result.ok) {
          let context = '';
          let charCount = 0;

          if (result.rows) {
            // Filter rows that contain the query term (case insensitive)
            const filteredRows = result.rows.filter(row => {
              const [id, timestamp, content, source, type] = row;
              return content.toLowerCase().includes(query.toLowerCase()) ||
                     source.toLowerCase().includes(query.toLowerCase());
            });

            // Sort by relevance (rows with query in content first, then in source)
            filteredRows.sort((a, b) => {
              const [a_id, a_timestamp, a_content, a_source, a_type] = a;
              const [b_id, b_timestamp, b_content, b_source, b_type] = b;

              const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());
              const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());

              // Prioritize content matches over source matches
              if (aContentMatch && !bContentMatch) return -1;
              if (!aContentMatch && bContentMatch) return 1;
              return 0;
            });

            for (const row of filteredRows) {
              const [id, timestamp, content, source, type] = row;
              const entryText = `### Source: ${source}\n${content}\n\n`;
              if (charCount + entryText.length > max_chars) {
                // Add partial content if we're near the limit
                const remainingChars = max_chars - charCount;
                context += entryText.substring(0, remainingChars);
                break;
              }
              context += entryText;
              charCount += entryText.length;
            }
          }

          res.json({ context: context || 'No results found.' });
        } else {
          res.status(500).json({ error: 'Search failed' });
        }
      } catch (error) {
        console.error('Memory search error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // POST /v1/system/spawn_shell endpoint (for index.html)
    app.post('/v1/system/spawn_shell', async (req, res) => {
      try {
        // For now, just return success - spawning a shell is complex and platform-dependent
        // In a real implementation, this would spawn a PowerShell terminal
        res.json({ success: true, message: "Shell spawned successfully" });
      } catch (error) {
        console.error('Spawn shell error:', error);
        res.status(500).json({ error: error.message });
      }
    });

    // GET /health endpoint
    app.get('/health', (req, res) => {
      res.json({ 
        status: 'Sovereign',
        timestamp: new Date().toISOString(),
        uptime: process.uptime()
      });
    });

    // Set up file watcher for context directory
    function setupFileWatcher() {
      const contextDir = path.join(__dirname, '..', 'context');
      
      // Ensure context directory exists
      if (!fs.existsSync(contextDir)) {
        fs.mkdirSync(contextDir, { recursive: true });
      }
      
      const watcher = chokidar.watch(contextDir, {
        ignored: /(^|[\/\\])\../, // ignore dotfiles
        persistent: true,
        ignoreInitial: true, // Don't trigger events for existing files
        awaitWriteFinish: {
          stabilityThreshold: 2000,
          pollInterval: 100
        }
      });

      watcher
        .on('add', filePath => handleFileChange(filePath))
        .on('change', filePath => handleFileChange(filePath))
        .on('error', error => console.error('Watcher error:', error));
        
      console.log('File watcher initialized for context directory');
    }

    async function handleFileChange(filePath) {
      console.log(`File changed: ${filePath}`);
      
      try {
        const content = fs.readFileSync(filePath, 'utf8');
        const relPath = path.relative(
          path.join(__dirname, '..', 'context'), 
          filePath
        );
        
        // Ingest the file content
        const query = `:insert memory {id, timestamp, content, source, type} <- $data`;
        const id = `file_${Date.now()}_${path.basename(filePath)}`;
        const params = {
          data: [[
            id,
            Date.now(),
            content,
            relPath,
            path.extname(filePath) || 'unknown'
          ]]
        };
        
        await db.run(query, params);
        console.log(`File ingested: ${relPath}`);
      } catch (error) {
        console.error(`Error processing file ${filePath}:`, error);
      }
    }

    // Initialize and start server
    async function startServer() {
      try {
        await initializeDb();
        setupFileWatcher();
        
        app.listen(PORT, () => {
          console.log(`Sovereign Context Engine listening on port ${PORT}`);
          console.log(`Health check: http://localhost:${PORT}/health`);
        });
      } catch (error) {
        console.error('Failed to start server:', error);
        process.exit(1);
      }
    }

    // Handle graceful shutdown
    process.on('SIGINT', async () => {
      console.log('Shutting down gracefully...');
      try {
        await db.close();
      } catch (e) {
        console.error('Error closing database:', e);
      }
      process.exit(0);
    });

    // Start the server
    startServer();

    module.exports = { db, app };
- role: system
  type: document
  source: migrate_history.js
  timestamp: 1767457063827
  content: |-
    const fs = require('fs');
    const path = require('path');
    const glob = require('glob');
    const yaml = require('js-yaml');

    // Migration script to consolidate legacy session files
    async function migrateHistory() {
      console.log('Starting legacy session migration...');

      // Find all session files
      const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');
      const pattern = path.join(sessionsDir, 'sessions_part_*.json');

      // Use glob to find all matching files
      const sessionFiles = glob.sync(pattern);

      if (sessionFiles.length === 0) {
        console.log('No session files found in the expected location.');
        // Try alternative path
        const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');
        const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');
        const altSessionFiles = glob.sync(altPattern);

        if (altSessionFiles.length === 0) {
          console.log('No session files found in alternative location either.');
          return;
        }

        console.log(`Found ${altSessionFiles.length} session files in alternative location.`);
        processSessionFiles(altSessionFiles);
        return;
      }

      console.log(`Found ${sessionFiles.length} session files`);
      processSessionFiles(sessionFiles);
    }

    function processSessionFiles(sessionFiles) {
      // Sort files numerically (part_1, part_2, ..., part_10, etc.)
      sessionFiles.sort((a, b) => {
        const matchA = a.match(/part_(\d+)/);
        const matchB = b.match(/part_(\d+)/);

        if (matchA && matchB) {
          return parseInt(matchA[1]) - parseInt(matchB[1]);
        }
        return a.localeCompare(b);
      });

      let allSessions = [];

      for (const file of sessionFiles) {
        console.log(`Processing: ${path.basename(file)}`);
        try {
          const content = fs.readFileSync(file, 'utf8');

          // Try to extract valid JSON from potentially corrupted files
          let data = extractValidJson(content);

          if (!data) {
            console.error(`Could not extract valid JSON from ${file}`);
            continue;
          }

          // Handle both list and object formats
          if (Array.isArray(data)) {
            allSessions = allSessions.concat(data);
          } else if (typeof data === 'object') {
            allSessions.push(data);
          } else {
            console.log(`Unexpected data format in ${file}, skipping...`);
          }
        } catch (error) {
          console.error(`Error reading ${file}:`, error.message);
        }
      }

      console.log(`Merged ${allSessions.length} total sessions`);

      // Save to YAML file
      const outputDir = path.join(__dirname, '..', '..', 'context');
      const outputFile = path.join(outputDir, 'full_history.yaml');

      // Custom YAML representer for multiline strings
      yaml.representer = {
        ...yaml.representer,
        string: (data) => {
          if (data.includes('\n')) {
            return new yaml.types.Str(data, { style: '|' });
          }
          return data;
        }
      };

      try {
        const yamlContent = yaml.dump(allSessions, {
          lineWidth: -1,
          noRefs: true,
          skipInvalid: true
        });

        fs.writeFileSync(outputFile, yamlContent, 'utf8');
        console.log(`YAML file created: ${outputFile}`);

        // Also save as JSON for compatibility
        const jsonOutputFile = path.join(outputDir, 'full_history.json');
        fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');
        console.log(`JSON file created: ${jsonOutputFile}`);

      } catch (error) {
        console.error('Error saving YAML file:', error.message);
        return;
      }

      console.log('Migration completed successfully!');
    }

    // Function to extract valid JSON from potentially corrupted files
    function extractValidJson(content) {
      try {
        // First, try to parse as regular JSON
        return JSON.parse(content);
      } catch (e) {
        // If that fails, clean the content and try again
        try {
          // Remove null bytes and other control characters that often corrupt JSON
          let cleanContent = content.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]/g, '');

          // Try to parse the cleaned content
          return JSON.parse(cleanContent);
        } catch (e2) {
          // If still failing, try to extract JSON array from the content
          try {
            // Find the main JSON array by looking for opening [ and closing ]
            const startIdx = cleanContent.indexOf('[');
            const endIdx = cleanContent.lastIndexOf(']');

            if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {
              const arrayContent = cleanContent.substring(startIdx, endIdx + 1);

              // Try to parse the extracted array
              return JSON.parse(arrayContent);
            }
          } catch (e3) {
            // If all attempts fail, return null
            return null;
          }
        }
      }

      return null;
    }

    // Run migration if this file is executed directly
    if (require.main === module) {
      migrateHistory().catch(console.error);
    }

    module.exports = { migrateHistory };
- role: system
  type: document
  source: read_all.js
  timestamp: 1767457667701
  content: |-
    const fs = require('fs');
    const path = require('path');
    const yaml = require('js-yaml');

    /**
     * Aggregates all readable text content from a directory and its subdirectories
     * into:
     * 1. A single text corpus (combined_text.txt) for human reading.
     * 2. A structured JSON memory file (combined_memory.json) for Sovereign DB ingestion.
     * 3. A structured YAML memory file (combined_memory.yaml) for easier processing and migration.
     */
    function createFullCorpusRecursive() {
      // Set the root directory to scan as the directory containing this script
      const rootDirToScan = path.dirname(__filename);

      const outputTextFile = path.join(rootDirToScan, 'combined_text.txt');
      const outputJsonFile = path.join(rootDirToScan, 'combined_memory.json');
      const outputYamlFile = path.join(rootDirToScan, 'combined_memory.yaml');

      console.log(`Scanning Target Directory: ${rootDirToScan}`);

      const textExtensions = new Set([
        '.json', '.md', '.poml', '.yaml', '.yml', '.txt', 
        '.py', '.js', '.ts', '.css', '.sh', '.ps1', '.html', '.bat'
      ]);

      const excludeDirs = new Set([
        '.venv', '.git', '.vscode', '__pycache__', 
        'node_modules', '.obsidian', 'random', 'archive', 
        'build', 'dist', 'logs'
      ]);

      // Files to exclude from the corpus itself to avoid recursion
      const excludeFiles = new Set([
        path.basename(outputTextFile),
        path.basename(outputJsonFile),
        path.basename(outputYamlFile),
        'package-lock.json',
        'yarn.lock'
      ]);

      const filesToProcess = [];

      function walkDirectory(currentPath) {
        const items = fs.readdirSync(currentPath);

        for (const item of items) {
          const itemPath = path.join(currentPath, item);
          const stat = fs.statSync(itemPath);

          if (stat.isDirectory()) {
            if (!excludeDirs.has(item)) {
              walkDirectory(itemPath);
            }
          } else if (stat.isFile()) {
            const ext = path.extname(item).toLowerCase();
            if (textExtensions.has(ext) && !excludeFiles.has(item)) {
              filesToProcess.push(itemPath);
            }
          }
        }
      }

      walkDirectory(rootDirToScan);
      filesToProcess.sort();

      if (filesToProcess.length === 0) {
        console.log(`No processable files found in '${rootDirToScan}'.`);
        return;
      }

      console.log(`Found ${filesToProcess.length} files to process.`);

      const memoryRecords = [];

      // 1. Generate Text Corpus
      const textStream = fs.createWriteStream(outputTextFile, { encoding: 'utf-8' });

      for (const filePath of filesToProcess) {
        console.log(`Processing '${filePath}'...`);
        try {
          // Get file metadata
          const fileStats = fs.statSync(filePath);
          const modTime = fileStats.mtimeMs; // milliseconds timestamp
          const relPath = path.relative(rootDirToScan, filePath);

          // Read file content
          const rawContent = fs.readFileSync(filePath);
          // For simplicity in JS, we'll assume UTF-8, but could implement encoding detection
          const decodedContent = rawContent.toString('utf-8');

          // Write to Text File
          textStream.write(`--- START OF FILE: ${relPath} ---\n`);
          textStream.write(decodedContent + "\n");
          textStream.write(`--- END OF FILE: ${relPath} ---\n\n`);

          // Add to Memory Records
          memoryRecords.push({
            role: 'system',
            type: 'document',
            source: relPath,
            timestamp: Math.floor(modTime), // Convert to integer
            content: decodedContent
          });

        } catch (e) {
          console.log(`An unexpected error occurred with file '${filePath}': ${e.message}`);
        }
      }

      textStream.end();

      // 2. Generate JSON Memory File
      console.log(`Generating Structured Memory: ${outputJsonFile}`);
      fs.writeFileSync(outputJsonFile, JSON.stringify(memoryRecords, null, 2), 'utf-8');

      // 3. Generate YAML Memory File
      console.log(`Generating YAML Memory: ${outputYamlFile}`);

      // Custom YAML representer for multiline strings
      const schema = yaml.DEFAULT_SCHEMA.extend([
        new yaml.Type('!long-string', {
          kind: 'scalar',
          predicate: (data) => typeof data === 'string' && data.includes('\n'),
          represent: (data) => ({ value: data, style: '|' })
        })
      ]);

      // Use default representer with multiline string style
      const yamlContent = yaml.dump(memoryRecords, {
        lineWidth: -1, // Don't wrap lines
        noRefs: true,
        quotingType: '"', // Use double quotes when needed
        forceQuotes: false
      });

      fs.writeFileSync(outputYamlFile, yamlContent, 'utf-8');

      console.log('\nCorpus aggregation complete.');
      console.log(`1. Text Corpus: '${outputTextFile}'`);
      console.log(`2. JSON Memory: '${outputJsonFile}' (Drop this into Coda Console)`);
      console.log(`3. YAML Memory: '${outputYamlFile}' (Alternative format for easier processing)`);
    }

    // Run the function if this script is executed directly
    if (require.main === module) {
      createFullCorpusRecursive();
    }

    module.exports = { createFullCorpusRecursive };
